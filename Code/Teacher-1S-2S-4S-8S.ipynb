{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "akllrun",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "57a2db239d1c4f0d89d4fad3ecc0ae82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7ae36c96fe44dd5a527284d34369d20",
              "IPY_MODEL_2b1dccbdf13942adb7656fb3a7bede44",
              "IPY_MODEL_076dbb34b15d4517962db93bfde032e8"
            ],
            "layout": "IPY_MODEL_638a41373c4b4deaa1aafaa274c3e15a"
          }
        },
        "d7ae36c96fe44dd5a527284d34369d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edbbfdc8486a42c187bf96453e588e15",
            "placeholder": "​",
            "style": "IPY_MODEL_97227bfb02d84b85a3a16bb8ba00ea3a",
            "value": ""
          }
        },
        "2b1dccbdf13942adb7656fb3a7bede44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5688bec2c1e4eda9adc2530d615314e",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_547065fe5ad84ca3bf826e2d268aae53",
            "value": 170498071
          }
        },
        "076dbb34b15d4517962db93bfde032e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cff5473352540b5a45e2a17e2a778cd",
            "placeholder": "​",
            "style": "IPY_MODEL_0d46f2ec8c6e491cada32254889bcbab",
            "value": " 170499072/? [00:02&lt;00:00, 64525808.46it/s]"
          }
        },
        "638a41373c4b4deaa1aafaa274c3e15a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edbbfdc8486a42c187bf96453e588e15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97227bfb02d84b85a3a16bb8ba00ea3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5688bec2c1e4eda9adc2530d615314e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "547065fe5ad84ca3bf826e2d268aae53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1cff5473352540b5a45e2a17e2a778cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d46f2ec8c6e491cada32254889bcbab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Knowledge Distillation \n",
        "We will impliment [TCN](https://www.bmvc2021-virtualconference.com/conference/papers/paper_0831.html) paper. It is a varient of knowledge distillation which uses dense feature vactors instead of logits to transfer knowledge from teacher to student.  "
      ],
      "metadata": {
        "id": "NM1cxLfnctDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71XqWTiofkMi",
        "outputId": "e8a092c7-949a-49b4-a380-1fd251f22dbc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training base teacher network\n",
        "This section is not graded"
      ],
      "metadata": {
        "id": "sOqQ-lRNesRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "import tqdm\n",
        "\n",
        "batch_size = 100\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "57a2db239d1c4f0d89d4fad3ecc0ae82",
            "d7ae36c96fe44dd5a527284d34369d20",
            "2b1dccbdf13942adb7656fb3a7bede44",
            "076dbb34b15d4517962db93bfde032e8",
            "638a41373c4b4deaa1aafaa274c3e15a",
            "edbbfdc8486a42c187bf96453e588e15",
            "97227bfb02d84b85a3a16bb8ba00ea3a",
            "c5688bec2c1e4eda9adc2530d615314e",
            "547065fe5ad84ca3bf826e2d268aae53",
            "1cff5473352540b5a45e2a17e2a778cd",
            "0d46f2ec8c6e491cada32254889bcbab"
          ]
        },
        "id": "KSnbXgTWmFsJ",
        "outputId": "c91ea952-ab1c-4c9f-ef09-051a5eef9c48"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57a2db239d1c4f0d89d4fad3ecc0ae82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "teacher = VGG('VGG16')\n",
        "teacher = teacher.to(device)"
      ],
      "metadata": {
        "id": "Up5ob6ujFKex"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(teacher.parameters(), lr=0.0001)\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    teacher.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        teacher.zero_grad()\n",
        "        outputs = teacher(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if(batch_idx % 200 == 0):\n",
        "          print(\"Accuracy : \",100.*correct/total,\" Loss : \", train_loss/(batch_idx+1))\n",
        "def test(epoch):\n",
        "    teacher.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = teacher(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\"Accuracy : \",100.*correct/total,\" Loss : \", test_loss/(batch_idx+1))"
      ],
      "metadata": {
        "id": "2wyVCEgqFxxo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "    train(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xfNOBETGFNR",
        "outputId": "f8111f75-8c9b-4c37-c9b5-bdabfe9c5b61"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Accuracy :  14.0  Loss :  2.3733975887298584\n",
            "Accuracy :  41.19900497512438  Loss :  1.5865661386233658\n",
            "Accuracy :  47.80548628428928  Loss :  1.41774071199341\n",
            "Validation: \n",
            "Accuracy :  67.0  Loss :  0.9107257127761841\n",
            "Accuracy :  65.33333333333333  Loss :  0.9950999220212301\n",
            "Accuracy :  64.17073170731707  Loss :  1.0056527911162958\n",
            "Accuracy :  64.47540983606558  Loss :  0.9981735944747925\n",
            "Accuracy :  64.24691358024691  Loss :  1.005907082999194\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy :  64.0  Loss :  0.9784829616546631\n",
            "Accuracy :  63.975124378109456  Loss :  1.007334947289519\n",
            "Accuracy :  65.65336658354114  Loss :  0.9614940865378725\n",
            "Validation: \n",
            "Accuracy :  75.0  Loss :  0.7087078094482422\n",
            "Accuracy :  73.0952380952381  Loss :  0.7782325758820489\n",
            "Accuracy :  72.07317073170732  Loss :  0.8009776901908037\n",
            "Accuracy :  72.45901639344262  Loss :  0.7891016011355353\n",
            "Accuracy :  72.55555555555556  Loss :  0.7947008230803926\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy :  64.0  Loss :  0.8603017330169678\n",
            "Accuracy :  72.07462686567165  Loss :  0.7997423499377806\n",
            "Accuracy :  72.94513715710723  Loss :  0.7742086663805041\n",
            "Validation: \n",
            "Accuracy :  79.0  Loss :  0.6711878180503845\n",
            "Accuracy :  74.76190476190476  Loss :  0.7303563171908969\n",
            "Accuracy :  74.34146341463415  Loss :  0.7372472046352014\n",
            "Accuracy :  74.85245901639344  Loss :  0.7317987011104333\n",
            "Accuracy :  74.92592592592592  Loss :  0.7305009972166132\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy :  79.0  Loss :  0.5780044794082642\n",
            "Accuracy :  76.72139303482587  Loss :  0.6739745083732984\n",
            "Accuracy :  77.12219451371571  Loss :  0.6625515817852686\n",
            "Validation: \n",
            "Accuracy :  77.0  Loss :  0.6510322093963623\n",
            "Accuracy :  76.38095238095238  Loss :  0.6998276596977597\n",
            "Accuracy :  76.53658536585365  Loss :  0.7156145616275508\n",
            "Accuracy :  76.21311475409836  Loss :  0.7109470562856706\n",
            "Accuracy :  76.12345679012346  Loss :  0.7078349800021561\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy :  77.0  Loss :  0.6177147030830383\n",
            "Accuracy :  79.4776119402985  Loss :  0.5986515867769422\n",
            "Accuracy :  79.75062344139651  Loss :  0.5906961390799715\n",
            "Validation: \n",
            "Accuracy :  84.0  Loss :  0.5090551376342773\n",
            "Accuracy :  78.66666666666667  Loss :  0.6368099678130377\n",
            "Accuracy :  78.0  Loss :  0.6627424199406694\n",
            "Accuracy :  77.98360655737704  Loss :  0.6582581303158744\n",
            "Accuracy :  78.0246913580247  Loss :  0.6549958369614165\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy :  75.0  Loss :  0.5102257132530212\n",
            "Accuracy :  81.50248756218906  Loss :  0.5391105981311988\n",
            "Accuracy :  81.57107231920199  Loss :  0.5350365822303325\n",
            "Validation: \n",
            "Accuracy :  83.0  Loss :  0.49757125973701477\n",
            "Accuracy :  82.61904761904762  Loss :  0.5151596452508654\n",
            "Accuracy :  82.0  Loss :  0.5324131686513017\n",
            "Accuracy :  82.01639344262296  Loss :  0.5297190204995578\n",
            "Accuracy :  82.06172839506173  Loss :  0.5265540598351278\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy :  83.0  Loss :  0.44234907627105713\n",
            "Accuracy :  83.60199004975124  Loss :  0.4862030011356173\n",
            "Accuracy :  83.44389027431421  Loss :  0.48413379181947497\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.3825371563434601\n",
            "Accuracy :  82.42857142857143  Loss :  0.5258803821745373\n",
            "Accuracy :  81.60975609756098  Loss :  0.5497385663230244\n",
            "Accuracy :  81.62295081967213  Loss :  0.5458922777019564\n",
            "Accuracy :  81.23456790123457  Loss :  0.5467918978797065\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy :  84.0  Loss :  0.36962124705314636\n",
            "Accuracy :  84.67164179104478  Loss :  0.44874195969519926\n",
            "Accuracy :  84.55860349127182  Loss :  0.44864371693936966\n",
            "Validation: \n",
            "Accuracy :  80.0  Loss :  0.5324685573577881\n",
            "Accuracy :  81.42857142857143  Loss :  0.5608684661842528\n",
            "Accuracy :  81.1219512195122  Loss :  0.5700179571058692\n",
            "Accuracy :  80.9672131147541  Loss :  0.57261781272341\n",
            "Accuracy :  81.07407407407408  Loss :  0.5733076157393279\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy :  85.0  Loss :  0.4485841691493988\n",
            "Accuracy :  85.57711442786069  Loss :  0.41721999919533137\n",
            "Accuracy :  85.56109725685785  Loss :  0.41495818572300036\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4822770953178406\n",
            "Accuracy :  82.52380952380952  Loss :  0.5443572174935114\n",
            "Accuracy :  82.2439024390244  Loss :  0.549246766218325\n",
            "Accuracy :  82.19672131147541  Loss :  0.547752359851462\n",
            "Accuracy :  82.18518518518519  Loss :  0.5443754104184516\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy :  93.0  Loss :  0.21290676295757294\n",
            "Accuracy :  86.90049751243781  Loss :  0.38545506681079295\n",
            "Accuracy :  86.8927680798005  Loss :  0.38235597234414404\n",
            "Validation: \n",
            "Accuracy :  85.0  Loss :  0.49091336131095886\n",
            "Accuracy :  83.23809523809524  Loss :  0.5201689849297205\n",
            "Accuracy :  82.97560975609755  Loss :  0.5335028255131187\n",
            "Accuracy :  83.0  Loss :  0.5326328812564005\n",
            "Accuracy :  82.88888888888889  Loss :  0.5320950560731652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_name = 'teacher.pt'\n",
        "path = F\"/content/gdrive/MyDrive/{model_save_name}\" \n",
        "#torch.save(teacher.state_dict(), path)\n",
        "teacher.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKgOR2afg0Ea",
        "outputId": "28391b64-04d3-480c-a752-f2c975666c2e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Dense Feature Dataset\n",
        "1.1 In this cell we remove the head of teacher network(i.e: last fullyconnected layer) and add a flatten layer at the end."
      ],
      "metadata": {
        "id": "mERdZFH7fBXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(teacher, (3, 32, 32))"
      ],
      "metadata": {
        "id": "Ubp5SeKSCRDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cc72c71-9e58-400e-c7a7-2c14feaab564"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          36,928\n",
            "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
            "              ReLU-6           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 64, 16, 16]               0\n",
            "            Conv2d-8          [-1, 128, 16, 16]          73,856\n",
            "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
            "             ReLU-10          [-1, 128, 16, 16]               0\n",
            "           Conv2d-11          [-1, 128, 16, 16]         147,584\n",
            "      BatchNorm2d-12          [-1, 128, 16, 16]             256\n",
            "             ReLU-13          [-1, 128, 16, 16]               0\n",
            "        MaxPool2d-14            [-1, 128, 8, 8]               0\n",
            "           Conv2d-15            [-1, 256, 8, 8]         295,168\n",
            "      BatchNorm2d-16            [-1, 256, 8, 8]             512\n",
            "             ReLU-17            [-1, 256, 8, 8]               0\n",
            "           Conv2d-18            [-1, 256, 8, 8]         590,080\n",
            "      BatchNorm2d-19            [-1, 256, 8, 8]             512\n",
            "             ReLU-20            [-1, 256, 8, 8]               0\n",
            "           Conv2d-21            [-1, 256, 8, 8]         590,080\n",
            "      BatchNorm2d-22            [-1, 256, 8, 8]             512\n",
            "             ReLU-23            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 512, 4, 4]       1,180,160\n",
            "      BatchNorm2d-26            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-27            [-1, 512, 4, 4]               0\n",
            "           Conv2d-28            [-1, 512, 4, 4]       2,359,808\n",
            "      BatchNorm2d-29            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-30            [-1, 512, 4, 4]               0\n",
            "           Conv2d-31            [-1, 512, 4, 4]       2,359,808\n",
            "      BatchNorm2d-32            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-33            [-1, 512, 4, 4]               0\n",
            "        MaxPool2d-34            [-1, 512, 2, 2]               0\n",
            "           Conv2d-35            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-36            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-37            [-1, 512, 2, 2]               0\n",
            "           Conv2d-38            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-39            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-40            [-1, 512, 2, 2]               0\n",
            "           Conv2d-41            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-42            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-43            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-44            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-45            [-1, 512, 1, 1]               0\n",
            "           Linear-46                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 14,728,266\n",
            "Trainable params: 14,728,266\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 6.57\n",
            "Params size (MB): 56.18\n",
            "Estimated Total Size (MB): 62.77\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_WOH = nn.Sequential(*list(teacher.children())[:-1],nn.Flatten())"
      ],
      "metadata": {
        "id": "OIK5Vcfo8Y19"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The summery of the new teacher without head :"
      ],
      "metadata": {
        "id": "JG2u-KMYbvI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "55V5zBa_vZjX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from torchsummary import summary\n",
        "summary(teacher_WOH, (3, 32, 32))"
      ],
      "metadata": {
        "id": "rFtzu7DD8uev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c254645-eab8-4fd9-ed1f-3533b154d419"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          36,928\n",
            "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
            "              ReLU-6           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 64, 16, 16]               0\n",
            "            Conv2d-8          [-1, 128, 16, 16]          73,856\n",
            "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
            "             ReLU-10          [-1, 128, 16, 16]               0\n",
            "           Conv2d-11          [-1, 128, 16, 16]         147,584\n",
            "      BatchNorm2d-12          [-1, 128, 16, 16]             256\n",
            "             ReLU-13          [-1, 128, 16, 16]               0\n",
            "        MaxPool2d-14            [-1, 128, 8, 8]               0\n",
            "           Conv2d-15            [-1, 256, 8, 8]         295,168\n",
            "      BatchNorm2d-16            [-1, 256, 8, 8]             512\n",
            "             ReLU-17            [-1, 256, 8, 8]               0\n",
            "           Conv2d-18            [-1, 256, 8, 8]         590,080\n",
            "      BatchNorm2d-19            [-1, 256, 8, 8]             512\n",
            "             ReLU-20            [-1, 256, 8, 8]               0\n",
            "           Conv2d-21            [-1, 256, 8, 8]         590,080\n",
            "      BatchNorm2d-22            [-1, 256, 8, 8]             512\n",
            "             ReLU-23            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 512, 4, 4]       1,180,160\n",
            "      BatchNorm2d-26            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-27            [-1, 512, 4, 4]               0\n",
            "           Conv2d-28            [-1, 512, 4, 4]       2,359,808\n",
            "      BatchNorm2d-29            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-30            [-1, 512, 4, 4]               0\n",
            "           Conv2d-31            [-1, 512, 4, 4]       2,359,808\n",
            "      BatchNorm2d-32            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-33            [-1, 512, 4, 4]               0\n",
            "        MaxPool2d-34            [-1, 512, 2, 2]               0\n",
            "           Conv2d-35            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-36            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-37            [-1, 512, 2, 2]               0\n",
            "           Conv2d-38            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-39            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-40            [-1, 512, 2, 2]               0\n",
            "           Conv2d-41            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-42            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-43            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-44            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-45            [-1, 512, 1, 1]               0\n",
            "          Flatten-46                  [-1, 512]               0\n",
            "================================================================\n",
            "Total params: 14,723,136\n",
            "Trainable params: 14,723,136\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 6.57\n",
            "Params size (MB): 56.16\n",
            "Estimated Total Size (MB): 62.75\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 In this cell you have to create dense feature labels dataset(i.e: the outputs of teacher network without head). For that you have to do forward pass on whole dataset and append the outputs in a variable. "
      ],
      "metadata": {
        "id": "FJsum1ioc2tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_WOH.eval()\n",
        "DenseTrain = None\n",
        "DenseTest = None\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = teacher_WOH(inputs)\n",
        "        if(DenseTrain == None):\n",
        "            DenseTrain = outputs\n",
        "        else:\n",
        "            DenseTrain = torch.cat((DenseTrain,outputs))\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = teacher_WOH(inputs)\n",
        "        if(DenseTest == None):\n",
        "            DenseTest = outputs\n",
        "        else:\n",
        "            DenseTest = torch.cat((DenseTest,outputs))"
      ],
      "metadata": {
        "id": "jb8FytxoHM62"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating ad-hoc student network\n",
        "we create an ad-hoc student network "
      ],
      "metadata": {
        "id": "xzcPi5zuhqOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "    'VGGS': [32, 32, 'M', 64, 64, 'M', 128, 128, 'M', 256, 256, 'M',512,'M'],\n",
        "\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(512, 512)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "s1 = VGG('VGGS')\n",
        "s1 = s1.to(device)\n",
        "summary(s1, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9mnHRpJhpl5",
        "outputId": "2f006528-05a4-48c1-af73-60e280a864c1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "           Conv2d-29            [-1, 512, 2, 2]       1,180,160\n",
            "      BatchNorm2d-30            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-31            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-32            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-33            [-1, 512, 1, 1]               0\n",
            "           Linear-34                  [-1, 512]         262,656\n",
            "================================================================\n",
            "Total params: 2,618,016\n",
            "Trainable params: 2,618,016\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.99\n",
            "Params size (MB): 9.99\n",
            "Estimated Total Size (MB): 12.99\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Student\n",
        "We will train the student network using Dense Features that we created.\n",
        "Dataset datagen will provide data in batches so we need to extract the corresponding batch of targets from our Dense feature variable from 1.2, for this we use the following formula:\n",
        "\n",
        "batch_index * batch_size --> (batch_index * batch_size) + batch_size"
      ],
      "metadata": {
        "id": "8z6cgCZ7h8F5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(s1.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    s1.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        targets = DenseTrain[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "        \n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        \n",
        "        s1.zero_grad()\n",
        "        outputs = s1(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        if(batch_idx % 10 == 0):\n",
        "          print(\"Loss : \", train_loss/(batch_idx+1))\n",
        "def test(epoch):\n",
        "    s1.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            targets = DenseTest[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "            outputs = s1(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\" Loss : \", test_loss/(batch_idx+1))"
      ],
      "metadata": {
        "id": "eXXtMar7h6jJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+60):\n",
        "    train(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1IThg_HiK6q",
        "outputId": "53886561-b8c4-44d3-d2b6-42c049ccad86"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Loss :  3.6982531547546387\n",
            "Loss :  2.6693457798524336\n",
            "Loss :  2.040993488970257\n",
            "Loss :  1.6936085147242392\n",
            "Loss :  1.467016842307114\n",
            "Loss :  1.3230298921173693\n",
            "Loss :  1.2173858148152712\n",
            "Loss :  1.1410718870834566\n",
            "Loss :  1.0773069829116633\n",
            "Loss :  1.0279522839483324\n",
            "Loss :  0.9826088010674656\n",
            "Loss :  0.9441749147466711\n",
            "Loss :  0.913011798188706\n",
            "Loss :  0.8847250028420951\n",
            "Loss :  0.8595928154515882\n",
            "Loss :  0.8373164838513002\n",
            "Loss :  0.8162185294287545\n",
            "Loss :  0.7974991399293755\n",
            "Loss :  0.7824804616567179\n",
            "Loss :  0.7662618968499268\n",
            "Loss :  0.751274244405737\n",
            "Loss :  0.7378353629067046\n",
            "Loss :  0.7251876513073348\n",
            "Loss :  0.7137788181955164\n",
            "Loss :  0.7028869803515707\n",
            "Loss :  0.6934796735584974\n",
            "Loss :  0.683782813078599\n",
            "Loss :  0.6741902095805234\n",
            "Loss :  0.6649867340763268\n",
            "Loss :  0.65718315843864\n",
            "Loss :  0.6493550655849748\n",
            "Loss :  0.642017930650251\n",
            "Loss :  0.634771172595544\n",
            "Loss :  0.6277699426401778\n",
            "Loss :  0.6213034062791081\n",
            "Loss :  0.6148867536643972\n",
            "Loss :  0.6086351433123908\n",
            "Loss :  0.60276378952268\n",
            "Loss :  0.5964537517135851\n",
            "Loss :  0.590879879949038\n",
            "Loss :  0.5854624561092205\n",
            "Loss :  0.5800766229194446\n",
            "Loss :  0.5750415806532473\n",
            "Loss :  0.5699751790604293\n",
            "Loss :  0.565402856692165\n",
            "Loss :  0.5606944153287723\n",
            "Loss :  0.556215103950004\n",
            "Loss :  0.5516340666136165\n",
            "Loss :  0.5476758462103886\n",
            "Loss :  0.5432978083669775\n",
            "Validation: \n",
            " Loss :  0.30649250745773315\n",
            " Loss :  0.3378171239580427\n",
            " Loss :  0.3386595416359785\n",
            " Loss :  0.33641758463421806\n",
            " Loss :  0.33542635374599034\n",
            "\n",
            "Epoch: 2\n",
            "Loss :  0.3453124165534973\n",
            "Loss :  0.3395089100707661\n",
            "Loss :  0.3359364285355523\n",
            "Loss :  0.33559507323849586\n",
            "Loss :  0.33323589912274987\n",
            "Loss :  0.33278703689575195\n",
            "Loss :  0.3289629880522118\n",
            "Loss :  0.32678995669727595\n",
            "Loss :  0.32343184727209584\n",
            "Loss :  0.32060641115838356\n",
            "Loss :  0.3180653718438479\n",
            "Loss :  0.3157094601575319\n",
            "Loss :  0.3135690538843801\n",
            "Loss :  0.3123232349184633\n",
            "Loss :  0.31033879666463704\n",
            "Loss :  0.3090059044740058\n",
            "Loss :  0.30723855778667497\n",
            "Loss :  0.30615733546471735\n",
            "Loss :  0.30530184449741193\n",
            "Loss :  0.3039969348626611\n",
            "Loss :  0.3027485862596711\n",
            "Loss :  0.30187874192027686\n",
            "Loss :  0.3004751420533495\n",
            "Loss :  0.2992846324588313\n",
            "Loss :  0.29758340893197355\n",
            "Loss :  0.29689817373971067\n",
            "Loss :  0.295948617131774\n",
            "Loss :  0.2945846964732307\n",
            "Loss :  0.2932850057226059\n",
            "Loss :  0.2925554873914653\n",
            "Loss :  0.29147096908963793\n",
            "Loss :  0.2903700003765787\n",
            "Loss :  0.2892078389167043\n",
            "Loss :  0.2882967150553476\n",
            "Loss :  0.2874440227872815\n",
            "Loss :  0.2865203149043597\n",
            "Loss :  0.28566781999001545\n",
            "Loss :  0.28465449532890574\n",
            "Loss :  0.28340626944081365\n",
            "Loss :  0.282232231381909\n",
            "Loss :  0.2812218281470629\n",
            "Loss :  0.2803345363578077\n",
            "Loss :  0.2795526964364312\n",
            "Loss :  0.2785109686395132\n",
            "Loss :  0.277784326280596\n",
            "Loss :  0.27703229719545786\n",
            "Loss :  0.27627762644642606\n",
            "Loss :  0.2753514422226357\n",
            "Loss :  0.27471211986333566\n",
            "Loss :  0.2736089423511276\n",
            "Validation: \n",
            " Loss :  0.20474176108837128\n",
            " Loss :  0.22717477026439847\n",
            " Loss :  0.22691406709391895\n",
            " Loss :  0.22566280233078315\n",
            " Loss :  0.22603972053822177\n",
            "\n",
            "Epoch: 3\n",
            "Loss :  0.2517645061016083\n",
            "Loss :  0.22620063613761554\n",
            "Loss :  0.23045008948871068\n",
            "Loss :  0.22976429039432156\n",
            "Loss :  0.2289476885301311\n",
            "Loss :  0.22875907315927393\n",
            "Loss :  0.22587587330185\n",
            "Loss :  0.22627878126124262\n",
            "Loss :  0.22464136834497805\n",
            "Loss :  0.2226736226252147\n",
            "Loss :  0.22040974695493679\n",
            "Loss :  0.2191299000033387\n",
            "Loss :  0.21863673528856484\n",
            "Loss :  0.21862557302904492\n",
            "Loss :  0.2181914688636225\n",
            "Loss :  0.21798042124075606\n",
            "Loss :  0.2171848788394691\n",
            "Loss :  0.2169962457397528\n",
            "Loss :  0.21721155653342358\n",
            "Loss :  0.2165050810856345\n",
            "Loss :  0.21610040821839327\n",
            "Loss :  0.2164073916973096\n",
            "Loss :  0.21600888696461243\n",
            "Loss :  0.2158858058772562\n",
            "Loss :  0.215392130044486\n",
            "Loss :  0.21514970111656948\n",
            "Loss :  0.21464041172316248\n",
            "Loss :  0.2141067688645472\n",
            "Loss :  0.21369533505940352\n",
            "Loss :  0.21372181694327352\n",
            "Loss :  0.213568601497384\n",
            "Loss :  0.21276638434055917\n",
            "Loss :  0.21218505873115634\n",
            "Loss :  0.21164354465879343\n",
            "Loss :  0.21120504619788563\n",
            "Loss :  0.21093432074598437\n",
            "Loss :  0.21070822251965796\n",
            "Loss :  0.2101616445738029\n",
            "Loss :  0.20954788634626884\n",
            "Loss :  0.2090496803488573\n",
            "Loss :  0.2085881682108168\n",
            "Loss :  0.20824384174497748\n",
            "Loss :  0.20806160063047024\n",
            "Loss :  0.20756165617579928\n",
            "Loss :  0.2072549958348004\n",
            "Loss :  0.20698499643221135\n",
            "Loss :  0.2066955699256082\n",
            "Loss :  0.20638451445254552\n",
            "Loss :  0.20625571853901392\n",
            "Loss :  0.2056330046682882\n",
            "Validation: \n",
            " Loss :  0.16627654433250427\n",
            " Loss :  0.17772453881445385\n",
            " Loss :  0.1773947651793317\n",
            " Loss :  0.17455215800981053\n",
            " Loss :  0.17496148506064474\n",
            "\n",
            "Epoch: 4\n",
            "Loss :  0.21935003995895386\n",
            "Loss :  0.1865910440683365\n",
            "Loss :  0.18488500444662004\n",
            "Loss :  0.18518177203593716\n",
            "Loss :  0.18538091567958273\n",
            "Loss :  0.1855055888493856\n",
            "Loss :  0.18343696071476231\n",
            "Loss :  0.18322345383570227\n",
            "Loss :  0.1828488924621064\n",
            "Loss :  0.18176368877782925\n",
            "Loss :  0.18060693572653402\n",
            "Loss :  0.1795629073639174\n",
            "Loss :  0.1792623013996881\n",
            "Loss :  0.17950734621240894\n",
            "Loss :  0.1788713018944923\n",
            "Loss :  0.1789802455941573\n",
            "Loss :  0.17889064689230474\n",
            "Loss :  0.17880696624691722\n",
            "Loss :  0.17884602286539025\n",
            "Loss :  0.17856750032664592\n",
            "Loss :  0.17821484136937268\n",
            "Loss :  0.17833745204159435\n",
            "Loss :  0.17827772231123565\n",
            "Loss :  0.17806470994051402\n",
            "Loss :  0.17770380721547296\n",
            "Loss :  0.17773440438675214\n",
            "Loss :  0.17753417281812178\n",
            "Loss :  0.17732164065776276\n",
            "Loss :  0.1770701704818583\n",
            "Loss :  0.17712825413831731\n",
            "Loss :  0.17713158095001777\n",
            "Loss :  0.17660198549940656\n",
            "Loss :  0.17630622329370255\n",
            "Loss :  0.1762032539012569\n",
            "Loss :  0.17605535266511252\n",
            "Loss :  0.17594606817787528\n",
            "Loss :  0.17578614992283056\n",
            "Loss :  0.1754848935372746\n",
            "Loss :  0.1751738318542796\n",
            "Loss :  0.1746587021957578\n",
            "Loss :  0.1745346425254446\n",
            "Loss :  0.1744564466740383\n",
            "Loss :  0.17427110441931637\n",
            "Loss :  0.1739059499618349\n",
            "Loss :  0.17376816992451544\n",
            "Loss :  0.17365499243900148\n",
            "Loss :  0.17347887446073545\n",
            "Loss :  0.17324877070013883\n",
            "Loss :  0.17322360428356084\n",
            "Loss :  0.1727492909749511\n",
            "Validation: \n",
            " Loss :  0.1424711048603058\n",
            " Loss :  0.15739396924064272\n",
            " Loss :  0.15585817687395143\n",
            " Loss :  0.1532519733319517\n",
            " Loss :  0.1537072899532907\n",
            "\n",
            "Epoch: 5\n",
            "Loss :  0.16409868001937866\n",
            "Loss :  0.16059130836616864\n",
            "Loss :  0.15991567217168354\n",
            "Loss :  0.1606549471616745\n",
            "Loss :  0.16112094245305875\n",
            "Loss :  0.16101580975102445\n",
            "Loss :  0.15973594638167835\n",
            "Loss :  0.15948726908421854\n",
            "Loss :  0.1595777330207236\n",
            "Loss :  0.15900212215198264\n",
            "Loss :  0.15725805979258944\n",
            "Loss :  0.15625733841915387\n",
            "Loss :  0.1559887137417951\n",
            "Loss :  0.1567234058530276\n",
            "Loss :  0.15609430088430432\n",
            "Loss :  0.15579261038674425\n",
            "Loss :  0.15539481083613746\n",
            "Loss :  0.15553669017135052\n",
            "Loss :  0.15571787743443283\n",
            "Loss :  0.15529670285460836\n",
            "Loss :  0.15508080069995045\n",
            "Loss :  0.15516545662382766\n",
            "Loss :  0.155306575592287\n",
            "Loss :  0.15562725802520652\n",
            "Loss :  0.15557571571644907\n",
            "Loss :  0.1555275253921866\n",
            "Loss :  0.15535833050008022\n",
            "Loss :  0.1550825900807152\n",
            "Loss :  0.15501729682671217\n",
            "Loss :  0.15522647620886051\n",
            "Loss :  0.15512931802344085\n",
            "Loss :  0.15482126703407986\n",
            "Loss :  0.15475051541380422\n",
            "Loss :  0.154660213637388\n",
            "Loss :  0.15453173397136225\n",
            "Loss :  0.15447380750352502\n",
            "Loss :  0.15429217867821537\n",
            "Loss :  0.15411954983627058\n",
            "Loss :  0.1539685192305272\n",
            "Loss :  0.153705810189552\n",
            "Loss :  0.15372302072898408\n",
            "Loss :  0.15370737063333645\n",
            "Loss :  0.15374057513447104\n",
            "Loss :  0.15354740464078853\n",
            "Loss :  0.15349682060634198\n",
            "Loss :  0.15351899754578152\n",
            "Loss :  0.15338837647903508\n",
            "Loss :  0.15328124137061416\n",
            "Loss :  0.15321606837538324\n",
            "Loss :  0.1529051237976721\n",
            "Validation: \n",
            " Loss :  0.13179747760295868\n",
            " Loss :  0.13752470449322746\n",
            " Loss :  0.1359674525333614\n",
            " Loss :  0.1341034897038194\n",
            " Loss :  0.13475935492250654\n",
            "\n",
            "Epoch: 6\n",
            "Loss :  0.14827799797058105\n",
            "Loss :  0.1437982056628574\n",
            "Loss :  0.14505051111891157\n",
            "Loss :  0.14470099994251806\n",
            "Loss :  0.1451499385804665\n",
            "Loss :  0.14495146230739706\n",
            "Loss :  0.1436963276784928\n",
            "Loss :  0.14316638831941175\n",
            "Loss :  0.1431932766680364\n",
            "Loss :  0.14282486080140858\n",
            "Loss :  0.14173391284328876\n",
            "Loss :  0.14132884225329836\n",
            "Loss :  0.14120402174793983\n",
            "Loss :  0.14162348898995014\n",
            "Loss :  0.14117331147616638\n",
            "Loss :  0.141192353916484\n",
            "Loss :  0.1407812335383818\n",
            "Loss :  0.1409287738956903\n",
            "Loss :  0.1412605203235347\n",
            "Loss :  0.14111157327266263\n",
            "Loss :  0.14082104560747669\n",
            "Loss :  0.14094954672582907\n",
            "Loss :  0.1410381220179985\n",
            "Loss :  0.14112157387521876\n",
            "Loss :  0.14101073065486686\n",
            "Loss :  0.14107714504359728\n",
            "Loss :  0.14096271143905048\n",
            "Loss :  0.14076785721242208\n",
            "Loss :  0.14065795031200523\n",
            "Loss :  0.14096498125812032\n",
            "Loss :  0.14109533745486078\n",
            "Loss :  0.14073718190672313\n",
            "Loss :  0.14060810932489198\n",
            "Loss :  0.14054650241604744\n",
            "Loss :  0.1404488789808016\n",
            "Loss :  0.140471987192787\n",
            "Loss :  0.14019374103592375\n",
            "Loss :  0.14004386288698792\n",
            "Loss :  0.13985743005992235\n",
            "Loss :  0.13959431244284295\n",
            "Loss :  0.13952624935313057\n",
            "Loss :  0.13962376665169884\n",
            "Loss :  0.13967574536446437\n",
            "Loss :  0.13941064003310458\n",
            "Loss :  0.13936562224580587\n",
            "Loss :  0.13930684503192647\n",
            "Loss :  0.13917868626415084\n",
            "Loss :  0.1391327909002132\n",
            "Loss :  0.13912163958294227\n",
            "Loss :  0.13885151684405606\n",
            "Validation: \n",
            " Loss :  0.12932391464710236\n",
            " Loss :  0.1358499512785957\n",
            " Loss :  0.1348891563531829\n",
            " Loss :  0.13253659925988462\n",
            " Loss :  0.1327569474592621\n",
            "\n",
            "Epoch: 7\n",
            "Loss :  0.13597576320171356\n",
            "Loss :  0.13024542412974618\n",
            "Loss :  0.1310151606088593\n",
            "Loss :  0.1325588291210513\n",
            "Loss :  0.13295839090899722\n",
            "Loss :  0.13311483637959348\n",
            "Loss :  0.13298852558507293\n",
            "Loss :  0.13329672383170732\n",
            "Loss :  0.13322548669429474\n",
            "Loss :  0.1323074358668956\n",
            "Loss :  0.13145954921694086\n",
            "Loss :  0.1307041459792369\n",
            "Loss :  0.13060527393394264\n",
            "Loss :  0.13104587110854288\n",
            "Loss :  0.13077264791684792\n",
            "Loss :  0.13078807294368744\n",
            "Loss :  0.13065168455890988\n",
            "Loss :  0.13069212807026523\n",
            "Loss :  0.13092404871520416\n",
            "Loss :  0.1309756896414682\n",
            "Loss :  0.13089775360787093\n",
            "Loss :  0.13119302287485926\n",
            "Loss :  0.13126077098409514\n",
            "Loss :  0.13129143491064832\n",
            "Loss :  0.13099271915389293\n",
            "Loss :  0.13095081947832943\n",
            "Loss :  0.13080127223241375\n",
            "Loss :  0.13072508276608596\n",
            "Loss :  0.13078069374018292\n",
            "Loss :  0.13101862386329888\n",
            "Loss :  0.13103461728440566\n",
            "Loss :  0.13091330544070798\n",
            "Loss :  0.1308497591321342\n",
            "Loss :  0.13077299312612442\n",
            "Loss :  0.1307375231359012\n",
            "Loss :  0.1308022104522102\n",
            "Loss :  0.13077195013494042\n",
            "Loss :  0.13060384880339682\n",
            "Loss :  0.1304738837003395\n",
            "Loss :  0.1302210011369432\n",
            "Loss :  0.13026587384523003\n",
            "Loss :  0.13025994640559754\n",
            "Loss :  0.1302757352909113\n",
            "Loss :  0.12996512397328552\n",
            "Loss :  0.1299294952869145\n",
            "Loss :  0.12986884576716604\n",
            "Loss :  0.12971255697164516\n",
            "Loss :  0.12971358896567312\n",
            "Loss :  0.12967860611957224\n",
            "Loss :  0.12947430933565812\n",
            "Validation: \n",
            " Loss :  0.11391229182481766\n",
            " Loss :  0.11811758223034087\n",
            " Loss :  0.11731488148613674\n",
            " Loss :  0.11565628134813465\n",
            " Loss :  0.11571743238119432\n",
            "\n",
            "Epoch: 8\n",
            "Loss :  0.13323912024497986\n",
            "Loss :  0.12093311074105176\n",
            "Loss :  0.12363736295983904\n",
            "Loss :  0.12420976931048978\n",
            "Loss :  0.1248838327279905\n",
            "Loss :  0.12464200179366504\n",
            "Loss :  0.1241217249485313\n",
            "Loss :  0.12407046818817166\n",
            "Loss :  0.12398980585513292\n",
            "Loss :  0.1231826350584135\n",
            "Loss :  0.12256217489738276\n",
            "Loss :  0.12176832160702697\n",
            "Loss :  0.12150856095158365\n",
            "Loss :  0.12178630415947382\n",
            "Loss :  0.12138040881630377\n",
            "Loss :  0.12166766506551907\n",
            "Loss :  0.12170328431247925\n",
            "Loss :  0.12180798448491514\n",
            "Loss :  0.12191058324845457\n",
            "Loss :  0.12174774604942162\n",
            "Loss :  0.12191334848676748\n",
            "Loss :  0.1222617561924514\n",
            "Loss :  0.12245674522349198\n",
            "Loss :  0.12273382772872975\n",
            "Loss :  0.12272571902181104\n",
            "Loss :  0.12290545719197071\n",
            "Loss :  0.12280567292966148\n",
            "Loss :  0.12269491831534902\n",
            "Loss :  0.12268715805218314\n",
            "Loss :  0.12286042959726963\n",
            "Loss :  0.12291892635267834\n",
            "Loss :  0.12269085842121835\n",
            "Loss :  0.12262530005145296\n",
            "Loss :  0.12258157654021082\n",
            "Loss :  0.12250126668872022\n",
            "Loss :  0.12258568401859696\n",
            "Loss :  0.12248538529443609\n",
            "Loss :  0.12237706069515722\n",
            "Loss :  0.12231478357096044\n",
            "Loss :  0.1222008813143996\n",
            "Loss :  0.12222296809615042\n",
            "Loss :  0.1223637917761095\n",
            "Loss :  0.1224421995057063\n",
            "Loss :  0.12225772499208384\n",
            "Loss :  0.12222888803874013\n",
            "Loss :  0.12222995747814157\n",
            "Loss :  0.12210855443927575\n",
            "Loss :  0.12207993347743515\n",
            "Loss :  0.12215269366259882\n",
            "Loss :  0.12190466624534786\n",
            "Validation: \n",
            " Loss :  0.10476729273796082\n",
            " Loss :  0.10889840126037598\n",
            " Loss :  0.10774664362756217\n",
            " Loss :  0.10608530178910396\n",
            " Loss :  0.10633854733573066\n",
            "\n",
            "Epoch: 9\n",
            "Loss :  0.11848090589046478\n",
            "Loss :  0.11764514378525993\n",
            "Loss :  0.11629418141785122\n",
            "Loss :  0.11622757584817948\n",
            "Loss :  0.11681063782151152\n",
            "Loss :  0.11721871705616221\n",
            "Loss :  0.11653888872900947\n",
            "Loss :  0.11645869879235685\n",
            "Loss :  0.11685530684980346\n",
            "Loss :  0.11623121838975739\n",
            "Loss :  0.11554212750184654\n",
            "Loss :  0.11519978987472551\n",
            "Loss :  0.11500131443512342\n",
            "Loss :  0.11555326275015605\n",
            "Loss :  0.11546383206303237\n",
            "Loss :  0.11554630337566729\n",
            "Loss :  0.11532654428148861\n",
            "Loss :  0.11561876975479182\n",
            "Loss :  0.11570192428912905\n",
            "Loss :  0.1157693386233914\n",
            "Loss :  0.11583050870480228\n",
            "Loss :  0.11605520620589008\n",
            "Loss :  0.1162050449335737\n",
            "Loss :  0.11635008831689884\n",
            "Loss :  0.1163569376552748\n",
            "Loss :  0.11639445144460496\n",
            "Loss :  0.11641852266487034\n",
            "Loss :  0.11627164829481132\n",
            "Loss :  0.11635031508699431\n",
            "Loss :  0.11649125509757766\n",
            "Loss :  0.11643568230823821\n",
            "Loss :  0.11624590204459678\n",
            "Loss :  0.11626666956044432\n",
            "Loss :  0.11628627221119728\n",
            "Loss :  0.11637957278322264\n",
            "Loss :  0.11651989280583172\n",
            "Loss :  0.11652364119515855\n",
            "Loss :  0.11646460097112424\n",
            "Loss :  0.11637278575831511\n",
            "Loss :  0.11620346142355438\n",
            "Loss :  0.11618176847696304\n",
            "Loss :  0.11622809528071805\n",
            "Loss :  0.11631317985312672\n",
            "Loss :  0.1161998267827897\n",
            "Loss :  0.11612402235502019\n",
            "Loss :  0.11615894318552081\n",
            "Loss :  0.11609638000711185\n",
            "Loss :  0.11606722519655896\n",
            "Loss :  0.1160886118385995\n",
            "Loss :  0.11589666882381905\n",
            "Validation: \n",
            " Loss :  0.1098499670624733\n",
            " Loss :  0.1123905795670691\n",
            " Loss :  0.11113504520276697\n",
            " Loss :  0.10966949103797069\n",
            " Loss :  0.11011484099758996\n",
            "\n",
            "Epoch: 10\n",
            "Loss :  0.11409494280815125\n",
            "Loss :  0.11226073584773323\n",
            "Loss :  0.11172286704892204\n",
            "Loss :  0.11031674449482272\n",
            "Loss :  0.11094995133760499\n",
            "Loss :  0.11090283022791732\n",
            "Loss :  0.11046934933936009\n",
            "Loss :  0.11027780741872922\n",
            "Loss :  0.11036868908523041\n",
            "Loss :  0.11013228798305595\n",
            "Loss :  0.10974029001623097\n",
            "Loss :  0.10949227838097392\n",
            "Loss :  0.10939839617772536\n",
            "Loss :  0.10974175551237951\n",
            "Loss :  0.1098088504786187\n",
            "Loss :  0.10993059295297458\n",
            "Loss :  0.1098590945790273\n",
            "Loss :  0.11006738319557313\n",
            "Loss :  0.11034117231546844\n",
            "Loss :  0.11050956048734525\n",
            "Loss :  0.11044743497721592\n",
            "Loss :  0.11070024850667935\n",
            "Loss :  0.11087112391696256\n",
            "Loss :  0.11108262914341765\n",
            "Loss :  0.11107646587355008\n",
            "Loss :  0.1110851814250547\n",
            "Loss :  0.11106350795290935\n",
            "Loss :  0.11095808748829408\n",
            "Loss :  0.11108023392242045\n",
            "Loss :  0.11126602924976152\n",
            "Loss :  0.11131375599838174\n",
            "Loss :  0.1111408375754617\n",
            "Loss :  0.11110779828743028\n",
            "Loss :  0.11110063192077274\n",
            "Loss :  0.11112888065601025\n",
            "Loss :  0.11120156173267935\n",
            "Loss :  0.11116459800596053\n",
            "Loss :  0.11107027122996888\n",
            "Loss :  0.11091980715514481\n",
            "Loss :  0.11077159776559571\n",
            "Loss :  0.11071180838376209\n",
            "Loss :  0.11069293331055746\n",
            "Loss :  0.11079055716830025\n",
            "Loss :  0.11067678739604153\n",
            "Loss :  0.11067709990707385\n",
            "Loss :  0.11063761411114965\n",
            "Loss :  0.11053154866439402\n",
            "Loss :  0.11050880062858517\n",
            "Loss :  0.11051436347921773\n",
            "Loss :  0.1103187920241152\n",
            "Validation: \n",
            " Loss :  0.10402581095695496\n",
            " Loss :  0.10551298303263527\n",
            " Loss :  0.10396078864975673\n",
            " Loss :  0.10258800915030182\n",
            " Loss :  0.10261549993797585\n",
            "\n",
            "Epoch: 11\n",
            "Loss :  0.11310729384422302\n",
            "Loss :  0.10588148371739821\n",
            "Loss :  0.10528435451643807\n",
            "Loss :  0.10607156469937294\n",
            "Loss :  0.10681230938289224\n",
            "Loss :  0.10713371064733057\n",
            "Loss :  0.10679354152229965\n",
            "Loss :  0.10615617099782111\n",
            "Loss :  0.1060699564807209\n",
            "Loss :  0.10549907844800216\n",
            "Loss :  0.10500273477322984\n",
            "Loss :  0.10485859779087273\n",
            "Loss :  0.10513615195662522\n",
            "Loss :  0.10576163516699813\n",
            "Loss :  0.10574171808383144\n",
            "Loss :  0.10579938436580809\n",
            "Loss :  0.10590631880375169\n",
            "Loss :  0.10623265341011404\n",
            "Loss :  0.10638441289492075\n",
            "Loss :  0.10617539606481323\n",
            "Loss :  0.1061900947520982\n",
            "Loss :  0.10633663328196766\n",
            "Loss :  0.10655550763213256\n",
            "Loss :  0.10682613118773415\n",
            "Loss :  0.10678964813096889\n",
            "Loss :  0.10684307447942605\n",
            "Loss :  0.10682833060565122\n",
            "Loss :  0.10671664548975955\n",
            "Loss :  0.10667795987527989\n",
            "Loss :  0.10689149128714788\n",
            "Loss :  0.10686891964107653\n",
            "Loss :  0.10670722589807112\n",
            "Loss :  0.1067109492801803\n",
            "Loss :  0.10664896851579948\n",
            "Loss :  0.10669127753403179\n",
            "Loss :  0.10669305190401539\n",
            "Loss :  0.10663426190697255\n",
            "Loss :  0.10651551534464417\n",
            "Loss :  0.10641184026800742\n",
            "Loss :  0.10630530246612056\n",
            "Loss :  0.10630765564721124\n",
            "Loss :  0.10642584563751871\n",
            "Loss :  0.10657905873946509\n",
            "Loss :  0.1064469813090466\n",
            "Loss :  0.10647278899103065\n",
            "Loss :  0.10644439407965033\n",
            "Loss :  0.1064384292463677\n",
            "Loss :  0.10645947831578062\n",
            "Loss :  0.10641267460435938\n",
            "Loss :  0.10626733551928565\n",
            "Validation: \n",
            " Loss :  0.10214588791131973\n",
            " Loss :  0.10314772810254778\n",
            " Loss :  0.10236600804619672\n",
            " Loss :  0.1010466068983078\n",
            " Loss :  0.10096087637874815\n",
            "\n",
            "Epoch: 12\n",
            "Loss :  0.10775481909513474\n",
            "Loss :  0.10163313759998842\n",
            "Loss :  0.10047505689518792\n",
            "Loss :  0.10115605376420482\n",
            "Loss :  0.10178683316562234\n",
            "Loss :  0.10264534430176604\n",
            "Loss :  0.10192531970192174\n",
            "Loss :  0.1018570279571372\n",
            "Loss :  0.10182068395761797\n",
            "Loss :  0.10181486115350828\n",
            "Loss :  0.10143459632550136\n",
            "Loss :  0.10090581985475781\n",
            "Loss :  0.10098464405241091\n",
            "Loss :  0.10139187658561095\n",
            "Loss :  0.1012873278653368\n",
            "Loss :  0.10153810665110088\n",
            "Loss :  0.10150838809909288\n",
            "Loss :  0.10170034245092269\n",
            "Loss :  0.10189406952475974\n",
            "Loss :  0.10186187649896632\n",
            "Loss :  0.10179175336414309\n",
            "Loss :  0.10202278871248119\n",
            "Loss :  0.10221463374422686\n",
            "Loss :  0.10256991245142826\n",
            "Loss :  0.10250960982934074\n",
            "Loss :  0.1024437543168011\n",
            "Loss :  0.10242033549994801\n",
            "Loss :  0.10235327652679599\n",
            "Loss :  0.10214950899954793\n",
            "Loss :  0.10237380741900186\n",
            "Loss :  0.10239301775381018\n",
            "Loss :  0.10225597922345833\n",
            "Loss :  0.10218124421214761\n",
            "Loss :  0.10211974092030453\n",
            "Loss :  0.102184423559572\n",
            "Loss :  0.10232922823255898\n",
            "Loss :  0.102224241168215\n",
            "Loss :  0.10214648423892148\n",
            "Loss :  0.10206235978468822\n",
            "Loss :  0.10199990035856471\n",
            "Loss :  0.1020563067380627\n",
            "Loss :  0.10221178871364199\n",
            "Loss :  0.10230286318516787\n",
            "Loss :  0.10211794277147461\n",
            "Loss :  0.10211665164721526\n",
            "Loss :  0.10213663615699353\n",
            "Loss :  0.10210117503774399\n",
            "Loss :  0.1021156610482058\n",
            "Loss :  0.10217527823673712\n",
            "Loss :  0.10198778328305592\n",
            "Validation: \n",
            " Loss :  0.09562219679355621\n",
            " Loss :  0.09895302000499907\n",
            " Loss :  0.09793091856124925\n",
            " Loss :  0.09680646987723522\n",
            " Loss :  0.09715473348343814\n",
            "\n",
            "Epoch: 13\n",
            "Loss :  0.11168307811021805\n",
            "Loss :  0.09971408884633672\n",
            "Loss :  0.0996245387054625\n",
            "Loss :  0.10031460297684516\n",
            "Loss :  0.10047508358228498\n",
            "Loss :  0.10065568662157245\n",
            "Loss :  0.10022102613918117\n",
            "Loss :  0.09996584844841085\n",
            "Loss :  0.09968176585288695\n",
            "Loss :  0.09941382697977863\n",
            "Loss :  0.09880324342463276\n",
            "Loss :  0.09842540907698709\n",
            "Loss :  0.09823511473157188\n",
            "Loss :  0.09855724757409277\n",
            "Loss :  0.09850397642622603\n",
            "Loss :  0.09858364049369926\n",
            "Loss :  0.09864963813227896\n",
            "Loss :  0.09886323843608823\n",
            "Loss :  0.09903690815795192\n",
            "Loss :  0.09895876674127828\n",
            "Loss :  0.09889340497071471\n",
            "Loss :  0.09896148529380419\n",
            "Loss :  0.09903303786640254\n",
            "Loss :  0.09955390636281018\n",
            "Loss :  0.09955003409341164\n",
            "Loss :  0.0995820760133257\n",
            "Loss :  0.09955585719410943\n",
            "Loss :  0.09949006772569184\n",
            "Loss :  0.09954800006015445\n",
            "Loss :  0.09963791072368622\n",
            "Loss :  0.09978373529408065\n",
            "Loss :  0.0997165530872115\n",
            "Loss :  0.09976293466915594\n",
            "Loss :  0.09984920185138092\n",
            "Loss :  0.09992674624552825\n",
            "Loss :  0.09998768319686253\n",
            "Loss :  0.09993275700761341\n",
            "Loss :  0.099854648595229\n",
            "Loss :  0.09974873173502799\n",
            "Loss :  0.09970016310663174\n",
            "Loss :  0.0996820380414216\n",
            "Loss :  0.09970571537595016\n",
            "Loss :  0.0997364454130662\n",
            "Loss :  0.09959009692522323\n",
            "Loss :  0.09966408964795591\n",
            "Loss :  0.09966230446972497\n",
            "Loss :  0.09951638452983472\n",
            "Loss :  0.09954575431827781\n",
            "Loss :  0.09958491544094006\n",
            "Loss :  0.09945219098125602\n",
            "Validation: \n",
            " Loss :  0.09092225879430771\n",
            " Loss :  0.09507480050836291\n",
            " Loss :  0.09418621750139608\n",
            " Loss :  0.09285164832091722\n",
            " Loss :  0.09316060682873667\n",
            "\n",
            "Epoch: 14\n",
            "Loss :  0.11124810576438904\n",
            "Loss :  0.09388496184890921\n",
            "Loss :  0.09592266096955254\n",
            "Loss :  0.09621082054030511\n",
            "Loss :  0.0966532959080324\n",
            "Loss :  0.0965125803269592\n",
            "Loss :  0.09612272582093223\n",
            "Loss :  0.09620935249496514\n",
            "Loss :  0.09587118268748861\n",
            "Loss :  0.09576422557398513\n",
            "Loss :  0.09550566063954098\n",
            "Loss :  0.09512330772908958\n",
            "Loss :  0.09509326763882124\n",
            "Loss :  0.0953652385536951\n",
            "Loss :  0.09554611500484723\n",
            "Loss :  0.09589076757628397\n",
            "Loss :  0.09607901692575549\n",
            "Loss :  0.09619107611520945\n",
            "Loss :  0.09632400522080574\n",
            "Loss :  0.09634588752429522\n",
            "Loss :  0.09617332698990456\n",
            "Loss :  0.09631116246866389\n",
            "Loss :  0.09647696687759857\n",
            "Loss :  0.09689632467764281\n",
            "Loss :  0.09673408870256787\n",
            "Loss :  0.09667580491636854\n",
            "Loss :  0.09671599046823165\n",
            "Loss :  0.0966917954907646\n",
            "Loss :  0.09666356216333939\n",
            "Loss :  0.09689590365616317\n",
            "Loss :  0.09697736151194651\n",
            "Loss :  0.0968119351618543\n",
            "Loss :  0.09680002008643106\n",
            "Loss :  0.09682408391619017\n",
            "Loss :  0.09691015381879471\n",
            "Loss :  0.09711696706351391\n",
            "Loss :  0.09711187378273776\n",
            "Loss :  0.09711100673177493\n",
            "Loss :  0.09716451048772792\n",
            "Loss :  0.09698627267957038\n",
            "Loss :  0.09697991390329347\n",
            "Loss :  0.09697396761382003\n",
            "Loss :  0.0970011059362928\n",
            "Loss :  0.0969000633659053\n",
            "Loss :  0.09694696775301784\n",
            "Loss :  0.09690052470782907\n",
            "Loss :  0.09685736605311682\n",
            "Loss :  0.0968519702648661\n",
            "Loss :  0.09690856287670235\n",
            "Loss :  0.096815204562699\n",
            "Validation: \n",
            " Loss :  0.0954829752445221\n",
            " Loss :  0.09802990122919991\n",
            " Loss :  0.09702519799877958\n",
            " Loss :  0.09591804261578889\n",
            " Loss :  0.0962790267335044\n",
            "\n",
            "Epoch: 15\n",
            "Loss :  0.10563857853412628\n",
            "Loss :  0.09243084558031776\n",
            "Loss :  0.09097885943594433\n",
            "Loss :  0.09292769984852883\n",
            "Loss :  0.09344408642954943\n",
            "Loss :  0.09372395759119707\n",
            "Loss :  0.09294453739631371\n",
            "Loss :  0.09297954752831392\n",
            "Loss :  0.09303490265651986\n",
            "Loss :  0.0929893685893698\n",
            "Loss :  0.09254539558793058\n",
            "Loss :  0.09198852769426398\n",
            "Loss :  0.09213349099986809\n",
            "Loss :  0.09236280386912003\n",
            "Loss :  0.09221892803907394\n",
            "Loss :  0.09245222180292306\n",
            "Loss :  0.09249691623523369\n",
            "Loss :  0.09269187551492836\n",
            "Loss :  0.09288467705579094\n",
            "Loss :  0.09292062223737776\n",
            "Loss :  0.0928780406861756\n",
            "Loss :  0.09291132906743135\n",
            "Loss :  0.09308303693705554\n",
            "Loss :  0.09344024794958371\n",
            "Loss :  0.09341794230754939\n",
            "Loss :  0.09334352924173096\n",
            "Loss :  0.09338058990879534\n",
            "Loss :  0.09336303985536758\n",
            "Loss :  0.09331022648412562\n",
            "Loss :  0.09353537773041382\n",
            "Loss :  0.0936036929488182\n",
            "Loss :  0.09351790229223932\n",
            "Loss :  0.09353252716153582\n",
            "Loss :  0.09363745044905614\n",
            "Loss :  0.09372256005789179\n",
            "Loss :  0.09387541299107408\n",
            "Loss :  0.09379258343222399\n",
            "Loss :  0.09369622345642259\n",
            "Loss :  0.093611832776564\n",
            "Loss :  0.09352725854767557\n",
            "Loss :  0.09349365092052189\n",
            "Loss :  0.0935426661676734\n",
            "Loss :  0.09359488797301069\n",
            "Loss :  0.093507846208017\n",
            "Loss :  0.09351313767992721\n",
            "Loss :  0.09352269025308858\n",
            "Loss :  0.09345353493333639\n",
            "Loss :  0.09349534499227621\n",
            "Loss :  0.09350562442612995\n",
            "Loss :  0.0933491883992906\n",
            "Validation: \n",
            " Loss :  0.09083449840545654\n",
            " Loss :  0.09288446073021207\n",
            " Loss :  0.09233852494053724\n",
            " Loss :  0.09133500434824678\n",
            " Loss :  0.09149585093980954\n",
            "\n",
            "Epoch: 16\n",
            "Loss :  0.10070464760065079\n",
            "Loss :  0.09381381896409122\n",
            "Loss :  0.09175448190598261\n",
            "Loss :  0.0923921360123542\n",
            "Loss :  0.09241960252203592\n",
            "Loss :  0.092754554076522\n",
            "Loss :  0.09211906754091138\n",
            "Loss :  0.09191476207383921\n",
            "Loss :  0.09192634962591124\n",
            "Loss :  0.09176270311677849\n",
            "Loss :  0.09144776875134741\n",
            "Loss :  0.0911682292416289\n",
            "Loss :  0.09090330113064159\n",
            "Loss :  0.09101334254022773\n",
            "Loss :  0.09117659741471\n",
            "Loss :  0.09122555234179591\n",
            "Loss :  0.09132802731687238\n",
            "Loss :  0.09165008928169284\n",
            "Loss :  0.09187917304302447\n",
            "Loss :  0.09180035629353599\n",
            "Loss :  0.09181694074797986\n",
            "Loss :  0.09198496815576372\n",
            "Loss :  0.09200889162078701\n",
            "Loss :  0.09219853815449265\n",
            "Loss :  0.09212681202843971\n",
            "Loss :  0.09215625093515176\n",
            "Loss :  0.09207546571091217\n",
            "Loss :  0.0919113137735212\n",
            "Loss :  0.09196700789432084\n",
            "Loss :  0.09205304764697642\n",
            "Loss :  0.09211643857021268\n",
            "Loss :  0.09196859983865088\n",
            "Loss :  0.09194556222155087\n",
            "Loss :  0.09194239465311575\n",
            "Loss :  0.09195363386110826\n",
            "Loss :  0.09200515757259141\n",
            "Loss :  0.09196327007543348\n",
            "Loss :  0.09193856176742003\n",
            "Loss :  0.09195915480532985\n",
            "Loss :  0.09189918707779911\n",
            "Loss :  0.09192536377401424\n",
            "Loss :  0.09195791904581144\n",
            "Loss :  0.09200457111885882\n",
            "Loss :  0.09190753138604685\n",
            "Loss :  0.0919466492067389\n",
            "Loss :  0.09195746706092436\n",
            "Loss :  0.09188594790316973\n",
            "Loss :  0.09192361703217662\n",
            "Loss :  0.09196110636677415\n",
            "Loss :  0.09185994281424038\n",
            "Validation: \n",
            " Loss :  0.08358583599328995\n",
            " Loss :  0.08835762561786742\n",
            " Loss :  0.08734325192323546\n",
            " Loss :  0.08633270395583793\n",
            " Loss :  0.08646364068543469\n",
            "\n",
            "Epoch: 17\n",
            "Loss :  0.09448143094778061\n",
            "Loss :  0.08685297993096439\n",
            "Loss :  0.08747539598317373\n",
            "Loss :  0.08779562120476077\n",
            "Loss :  0.08863666998903925\n",
            "Loss :  0.0888843815408501\n",
            "Loss :  0.08874520525091985\n",
            "Loss :  0.08855634287629328\n",
            "Loss :  0.08881460148611187\n",
            "Loss :  0.08875358866138773\n",
            "Loss :  0.08866502337231494\n",
            "Loss :  0.08816408124324437\n",
            "Loss :  0.08814588336905171\n",
            "Loss :  0.08839513594640121\n",
            "Loss :  0.08832340243648976\n",
            "Loss :  0.08855013381566433\n",
            "Loss :  0.08865609718775898\n",
            "Loss :  0.08890403946589308\n",
            "Loss :  0.08876845191196842\n",
            "Loss :  0.08862543675599922\n",
            "Loss :  0.08868302126873785\n",
            "Loss :  0.08873828814775458\n",
            "Loss :  0.0888299470708381\n",
            "Loss :  0.08906337744616843\n",
            "Loss :  0.08911739521625131\n",
            "Loss :  0.0890576815996987\n",
            "Loss :  0.08902251001062064\n",
            "Loss :  0.08887654595159517\n",
            "Loss :  0.08895802924747569\n",
            "Loss :  0.08912248038958848\n",
            "Loss :  0.0892266101664879\n",
            "Loss :  0.08917457763692574\n",
            "Loss :  0.08916538603302103\n",
            "Loss :  0.0892320925150393\n",
            "Loss :  0.08930274767697381\n",
            "Loss :  0.08945907849786627\n",
            "Loss :  0.08942193279966423\n",
            "Loss :  0.08934915840947082\n",
            "Loss :  0.08927876240234049\n",
            "Loss :  0.08920512291247887\n",
            "Loss :  0.08920118090071881\n",
            "Loss :  0.08925769287739357\n",
            "Loss :  0.08930174581653431\n",
            "Loss :  0.08920465516933432\n",
            "Loss :  0.08923727152298908\n",
            "Loss :  0.08925901832443119\n",
            "Loss :  0.08920252250422625\n",
            "Loss :  0.08924308327479474\n",
            "Loss :  0.08935196319761494\n",
            "Loss :  0.08925744466286327\n",
            "Validation: \n",
            " Loss :  0.08796583861112595\n",
            " Loss :  0.09163175984507516\n",
            " Loss :  0.09164272793909399\n",
            " Loss :  0.09061363107356869\n",
            " Loss :  0.09078306649570111\n",
            "\n",
            "Epoch: 18\n",
            "Loss :  0.08913469314575195\n",
            "Loss :  0.08605759319933978\n",
            "Loss :  0.0850853721300761\n",
            "Loss :  0.08583508359809075\n",
            "Loss :  0.08648070529466723\n",
            "Loss :  0.0868408649283297\n",
            "Loss :  0.0864842068709311\n",
            "Loss :  0.08626423975531484\n",
            "Loss :  0.0864046636370965\n",
            "Loss :  0.08635440542475208\n",
            "Loss :  0.08588355337039079\n",
            "Loss :  0.08563203002149994\n",
            "Loss :  0.0854298753186691\n",
            "Loss :  0.08589012158736017\n",
            "Loss :  0.08596993005233454\n",
            "Loss :  0.08641440100622493\n",
            "Loss :  0.08646989484196124\n",
            "Loss :  0.08677706531962456\n",
            "Loss :  0.08700169779319131\n",
            "Loss :  0.08690038541848746\n",
            "Loss :  0.08700955536828112\n",
            "Loss :  0.08721214818869721\n",
            "Loss :  0.08732228368790441\n",
            "Loss :  0.08766844097799037\n",
            "Loss :  0.08763899202787036\n",
            "Loss :  0.08766412114598361\n",
            "Loss :  0.08772923075147972\n",
            "Loss :  0.08760262143238004\n",
            "Loss :  0.08757298333683047\n",
            "Loss :  0.08770985947441809\n",
            "Loss :  0.08767154892021635\n",
            "Loss :  0.08761623609583477\n",
            "Loss :  0.0875746779306284\n",
            "Loss :  0.087673927249325\n",
            "Loss :  0.08765277303646038\n",
            "Loss :  0.08778607870778467\n",
            "Loss :  0.08774621281102093\n",
            "Loss :  0.08772459543859862\n",
            "Loss :  0.0876604150247386\n",
            "Loss :  0.08760468547453966\n",
            "Loss :  0.08754787092419931\n",
            "Loss :  0.08758490092127863\n",
            "Loss :  0.0876307654196746\n",
            "Loss :  0.08744743668631999\n",
            "Loss :  0.08750491173697167\n",
            "Loss :  0.08757784990738342\n",
            "Loss :  0.08754279189733001\n",
            "Loss :  0.08762643895964208\n",
            "Loss :  0.08765946942034977\n",
            "Loss :  0.08757629482238696\n",
            "Validation: \n",
            " Loss :  0.08501657098531723\n",
            " Loss :  0.0840288024573099\n",
            " Loss :  0.08307420407853476\n",
            " Loss :  0.0818054698041228\n",
            " Loss :  0.08210221567639599\n",
            "\n",
            "Epoch: 19\n",
            "Loss :  0.09487462788820267\n",
            "Loss :  0.08307448300448331\n",
            "Loss :  0.08318474995238441\n",
            "Loss :  0.08408753670031024\n",
            "Loss :  0.08516337359096945\n",
            "Loss :  0.08555452014301337\n",
            "Loss :  0.08533364851943782\n",
            "Loss :  0.08564977494763656\n",
            "Loss :  0.08575439775063667\n",
            "Loss :  0.0855621554694333\n",
            "Loss :  0.08520646135110667\n",
            "Loss :  0.08474774846622536\n",
            "Loss :  0.08442901462809113\n",
            "Loss :  0.0850257849079052\n",
            "Loss :  0.0851150837244717\n",
            "Loss :  0.08539066054173652\n",
            "Loss :  0.08559086789255557\n",
            "Loss :  0.08571281458376444\n",
            "Loss :  0.08574856126176718\n",
            "Loss :  0.08565944522931314\n",
            "Loss :  0.08575755669109857\n",
            "Loss :  0.0858967904160373\n",
            "Loss :  0.08594038570089038\n",
            "Loss :  0.08613518567441346\n",
            "Loss :  0.08617889998612067\n",
            "Loss :  0.0861472951107291\n",
            "Loss :  0.08621491734438015\n",
            "Loss :  0.08613820371262702\n",
            "Loss :  0.08612358996982676\n",
            "Loss :  0.08619180536761727\n",
            "Loss :  0.08627747345802396\n",
            "Loss :  0.08620027629987986\n",
            "Loss :  0.08614853479409143\n",
            "Loss :  0.08618591361175491\n",
            "Loss :  0.0862167696755303\n",
            "Loss :  0.08635260776067391\n",
            "Loss :  0.08634762765215374\n",
            "Loss :  0.08634827237807194\n",
            "Loss :  0.08628551941609446\n",
            "Loss :  0.08620059703622023\n",
            "Loss :  0.0861244111732949\n",
            "Loss :  0.08617936832481347\n",
            "Loss :  0.08620051535249039\n",
            "Loss :  0.08604173848012207\n",
            "Loss :  0.08602332969168687\n",
            "Loss :  0.08603264665656501\n",
            "Loss :  0.08603160466328101\n",
            "Loss :  0.08599803527132974\n",
            "Loss :  0.08604417487266405\n",
            "Loss :  0.08595607123221982\n",
            "Validation: \n",
            " Loss :  0.08526955544948578\n",
            " Loss :  0.0867279622526396\n",
            " Loss :  0.0864815304918987\n",
            " Loss :  0.08541325736241262\n",
            " Loss :  0.08580832615678693\n",
            "\n",
            "Epoch: 20\n",
            "Loss :  0.09164559841156006\n",
            "Loss :  0.08422259783202951\n",
            "Loss :  0.0844541419120062\n",
            "Loss :  0.0849248291023316\n",
            "Loss :  0.08464042587978084\n",
            "Loss :  0.08435756625498042\n",
            "Loss :  0.08421914572598505\n",
            "Loss :  0.08399781301407747\n",
            "Loss :  0.08412259725140936\n",
            "Loss :  0.08417897728773263\n",
            "Loss :  0.08389661129158323\n",
            "Loss :  0.08357103457590481\n",
            "Loss :  0.08349114933536073\n",
            "Loss :  0.08394641986557545\n",
            "Loss :  0.08383702132718783\n",
            "Loss :  0.08383160010473617\n",
            "Loss :  0.08400818733324916\n",
            "Loss :  0.08420394452517493\n",
            "Loss :  0.08435618543987117\n",
            "Loss :  0.08424234909068852\n",
            "Loss :  0.08407326865552077\n",
            "Loss :  0.08412664447209281\n",
            "Loss :  0.08415396761031173\n",
            "Loss :  0.08440797211545886\n",
            "Loss :  0.08454722274264855\n",
            "Loss :  0.0844014036762287\n",
            "Loss :  0.0843668537521271\n",
            "Loss :  0.0842446797008444\n",
            "Loss :  0.0842696285120533\n",
            "Loss :  0.08445555081789437\n",
            "Loss :  0.08439188878797614\n",
            "Loss :  0.08436537232142169\n",
            "Loss :  0.08436920036593702\n",
            "Loss :  0.08449414275654132\n",
            "Loss :  0.08459251186103066\n",
            "Loss :  0.08476196736776591\n",
            "Loss :  0.08473159044650783\n",
            "Loss :  0.08464480625249626\n",
            "Loss :  0.08461692712203724\n",
            "Loss :  0.08455894502532452\n",
            "Loss :  0.08454061468640468\n",
            "Loss :  0.08454867218532701\n",
            "Loss :  0.0845451579913674\n",
            "Loss :  0.08446374613350893\n",
            "Loss :  0.08451219494380648\n",
            "Loss :  0.08445570890496416\n",
            "Loss :  0.08439941510331864\n",
            "Loss :  0.08447021709442645\n",
            "Loss :  0.08446889541067354\n",
            "Loss :  0.084400371141929\n",
            "Validation: \n",
            " Loss :  0.08863324671983719\n",
            " Loss :  0.09235123580410368\n",
            " Loss :  0.0925036403464108\n",
            " Loss :  0.09163288735463972\n",
            " Loss :  0.09214266132057448\n",
            "\n",
            "Epoch: 21\n",
            "Loss :  0.08409183472394943\n",
            "Loss :  0.08177218247543681\n",
            "Loss :  0.08128265539805095\n",
            "Loss :  0.08263157812818404\n",
            "Loss :  0.08325800208783732\n",
            "Loss :  0.08271752750756693\n",
            "Loss :  0.08278821921739422\n",
            "Loss :  0.0828063945535203\n",
            "Loss :  0.08313064810670452\n",
            "Loss :  0.08272481603281838\n",
            "Loss :  0.08255010902291478\n",
            "Loss :  0.08224999931481508\n",
            "Loss :  0.08195336760321925\n",
            "Loss :  0.08245701740943749\n",
            "Loss :  0.0825843907205771\n",
            "Loss :  0.08286435482715139\n",
            "Loss :  0.08297857484832313\n",
            "Loss :  0.08316500049236922\n",
            "Loss :  0.08326777358904727\n",
            "Loss :  0.08313113271565961\n",
            "Loss :  0.08323253583700503\n",
            "Loss :  0.08336601593483116\n",
            "Loss :  0.08337038907125525\n",
            "Loss :  0.08361872914549592\n",
            "Loss :  0.08365254765724245\n",
            "Loss :  0.08353653977591678\n",
            "Loss :  0.08350752776495798\n",
            "Loss :  0.08339560914413516\n",
            "Loss :  0.08334910294233268\n",
            "Loss :  0.08344883182409293\n",
            "Loss :  0.08348141717059272\n",
            "Loss :  0.08338538993305715\n",
            "Loss :  0.08337637048644068\n",
            "Loss :  0.08341215523888337\n",
            "Loss :  0.08350893900978251\n",
            "Loss :  0.08365268502225223\n",
            "Loss :  0.08362476917762836\n",
            "Loss :  0.08355559117469505\n",
            "Loss :  0.08347064485465448\n",
            "Loss :  0.08342344060425869\n",
            "Loss :  0.08337346592597533\n",
            "Loss :  0.0833942148553484\n",
            "Loss :  0.0833995970864194\n",
            "Loss :  0.08326224498710057\n",
            "Loss :  0.08325270943495693\n",
            "Loss :  0.08327090138870967\n",
            "Loss :  0.08323401589261735\n",
            "Loss :  0.08323006760605835\n",
            "Loss :  0.08328668937365875\n",
            "Loss :  0.08318100344624879\n",
            "Validation: \n",
            " Loss :  0.08460782468318939\n",
            " Loss :  0.08787946651379268\n",
            " Loss :  0.0868438382337733\n",
            " Loss :  0.08627789638570098\n",
            " Loss :  0.08656421036999902\n",
            "\n",
            "Epoch: 22\n",
            "Loss :  0.0891968160867691\n",
            "Loss :  0.08325265077027408\n",
            "Loss :  0.08202553966215678\n",
            "Loss :  0.08155435684227175\n",
            "Loss :  0.08097003291292888\n",
            "Loss :  0.08103123876978369\n",
            "Loss :  0.08062809644663921\n",
            "Loss :  0.08070691517541106\n",
            "Loss :  0.08035994468279826\n",
            "Loss :  0.08024732703036004\n",
            "Loss :  0.0797878687482069\n",
            "Loss :  0.0796239174701072\n",
            "Loss :  0.0794066307096442\n",
            "Loss :  0.07980201939362606\n",
            "Loss :  0.07992671979657302\n",
            "Loss :  0.08027726959511144\n",
            "Loss :  0.08057224537645068\n",
            "Loss :  0.08074600070889233\n",
            "Loss :  0.08097416870501818\n",
            "Loss :  0.08086093156281567\n",
            "Loss :  0.08090394286818765\n",
            "Loss :  0.08091302060685451\n",
            "Loss :  0.08106844794696273\n",
            "Loss :  0.08141956781541114\n",
            "Loss :  0.08146241186689045\n",
            "Loss :  0.08130601277033171\n",
            "Loss :  0.08127419073919684\n",
            "Loss :  0.08123170007088967\n",
            "Loss :  0.0813270445077869\n",
            "Loss :  0.0814178162526429\n",
            "Loss :  0.08150584072468685\n",
            "Loss :  0.08150562579321324\n",
            "Loss :  0.08144506942074618\n",
            "Loss :  0.08153532994657844\n",
            "Loss :  0.08157712203666262\n",
            "Loss :  0.08167997435626821\n",
            "Loss :  0.08161187607413184\n",
            "Loss :  0.08156380109549212\n",
            "Loss :  0.0815295374260487\n",
            "Loss :  0.08144444204352395\n",
            "Loss :  0.08151732587680555\n",
            "Loss :  0.08162639830306789\n",
            "Loss :  0.08168861924752487\n",
            "Loss :  0.08157325760740139\n",
            "Loss :  0.08161394196708187\n",
            "Loss :  0.08161157364657608\n",
            "Loss :  0.0815195909776036\n",
            "Loss :  0.08158508601208908\n",
            "Loss :  0.08161426882362167\n",
            "Loss :  0.08154644521215054\n",
            "Validation: \n",
            " Loss :  0.08013299852609634\n",
            " Loss :  0.08723082961071105\n",
            " Loss :  0.08698170141475957\n",
            " Loss :  0.08625415297316723\n",
            " Loss :  0.08660691792582288\n",
            "\n",
            "Epoch: 23\n",
            "Loss :  0.08974447101354599\n",
            "Loss :  0.07956567677584561\n",
            "Loss :  0.07843526239906039\n",
            "Loss :  0.07980941524428706\n",
            "Loss :  0.08004729704159062\n",
            "Loss :  0.08030186681186452\n",
            "Loss :  0.07949420431109726\n",
            "Loss :  0.07938404097943239\n",
            "Loss :  0.07953670004635681\n",
            "Loss :  0.07947396405123092\n",
            "Loss :  0.07912211968462066\n",
            "Loss :  0.07879641430603491\n",
            "Loss :  0.07852531070551597\n",
            "Loss :  0.0788868413740442\n",
            "Loss :  0.07897084486399981\n",
            "Loss :  0.07925001674929992\n",
            "Loss :  0.07944450064660599\n",
            "Loss :  0.0798069900500844\n",
            "Loss :  0.07983478158712387\n",
            "Loss :  0.07980583210266073\n",
            "Loss :  0.0797968287002388\n",
            "Loss :  0.0799212282373442\n",
            "Loss :  0.08009457652250566\n",
            "Loss :  0.08034923256604702\n",
            "Loss :  0.08037107617157624\n",
            "Loss :  0.08034252621738085\n",
            "Loss :  0.08029597923445062\n",
            "Loss :  0.08020273527316062\n",
            "Loss :  0.08034479000284155\n",
            "Loss :  0.08041988830386158\n",
            "Loss :  0.08042869862923986\n",
            "Loss :  0.08039323726842641\n",
            "Loss :  0.08039360449321545\n",
            "Loss :  0.08048693243740548\n",
            "Loss :  0.08052213085798923\n",
            "Loss :  0.08065154087169897\n",
            "Loss :  0.08067298152192477\n",
            "Loss :  0.08063938743823944\n",
            "Loss :  0.08060053201133185\n",
            "Loss :  0.08052656893878032\n",
            "Loss :  0.08049764527830103\n",
            "Loss :  0.08052365515390161\n",
            "Loss :  0.08056448008024494\n",
            "Loss :  0.08043265625350436\n",
            "Loss :  0.08050569509263752\n",
            "Loss :  0.0805213317985413\n",
            "Loss :  0.08048768571846655\n",
            "Loss :  0.08042986011587384\n",
            "Loss :  0.0804586164467424\n",
            "Loss :  0.08039259439421526\n",
            "Validation: \n",
            " Loss :  0.0762125626206398\n",
            " Loss :  0.07926873720827557\n",
            " Loss :  0.07891941615721075\n",
            " Loss :  0.07833290210024255\n",
            " Loss :  0.07868301344138605\n",
            "\n",
            "Epoch: 24\n",
            "Loss :  0.08455126732587814\n",
            "Loss :  0.0788967643271793\n",
            "Loss :  0.07724528937112718\n",
            "Loss :  0.07803626430611457\n",
            "Loss :  0.0779679297673993\n",
            "Loss :  0.07820281196458667\n",
            "Loss :  0.07813157397704046\n",
            "Loss :  0.0780326077635859\n",
            "Loss :  0.07808296180065767\n",
            "Loss :  0.07795350286331805\n",
            "Loss :  0.07780831203897401\n",
            "Loss :  0.07754117740435643\n",
            "Loss :  0.07759376048795448\n",
            "Loss :  0.07791491347642346\n",
            "Loss :  0.07789868422856568\n",
            "Loss :  0.07819785187575991\n",
            "Loss :  0.07813905137851372\n",
            "Loss :  0.07843926779882253\n",
            "Loss :  0.07852879022531088\n",
            "Loss :  0.07854980366860384\n",
            "Loss :  0.07861878682131791\n",
            "Loss :  0.0787080863217042\n",
            "Loss :  0.07872490588221615\n",
            "Loss :  0.0789219963795695\n",
            "Loss :  0.07897612709103778\n",
            "Loss :  0.07890559991161186\n",
            "Loss :  0.07885699022661223\n",
            "Loss :  0.07873417225381105\n",
            "Loss :  0.07871796726332016\n",
            "Loss :  0.07884093906563991\n",
            "Loss :  0.07878784278401504\n",
            "Loss :  0.07878295116581717\n",
            "Loss :  0.07883921541807436\n",
            "Loss :  0.07888951370903373\n",
            "Loss :  0.07896926688833321\n",
            "Loss :  0.07906499137820681\n",
            "Loss :  0.07900203289747898\n",
            "Loss :  0.07898684095018957\n",
            "Loss :  0.0789591021777138\n",
            "Loss :  0.07889533418295024\n",
            "Loss :  0.07891630009745719\n",
            "Loss :  0.07891874405081835\n",
            "Loss :  0.07890735957727296\n",
            "Loss :  0.07878200243078086\n",
            "Loss :  0.07876536854946153\n",
            "Loss :  0.07878513019655867\n",
            "Loss :  0.07870575984426796\n",
            "Loss :  0.07873001980996688\n",
            "Loss :  0.07879166376318109\n",
            "Loss :  0.07868036069355283\n",
            "Validation: \n",
            " Loss :  0.08166925609111786\n",
            " Loss :  0.08407757466747648\n",
            " Loss :  0.08339841173189443\n",
            " Loss :  0.08265969987775458\n",
            " Loss :  0.08311525474727889\n",
            "\n",
            "Epoch: 25\n",
            "Loss :  0.08559553325176239\n",
            "Loss :  0.07578678564591841\n",
            "Loss :  0.07595315362725939\n",
            "Loss :  0.07643855218925784\n",
            "Loss :  0.07709665451107955\n",
            "Loss :  0.07715697279747795\n",
            "Loss :  0.07707599355060546\n",
            "Loss :  0.0773838576926312\n",
            "Loss :  0.0774732768351649\n",
            "Loss :  0.07744308945896862\n",
            "Loss :  0.07720459599306087\n",
            "Loss :  0.07691961667827658\n",
            "Loss :  0.07659765262125938\n",
            "Loss :  0.07685621580436029\n",
            "Loss :  0.07701701921879822\n",
            "Loss :  0.0771813764428066\n",
            "Loss :  0.07723175977235255\n",
            "Loss :  0.07750213632022428\n",
            "Loss :  0.07762642589639564\n",
            "Loss :  0.07759333793951578\n",
            "Loss :  0.0774590603599501\n",
            "Loss :  0.07759397173238591\n",
            "Loss :  0.07757522037665768\n",
            "Loss :  0.07776905360805007\n",
            "Loss :  0.07778012879285574\n",
            "Loss :  0.07776477101789528\n",
            "Loss :  0.07776205786914205\n",
            "Loss :  0.07764985569068866\n",
            "Loss :  0.07766285689806175\n",
            "Loss :  0.07774258883753184\n",
            "Loss :  0.07783362633267114\n",
            "Loss :  0.07789009737623466\n",
            "Loss :  0.07788983572309262\n",
            "Loss :  0.07792568812287466\n",
            "Loss :  0.07801675925419128\n",
            "Loss :  0.07817446518997181\n",
            "Loss :  0.07816487755058875\n",
            "Loss :  0.0781244698239144\n",
            "Loss :  0.07809606403738183\n",
            "Loss :  0.07802494290425345\n",
            "Loss :  0.07801756759794276\n",
            "Loss :  0.07796351585328724\n",
            "Loss :  0.0780374631728101\n",
            "Loss :  0.07800805344831915\n",
            "Loss :  0.07805675549470649\n",
            "Loss :  0.078071652364374\n",
            "Loss :  0.07797033864188609\n",
            "Loss :  0.0780120813020855\n",
            "Loss :  0.07805959964771281\n",
            "Loss :  0.07797260246336339\n",
            "Validation: \n",
            " Loss :  0.08386821299791336\n",
            " Loss :  0.08596976740019661\n",
            " Loss :  0.08505835856606321\n",
            " Loss :  0.08449612948738161\n",
            " Loss :  0.08500881014782706\n",
            "\n",
            "Epoch: 26\n",
            "Loss :  0.07582227140665054\n",
            "Loss :  0.07399629124186256\n",
            "Loss :  0.0742921130288215\n",
            "Loss :  0.07547304779291153\n",
            "Loss :  0.07611007043501226\n",
            "Loss :  0.07603286133677352\n",
            "Loss :  0.07583877248842208\n",
            "Loss :  0.0755783676786322\n",
            "Loss :  0.07570796390926396\n",
            "Loss :  0.07560150980294406\n",
            "Loss :  0.07528105569948064\n",
            "Loss :  0.07500982687279985\n",
            "Loss :  0.07497336506104667\n",
            "Loss :  0.07508598876590947\n",
            "Loss :  0.07516044853849614\n",
            "Loss :  0.07532184901616432\n",
            "Loss :  0.07547792815458701\n",
            "Loss :  0.07586992871865891\n",
            "Loss :  0.07593650350254544\n",
            "Loss :  0.07576264729674574\n",
            "Loss :  0.07584222928801579\n",
            "Loss :  0.07593841568271131\n",
            "Loss :  0.07600090079582654\n",
            "Loss :  0.07631782945487406\n",
            "Loss :  0.07646018721132358\n",
            "Loss :  0.0764924688524459\n",
            "Loss :  0.07643067953801247\n",
            "Loss :  0.07643828249615497\n",
            "Loss :  0.07645481633970322\n",
            "Loss :  0.07652833983558151\n",
            "Loss :  0.0765808227400843\n",
            "Loss :  0.07660003880883337\n",
            "Loss :  0.07659487270770414\n",
            "Loss :  0.07661323411407067\n",
            "Loss :  0.07670405111617007\n",
            "Loss :  0.07682967236918262\n",
            "Loss :  0.07676795892768289\n",
            "Loss :  0.0767355287532922\n",
            "Loss :  0.07669251937213845\n",
            "Loss :  0.07663905816843443\n",
            "Loss :  0.0766329388145794\n",
            "Loss :  0.07675767706258453\n",
            "Loss :  0.0767673961906705\n",
            "Loss :  0.0767215812392965\n",
            "Loss :  0.07674557036485802\n",
            "Loss :  0.07678254697050593\n",
            "Loss :  0.07671682659431549\n",
            "Loss :  0.0767554020204615\n",
            "Loss :  0.07684824866651249\n",
            "Loss :  0.07679703323333667\n",
            "Validation: \n",
            " Loss :  0.08294347673654556\n",
            " Loss :  0.08245222980067843\n",
            " Loss :  0.08233491985536204\n",
            " Loss :  0.08161912174498448\n",
            " Loss :  0.08167994307515061\n",
            "\n",
            "Epoch: 27\n",
            "Loss :  0.09296771883964539\n",
            "Loss :  0.07718444480137392\n",
            "Loss :  0.07580847293138504\n",
            "Loss :  0.07591409620738798\n",
            "Loss :  0.07631994402263223\n",
            "Loss :  0.07625163448791877\n",
            "Loss :  0.07600314463259744\n",
            "Loss :  0.07598577534228983\n",
            "Loss :  0.0760428473169421\n",
            "Loss :  0.07590765113031471\n",
            "Loss :  0.07566887582882796\n",
            "Loss :  0.07530116061638067\n",
            "Loss :  0.07520087007894989\n",
            "Loss :  0.07555294594236912\n",
            "Loss :  0.07557666079795107\n",
            "Loss :  0.07574701013154542\n",
            "Loss :  0.07586694124692715\n",
            "Loss :  0.0760656554709401\n",
            "Loss :  0.07609929971767394\n",
            "Loss :  0.07603735465029772\n",
            "Loss :  0.0759624342494343\n",
            "Loss :  0.0760270957945365\n",
            "Loss :  0.07612727860587215\n",
            "Loss :  0.07634845577599682\n",
            "Loss :  0.07635697597228146\n",
            "Loss :  0.07627842315580266\n",
            "Loss :  0.07622763555910853\n",
            "Loss :  0.07612665408596782\n",
            "Loss :  0.07621977344367428\n",
            "Loss :  0.0763290855406281\n",
            "Loss :  0.07636897722599514\n",
            "Loss :  0.07623374311199525\n",
            "Loss :  0.07616704335650923\n",
            "Loss :  0.07618574939915781\n",
            "Loss :  0.07618667220265285\n",
            "Loss :  0.07626003748670943\n",
            "Loss :  0.07623605857347848\n",
            "Loss :  0.07621662198211948\n",
            "Loss :  0.0761602898164997\n",
            "Loss :  0.07606470659184639\n",
            "Loss :  0.07606308440912395\n",
            "Loss :  0.07612624994667197\n",
            "Loss :  0.0761539079868312\n",
            "Loss :  0.07609088015321236\n",
            "Loss :  0.07611124508099762\n",
            "Loss :  0.07613356603090621\n",
            "Loss :  0.07606933484108484\n",
            "Loss :  0.07609043681634206\n",
            "Loss :  0.07614431724633844\n",
            "Loss :  0.07603749635111533\n",
            "Validation: \n",
            " Loss :  0.0827886164188385\n",
            " Loss :  0.0846114701458386\n",
            " Loss :  0.08446143549389956\n",
            " Loss :  0.08354986837652863\n",
            " Loss :  0.08396186642808678\n",
            "\n",
            "Epoch: 28\n",
            "Loss :  0.08918624371290207\n",
            "Loss :  0.07442126219922846\n",
            "Loss :  0.07346191292717344\n",
            "Loss :  0.07373554331641044\n",
            "Loss :  0.07430306949266573\n",
            "Loss :  0.07435522389178183\n",
            "Loss :  0.07431473521912685\n",
            "Loss :  0.07418223706559396\n",
            "Loss :  0.07383155404233638\n",
            "Loss :  0.07384609672558176\n",
            "Loss :  0.07364490971264273\n",
            "Loss :  0.07337771681649191\n",
            "Loss :  0.07315793538019677\n",
            "Loss :  0.07345808067053329\n",
            "Loss :  0.07341616928366058\n",
            "Loss :  0.07373657531493547\n",
            "Loss :  0.07380672912142291\n",
            "Loss :  0.07403978524588005\n",
            "Loss :  0.07404197808046367\n",
            "Loss :  0.07393114684217887\n",
            "Loss :  0.07386996287537452\n",
            "Loss :  0.0740194848045636\n",
            "Loss :  0.07415118242078776\n",
            "Loss :  0.07444925220820295\n",
            "Loss :  0.07454672630458947\n",
            "Loss :  0.07446580693363193\n",
            "Loss :  0.0745209133059814\n",
            "Loss :  0.07441273522717927\n",
            "Loss :  0.07444645844903705\n",
            "Loss :  0.07464301246906474\n",
            "Loss :  0.07479802413883796\n",
            "Loss :  0.07477889503457155\n",
            "Loss :  0.07482657181186096\n",
            "Loss :  0.07487034699752972\n",
            "Loss :  0.07495582224992363\n",
            "Loss :  0.07513965595184569\n",
            "Loss :  0.07515606945612754\n",
            "Loss :  0.07512443324907449\n",
            "Loss :  0.07512643779786866\n",
            "Loss :  0.07508675827432776\n",
            "Loss :  0.07507335659385916\n",
            "Loss :  0.07506859791068556\n",
            "Loss :  0.07509079231325351\n",
            "Loss :  0.07503756700113866\n",
            "Loss :  0.07503974712253157\n",
            "Loss :  0.07507455237457598\n",
            "Loss :  0.0750538233966605\n",
            "Loss :  0.07506087959413822\n",
            "Loss :  0.0751128395682437\n",
            "Loss :  0.07502348904636387\n",
            "Validation: \n",
            " Loss :  0.08130841702222824\n",
            " Loss :  0.08537928476220086\n",
            " Loss :  0.0847086506645854\n",
            " Loss :  0.08428862202362936\n",
            " Loss :  0.08463043636745876\n",
            "\n",
            "Epoch: 29\n",
            "Loss :  0.07977738976478577\n",
            "Loss :  0.07425885647535324\n",
            "Loss :  0.07337139546871185\n",
            "Loss :  0.07316236342153241\n",
            "Loss :  0.07308810985669857\n",
            "Loss :  0.07327099093327336\n",
            "Loss :  0.07328260024307204\n",
            "Loss :  0.07327273288663004\n",
            "Loss :  0.0732920859698896\n",
            "Loss :  0.07319033186841797\n",
            "Loss :  0.07317245972923714\n",
            "Loss :  0.07302481130705224\n",
            "Loss :  0.07272297670402803\n",
            "Loss :  0.07309515253846882\n",
            "Loss :  0.07310216797264756\n",
            "Loss :  0.0733441925354746\n",
            "Loss :  0.07347814417126015\n",
            "Loss :  0.07373634941483799\n",
            "Loss :  0.07385703787536911\n",
            "Loss :  0.0737611975774403\n",
            "Loss :  0.07368002641037923\n",
            "Loss :  0.07376650026977345\n",
            "Loss :  0.07391265117272533\n",
            "Loss :  0.07422320356036162\n",
            "Loss :  0.07433411619660765\n",
            "Loss :  0.07428072857488674\n",
            "Loss :  0.07425679407756904\n",
            "Loss :  0.07419135296421737\n",
            "Loss :  0.07412827167381596\n",
            "Loss :  0.07424385654916059\n",
            "Loss :  0.07427631812386734\n",
            "Loss :  0.07418015001527366\n",
            "Loss :  0.07415516854400205\n",
            "Loss :  0.07424686667917359\n",
            "Loss :  0.07430274335563707\n",
            "Loss :  0.07446813172636887\n",
            "Loss :  0.07443707237159446\n",
            "Loss :  0.07439612377324516\n",
            "Loss :  0.074353428415739\n",
            "Loss :  0.07434105397680836\n",
            "Loss :  0.07434797917778355\n",
            "Loss :  0.07443121428218491\n",
            "Loss :  0.07443756481849383\n",
            "Loss :  0.0743010186021953\n",
            "Loss :  0.07427072414363864\n",
            "Loss :  0.07426671664790145\n",
            "Loss :  0.07420786483104214\n",
            "Loss :  0.07423986496123032\n",
            "Loss :  0.07429383746241829\n",
            "Loss :  0.07418366222663227\n",
            "Validation: \n",
            " Loss :  0.08231557905673981\n",
            " Loss :  0.08371000062851679\n",
            " Loss :  0.08333573926512788\n",
            " Loss :  0.0828242521794116\n",
            " Loss :  0.08338530048911955\n",
            "\n",
            "Epoch: 30\n",
            "Loss :  0.07369528710842133\n",
            "Loss :  0.07094934718175368\n",
            "Loss :  0.07034516086181004\n",
            "Loss :  0.07170169127564277\n",
            "Loss :  0.07179012476671033\n",
            "Loss :  0.07164994583410375\n",
            "Loss :  0.07157112847341866\n",
            "Loss :  0.07155817196192876\n",
            "Loss :  0.07158804211167642\n",
            "Loss :  0.07175482477951836\n",
            "Loss :  0.0716040823055376\n",
            "Loss :  0.07150948705436948\n",
            "Loss :  0.07141247631843425\n",
            "Loss :  0.07174392459729245\n",
            "Loss :  0.07178551069599517\n",
            "Loss :  0.07195457298037232\n",
            "Loss :  0.07195394351985884\n",
            "Loss :  0.07222818949243479\n",
            "Loss :  0.0724268386047848\n",
            "Loss :  0.0722755220040913\n",
            "Loss :  0.07240739807634805\n",
            "Loss :  0.07255820866444665\n",
            "Loss :  0.07263090643542924\n",
            "Loss :  0.07280698989505892\n",
            "Loss :  0.07281500884791628\n",
            "Loss :  0.0727563879909031\n",
            "Loss :  0.07289897551995585\n",
            "Loss :  0.07292466097927182\n",
            "Loss :  0.07289453491144333\n",
            "Loss :  0.07296450829126991\n",
            "Loss :  0.0729788593774618\n",
            "Loss :  0.07298373131506695\n",
            "Loss :  0.07297668773158689\n",
            "Loss :  0.07310774846964732\n",
            "Loss :  0.07316502665450846\n",
            "Loss :  0.07328430959089868\n",
            "Loss :  0.07325377727025434\n",
            "Loss :  0.07323986756793573\n",
            "Loss :  0.07317301495064275\n",
            "Loss :  0.07314479632107802\n",
            "Loss :  0.0731476254463939\n",
            "Loss :  0.07324729708460706\n",
            "Loss :  0.07324435052211947\n",
            "Loss :  0.07311207948040796\n",
            "Loss :  0.07311385924043028\n",
            "Loss :  0.07308077549524687\n",
            "Loss :  0.07300796106037503\n",
            "Loss :  0.07304101415184147\n",
            "Loss :  0.07309807265436823\n",
            "Loss :  0.07305869179381372\n",
            "Validation: \n",
            " Loss :  0.08340305835008621\n",
            " Loss :  0.08424407527560279\n",
            " Loss :  0.08425937611155393\n",
            " Loss :  0.083634866187807\n",
            " Loss :  0.08403245249280224\n",
            "\n",
            "Epoch: 31\n",
            "Loss :  0.07915142178535461\n",
            "Loss :  0.07103596830909903\n",
            "Loss :  0.07085686017360006\n",
            "Loss :  0.071379738349107\n",
            "Loss :  0.0719851392616586\n",
            "Loss :  0.0719462178063159\n",
            "Loss :  0.07202513778551681\n",
            "Loss :  0.07215698918616267\n",
            "Loss :  0.07215331398226597\n",
            "Loss :  0.07221563811321835\n",
            "Loss :  0.0720421428432559\n",
            "Loss :  0.07170239768855206\n",
            "Loss :  0.07155494887597305\n",
            "Loss :  0.0718355784548148\n",
            "Loss :  0.07198951837230236\n",
            "Loss :  0.07214242688670064\n",
            "Loss :  0.07242289791751352\n",
            "Loss :  0.07262325661572797\n",
            "Loss :  0.07265161932831969\n",
            "Loss :  0.07261863046603677\n",
            "Loss :  0.07266027382134799\n",
            "Loss :  0.07279125920616054\n",
            "Loss :  0.07288741467023327\n",
            "Loss :  0.07303295360305609\n",
            "Loss :  0.07296471268371428\n",
            "Loss :  0.0729167517497245\n",
            "Loss :  0.07291675557407383\n",
            "Loss :  0.07283279730000179\n",
            "Loss :  0.07292828784453487\n",
            "Loss :  0.07308850218493913\n",
            "Loss :  0.07313962652041667\n",
            "Loss :  0.07311961373428057\n",
            "Loss :  0.07313272296639618\n",
            "Loss :  0.07319008859487819\n",
            "Loss :  0.07325110343063682\n",
            "Loss :  0.07330474980239175\n",
            "Loss :  0.07324737045201898\n",
            "Loss :  0.07320966355683348\n",
            "Loss :  0.0731486585333435\n",
            "Loss :  0.07306142548656525\n",
            "Loss :  0.07307472183111302\n",
            "Loss :  0.07307269778601154\n",
            "Loss :  0.07309818128898138\n",
            "Loss :  0.0729808400789159\n",
            "Loss :  0.07299504197643998\n",
            "Loss :  0.07296797053314366\n",
            "Loss :  0.07290498426672952\n",
            "Loss :  0.07289656894696746\n",
            "Loss :  0.07291712406674195\n",
            "Loss :  0.07285658880127423\n",
            "Validation: \n",
            " Loss :  0.07603345066308975\n",
            " Loss :  0.07613119163683482\n",
            " Loss :  0.07581730350488569\n",
            " Loss :  0.07535257483603525\n",
            " Loss :  0.07562508719202912\n",
            "\n",
            "Epoch: 32\n",
            "Loss :  0.07528532296419144\n",
            "Loss :  0.06933298774740913\n",
            "Loss :  0.06890518856900078\n",
            "Loss :  0.0694461427628994\n",
            "Loss :  0.06977914346427452\n",
            "Loss :  0.06995540696616266\n",
            "Loss :  0.06986370157511508\n",
            "Loss :  0.06989495996648157\n",
            "Loss :  0.06992673266817022\n",
            "Loss :  0.06986028656035989\n",
            "Loss :  0.06966914952096373\n",
            "Loss :  0.06949579568059595\n",
            "Loss :  0.06925112763342779\n",
            "Loss :  0.0695944644851994\n",
            "Loss :  0.0697705218608075\n",
            "Loss :  0.07012210704928992\n",
            "Loss :  0.07016690262818928\n",
            "Loss :  0.070465347724177\n",
            "Loss :  0.07045176903104913\n",
            "Loss :  0.07048593670207792\n",
            "Loss :  0.07060561629373636\n",
            "Loss :  0.07089524867975316\n",
            "Loss :  0.07092565582955584\n",
            "Loss :  0.0711500993183939\n",
            "Loss :  0.07119790169398814\n",
            "Loss :  0.07115033705277272\n",
            "Loss :  0.07114174433937474\n",
            "Loss :  0.07108944035874082\n",
            "Loss :  0.07115077439470223\n",
            "Loss :  0.07131758042934425\n",
            "Loss :  0.0713164806242203\n",
            "Loss :  0.07130230990780512\n",
            "Loss :  0.07128590607661696\n",
            "Loss :  0.0713297656745709\n",
            "Loss :  0.07139796931897441\n",
            "Loss :  0.07142749456343828\n",
            "Loss :  0.07144032263937419\n",
            "Loss :  0.07143712545822894\n",
            "Loss :  0.07137351377507833\n",
            "Loss :  0.07130955626516391\n",
            "Loss :  0.07130510057148494\n",
            "Loss :  0.07132939582831088\n",
            "Loss :  0.07134197706316825\n",
            "Loss :  0.07126035290638422\n",
            "Loss :  0.07126782530424547\n",
            "Loss :  0.07124560026497376\n",
            "Loss :  0.07118165065532393\n",
            "Loss :  0.07118016312930994\n",
            "Loss :  0.07123289821890189\n",
            "Loss :  0.07116699274894905\n",
            "Validation: \n",
            " Loss :  0.08003101497888565\n",
            " Loss :  0.08833418360778264\n",
            " Loss :  0.08772366784694718\n",
            " Loss :  0.08715434164785948\n",
            " Loss :  0.08753168509330278\n",
            "\n",
            "Epoch: 33\n",
            "Loss :  0.07570164650678635\n",
            "Loss :  0.07092757421461018\n",
            "Loss :  0.06995029179822831\n",
            "Loss :  0.07026349128253999\n",
            "Loss :  0.06998745751817052\n",
            "Loss :  0.07044632564864907\n",
            "Loss :  0.07034038880565127\n",
            "Loss :  0.06991725304806737\n",
            "Loss :  0.06982998407365364\n",
            "Loss :  0.06969336956575677\n",
            "Loss :  0.06953792093266355\n",
            "Loss :  0.06948204241223163\n",
            "Loss :  0.06947806403656637\n",
            "Loss :  0.06997778494166963\n",
            "Loss :  0.07015912176658076\n",
            "Loss :  0.07041143436009521\n",
            "Loss :  0.07049029836465853\n",
            "Loss :  0.07079431909131027\n",
            "Loss :  0.0706836662116301\n",
            "Loss :  0.07061087150926365\n",
            "Loss :  0.0705938686556484\n",
            "Loss :  0.0707177160334248\n",
            "Loss :  0.07073798448656479\n",
            "Loss :  0.07094600516093241\n",
            "Loss :  0.07095277402163541\n",
            "Loss :  0.07085757597212297\n",
            "Loss :  0.07082653281161155\n",
            "Loss :  0.07080461204436872\n",
            "Loss :  0.07082144641632288\n",
            "Loss :  0.07092643376836662\n",
            "Loss :  0.07091278090231443\n",
            "Loss :  0.07093759901653915\n",
            "Loss :  0.07086780478139161\n",
            "Loss :  0.0709511736616087\n",
            "Loss :  0.07099977117086435\n",
            "Loss :  0.07112925713006248\n",
            "Loss :  0.07110493078084863\n",
            "Loss :  0.07111145665422283\n",
            "Loss :  0.07109552774373001\n",
            "Loss :  0.07104237716826027\n",
            "Loss :  0.07102972365674236\n",
            "Loss :  0.07103307570564196\n",
            "Loss :  0.07101581696058112\n",
            "Loss :  0.07095517779067331\n",
            "Loss :  0.07097813303744982\n",
            "Loss :  0.07100922311191549\n",
            "Loss :  0.07093974581865065\n",
            "Loss :  0.07100146411742121\n",
            "Loss :  0.07101825701378735\n",
            "Loss :  0.070947933304759\n",
            "Validation: \n",
            " Loss :  0.0797184631228447\n",
            " Loss :  0.08657772306885038\n",
            " Loss :  0.08572421913466803\n",
            " Loss :  0.08498552319456319\n",
            " Loss :  0.08533781582926526\n",
            "\n",
            "Epoch: 34\n",
            "Loss :  0.07571611553430557\n",
            "Loss :  0.06958352367986333\n",
            "Loss :  0.06840742627779643\n",
            "Loss :  0.06908859360602594\n",
            "Loss :  0.06918081804746534\n",
            "Loss :  0.06893786846422682\n",
            "Loss :  0.06840006793375875\n",
            "Loss :  0.06831704289980338\n",
            "Loss :  0.06871033009187674\n",
            "Loss :  0.06878297489423019\n",
            "Loss :  0.06869593144643425\n",
            "Loss :  0.0685623100883252\n",
            "Loss :  0.06846632131121376\n",
            "Loss :  0.06877668543171336\n",
            "Loss :  0.06911917279163997\n",
            "Loss :  0.06920780887864283\n",
            "Loss :  0.06951642041065678\n",
            "Loss :  0.06963429547715605\n",
            "Loss :  0.0696368805693658\n",
            "Loss :  0.06956829659211698\n",
            "Loss :  0.06957893966887127\n",
            "Loss :  0.06965517777049146\n",
            "Loss :  0.0698154925524649\n",
            "Loss :  0.07004223762330039\n",
            "Loss :  0.070213531943896\n",
            "Loss :  0.07005610708815169\n",
            "Loss :  0.0700380845402164\n",
            "Loss :  0.07004792169011387\n",
            "Loss :  0.070078508285441\n",
            "Loss :  0.07020530690954313\n",
            "Loss :  0.07023330738279114\n",
            "Loss :  0.07017507061506008\n",
            "Loss :  0.07016756745328041\n",
            "Loss :  0.07021080553981833\n",
            "Loss :  0.0702671695314894\n",
            "Loss :  0.07039496311095365\n",
            "Loss :  0.07038938610136014\n",
            "Loss :  0.0703569238257376\n",
            "Loss :  0.0703228437231751\n",
            "Loss :  0.07024965238998003\n",
            "Loss :  0.07022343713446448\n",
            "Loss :  0.07017571575160154\n",
            "Loss :  0.07013195100950516\n",
            "Loss :  0.07004434324092212\n",
            "Loss :  0.07008893868110888\n",
            "Loss :  0.07008354627669783\n",
            "Loss :  0.0699983624632343\n",
            "Loss :  0.07003337342584716\n",
            "Loss :  0.07004999146330133\n",
            "Loss :  0.06997280705970563\n",
            "Validation: \n",
            " Loss :  0.08283742517232895\n",
            " Loss :  0.0861883486310641\n",
            " Loss :  0.08550560856010855\n",
            " Loss :  0.08485474803897201\n",
            " Loss :  0.08509792350692513\n",
            "\n",
            "Epoch: 35\n",
            "Loss :  0.0743994489312172\n",
            "Loss :  0.0677421350370754\n",
            "Loss :  0.06818484053725288\n",
            "Loss :  0.0687807607314279\n",
            "Loss :  0.06873465774626267\n",
            "Loss :  0.068613544471708\n",
            "Loss :  0.06853698261204313\n",
            "Loss :  0.06832183456756699\n",
            "Loss :  0.06869161657897042\n",
            "Loss :  0.06864459841297223\n",
            "Loss :  0.06838901285635363\n",
            "Loss :  0.0679987952583008\n",
            "Loss :  0.06788113907225861\n",
            "Loss :  0.06816358682308488\n",
            "Loss :  0.0682948926556195\n",
            "Loss :  0.06844585818170712\n",
            "Loss :  0.06858872867519071\n",
            "Loss :  0.06884526762000301\n",
            "Loss :  0.06891014966187556\n",
            "Loss :  0.06886069833530181\n",
            "Loss :  0.06884243778550803\n",
            "Loss :  0.0689213956525258\n",
            "Loss :  0.06905937162796837\n",
            "Loss :  0.06932339909208285\n",
            "Loss :  0.06950606849369172\n",
            "Loss :  0.06949988316254786\n",
            "Loss :  0.06947445692458828\n",
            "Loss :  0.06940969466008383\n",
            "Loss :  0.06936889185546981\n",
            "Loss :  0.06953354729051442\n",
            "Loss :  0.06952325601217359\n",
            "Loss :  0.06953810242834199\n",
            "Loss :  0.06949669151709087\n",
            "Loss :  0.06951641033648365\n",
            "Loss :  0.06951140801359482\n",
            "Loss :  0.06955168292223558\n",
            "Loss :  0.06951093161865615\n",
            "Loss :  0.06951341679636037\n",
            "Loss :  0.06952023517897749\n",
            "Loss :  0.06948224384613964\n",
            "Loss :  0.0694833042522767\n",
            "Loss :  0.06948896716836016\n",
            "Loss :  0.06947700556305696\n",
            "Loss :  0.06935103854765196\n",
            "Loss :  0.06936895431709938\n",
            "Loss :  0.06937960842562356\n",
            "Loss :  0.06929373854282363\n",
            "Loss :  0.06930892310261473\n",
            "Loss :  0.06939335700256165\n",
            "Loss :  0.06932139460399535\n",
            "Validation: \n",
            " Loss :  0.08420433849096298\n",
            " Loss :  0.0877522627512614\n",
            " Loss :  0.08661668620458464\n",
            " Loss :  0.0858353259133511\n",
            " Loss :  0.0861503349410163\n",
            "\n",
            "Epoch: 36\n",
            "Loss :  0.07030883431434631\n",
            "Loss :  0.06869870288805528\n",
            "Loss :  0.06765028390856016\n",
            "Loss :  0.06789750305394973\n",
            "Loss :  0.06759278112795294\n",
            "Loss :  0.06733620429740232\n",
            "Loss :  0.0671997172910659\n",
            "Loss :  0.06727788314013414\n",
            "Loss :  0.06755536058802664\n",
            "Loss :  0.06771472374816517\n",
            "Loss :  0.06752933923265722\n",
            "Loss :  0.06720927535547866\n",
            "Loss :  0.06710856752701042\n",
            "Loss :  0.06743841385113374\n",
            "Loss :  0.06753951194861256\n",
            "Loss :  0.06775857508182526\n",
            "Loss :  0.06793868995231131\n",
            "Loss :  0.06822218976871312\n",
            "Loss :  0.06820192309456635\n",
            "Loss :  0.06808034129713843\n",
            "Loss :  0.06814225535116979\n",
            "Loss :  0.0681420999327542\n",
            "Loss :  0.06821998638602403\n",
            "Loss :  0.06839983534567799\n",
            "Loss :  0.06850736135330933\n",
            "Loss :  0.0683584532949079\n",
            "Loss :  0.06842113477516905\n",
            "Loss :  0.06834076709248044\n",
            "Loss :  0.06834415091844641\n",
            "Loss :  0.06847240894879263\n",
            "Loss :  0.06857746273922762\n",
            "Loss :  0.06856349520265481\n",
            "Loss :  0.0685387746636927\n",
            "Loss :  0.06857377294102104\n",
            "Loss :  0.06864807932826081\n",
            "Loss :  0.06874216578242785\n",
            "Loss :  0.06876014567230547\n",
            "Loss :  0.06863837963286436\n",
            "Loss :  0.06864306441168459\n",
            "Loss :  0.06863363412068323\n",
            "Loss :  0.06860904460610297\n",
            "Loss :  0.06871061201078178\n",
            "Loss :  0.06863556378421477\n",
            "Loss :  0.06858727028226078\n",
            "Loss :  0.06857710287752065\n",
            "Loss :  0.06861553749197602\n",
            "Loss :  0.0685514264422875\n",
            "Loss :  0.06854443512303308\n",
            "Loss :  0.06859102267423439\n",
            "Loss :  0.06857520093014673\n",
            "Validation: \n",
            " Loss :  0.08282167464494705\n",
            " Loss :  0.08433806186630613\n",
            " Loss :  0.08345920574374316\n",
            " Loss :  0.08287372513384116\n",
            " Loss :  0.0829470216492076\n",
            "\n",
            "Epoch: 37\n",
            "Loss :  0.06700814515352249\n",
            "Loss :  0.06630723652514545\n",
            "Loss :  0.06610686704516411\n",
            "Loss :  0.06728210816940954\n",
            "Loss :  0.06772443043386064\n",
            "Loss :  0.06763921188665371\n",
            "Loss :  0.06773099822343373\n",
            "Loss :  0.06746932224068843\n",
            "Loss :  0.06751538537166736\n",
            "Loss :  0.06755600750937567\n",
            "Loss :  0.06747511663649342\n",
            "Loss :  0.06731878929175772\n",
            "Loss :  0.06711708695804777\n",
            "Loss :  0.06741546694445245\n",
            "Loss :  0.06752017843173751\n",
            "Loss :  0.06780392720999308\n",
            "Loss :  0.06799088140822346\n",
            "Loss :  0.0681102986049931\n",
            "Loss :  0.0681315719957839\n",
            "Loss :  0.06803744762199711\n",
            "Loss :  0.06812276932137523\n",
            "Loss :  0.06825707609195845\n",
            "Loss :  0.06836007006162971\n",
            "Loss :  0.06851466611285746\n",
            "Loss :  0.06851401370157839\n",
            "Loss :  0.06844525162382904\n",
            "Loss :  0.06842367225451487\n",
            "Loss :  0.06835812412724723\n",
            "Loss :  0.06831282190315664\n",
            "Loss :  0.06836891139751856\n",
            "Loss :  0.06839759442398319\n",
            "Loss :  0.06834103177837621\n",
            "Loss :  0.06837032446376631\n",
            "Loss :  0.06839603582942955\n",
            "Loss :  0.06846569639103503\n",
            "Loss :  0.06854927895373089\n",
            "Loss :  0.06857235590382957\n",
            "Loss :  0.06853444636309886\n",
            "Loss :  0.0684803314115901\n",
            "Loss :  0.06840299359520378\n",
            "Loss :  0.0684044186555388\n",
            "Loss :  0.06843247029866906\n",
            "Loss :  0.06846267001752049\n",
            "Loss :  0.0684044181764679\n",
            "Loss :  0.06837979603617911\n",
            "Loss :  0.06837946724997392\n",
            "Loss :  0.06828891796235666\n",
            "Loss :  0.06831263000987897\n",
            "Loss :  0.06833241940350146\n",
            "Loss :  0.06829320333609756\n",
            "Validation: \n",
            " Loss :  0.07253504544496536\n",
            " Loss :  0.07981230673335847\n",
            " Loss :  0.07911324446521155\n",
            " Loss :  0.07872889007701249\n",
            " Loss :  0.07897996930060563\n",
            "\n",
            "Epoch: 38\n",
            "Loss :  0.08097332715988159\n",
            "Loss :  0.06649654392491687\n",
            "Loss :  0.06658672151111421\n",
            "Loss :  0.0668232170564513\n",
            "Loss :  0.06688620568048663\n",
            "Loss :  0.06743053361481312\n",
            "Loss :  0.06722554955326143\n",
            "Loss :  0.06691757857169904\n",
            "Loss :  0.06703071664144963\n",
            "Loss :  0.06712393249784197\n",
            "Loss :  0.06710971061988633\n",
            "Loss :  0.06666430280552255\n",
            "Loss :  0.0663921918011894\n",
            "Loss :  0.06668522388084244\n",
            "Loss :  0.06679001931391709\n",
            "Loss :  0.06692445968950031\n",
            "Loss :  0.06699879039426028\n",
            "Loss :  0.0672882957198815\n",
            "Loss :  0.06730688147205674\n",
            "Loss :  0.06721708231181374\n",
            "Loss :  0.06730257590018694\n",
            "Loss :  0.06729360297322273\n",
            "Loss :  0.0673311657327063\n",
            "Loss :  0.06760966310124376\n",
            "Loss :  0.06761256143203415\n",
            "Loss :  0.06746915968290838\n",
            "Loss :  0.06743587850382501\n",
            "Loss :  0.06740983152191578\n",
            "Loss :  0.06740878631105626\n",
            "Loss :  0.0675199305273823\n",
            "Loss :  0.0676034851946506\n",
            "Loss :  0.06763487611073773\n",
            "Loss :  0.0676286369213991\n",
            "Loss :  0.06764810896982239\n",
            "Loss :  0.06771858886452364\n",
            "Loss :  0.06778821637827447\n",
            "Loss :  0.06778307761635807\n",
            "Loss :  0.06773811561680547\n",
            "Loss :  0.0676931672854217\n",
            "Loss :  0.06760672545608352\n",
            "Loss :  0.06761784450223024\n",
            "Loss :  0.06762154503677884\n",
            "Loss :  0.06763687897075384\n",
            "Loss :  0.06752268810110414\n",
            "Loss :  0.06753165174334769\n",
            "Loss :  0.06754122494395715\n",
            "Loss :  0.06742172285817451\n",
            "Loss :  0.06741453389263456\n",
            "Loss :  0.06743189200696975\n",
            "Loss :  0.06738850211083525\n",
            "Validation: \n",
            " Loss :  0.08193344622850418\n",
            " Loss :  0.08933911472558975\n",
            " Loss :  0.08882156850361242\n",
            " Loss :  0.08851815393713654\n",
            " Loss :  0.08909708500644307\n",
            "\n",
            "Epoch: 39\n",
            "Loss :  0.07646527141332626\n",
            "Loss :  0.06866294992240993\n",
            "Loss :  0.06687559755075545\n",
            "Loss :  0.06683253184441597\n",
            "Loss :  0.06695035081811068\n",
            "Loss :  0.06686684956737593\n",
            "Loss :  0.06654992467555844\n",
            "Loss :  0.06657331468353808\n",
            "Loss :  0.06666516760985057\n",
            "Loss :  0.0666890595476706\n",
            "Loss :  0.06656504669549441\n",
            "Loss :  0.06618016169549108\n",
            "Loss :  0.06592463680412158\n",
            "Loss :  0.06617022191977683\n",
            "Loss :  0.06633719114969808\n",
            "Loss :  0.06654159626029185\n",
            "Loss :  0.06658044485201747\n",
            "Loss :  0.0667374782830651\n",
            "Loss :  0.06684461777299149\n",
            "Loss :  0.06681062930619529\n",
            "Loss :  0.06677603680844331\n",
            "Loss :  0.0667861034625797\n",
            "Loss :  0.06688383295794957\n",
            "Loss :  0.06709418813516567\n",
            "Loss :  0.06716263430247169\n",
            "Loss :  0.0670158741216498\n",
            "Loss :  0.06693729631231662\n",
            "Loss :  0.06677157521192878\n",
            "Loss :  0.0668261953069434\n",
            "Loss :  0.06685491327413988\n",
            "Loss :  0.06690331667265623\n",
            "Loss :  0.06692442246428257\n",
            "Loss :  0.06688062883797465\n",
            "Loss :  0.06686374874858697\n",
            "Loss :  0.0669216572845087\n",
            "Loss :  0.06701568103371522\n",
            "Loss :  0.06696925979943487\n",
            "Loss :  0.06695238977791164\n",
            "Loss :  0.0669239087719617\n",
            "Loss :  0.06683421210216745\n",
            "Loss :  0.0668193743171686\n",
            "Loss :  0.06685072385735466\n",
            "Loss :  0.0668456377439431\n",
            "Loss :  0.0668031309413661\n",
            "Loss :  0.06681729332795219\n",
            "Loss :  0.06681963527149742\n",
            "Loss :  0.06677814413856012\n",
            "Loss :  0.06682932943959904\n",
            "Loss :  0.06683489983115276\n",
            "Loss :  0.06682628385586069\n",
            "Validation: \n",
            " Loss :  0.07133565098047256\n",
            " Loss :  0.07645606657578832\n",
            " Loss :  0.0761252624414316\n",
            " Loss :  0.07587842234089727\n",
            " Loss :  0.07603433999566385\n",
            "\n",
            "Epoch: 40\n",
            "Loss :  0.07586167007684708\n",
            "Loss :  0.06579279188405383\n",
            "Loss :  0.06512586222518058\n",
            "Loss :  0.06538622213467475\n",
            "Loss :  0.06591615426104243\n",
            "Loss :  0.06587045296442275\n",
            "Loss :  0.06562401519202796\n",
            "Loss :  0.06549903063077323\n",
            "Loss :  0.06563819397562816\n",
            "Loss :  0.06556572617737802\n",
            "Loss :  0.06533969573602819\n",
            "Loss :  0.0651844972828487\n",
            "Loss :  0.0649397173696313\n",
            "Loss :  0.06517732333932214\n",
            "Loss :  0.06547824285448865\n",
            "Loss :  0.06554450785482166\n",
            "Loss :  0.06549274400802133\n",
            "Loss :  0.06566175162705065\n",
            "Loss :  0.0656544077338764\n",
            "Loss :  0.06557028736743628\n",
            "Loss :  0.06566477203695335\n",
            "Loss :  0.06578588206762385\n",
            "Loss :  0.06583965844495804\n",
            "Loss :  0.066085441755655\n",
            "Loss :  0.06617059373138356\n",
            "Loss :  0.06616397387537348\n",
            "Loss :  0.06624466728890079\n",
            "Loss :  0.06619995910798052\n",
            "Loss :  0.0662425975464417\n",
            "Loss :  0.06636451101538651\n",
            "Loss :  0.0664212540089094\n",
            "Loss :  0.0663391710573454\n",
            "Loss :  0.06627668831439404\n",
            "Loss :  0.06626789651113335\n",
            "Loss :  0.06633917194220328\n",
            "Loss :  0.06643302641023598\n",
            "Loss :  0.06640332254098723\n",
            "Loss :  0.0663199458080482\n",
            "Loss :  0.06630423149763756\n",
            "Loss :  0.06620919299514397\n",
            "Loss :  0.06619888805131663\n",
            "Loss :  0.06621184570764684\n",
            "Loss :  0.06620977322555495\n",
            "Loss :  0.06614178985253565\n",
            "Loss :  0.06614529525611947\n",
            "Loss :  0.06608343228466496\n",
            "Loss :  0.06602502236882156\n",
            "Loss :  0.06605939892162184\n",
            "Loss :  0.0660805246035671\n",
            "Loss :  0.06603953233197361\n",
            "Validation: \n",
            " Loss :  0.07271931320428848\n",
            " Loss :  0.07845862458149593\n",
            " Loss :  0.07820109114414309\n",
            " Loss :  0.07756709747138571\n",
            " Loss :  0.07796547322729487\n",
            "\n",
            "Epoch: 41\n",
            "Loss :  0.07041511684656143\n",
            "Loss :  0.06418561021035368\n",
            "Loss :  0.06411368932042803\n",
            "Loss :  0.06467021893589728\n",
            "Loss :  0.06469862453821229\n",
            "Loss :  0.06526286257248298\n",
            "Loss :  0.0650860065685921\n",
            "Loss :  0.06480877746788549\n",
            "Loss :  0.06494363852673107\n",
            "Loss :  0.06504718234742081\n",
            "Loss :  0.06471208619451758\n",
            "Loss :  0.06443839765212557\n",
            "Loss :  0.06428256578558733\n",
            "Loss :  0.0643585451402282\n",
            "Loss :  0.06454371988561981\n",
            "Loss :  0.06466476962167696\n",
            "Loss :  0.06467174923753147\n",
            "Loss :  0.06474870319167773\n",
            "Loss :  0.06485364674616255\n",
            "Loss :  0.06495984156094296\n",
            "Loss :  0.06499422308224351\n",
            "Loss :  0.06504739317778163\n",
            "Loss :  0.06504758516520397\n",
            "Loss :  0.0652988874131725\n",
            "Loss :  0.0653342829082022\n",
            "Loss :  0.0652890765512607\n",
            "Loss :  0.06523044124759476\n",
            "Loss :  0.06514002966979773\n",
            "Loss :  0.06517472969255414\n",
            "Loss :  0.06527412436979334\n",
            "Loss :  0.06531274112009527\n",
            "Loss :  0.06533254468603916\n",
            "Loss :  0.06530607650723784\n",
            "Loss :  0.06528802990373168\n",
            "Loss :  0.0654344867008173\n",
            "Loss :  0.06554650495152528\n",
            "Loss :  0.06559042150665519\n",
            "Loss :  0.065587012754939\n",
            "Loss :  0.06551538664955167\n",
            "Loss :  0.0654457834980372\n",
            "Loss :  0.06541314364371455\n",
            "Loss :  0.06544144244053358\n",
            "Loss :  0.065390023723634\n",
            "Loss :  0.06532293582322149\n",
            "Loss :  0.06530981300538088\n",
            "Loss :  0.06538532272873326\n",
            "Loss :  0.06539832125872438\n",
            "Loss :  0.06539628127957606\n",
            "Loss :  0.06544592392605704\n",
            "Loss :  0.0654154587423243\n",
            "Validation: \n",
            " Loss :  0.08064386248588562\n",
            " Loss :  0.09008865058422089\n",
            " Loss :  0.08870286094706233\n",
            " Loss :  0.08850441714290713\n",
            " Loss :  0.0883986563594253\n",
            "\n",
            "Epoch: 42\n",
            "Loss :  0.0686054602265358\n",
            "Loss :  0.06415171447125348\n",
            "Loss :  0.06394490688329652\n",
            "Loss :  0.06433098102288862\n",
            "Loss :  0.06431139451338024\n",
            "Loss :  0.06475944619844942\n",
            "Loss :  0.06468912922456617\n",
            "Loss :  0.06467218025469444\n",
            "Loss :  0.06472253256741865\n",
            "Loss :  0.06466864753555465\n",
            "Loss :  0.0647001530200538\n",
            "Loss :  0.0643245822808764\n",
            "Loss :  0.06419945455902865\n",
            "Loss :  0.06435636214855063\n",
            "Loss :  0.06455387537043991\n",
            "Loss :  0.0647986437508602\n",
            "Loss :  0.06489566840833018\n",
            "Loss :  0.06513658372892274\n",
            "Loss :  0.0651698949751933\n",
            "Loss :  0.06508804048309151\n",
            "Loss :  0.06508481167071495\n",
            "Loss :  0.06514100456760392\n",
            "Loss :  0.0651739791912191\n",
            "Loss :  0.06527975124198121\n",
            "Loss :  0.06539838482853783\n",
            "Loss :  0.06535725341553232\n",
            "Loss :  0.06532580085755307\n",
            "Loss :  0.06524864475610512\n",
            "Loss :  0.0652591953869392\n",
            "Loss :  0.06534572270830062\n",
            "Loss :  0.06535762663546987\n",
            "Loss :  0.06534374082443031\n",
            "Loss :  0.06531449540799653\n",
            "Loss :  0.06530640168492527\n",
            "Loss :  0.06534186068955056\n",
            "Loss :  0.06543076394969581\n",
            "Loss :  0.06538319346234409\n",
            "Loss :  0.06538670321278817\n",
            "Loss :  0.06540036295342633\n",
            "Loss :  0.065313903181373\n",
            "Loss :  0.06532850341309336\n",
            "Loss :  0.06532384919279104\n",
            "Loss :  0.0652655323971762\n",
            "Loss :  0.06518791685974239\n",
            "Loss :  0.06521136760542723\n",
            "Loss :  0.06524695198313889\n",
            "Loss :  0.06517425695042289\n",
            "Loss :  0.06517006422050946\n",
            "Loss :  0.06521946968599814\n",
            "Loss :  0.06519245223546222\n",
            "Validation: \n",
            " Loss :  0.08482123911380768\n",
            " Loss :  0.0883462056517601\n",
            " Loss :  0.08746573201766829\n",
            " Loss :  0.08685100811426757\n",
            " Loss :  0.08718054346096368\n",
            "\n",
            "Epoch: 43\n",
            "Loss :  0.06363862007856369\n",
            "Loss :  0.06483078002929688\n",
            "Loss :  0.06368013968070348\n",
            "Loss :  0.06384977626223717\n",
            "Loss :  0.06404853139708681\n",
            "Loss :  0.06391751839249742\n",
            "Loss :  0.06381639942038254\n",
            "Loss :  0.06368362310696656\n",
            "Loss :  0.06364835913718482\n",
            "Loss :  0.06377328928191583\n",
            "Loss :  0.06378011239489706\n",
            "Loss :  0.06341218814119562\n",
            "Loss :  0.06324031752003126\n",
            "Loss :  0.06353996563276262\n",
            "Loss :  0.06377896280787515\n",
            "Loss :  0.06396893920093183\n",
            "Loss :  0.06412440637901703\n",
            "Loss :  0.06430509045981524\n",
            "Loss :  0.06436725487695873\n",
            "Loss :  0.06417727979455942\n",
            "Loss :  0.0641993629880509\n",
            "Loss :  0.06419742525824439\n",
            "Loss :  0.06423034662237534\n",
            "Loss :  0.06444671417856629\n",
            "Loss :  0.06450772126247774\n",
            "Loss :  0.06449660587595754\n",
            "Loss :  0.06447375148991516\n",
            "Loss :  0.0644627037660882\n",
            "Loss :  0.06453626305925464\n",
            "Loss :  0.06465532168541167\n",
            "Loss :  0.06468700539580612\n",
            "Loss :  0.06464424608365132\n",
            "Loss :  0.06464526138480207\n",
            "Loss :  0.06465170342261338\n",
            "Loss :  0.06471543563557161\n",
            "Loss :  0.06473835845321332\n",
            "Loss :  0.06469396006450098\n",
            "Loss :  0.06467315216229932\n",
            "Loss :  0.06466793539760307\n",
            "Loss :  0.06456721906581193\n",
            "Loss :  0.06454658831592808\n",
            "Loss :  0.06453406194410764\n",
            "Loss :  0.06449296279432089\n",
            "Loss :  0.06442598315720768\n",
            "Loss :  0.06440982390659737\n",
            "Loss :  0.06440038319900136\n",
            "Loss :  0.0643132725354777\n",
            "Loss :  0.0643613840000518\n",
            "Loss :  0.06440014103253269\n",
            "Loss :  0.06433096266296634\n",
            "Validation: \n",
            " Loss :  0.07865402847528458\n",
            " Loss :  0.08120575405302502\n",
            " Loss :  0.08071929979615095\n",
            " Loss :  0.08041639613812088\n",
            " Loss :  0.08060559446429029\n",
            "\n",
            "Epoch: 44\n",
            "Loss :  0.06969815492630005\n",
            "Loss :  0.06217741695317355\n",
            "Loss :  0.06242312863469124\n",
            "Loss :  0.0629498505544278\n",
            "Loss :  0.06291723587527508\n",
            "Loss :  0.06298044404270602\n",
            "Loss :  0.06282213989828454\n",
            "Loss :  0.06284002513742783\n",
            "Loss :  0.06313824552444765\n",
            "Loss :  0.06336610608703487\n",
            "Loss :  0.06320546228106659\n",
            "Loss :  0.06314816859525603\n",
            "Loss :  0.06311653766873454\n",
            "Loss :  0.06344788669402363\n",
            "Loss :  0.06346746431069171\n",
            "Loss :  0.06355411697499799\n",
            "Loss :  0.063585763438518\n",
            "Loss :  0.06379482274254163\n",
            "Loss :  0.06379716442023194\n",
            "Loss :  0.06381781286126031\n",
            "Loss :  0.06384060357637074\n",
            "Loss :  0.06394842265270897\n",
            "Loss :  0.06398988494910805\n",
            "Loss :  0.06416730340812113\n",
            "Loss :  0.06423148903918464\n",
            "Loss :  0.06410558227880067\n",
            "Loss :  0.06408877720720924\n",
            "Loss :  0.06397127144286113\n",
            "Loss :  0.06394848940741549\n",
            "Loss :  0.06405046017667682\n",
            "Loss :  0.06403583653295951\n",
            "Loss :  0.06401741394467676\n",
            "Loss :  0.0639983344695464\n",
            "Loss :  0.06407047643852377\n",
            "Loss :  0.06416287140980843\n",
            "Loss :  0.06424194251709854\n",
            "Loss :  0.064260254389609\n",
            "Loss :  0.06425988027509653\n",
            "Loss :  0.06420839706000693\n",
            "Loss :  0.06418589033815257\n",
            "Loss :  0.06413365671053492\n",
            "Loss :  0.06416552339809654\n",
            "Loss :  0.06415800022894866\n",
            "Loss :  0.06410599181000705\n",
            "Loss :  0.06410624017588405\n",
            "Loss :  0.06410323151133278\n",
            "Loss :  0.06403335140222065\n",
            "Loss :  0.06402198702191851\n",
            "Loss :  0.06407798208652564\n",
            "Loss :  0.06404067253477705\n",
            "Validation: \n",
            " Loss :  0.08052976429462433\n",
            " Loss :  0.08063127597173055\n",
            " Loss :  0.08018687094857053\n",
            " Loss :  0.07995509624969764\n",
            " Loss :  0.08044543236861994\n",
            "\n",
            "Epoch: 45\n",
            "Loss :  0.06985964626073837\n",
            "Loss :  0.06284679912708023\n",
            "Loss :  0.06175516989259493\n",
            "Loss :  0.06180152681566054\n",
            "Loss :  0.06232056030776442\n",
            "Loss :  0.06253810847798984\n",
            "Loss :  0.06232600152248242\n",
            "Loss :  0.06204585416216246\n",
            "Loss :  0.06224629365735584\n",
            "Loss :  0.062296434745683776\n",
            "Loss :  0.06226776536590982\n",
            "Loss :  0.062109751039528635\n",
            "Loss :  0.06212860847677081\n",
            "Loss :  0.06253483385302638\n",
            "Loss :  0.06267150531106806\n",
            "Loss :  0.06282711371976808\n",
            "Loss :  0.06298920433528675\n",
            "Loss :  0.06317849866828026\n",
            "Loss :  0.06313035188458901\n",
            "Loss :  0.06308358090476215\n",
            "Loss :  0.06303028341623682\n",
            "Loss :  0.06311569110406519\n",
            "Loss :  0.06309954161287973\n",
            "Loss :  0.06321339734963008\n",
            "Loss :  0.06337673332990452\n",
            "Loss :  0.06343738394845054\n",
            "Loss :  0.06342689229096946\n",
            "Loss :  0.06333401287833702\n",
            "Loss :  0.06327902179648867\n",
            "Loss :  0.0634161594909491\n",
            "Loss :  0.06340539519770994\n",
            "Loss :  0.06336362741384476\n",
            "Loss :  0.06339981362827099\n",
            "Loss :  0.06339101973932315\n",
            "Loss :  0.06346062492153162\n",
            "Loss :  0.06350492891932485\n",
            "Loss :  0.06347515470144491\n",
            "Loss :  0.06342198389439249\n",
            "Loss :  0.0634223482329545\n",
            "Loss :  0.06334289271965661\n",
            "Loss :  0.06330553162610739\n",
            "Loss :  0.06332484782285934\n",
            "Loss :  0.06333772791217739\n",
            "Loss :  0.06324581183026395\n",
            "Loss :  0.0632671023594414\n",
            "Loss :  0.06324284027295472\n",
            "Loss :  0.06322879713521604\n",
            "Loss :  0.06323668183541349\n",
            "Loss :  0.06324983832741973\n",
            "Loss :  0.06317193003451023\n",
            "Validation: \n",
            " Loss :  0.0766502246260643\n",
            " Loss :  0.08255001263959068\n",
            " Loss :  0.0823402604678782\n",
            " Loss :  0.08193093489428036\n",
            " Loss :  0.08196891209593525\n",
            "\n",
            "Epoch: 46\n",
            "Loss :  0.07431937009096146\n",
            "Loss :  0.06218510967763988\n",
            "Loss :  0.06216879774417196\n",
            "Loss :  0.06222710217679701\n",
            "Loss :  0.0625811685330984\n",
            "Loss :  0.06264897231377807\n",
            "Loss :  0.06254304401942941\n",
            "Loss :  0.06224492890104442\n",
            "Loss :  0.062236902936373226\n",
            "Loss :  0.062234226696111346\n",
            "Loss :  0.06211624312961456\n",
            "Loss :  0.06181359911958376\n",
            "Loss :  0.0614927789458066\n",
            "Loss :  0.06178067567694278\n",
            "Loss :  0.06190079278874059\n",
            "Loss :  0.06205107034831647\n",
            "Loss :  0.062235859286340865\n",
            "Loss :  0.06241081413208393\n",
            "Loss :  0.06249259914482496\n",
            "Loss :  0.062346331170560175\n",
            "Loss :  0.062401397644880396\n",
            "Loss :  0.06264671137759471\n",
            "Loss :  0.06270256110917928\n",
            "Loss :  0.06295730638039576\n",
            "Loss :  0.06301913147645373\n",
            "Loss :  0.06299528991618004\n",
            "Loss :  0.06300751685068526\n",
            "Loss :  0.06298656926603775\n",
            "Loss :  0.06290356508618572\n",
            "Loss :  0.06300890978259319\n",
            "Loss :  0.0631152567178308\n",
            "Loss :  0.06310822551177629\n",
            "Loss :  0.06309572400705094\n",
            "Loss :  0.06311947293016845\n",
            "Loss :  0.06322960443030005\n",
            "Loss :  0.06337675069089968\n",
            "Loss :  0.06335364272843767\n",
            "Loss :  0.06332427554455086\n",
            "Loss :  0.06327562348970904\n",
            "Loss :  0.06318645067798817\n",
            "Loss :  0.063196761733353\n",
            "Loss :  0.06324989324177269\n",
            "Loss :  0.06326034205861726\n",
            "Loss :  0.06319952330912902\n",
            "Loss :  0.06319260678323758\n",
            "Loss :  0.06320501554203932\n",
            "Loss :  0.06314604990426734\n",
            "Loss :  0.0631257808448909\n",
            "Loss :  0.06316948926033696\n",
            "Loss :  0.06309929216193327\n",
            "Validation: \n",
            " Loss :  0.08147046715021133\n",
            " Loss :  0.08376903548127129\n",
            " Loss :  0.0832309595695356\n",
            " Loss :  0.08293139946753861\n",
            " Loss :  0.08303670539164248\n",
            "\n",
            "Epoch: 47\n",
            "Loss :  0.07280800491571426\n",
            "Loss :  0.06282267821106044\n",
            "Loss :  0.0613771735557488\n",
            "Loss :  0.062167494768096555\n",
            "Loss :  0.0622675041781693\n",
            "Loss :  0.06207308896324214\n",
            "Loss :  0.06188810549554278\n",
            "Loss :  0.061807702964460344\n",
            "Loss :  0.0621380703408777\n",
            "Loss :  0.06209465787633435\n",
            "Loss :  0.06216558275541457\n",
            "Loss :  0.062142746606925585\n",
            "Loss :  0.06184092841365121\n",
            "Loss :  0.062018976866743944\n",
            "Loss :  0.062215480213681014\n",
            "Loss :  0.06234141066670418\n",
            "Loss :  0.06244552850445605\n",
            "Loss :  0.06253120775895508\n",
            "Loss :  0.06254049481113971\n",
            "Loss :  0.06244724635912486\n",
            "Loss :  0.062450820014844484\n",
            "Loss :  0.06249786860428715\n",
            "Loss :  0.06259299915840183\n",
            "Loss :  0.06283433674088804\n",
            "Loss :  0.06286834699731644\n",
            "Loss :  0.06282765479142448\n",
            "Loss :  0.0627737210000155\n",
            "Loss :  0.0626314704571043\n",
            "Loss :  0.06258485950131858\n",
            "Loss :  0.06267228023600332\n",
            "Loss :  0.0626988724814697\n",
            "Loss :  0.06271477013682629\n",
            "Loss :  0.06271468642576833\n",
            "Loss :  0.06271035247834067\n",
            "Loss :  0.06279384275991197\n",
            "Loss :  0.06288224973442548\n",
            "Loss :  0.06287823103554031\n",
            "Loss :  0.0628316332548455\n",
            "Loss :  0.06274640827080397\n",
            "Loss :  0.06270149072913257\n",
            "Loss :  0.06268377429306358\n",
            "Loss :  0.06269936618868742\n",
            "Loss :  0.06267975583991076\n",
            "Loss :  0.06259664201681132\n",
            "Loss :  0.06258889460036544\n",
            "Loss :  0.06260543290170492\n",
            "Loss :  0.06251609195306607\n",
            "Loss :  0.062536917903684\n",
            "Loss :  0.06253126198561425\n",
            "Loss :  0.062485110045025646\n",
            "Validation: \n",
            " Loss :  0.07678256928920746\n",
            " Loss :  0.07946072022120158\n",
            " Loss :  0.0788998156785965\n",
            " Loss :  0.07888466009839637\n",
            " Loss :  0.07908000502689386\n",
            "\n",
            "Epoch: 48\n",
            "Loss :  0.07267727702856064\n",
            "Loss :  0.06216052987358787\n",
            "Loss :  0.06099817121312732\n",
            "Loss :  0.06201472337688169\n",
            "Loss :  0.061823384427442785\n",
            "Loss :  0.06229493547888363\n",
            "Loss :  0.061891959033540035\n",
            "Loss :  0.061761344507546494\n",
            "Loss :  0.0617034318914384\n",
            "Loss :  0.06181049829983449\n",
            "Loss :  0.0618226356287994\n",
            "Loss :  0.061596965299801784\n",
            "Loss :  0.06151809527977439\n",
            "Loss :  0.061738584901540335\n",
            "Loss :  0.06189244414897675\n",
            "Loss :  0.06194618556475797\n",
            "Loss :  0.062027247356516976\n",
            "Loss :  0.06223696971323058\n",
            "Loss :  0.06226886942653366\n",
            "Loss :  0.062194312059598446\n",
            "Loss :  0.062254955704828994\n",
            "Loss :  0.06239367144014598\n",
            "Loss :  0.06253567895940526\n",
            "Loss :  0.06275897939161305\n",
            "Loss :  0.06274876646171962\n",
            "Loss :  0.06268023755920836\n",
            "Loss :  0.06263058813881144\n",
            "Loss :  0.06259882887241146\n",
            "Loss :  0.0625589854511502\n",
            "Loss :  0.06255735637447268\n",
            "Loss :  0.06251430056023835\n",
            "Loss :  0.062490939327374914\n",
            "Loss :  0.062478666933618976\n",
            "Loss :  0.06242396015099889\n",
            "Loss :  0.06251564837140072\n",
            "Loss :  0.06265039607203245\n",
            "Loss :  0.06263834787042517\n",
            "Loss :  0.06256442092298818\n",
            "Loss :  0.06249491569251213\n",
            "Loss :  0.06243489605500875\n",
            "Loss :  0.06240933335518599\n",
            "Loss :  0.062410030332722514\n",
            "Loss :  0.0624139731984382\n",
            "Loss :  0.06235357642761516\n",
            "Loss :  0.062344413723737474\n",
            "Loss :  0.062369197904402825\n",
            "Loss :  0.06228370241287474\n",
            "Loss :  0.062321606042721724\n",
            "Loss :  0.06240259745233768\n",
            "Loss :  0.062336500969292434\n",
            "Validation: \n",
            " Loss :  0.07554326951503754\n",
            " Loss :  0.0831608594883056\n",
            " Loss :  0.08205293137125852\n",
            " Loss :  0.08184684263389619\n",
            " Loss :  0.08181488421964056\n",
            "\n",
            "Epoch: 49\n",
            "Loss :  0.06645891070365906\n",
            "Loss :  0.059276332570747894\n",
            "Loss :  0.05918520440657934\n",
            "Loss :  0.06045086261245512\n",
            "Loss :  0.06053031235933304\n",
            "Loss :  0.06070742383599281\n",
            "Loss :  0.060742767558234635\n",
            "Loss :  0.06032572161983436\n",
            "Loss :  0.06058962712133372\n",
            "Loss :  0.06079143838404299\n",
            "Loss :  0.06061053670721479\n",
            "Loss :  0.060344232430866174\n",
            "Loss :  0.060247370711535464\n",
            "Loss :  0.060515934871580766\n",
            "Loss :  0.06081838982430755\n",
            "Loss :  0.06085997015632541\n",
            "Loss :  0.06098864451976296\n",
            "Loss :  0.061231497239473964\n",
            "Loss :  0.06119425502188956\n",
            "Loss :  0.06135451038860526\n",
            "Loss :  0.06135104978410759\n",
            "Loss :  0.06143141306618943\n",
            "Loss :  0.061478634950666945\n",
            "Loss :  0.0616583481463261\n",
            "Loss :  0.0617132247947311\n",
            "Loss :  0.0617460794153204\n",
            "Loss :  0.061697494581170464\n",
            "Loss :  0.06166245441925042\n",
            "Loss :  0.06169418964097508\n",
            "Loss :  0.061750408671668304\n",
            "Loss :  0.061805997712172545\n",
            "Loss :  0.06177138647370017\n",
            "Loss :  0.06174756133946303\n",
            "Loss :  0.06179827767961695\n",
            "Loss :  0.061800800100012605\n",
            "Loss :  0.061875973331962215\n",
            "Loss :  0.061896459733515236\n",
            "Loss :  0.06186320990163361\n",
            "Loss :  0.06183795891995505\n",
            "Loss :  0.061791519584405756\n",
            "Loss :  0.06181859450781732\n",
            "Loss :  0.06181215221139346\n",
            "Loss :  0.06178430144867535\n",
            "Loss :  0.06173425981139639\n",
            "Loss :  0.061743277921225206\n",
            "Loss :  0.06177471300582928\n",
            "Loss :  0.06173337600042401\n",
            "Loss :  0.061752052491257904\n",
            "Loss :  0.06177961454569922\n",
            "Loss :  0.061715133351609566\n",
            "Validation: \n",
            " Loss :  0.07193418592214584\n",
            " Loss :  0.07929912404645056\n",
            " Loss :  0.07868061605386617\n",
            " Loss :  0.07824637832455948\n",
            " Loss :  0.07829943386676871\n",
            "\n",
            "Epoch: 50\n",
            "Loss :  0.06673481315374374\n",
            "Loss :  0.05902540616013787\n",
            "Loss :  0.05965711921453476\n",
            "Loss :  0.06030485911234733\n",
            "Loss :  0.060426277812661196\n",
            "Loss :  0.06035890707782671\n",
            "Loss :  0.06046626576390423\n",
            "Loss :  0.060191065492764324\n",
            "Loss :  0.06039433503224526\n",
            "Loss :  0.06051962593427071\n",
            "Loss :  0.06052815939972896\n",
            "Loss :  0.06036700670783584\n",
            "Loss :  0.06020025890474477\n",
            "Loss :  0.06043576983777621\n",
            "Loss :  0.060583523137772335\n",
            "Loss :  0.06080198243556433\n",
            "Loss :  0.06091859541046694\n",
            "Loss :  0.06098519729679091\n",
            "Loss :  0.06106996283099796\n",
            "Loss :  0.06113299782051466\n",
            "Loss :  0.061185986174279776\n",
            "Loss :  0.06123677104429046\n",
            "Loss :  0.06126492558156743\n",
            "Loss :  0.06140265249328696\n",
            "Loss :  0.061415881335858985\n",
            "Loss :  0.06121695197495331\n",
            "Loss :  0.0611884748969955\n",
            "Loss :  0.0611368364266144\n",
            "Loss :  0.061112622402316734\n",
            "Loss :  0.0611901832712475\n",
            "Loss :  0.06122359898300266\n",
            "Loss :  0.06124700181833034\n",
            "Loss :  0.06121220800596234\n",
            "Loss :  0.06122678160397308\n",
            "Loss :  0.061274781954253524\n",
            "Loss :  0.06133172335235821\n",
            "Loss :  0.06131861235048632\n",
            "Loss :  0.061301528403379844\n",
            "Loss :  0.06126551050448355\n",
            "Loss :  0.06126490509723458\n",
            "Loss :  0.0612307883000136\n",
            "Loss :  0.06124913715134282\n",
            "Loss :  0.06123156589843032\n",
            "Loss :  0.06115418949149324\n",
            "Loss :  0.061159983446268266\n",
            "Loss :  0.06118735709039705\n",
            "Loss :  0.061125902479227605\n",
            "Loss :  0.061128223042009745\n",
            "Loss :  0.06119864541657749\n",
            "Loss :  0.06114506592515044\n",
            "Validation: \n",
            " Loss :  0.07326560467481613\n",
            " Loss :  0.07818414164440972\n",
            " Loss :  0.07794152545492823\n",
            " Loss :  0.07748992716679808\n",
            " Loss :  0.07767526409876199\n",
            "\n",
            "Epoch: 51\n",
            "Loss :  0.05889204889535904\n",
            "Loss :  0.058606517924503845\n",
            "Loss :  0.058739040401719865\n",
            "Loss :  0.05918894816310175\n",
            "Loss :  0.05977459415429976\n",
            "Loss :  0.06013831810331812\n",
            "Loss :  0.06007758882202086\n",
            "Loss :  0.05966215727614685\n",
            "Loss :  0.059957204225622576\n",
            "Loss :  0.06016064848709893\n",
            "Loss :  0.06015086716206947\n",
            "Loss :  0.05995301700927116\n",
            "Loss :  0.05986816688510012\n",
            "Loss :  0.060151212628333624\n",
            "Loss :  0.060270501287482306\n",
            "Loss :  0.06042095542644823\n",
            "Loss :  0.06055676805214112\n",
            "Loss :  0.060721698995919254\n",
            "Loss :  0.06083072302091187\n",
            "Loss :  0.06077967931306799\n",
            "Loss :  0.060744771872883414\n",
            "Loss :  0.06074742531465693\n",
            "Loss :  0.06075661017546826\n",
            "Loss :  0.060912575475968324\n",
            "Loss :  0.06101494324219672\n",
            "Loss :  0.060944804528438715\n",
            "Loss :  0.06092791541897018\n",
            "Loss :  0.06088230259908961\n",
            "Loss :  0.06087441180312336\n",
            "Loss :  0.06098638361006258\n",
            "Loss :  0.0609795613913639\n",
            "Loss :  0.06101659764430914\n",
            "Loss :  0.061032076514212885\n",
            "Loss :  0.0611059718576803\n",
            "Loss :  0.06118515810090775\n",
            "Loss :  0.06120200492335521\n",
            "Loss :  0.06120548067082989\n",
            "Loss :  0.061201177246246054\n",
            "Loss :  0.06117249150171367\n",
            "Loss :  0.0611188604837031\n",
            "Loss :  0.061080951782459036\n",
            "Loss :  0.06107709517842952\n",
            "Loss :  0.06102251016364528\n",
            "Loss :  0.06094843409350466\n",
            "Loss :  0.06094293919753055\n",
            "Loss :  0.06097771718007762\n",
            "Loss :  0.060948082421645684\n",
            "Loss :  0.060937528880095536\n",
            "Loss :  0.060971226612892074\n",
            "Loss :  0.06089119352172689\n",
            "Validation: \n",
            " Loss :  0.07207517325878143\n",
            " Loss :  0.07998184398526237\n",
            " Loss :  0.07926152065032865\n",
            " Loss :  0.07929178376178272\n",
            " Loss :  0.07951823851944488\n",
            "\n",
            "Epoch: 52\n",
            "Loss :  0.06371849775314331\n",
            "Loss :  0.06026010316881267\n",
            "Loss :  0.05949334906680243\n",
            "Loss :  0.059975300224558\n",
            "Loss :  0.059891147402728474\n",
            "Loss :  0.059628052308278924\n",
            "Loss :  0.06001316994184353\n",
            "Loss :  0.05975823329997734\n",
            "Loss :  0.059767382978289214\n",
            "Loss :  0.05994745962076135\n",
            "Loss :  0.05982268446742898\n",
            "Loss :  0.05984192942311098\n",
            "Loss :  0.0597573694857684\n",
            "Loss :  0.0600197704238746\n",
            "Loss :  0.06003695036819641\n",
            "Loss :  0.06011920037451169\n",
            "Loss :  0.06024678856689737\n",
            "Loss :  0.060252973559307074\n",
            "Loss :  0.060224958361018426\n",
            "Loss :  0.06016447607681389\n",
            "Loss :  0.0601944577560496\n",
            "Loss :  0.06027879744273791\n",
            "Loss :  0.060398245828723475\n",
            "Loss :  0.060485727552856715\n",
            "Loss :  0.060479889287617196\n",
            "Loss :  0.060479881338389274\n",
            "Loss :  0.06052333917254689\n",
            "Loss :  0.06048209826389802\n",
            "Loss :  0.060481136578576\n",
            "Loss :  0.06053159720043546\n",
            "Loss :  0.06062414739714112\n",
            "Loss :  0.06058381305007306\n",
            "Loss :  0.06052719470914279\n",
            "Loss :  0.06053696603168174\n",
            "Loss :  0.0605705796557962\n",
            "Loss :  0.060627034692852584\n",
            "Loss :  0.06062231386219696\n",
            "Loss :  0.06056580724661562\n",
            "Loss :  0.06052071125957909\n",
            "Loss :  0.060472186921578845\n",
            "Loss :  0.060443578459824114\n",
            "Loss :  0.06042184334456776\n",
            "Loss :  0.06039031132206214\n",
            "Loss :  0.06030288131220435\n",
            "Loss :  0.060360144806894854\n",
            "Loss :  0.060362560108733546\n",
            "Loss :  0.06028157870998352\n",
            "Loss :  0.06028392646786007\n",
            "Loss :  0.06030021842545878\n",
            "Loss :  0.06026502534018757\n",
            "Validation: \n",
            " Loss :  0.07955572754144669\n",
            " Loss :  0.08507920979034334\n",
            " Loss :  0.08464853810827906\n",
            " Loss :  0.08439190624678722\n",
            " Loss :  0.08454943494296369\n",
            "\n",
            "Epoch: 53\n",
            "Loss :  0.06244907155632973\n",
            "Loss :  0.05969602479176088\n",
            "Loss :  0.05929256478945414\n",
            "Loss :  0.059812184783720204\n",
            "Loss :  0.059608897421418165\n",
            "Loss :  0.059468139036028995\n",
            "Loss :  0.059218185114078836\n",
            "Loss :  0.0596061707277533\n",
            "Loss :  0.059668914035514546\n",
            "Loss :  0.05973643154560865\n",
            "Loss :  0.05957516673767921\n",
            "Loss :  0.05928291121984387\n",
            "Loss :  0.05906305890068535\n",
            "Loss :  0.0592104499406032\n",
            "Loss :  0.059226146350938376\n",
            "Loss :  0.05937166911679388\n",
            "Loss :  0.05956826315070531\n",
            "Loss :  0.059849843731400565\n",
            "Loss :  0.059822702255532226\n",
            "Loss :  0.05974070281180412\n",
            "Loss :  0.05968566813427417\n",
            "Loss :  0.05971423112809376\n",
            "Loss :  0.05989739138683582\n",
            "Loss :  0.06005608701667228\n",
            "Loss :  0.060124782451208204\n",
            "Loss :  0.05999208562283877\n",
            "Loss :  0.060019185159969145\n",
            "Loss :  0.05992748272155044\n",
            "Loss :  0.059867466246424195\n",
            "Loss :  0.059944064730835946\n",
            "Loss :  0.060006834665604206\n",
            "Loss :  0.059954898521735356\n",
            "Loss :  0.05994828503776191\n",
            "Loss :  0.059954534777161575\n",
            "Loss :  0.05995426487581821\n",
            "Loss :  0.060041645005812334\n",
            "Loss :  0.06000026411420751\n",
            "Loss :  0.05998315004446114\n",
            "Loss :  0.059981438979076276\n",
            "Loss :  0.059968959864066994\n",
            "Loss :  0.05995340758651272\n",
            "Loss :  0.05997170009389701\n",
            "Loss :  0.05997425539865913\n",
            "Loss :  0.059914989591405454\n",
            "Loss :  0.05994621583190905\n",
            "Loss :  0.059941331498704835\n",
            "Loss :  0.05988151020897702\n",
            "Loss :  0.05988747194694106\n",
            "Loss :  0.05990535889804487\n",
            "Loss :  0.059821539036853735\n",
            "Validation: \n",
            " Loss :  0.07001801580190659\n",
            " Loss :  0.07752219339211781\n",
            " Loss :  0.07708821700113576\n",
            " Loss :  0.07660254665085527\n",
            " Loss :  0.07659421796783988\n",
            "\n",
            "Epoch: 54\n",
            "Loss :  0.0693332850933075\n",
            "Loss :  0.060062272982163864\n",
            "Loss :  0.05850391923671677\n",
            "Loss :  0.059028991407925085\n",
            "Loss :  0.059451833094765504\n",
            "Loss :  0.05931144215020479\n",
            "Loss :  0.05916225152914641\n",
            "Loss :  0.05900359017328477\n",
            "Loss :  0.05899224112983103\n",
            "Loss :  0.058864511467598295\n",
            "Loss :  0.05883008640001316\n",
            "Loss :  0.05852118228469883\n",
            "Loss :  0.05831498723507913\n",
            "Loss :  0.058595254107286\n",
            "Loss :  0.058677585148219524\n",
            "Loss :  0.05886933020013847\n",
            "Loss :  0.05894573130037473\n",
            "Loss :  0.05904642331321337\n",
            "Loss :  0.059067474194488476\n",
            "Loss :  0.0590743121949478\n",
            "Loss :  0.05902604780980011\n",
            "Loss :  0.05913345773496899\n",
            "Loss :  0.059308008286613145\n",
            "Loss :  0.05946826871919941\n",
            "Loss :  0.05952644311046205\n",
            "Loss :  0.05948377365016368\n",
            "Loss :  0.059431230499484074\n",
            "Loss :  0.059346069846425986\n",
            "Loss :  0.059333966600831296\n",
            "Loss :  0.059401044334025724\n",
            "Loss :  0.05945726222918675\n",
            "Loss :  0.059362447003076316\n",
            "Loss :  0.05940575435804058\n",
            "Loss :  0.05943639700236277\n",
            "Loss :  0.0594963079586057\n",
            "Loss :  0.05951094678324512\n",
            "Loss :  0.05948549770128364\n",
            "Loss :  0.05949425541687847\n",
            "Loss :  0.059456734519618075\n",
            "Loss :  0.05940807141992442\n",
            "Loss :  0.05939536988549399\n",
            "Loss :  0.05938573418198711\n",
            "Loss :  0.0593797629013339\n",
            "Loss :  0.05932712824604627\n",
            "Loss :  0.05931256875361986\n",
            "Loss :  0.05935096661426011\n",
            "Loss :  0.05933242536871138\n",
            "Loss :  0.05936451828119102\n",
            "Loss :  0.05942126951352722\n",
            "Loss :  0.059381553345566614\n",
            "Validation: \n",
            " Loss :  0.07348376512527466\n",
            " Loss :  0.07958054844112623\n",
            " Loss :  0.07906072850270969\n",
            " Loss :  0.07878142503685638\n",
            " Loss :  0.07885957725438071\n",
            "\n",
            "Epoch: 55\n",
            "Loss :  0.06093607470393181\n",
            "Loss :  0.05925774303349582\n",
            "Loss :  0.05755300873092243\n",
            "Loss :  0.05812262335131245\n",
            "Loss :  0.05834873720276647\n",
            "Loss :  0.05819205742548494\n",
            "Loss :  0.058072104134031986\n",
            "Loss :  0.057737189069600174\n",
            "Loss :  0.05779944879957187\n",
            "Loss :  0.057882290824756516\n",
            "Loss :  0.057906197549978104\n",
            "Loss :  0.057860591121622035\n",
            "Loss :  0.05770879559034158\n",
            "Loss :  0.057905599133658954\n",
            "Loss :  0.05797319824919633\n",
            "Loss :  0.05814787207652401\n",
            "Loss :  0.05825788951160745\n",
            "Loss :  0.05846336616845856\n",
            "Loss :  0.058470721052990435\n",
            "Loss :  0.05840109489585093\n",
            "Loss :  0.05853876291741779\n",
            "Loss :  0.05875641284113247\n",
            "Loss :  0.058906787644009785\n",
            "Loss :  0.05909460834610514\n",
            "Loss :  0.05911944352121274\n",
            "Loss :  0.05908188643861577\n",
            "Loss :  0.05897661625368385\n",
            "Loss :  0.058894393271405755\n",
            "Loss :  0.05889853242506336\n",
            "Loss :  0.058912653040742544\n",
            "Loss :  0.0589147520768286\n",
            "Loss :  0.058945082643120233\n",
            "Loss :  0.05894799742882497\n",
            "Loss :  0.05897626701708647\n",
            "Loss :  0.05900715763443027\n",
            "Loss :  0.05907277122904093\n",
            "Loss :  0.05906418608785336\n",
            "Loss :  0.05905251135683124\n",
            "Loss :  0.05903905701488647\n",
            "Loss :  0.058993252399175064\n",
            "Loss :  0.05901336141320833\n",
            "Loss :  0.058987799270527205\n",
            "Loss :  0.058949247798981974\n",
            "Loss :  0.058888223221019914\n",
            "Loss :  0.05887687525578907\n",
            "Loss :  0.05888599283712402\n",
            "Loss :  0.05882781642184082\n",
            "Loss :  0.05884949652569056\n",
            "Loss :  0.05887752368643477\n",
            "Loss :  0.05883073543488615\n",
            "Validation: \n",
            " Loss :  0.07911770045757294\n",
            " Loss :  0.0855942712653251\n",
            " Loss :  0.0850142603967248\n",
            " Loss :  0.08434321914539962\n",
            " Loss :  0.08458456728193495\n",
            "\n",
            "Epoch: 56\n",
            "Loss :  0.060372285544872284\n",
            "Loss :  0.05580813369967721\n",
            "Loss :  0.056749657683429267\n",
            "Loss :  0.057520470071223476\n",
            "Loss :  0.05759804050733403\n",
            "Loss :  0.05725873641523661\n",
            "Loss :  0.05704114146408488\n",
            "Loss :  0.05710445407410743\n",
            "Loss :  0.05751842582299386\n",
            "Loss :  0.05756401217409542\n",
            "Loss :  0.057698341768861995\n",
            "Loss :  0.0575559352365163\n",
            "Loss :  0.05735065615620495\n",
            "Loss :  0.05756715261412941\n",
            "Loss :  0.0578634131514857\n",
            "Loss :  0.058072774936229185\n",
            "Loss :  0.058208933898380825\n",
            "Loss :  0.05821708825073744\n",
            "Loss :  0.0581910792758781\n",
            "Loss :  0.05810357462517254\n",
            "Loss :  0.05813124243966976\n",
            "Loss :  0.058226397781857946\n",
            "Loss :  0.05830241872916394\n",
            "Loss :  0.05845831513598368\n",
            "Loss :  0.05855438023558296\n",
            "Loss :  0.058516847377873035\n",
            "Loss :  0.0585761658447684\n",
            "Loss :  0.058529753100498134\n",
            "Loss :  0.05851190351612627\n",
            "Loss :  0.058482822709272\n",
            "Loss :  0.058536604083950734\n",
            "Loss :  0.05854935084029409\n",
            "Loss :  0.05853751355149664\n",
            "Loss :  0.05862301355087145\n",
            "Loss :  0.05866855146947844\n",
            "Loss :  0.058735564849909896\n",
            "Loss :  0.05867905636160658\n",
            "Loss :  0.058638047336568085\n",
            "Loss :  0.05858530811085476\n",
            "Loss :  0.05853315535218209\n",
            "Loss :  0.05848921298460473\n",
            "Loss :  0.05849167578593078\n",
            "Loss :  0.05844609815441514\n",
            "Loss :  0.05842898690126611\n",
            "Loss :  0.05840456208674545\n",
            "Loss :  0.05842838849368487\n",
            "Loss :  0.05840281432660179\n",
            "Loss :  0.058394250200938774\n",
            "Loss :  0.058404373752414064\n",
            "Loss :  0.05836670551365602\n",
            "Validation: \n",
            " Loss :  0.07652238756418228\n",
            " Loss :  0.08096530714205333\n",
            " Loss :  0.0803944790145246\n",
            " Loss :  0.08000298357400738\n",
            " Loss :  0.08022667550378376\n",
            "\n",
            "Epoch: 57\n",
            "Loss :  0.06429212540388107\n",
            "Loss :  0.05669154870239171\n",
            "Loss :  0.05684882455638477\n",
            "Loss :  0.05744878563188737\n",
            "Loss :  0.05699993897138572\n",
            "Loss :  0.056965828234074166\n",
            "Loss :  0.057104700657187916\n",
            "Loss :  0.05705239476872162\n",
            "Loss :  0.05719316736967475\n",
            "Loss :  0.0571116050193598\n",
            "Loss :  0.05715149338587676\n",
            "Loss :  0.05692572881643836\n",
            "Loss :  0.05683500615279537\n",
            "Loss :  0.05703088857290399\n",
            "Loss :  0.0573028842291088\n",
            "Loss :  0.05741032496686803\n",
            "Loss :  0.057543361547941006\n",
            "Loss :  0.05765739954703036\n",
            "Loss :  0.05772331182057686\n",
            "Loss :  0.05760014525258728\n",
            "Loss :  0.05766386462755464\n",
            "Loss :  0.057755660089130086\n",
            "Loss :  0.057762205196182116\n",
            "Loss :  0.05802200304029824\n",
            "Loss :  0.05804725608081244\n",
            "Loss :  0.05804665512832038\n",
            "Loss :  0.058062104729514466\n",
            "Loss :  0.057974263159661275\n",
            "Loss :  0.05802576791402284\n",
            "Loss :  0.0581480436718341\n",
            "Loss :  0.05824565651062319\n",
            "Loss :  0.05823678929370699\n",
            "Loss :  0.05822895389526061\n",
            "Loss :  0.05821097429470353\n",
            "Loss :  0.05829238137803819\n",
            "Loss :  0.05835701205749118\n",
            "Loss :  0.05833346509165711\n",
            "Loss :  0.05836378669521879\n",
            "Loss :  0.058412801161447536\n",
            "Loss :  0.05840145382086944\n",
            "Loss :  0.05841072612532654\n",
            "Loss :  0.058400717294274165\n",
            "Loss :  0.05836179236618187\n",
            "Loss :  0.05830268182396336\n",
            "Loss :  0.05832384250500575\n",
            "Loss :  0.05835538336846094\n",
            "Loss :  0.05831077129230582\n",
            "Loss :  0.0583195625596745\n",
            "Loss :  0.058346434986207195\n",
            "Loss :  0.05828730284773162\n",
            "Validation: \n",
            " Loss :  0.07748492062091827\n",
            " Loss :  0.07922624548276265\n",
            " Loss :  0.07839377715093333\n",
            " Loss :  0.07811969176667635\n",
            " Loss :  0.07804014653335382\n",
            "\n",
            "Epoch: 58\n",
            "Loss :  0.05859695374965668\n",
            "Loss :  0.056692360138351265\n",
            "Loss :  0.05571597300115086\n",
            "Loss :  0.0561500929536358\n",
            "Loss :  0.05597222560062641\n",
            "Loss :  0.05628523223248182\n",
            "Loss :  0.056248203408522685\n",
            "Loss :  0.056232516666952996\n",
            "Loss :  0.05630414799591641\n",
            "Loss :  0.056388397342883624\n",
            "Loss :  0.056411239549075023\n",
            "Loss :  0.0562384332286882\n",
            "Loss :  0.05618924111866754\n",
            "Loss :  0.05643149115543329\n",
            "Loss :  0.056661830673403774\n",
            "Loss :  0.05673510657734429\n",
            "Loss :  0.05691780874170132\n",
            "Loss :  0.05718679859014283\n",
            "Loss :  0.05721593421796409\n",
            "Loss :  0.057220528477617584\n",
            "Loss :  0.057233478503292474\n",
            "Loss :  0.057354649996729255\n",
            "Loss :  0.057431002817542307\n",
            "Loss :  0.05768667879107194\n",
            "Loss :  0.057815137616826294\n",
            "Loss :  0.057789458429433435\n",
            "Loss :  0.05776784446276011\n",
            "Loss :  0.057739063767489475\n",
            "Loss :  0.05773898552161943\n",
            "Loss :  0.0578224091467374\n",
            "Loss :  0.057853586204424254\n",
            "Loss :  0.057816146259043376\n",
            "Loss :  0.05782697853455291\n",
            "Loss :  0.05786024283876592\n",
            "Loss :  0.05795644276128137\n",
            "Loss :  0.058037935466932776\n",
            "Loss :  0.05803367932045889\n",
            "Loss :  0.05806979526968979\n",
            "Loss :  0.05805662436669893\n",
            "Loss :  0.0580178129360499\n",
            "Loss :  0.05801143777786645\n",
            "Loss :  0.05799372072746284\n",
            "Loss :  0.0579777888759015\n",
            "Loss :  0.057919442463501974\n",
            "Loss :  0.057915319762532676\n",
            "Loss :  0.057900404933418775\n",
            "Loss :  0.05784574930036921\n",
            "Loss :  0.057828781829707944\n",
            "Loss :  0.05785530316110956\n",
            "Loss :  0.057824760371944325\n",
            "Validation: \n",
            " Loss :  0.08194702863693237\n",
            " Loss :  0.08728968174684615\n",
            " Loss :  0.08681955152168506\n",
            " Loss :  0.0867576683398153\n",
            " Loss :  0.08677850930411139\n",
            "\n",
            "Epoch: 59\n",
            "Loss :  0.0613485611975193\n",
            "Loss :  0.05710194869474931\n",
            "Loss :  0.05648813645044962\n",
            "Loss :  0.05714664716393717\n",
            "Loss :  0.05727773641304272\n",
            "Loss :  0.05716419862765892\n",
            "Loss :  0.05712439699983988\n",
            "Loss :  0.0568739277586131\n",
            "Loss :  0.0572016512354215\n",
            "Loss :  0.057250319847038815\n",
            "Loss :  0.05711741513102361\n",
            "Loss :  0.05706060130719666\n",
            "Loss :  0.05699190996156251\n",
            "Loss :  0.05736275096886031\n",
            "Loss :  0.05740334378912094\n",
            "Loss :  0.05758361607197894\n",
            "Loss :  0.05761224459509672\n",
            "Loss :  0.05771240642109112\n",
            "Loss :  0.05768290069468772\n",
            "Loss :  0.05767649895857766\n",
            "Loss :  0.05772799436948193\n",
            "Loss :  0.05782198152059062\n",
            "Loss :  0.0578484168250906\n",
            "Loss :  0.05795069666200386\n",
            "Loss :  0.0579743334613895\n",
            "Loss :  0.057879722495359255\n",
            "Loss :  0.05793243940648448\n",
            "Loss :  0.057896323873218136\n",
            "Loss :  0.05790898415965966\n",
            "Loss :  0.057961034022041204\n",
            "Loss :  0.05796442721719758\n",
            "Loss :  0.05797309216435316\n",
            "Loss :  0.05798190074314209\n",
            "Loss :  0.05798982666788505\n",
            "Loss :  0.057982108449219256\n",
            "Loss :  0.058022619641659265\n",
            "Loss :  0.05796954919096506\n",
            "Loss :  0.05795616210711934\n",
            "Loss :  0.05785244105877526\n",
            "Loss :  0.05781633255388731\n",
            "Loss :  0.05777904436809761\n",
            "Loss :  0.0577721963398648\n",
            "Loss :  0.057752042911468945\n",
            "Loss :  0.05767119318903737\n",
            "Loss :  0.05767076947386303\n",
            "Loss :  0.05763477409989501\n",
            "Loss :  0.05756663956119802\n",
            "Loss :  0.05755344160581344\n",
            "Loss :  0.057556592152606924\n",
            "Loss :  0.05751264413966666\n",
            "Validation: \n",
            " Loss :  0.0772971361875534\n",
            " Loss :  0.08232522436550685\n",
            " Loss :  0.08159851410040041\n",
            " Loss :  0.08172662995877814\n",
            " Loss :  0.08200494475938656\n",
            "\n",
            "Epoch: 60\n",
            "Loss :  0.06250016391277313\n",
            "Loss :  0.055087213489142334\n",
            "Loss :  0.055307810327836444\n",
            "Loss :  0.05591589524861305\n",
            "Loss :  0.05606709157184857\n",
            "Loss :  0.05620090385862425\n",
            "Loss :  0.056378592721751476\n",
            "Loss :  0.05610567955693729\n",
            "Loss :  0.05619988070777905\n",
            "Loss :  0.05651773299489703\n",
            "Loss :  0.05643323296219996\n",
            "Loss :  0.05635807082593978\n",
            "Loss :  0.05627159016930367\n",
            "Loss :  0.05662991004135772\n",
            "Loss :  0.05679886753783158\n",
            "Loss :  0.05699940442742891\n",
            "Loss :  0.05705802815948954\n",
            "Loss :  0.057234405708766124\n",
            "Loss :  0.057295625298721356\n",
            "Loss :  0.05720687480107028\n",
            "Loss :  0.05720861495208385\n",
            "Loss :  0.05737143214703736\n",
            "Loss :  0.05742351704053749\n",
            "Loss :  0.057602745871910284\n",
            "Loss :  0.05770213257413188\n",
            "Loss :  0.05765917012653503\n",
            "Loss :  0.05761483214595765\n",
            "Loss :  0.057525272408974565\n",
            "Loss :  0.05748761504941564\n",
            "Loss :  0.05750945899187494\n",
            "Loss :  0.057498953308278936\n",
            "Loss :  0.05742784855450081\n",
            "Loss :  0.05739495391461337\n",
            "Loss :  0.057379940901728196\n",
            "Loss :  0.057463913773598906\n",
            "Loss :  0.05753063576089011\n",
            "Loss :  0.057504348002807584\n",
            "Loss :  0.05751131843484315\n",
            "Loss :  0.05746609196457963\n",
            "Loss :  0.057436468246419105\n",
            "Loss :  0.05739999582010909\n",
            "Loss :  0.057374725956696376\n",
            "Loss :  0.057363428078113995\n",
            "Loss :  0.057296596519355154\n",
            "Loss :  0.0572864567256974\n",
            "Loss :  0.05729469837013739\n",
            "Loss :  0.05719881144366399\n",
            "Loss :  0.05716784451860784\n",
            "Loss :  0.05715609129649934\n",
            "Loss :  0.05710230719927858\n",
            "Validation: \n",
            " Loss :  0.07652295380830765\n",
            " Loss :  0.08058544070947737\n",
            " Loss :  0.08002650556040973\n",
            " Loss :  0.07992396491472839\n",
            " Loss :  0.07989441787387118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Finetuning student on crossentropy\n",
        "Now the student is trained. In this cell we need to replace the classifier(i.e: Fully Connected layer) of student network from one with output shape of dense feature to one with shape of classes. e.g: nn.Linear(256,512) to nn.Linear(256,10). After this we need to freez Conv layers in the network and finetune the network using orignal dataset. "
      ],
      "metadata": {
        "id": "N5rEK4VmiN_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s1.classifier = nn.Linear(512, 10)\n",
        "for m in s1.modules():\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        m.weight.requires_grad = False\n",
        "        if m.bias is not None:\n",
        "            m.bias.requires_grad = False\n",
        "s1 = s1.to(device)\n",
        "summary(s1, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv_cGp5aiSbS",
        "outputId": "6104876f-028e-4505-9a9b-d0ab3c8954b8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "           Conv2d-29            [-1, 512, 2, 2]       1,180,160\n",
            "      BatchNorm2d-30            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-31            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-32            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-33            [-1, 512, 1, 1]               0\n",
            "           Linear-34                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 2,360,490\n",
            "Trainable params: 8,074\n",
            "Non-trainable params: 2,352,416\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.98\n",
            "Params size (MB): 9.00\n",
            "Estimated Total Size (MB): 12.00\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(s1.parameters(), lr=0.0001)\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    s1.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        s1.zero_grad()\n",
        "        outputs = s1(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if(batch_idx % 200 == 0):\n",
        "          print(\"Accuracy : \",100.*correct/total,\" Loss : \", train_loss/(batch_idx+1))\n",
        "def test(epoch):\n",
        "    s1.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = s1(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\"Accuracy : \",100.*correct/total,\" Loss : \", test_loss/(batch_idx+1))"
      ],
      "metadata": {
        "id": "t7gwqasZiseD"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "    train(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGgfdwA-iue-",
        "outputId": "042f3ea0-48aa-4d54-9cb0-acbe9756a4b3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Accuracy :  13.0  Loss :  2.3406362533569336\n",
            "Accuracy :  69.44278606965175  Loss :  1.4717139343717205\n",
            "Accuracy :  77.3640897755611  Loss :  1.1176168737268806\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.5915505886077881\n",
            "Accuracy :  84.14285714285714  Loss :  0.5783692726067134\n",
            "Accuracy :  83.82926829268293  Loss :  0.5829475471159307\n",
            "Accuracy :  84.09836065573771  Loss :  0.5820076895541832\n",
            "Accuracy :  84.11111111111111  Loss :  0.5806706735381374\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy :  82.0  Loss :  0.5544735193252563\n",
            "Accuracy :  85.71641791044776  Loss :  0.5207162867138042\n",
            "Accuracy :  86.07481296758105  Loss :  0.49043705888519856\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.4678022861480713\n",
            "Accuracy :  85.0  Loss :  0.4648697092419579\n",
            "Accuracy :  84.60975609756098  Loss :  0.4711946045480123\n",
            "Accuracy :  84.75409836065573  Loss :  0.46949492419352296\n",
            "Accuracy :  84.85185185185185  Loss :  0.4675458275977476\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy :  87.0  Loss :  0.40074536204338074\n",
            "Accuracy :  86.33830845771145  Loss :  0.4234069590248279\n",
            "Accuracy :  86.60598503740648  Loss :  0.41434788826546465\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.4359573721885681\n",
            "Accuracy :  85.04761904761905  Loss :  0.4403695875690097\n",
            "Accuracy :  84.85365853658537  Loss :  0.4470213804303146\n",
            "Accuracy :  85.06557377049181  Loss :  0.4447761070532877\n",
            "Accuracy :  85.1604938271605  Loss :  0.4426679692150634\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy :  89.0  Loss :  0.36340785026550293\n",
            "Accuracy :  86.72636815920399  Loss :  0.39733397708603396\n",
            "Accuracy :  86.88778054862843  Loss :  0.39068682811355354\n",
            "Validation: \n",
            "Accuracy :  88.0  Loss :  0.4188281297683716\n",
            "Accuracy :  85.23809523809524  Loss :  0.43245758754866465\n",
            "Accuracy :  85.09756097560975  Loss :  0.4392263460450056\n",
            "Accuracy :  85.21311475409836  Loss :  0.43702534378552044\n",
            "Accuracy :  85.32098765432099  Loss :  0.43483704917224836\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy :  88.0  Loss :  0.3661419153213501\n",
            "Accuracy :  86.8905472636816  Loss :  0.38450111826853967\n",
            "Accuracy :  87.01995012468828  Loss :  0.38024007675802324\n",
            "Validation: \n",
            "Accuracy :  88.0  Loss :  0.40844354033470154\n",
            "Accuracy :  85.42857142857143  Loss :  0.4295568572623389\n",
            "Accuracy :  85.1951219512195  Loss :  0.43673316044051474\n",
            "Accuracy :  85.22950819672131  Loss :  0.43440292824487214\n",
            "Accuracy :  85.33333333333333  Loss :  0.4321039092761499\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy :  92.0  Loss :  0.33555862307548523\n",
            "Accuracy :  87.12437810945273  Loss :  0.380191825960406\n",
            "Accuracy :  87.24688279301746  Loss :  0.375374685573459\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4065747857093811\n",
            "Accuracy :  85.23809523809524  Loss :  0.4284237083934602\n",
            "Accuracy :  85.1219512195122  Loss :  0.4354355538763651\n",
            "Accuracy :  85.26229508196721  Loss :  0.4327308882455357\n",
            "Accuracy :  85.4074074074074  Loss :  0.4303741723666956\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy :  91.0  Loss :  0.36680299043655396\n",
            "Accuracy :  86.96517412935323  Loss :  0.37606080691909316\n",
            "Accuracy :  87.23690773067332  Loss :  0.372890130354282\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4015743136405945\n",
            "Accuracy :  85.28571428571429  Loss :  0.427399259238016\n",
            "Accuracy :  85.17073170731707  Loss :  0.43431568727260683\n",
            "Accuracy :  85.31147540983606  Loss :  0.43208492583915836\n",
            "Accuracy :  85.46913580246914  Loss :  0.4296861316686795\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy :  89.0  Loss :  0.3054676055908203\n",
            "Accuracy :  87.13930348258707  Loss :  0.3697983798251223\n",
            "Accuracy :  87.28678304239402  Loss :  0.368000478741534\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.40024876594543457\n",
            "Accuracy :  85.38095238095238  Loss :  0.42641933049474445\n",
            "Accuracy :  85.21951219512195  Loss :  0.43326170197347313\n",
            "Accuracy :  85.39344262295081  Loss :  0.43038281651793936\n",
            "Accuracy :  85.50617283950618  Loss :  0.4277764750115665\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy :  90.0  Loss :  0.2875247299671173\n",
            "Accuracy :  87.28358208955224  Loss :  0.368846304603477\n",
            "Accuracy :  87.3790523690773  Loss :  0.3667525038531891\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.3947552442550659\n",
            "Accuracy :  85.28571428571429  Loss :  0.42599410599186305\n",
            "Accuracy :  85.29268292682927  Loss :  0.43284123009297903\n",
            "Accuracy :  85.42622950819673  Loss :  0.4302906450189528\n",
            "Accuracy :  85.60493827160494  Loss :  0.4276731468645143\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy :  88.0  Loss :  0.30968278646469116\n",
            "Accuracy :  87.45273631840796  Loss :  0.36800066370572615\n",
            "Accuracy :  87.53615960099751  Loss :  0.36524396882092863\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.3901783227920532\n",
            "Accuracy :  85.42857142857143  Loss :  0.42506549117111025\n",
            "Accuracy :  85.29268292682927  Loss :  0.4321807902760622\n",
            "Accuracy :  85.49180327868852  Loss :  0.42954785730995115\n",
            "Accuracy :  85.60493827160494  Loss :  0.4268613422726407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_name = '1student.pt'\n",
        "path = F\"/content/gdrive/MyDrive/{model_save_name}\" \n",
        "torch.save(s1.state_dict(), path)"
      ],
      "metadata": {
        "id": "285Zfq11iwx2"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_save_name = '1student.pt'\n",
        "# path = F\"/content/gdrive/MyDrive/{model_save_name}\" \n",
        "# s1.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "eeygMyzeeKF2"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extract denseFeatures from s1"
      ],
      "metadata": {
        "id": "SsHS6Sb4k4t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "S1_AS_TA_WOH = nn.Sequential(*list(s1.children())[:-1],nn.Flatten())\n",
        "summary(s1, (3, 32, 32))\n",
        "summary(S1_AS_TA_WOH, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rSy2A3uk8cF",
        "outputId": "2d73cd61-8ce6-4122-c283-4ef3dcbdc803"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "           Conv2d-29            [-1, 512, 2, 2]       1,180,160\n",
            "      BatchNorm2d-30            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-31            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-32            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-33            [-1, 512, 1, 1]               0\n",
            "           Linear-34                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 2,360,490\n",
            "Trainable params: 8,074\n",
            "Non-trainable params: 2,352,416\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.98\n",
            "Params size (MB): 9.00\n",
            "Estimated Total Size (MB): 12.00\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "           Conv2d-29            [-1, 512, 2, 2]       1,180,160\n",
            "      BatchNorm2d-30            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-31            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-32            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-33            [-1, 512, 1, 1]               0\n",
            "          Flatten-34                  [-1, 512]               0\n",
            "================================================================\n",
            "Total params: 2,355,360\n",
            "Trainable params: 2,944\n",
            "Non-trainable params: 2,352,416\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.99\n",
            "Params size (MB): 8.98\n",
            "Estimated Total Size (MB): 11.98\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "S1_AS_TA_WOH.eval()\n",
        "S1DenseTrain = None\n",
        "s1DenseTest = None\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = S1_AS_TA_WOH(inputs)\n",
        "        if(S1DenseTrain == None):\n",
        "            S1DenseTrain = outputs\n",
        "        else:\n",
        "            S1DenseTrain = torch.cat((S1DenseTrain,outputs))\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = S1_AS_TA_WOH(inputs)\n",
        "        if(s1DenseTest == None):\n",
        "            s1DenseTest = outputs\n",
        "        else:\n",
        "            s1DenseTest = torch.cat((s1DenseTest,outputs))"
      ],
      "metadata": {
        "id": "4w6BzS6LlRvU"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating 2 more students "
      ],
      "metadata": {
        "id": "guODYUgtigU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "    'VGGS': [32, 32, 'M', 64, 64, 'M', 128, 128, 'M', 256, 256, 'M','M'],\n",
        "    \n",
        "\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(256, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "s01 = VGG('VGGS')\n",
        "s01 = s01.to(device)\n",
        "summary(s01, (3, 32, 32))\n",
        "s2 = VGG('VGGS')\n",
        "s2 = s2.to(device)\n",
        "summary(s2, (3, 32, 32))"
      ],
      "metadata": {
        "id": "5TbTN57Ke7ZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4095ddd-58e2-4965-d39d-d17330999fdf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 256, 1, 1]               0\n",
            "           Linear-31                  [-1, 256]          65,792\n",
            "================================================================\n",
            "Total params: 1,239,968\n",
            "Trainable params: 1,239,968\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.94\n",
            "Params size (MB): 4.73\n",
            "Estimated Total Size (MB): 7.68\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 256, 1, 1]               0\n",
            "           Linear-31                  [-1, 256]          65,792\n",
            "================================================================\n",
            "Total params: 1,239,968\n",
            "Trainable params: 1,239,968\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.94\n",
            "Params size (MB): 4.73\n",
            "Estimated Total Size (MB): 7.68\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Multi Students\n",
        "1.6 In this step you will train two students instead of one. In the training loop you will pass the input from both students and then backwark both the losses. "
      ],
      "metadata": {
        "id": "ZYIy2HShm_Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer1 = optim.Adam(s01.parameters(), lr=0.0001)\n",
        "optimizer2 = optim.Adam(s2.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    s01.train()\n",
        "    train_loss1 = 0\n",
        "    train_loss2 = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        #code here\n",
        "        targets = S1DenseTrain[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "        targets.to(device)\n",
        "        s01.zero_grad()\n",
        "        s2.zero_grad()\n",
        "        output1 = s01(inputs)\n",
        "        output2 = s2(inputs)\n",
        "        loss1 = criterion(output1, targets[:,:256])\n",
        "        loss2 = criterion(output2, targets[:,256:])\n",
        "        loss1.backward()\n",
        "        loss2.backward()\n",
        "        optimizer1.step()\n",
        "        optimizer2.step()\n",
        "\n",
        "        train_loss1 += loss1.item()\n",
        "        train_loss2 += loss2.item()\n",
        "        if(batch_idx % 10 == 0):\n",
        "          print(\"Loss S01: \", train_loss1/(batch_idx+1))\n",
        "          print(\"Loss S2: \", train_loss2/(batch_idx+1))\n",
        "def test(epoch):\n",
        "    s01.eval()\n",
        "    test_loss1 = 0\n",
        "    test_loss2 = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            #code here\n",
        "            targets = s1DenseTest[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "            targets.to(device)\n",
        "            output1 = s01(inputs)\n",
        "            output2 = s2(inputs)\n",
        "            loss1 = criterion(output1, targets[:,:256])\n",
        "            loss2 = criterion(output2, targets[:,256:])\n",
        "            test_loss1 += loss1.item()\n",
        "            test_loss2 += loss2.item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\" Loss S01: \", test_loss1/(batch_idx+1))\n",
        "              print(\" Loss S2: \", test_loss2/(batch_idx+1))"
      ],
      "metadata": {
        "id": "kAcYq4NrnBBv"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+60):\n",
        "    train(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test(epoch)"
      ],
      "metadata": {
        "id": "RQJvw4e4nDwb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd49bde-4c90-4388-ac62-2ea02cd85687"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Loss S2:  0.04036626392871214\n",
            "Loss S01:  0.03632142970387132\n",
            "Loss S2:  0.040360466796117274\n",
            "Loss S01:  0.03629940873111345\n",
            "Loss S2:  0.0403504743999229\n",
            "Loss S01:  0.03630372395743183\n",
            "Loss S2:  0.04032999410583319\n",
            "Loss S01:  0.03630445951682633\n",
            "Loss S2:  0.04032467578242465\n",
            "Loss S01:  0.036333039148413696\n",
            "Loss S2:  0.04036824176504141\n",
            "Loss S01:  0.03631540529050265\n",
            "Loss S2:  0.040362188375046806\n",
            "Loss S01:  0.03633014939477563\n",
            "Loss S2:  0.04037490501813978\n",
            "Loss S01:  0.03628808829379543\n",
            "Loss S2:  0.040331798406458196\n",
            "Validation: \n",
            " Loss S01:  0.03285142406821251\n",
            " Loss S2:  0.04838593304157257\n",
            " Loss S01:  0.03590654262474605\n",
            " Loss S2:  0.05235871459756579\n",
            " Loss S01:  0.035632651299238205\n",
            " Loss S2:  0.05273262892917889\n",
            " Loss S01:  0.03584071083879862\n",
            " Loss S2:  0.052420808399309876\n",
            " Loss S01:  0.03592359982890847\n",
            " Loss S2:  0.05228182921806971\n",
            "\n",
            "Epoch: 17\n",
            "Loss S01:  0.03684191033244133\n",
            "Loss S2:  0.04232829064130783\n",
            "Loss S01:  0.0353022925555706\n",
            "Loss S2:  0.03960537842728875\n",
            "Loss S01:  0.035376682079264095\n",
            "Loss S2:  0.03995685669637862\n",
            "Loss S01:  0.035478809367745154\n",
            "Loss S2:  0.03967174491094005\n",
            "Loss S01:  0.035752486937293194\n",
            "Loss S2:  0.03983752011525922\n",
            "Loss S01:  0.03600980155169964\n",
            "Loss S2:  0.04018260100308586\n",
            "Loss S01:  0.03578819282597206\n",
            "Loss S2:  0.03999421195905717\n",
            "Loss S01:  0.03603009562152372\n",
            "Loss S2:  0.040152288028891654\n",
            "Loss S01:  0.03576680284315421\n",
            "Loss S2:  0.03991459521614475\n",
            "Loss S01:  0.0356127132999373\n",
            "Loss S2:  0.03974795275992089\n",
            "Loss S01:  0.035490892716858645\n",
            "Loss S2:  0.03957644440602548\n",
            "Loss S01:  0.03552514999299436\n",
            "Loss S2:  0.03953968132804106\n",
            "Loss S01:  0.03556934226889256\n",
            "Loss S2:  0.039632436072777125\n",
            "Loss S01:  0.03562920045761662\n",
            "Loss S2:  0.03970893859180785\n",
            "Loss S01:  0.035632093654985125\n",
            "Loss S2:  0.03964195059652024\n",
            "Loss S01:  0.035647426330984035\n",
            "Loss S2:  0.039713814805280294\n",
            "Loss S01:  0.035592564283320624\n",
            "Loss S2:  0.03971401146202354\n",
            "Loss S01:  0.035635345664463546\n",
            "Loss S2:  0.039769098707284144\n",
            "Loss S01:  0.03562700495288517\n",
            "Loss S2:  0.03982933394569718\n",
            "Loss S01:  0.035576632217118875\n",
            "Loss S2:  0.03973936192028185\n",
            "Loss S01:  0.03549327881107876\n",
            "Loss S2:  0.03963975410037373\n",
            "Loss S01:  0.03549508545636001\n",
            "Loss S2:  0.0396619128036838\n",
            "Loss S01:  0.03550451260683763\n",
            "Loss S2:  0.03967056131807927\n",
            "Loss S01:  0.035526819720670774\n",
            "Loss S2:  0.03964535343698609\n",
            "Loss S01:  0.03555048980529872\n",
            "Loss S2:  0.039670581562266806\n",
            "Loss S01:  0.03556786575521606\n",
            "Loss S2:  0.03965703718572024\n",
            "Loss S01:  0.03560772662361463\n",
            "Loss S2:  0.03966666088650053\n",
            "Loss S01:  0.03559172803680395\n",
            "Loss S2:  0.03960872295817766\n",
            "Loss S01:  0.03560947363009657\n",
            "Loss S2:  0.03958575263142162\n",
            "Loss S01:  0.035602799664760376\n",
            "Loss S2:  0.03961565419775514\n",
            "Loss S01:  0.035628298130443325\n",
            "Loss S2:  0.03963618101768716\n",
            "Loss S01:  0.03559706448742048\n",
            "Loss S2:  0.03962863148025378\n",
            "Loss S01:  0.03557246515521565\n",
            "Loss S2:  0.03960383706653601\n",
            "Loss S01:  0.03555807726193231\n",
            "Loss S2:  0.03958449824474009\n",
            "Loss S01:  0.035557078842150154\n",
            "Loss S2:  0.039570087648906314\n",
            "Loss S01:  0.035557031031814734\n",
            "Loss S2:  0.039592390630574646\n",
            "Loss S01:  0.03556332724308208\n",
            "Loss S2:  0.03960231667864356\n",
            "Loss S01:  0.035511311414749475\n",
            "Loss S2:  0.03954312270582525\n",
            "Loss S01:  0.035469223087619295\n",
            "Loss S2:  0.039529166851691375\n",
            "Loss S01:  0.03543677262942809\n",
            "Loss S2:  0.03948341740671631\n",
            "Loss S01:  0.035438314470418376\n",
            "Loss S2:  0.03947016713848138\n",
            "Loss S01:  0.03546123310397866\n",
            "Loss S2:  0.03948142772898477\n",
            "Loss S01:  0.03547032718544477\n",
            "Loss S2:  0.03948196946618959\n",
            "Loss S01:  0.03545129650516604\n",
            "Loss S2:  0.03945333816612403\n",
            "Loss S01:  0.035441515460407656\n",
            "Loss S2:  0.03944963145823706\n",
            "Loss S01:  0.035433141679860274\n",
            "Loss S2:  0.039448725931065576\n",
            "Loss S01:  0.03542926388342184\n",
            "Loss S2:  0.03946968768430377\n",
            "Loss S01:  0.035416251523004975\n",
            "Loss S2:  0.03946349385437692\n",
            "Loss S01:  0.035423913312274305\n",
            "Loss S2:  0.03947315049084705\n",
            "Loss S01:  0.035379227315305205\n",
            "Loss S2:  0.03941014072954533\n",
            "Validation: \n",
            " Loss S01:  0.030476590618491173\n",
            " Loss S2:  0.049031488597393036\n",
            " Loss S01:  0.033707690824355395\n",
            " Loss S2:  0.05317801538677443\n",
            " Loss S01:  0.03359229013142062\n",
            " Loss S2:  0.05363792835212335\n",
            " Loss S01:  0.03367043637716379\n",
            " Loss S2:  0.05328862124779185\n",
            " Loss S01:  0.033773478097569795\n",
            " Loss S2:  0.053108000185018704\n",
            "\n",
            "Epoch: 18\n",
            "Loss S01:  0.03682756796479225\n",
            "Loss S2:  0.04154660925269127\n",
            "Loss S01:  0.03428516181355173\n",
            "Loss S2:  0.03839054161852056\n",
            "Loss S01:  0.03419768056344418\n",
            "Loss S2:  0.038623511081650144\n",
            "Loss S01:  0.03437135951413262\n",
            "Loss S2:  0.03849054608614214\n",
            "Loss S01:  0.034556291588559385\n",
            "Loss S2:  0.038719026326406294\n",
            "Loss S01:  0.034781228130062423\n",
            "Loss S2:  0.038988649114674215\n",
            "Loss S01:  0.034786382873283055\n",
            "Loss S2:  0.038822948077663046\n",
            "Loss S01:  0.0349552443801937\n",
            "Loss S2:  0.03914905161085263\n",
            "Loss S01:  0.034782139124509726\n",
            "Loss S2:  0.0390267004661354\n",
            "Loss S01:  0.03466269527408448\n",
            "Loss S2:  0.03891479543277195\n",
            "Loss S01:  0.03459582450144952\n",
            "Loss S2:  0.03883369285428878\n",
            "Loss S01:  0.034548956212830974\n",
            "Loss S2:  0.038821997976786385\n",
            "Loss S01:  0.03459498341664795\n",
            "Loss S2:  0.03887209702621807\n",
            "Loss S01:  0.034621281563553194\n",
            "Loss S2:  0.03885708089548213\n",
            "Loss S01:  0.034643965542105075\n",
            "Loss S2:  0.03878070036253185\n",
            "Loss S01:  0.03465544945554228\n",
            "Loss S2:  0.03884697875735776\n",
            "Loss S01:  0.034605891935481047\n",
            "Loss S2:  0.03882011755005173\n",
            "Loss S01:  0.034665156950988964\n",
            "Loss S2:  0.03892344234195369\n",
            "Loss S01:  0.03475963562027196\n",
            "Loss S2:  0.039027489413049336\n",
            "Loss S01:  0.03470569561328251\n",
            "Loss S2:  0.03895807872814034\n",
            "Loss S01:  0.034636606874676484\n",
            "Loss S2:  0.0388827714605711\n",
            "Loss S01:  0.03462460599133471\n",
            "Loss S2:  0.03886328069095928\n",
            "Loss S01:  0.034643192577483425\n",
            "Loss S2:  0.03887118540737963\n",
            "Loss S01:  0.0346492048417464\n",
            "Loss S2:  0.03883764673492093\n",
            "Loss S01:  0.03465767176164879\n",
            "Loss S2:  0.03879931366604394\n",
            "Loss S01:  0.034671167119506345\n",
            "Loss S2:  0.03877903808991748\n",
            "Loss S01:  0.03468989598711103\n",
            "Loss S2:  0.03879121376979397\n",
            "Loss S01:  0.034678205964585075\n",
            "Loss S2:  0.0387346199627732\n",
            "Loss S01:  0.03465939103045709\n",
            "Loss S2:  0.03871085395327242\n",
            "Loss S01:  0.03465377875440514\n",
            "Loss S2:  0.038737085139014055\n",
            "Loss S01:  0.03466618040644647\n",
            "Loss S2:  0.038720018221096345\n",
            "Loss S01:  0.03463505504359386\n",
            "Loss S2:  0.038720895275713166\n",
            "Loss S01:  0.03460241999256648\n",
            "Loss S2:  0.038685407164590756\n",
            "Loss S01:  0.03458123880345656\n",
            "Loss S2:  0.03866666344942643\n",
            "Loss S01:  0.03457408637092435\n",
            "Loss S2:  0.0386620127757099\n",
            "Loss S01:  0.03458325115385388\n",
            "Loss S2:  0.03866081369610933\n",
            "Loss S01:  0.0345869276651963\n",
            "Loss S2:  0.03866707140430189\n",
            "Loss S01:  0.034557553078487235\n",
            "Loss S2:  0.03861268946142852\n",
            "Loss S01:  0.03453712292465325\n",
            "Loss S2:  0.038582861208383806\n",
            "Loss S01:  0.03452358868382776\n",
            "Loss S2:  0.03852322021180102\n",
            "Loss S01:  0.03453969612643309\n",
            "Loss S2:  0.038497267973764876\n",
            "Loss S01:  0.03455020911502142\n",
            "Loss S2:  0.0384838033802898\n",
            "Loss S01:  0.03454706558599325\n",
            "Loss S2:  0.038497889631222096\n",
            "Loss S01:  0.03452822085893486\n",
            "Loss S2:  0.03848280707534117\n",
            "Loss S01:  0.034519639009791436\n",
            "Loss S2:  0.03846475640797561\n",
            "Loss S01:  0.03451548829806882\n",
            "Loss S2:  0.03847731885287556\n",
            "Loss S01:  0.034511248049792914\n",
            "Loss S2:  0.038488601696012856\n",
            "Loss S01:  0.034502117963796465\n",
            "Loss S2:  0.038477978698766915\n",
            "Loss S01:  0.034506147003223395\n",
            "Loss S2:  0.038494002639379916\n",
            "Loss S01:  0.0344764910365195\n",
            "Loss S2:  0.03846496863332397\n",
            "Validation: \n",
            " Loss S01:  0.03014843910932541\n",
            " Loss S2:  0.047598447650671005\n",
            " Loss S01:  0.03316247658360572\n",
            " Loss S2:  0.052341779605263754\n",
            " Loss S01:  0.03298713952848097\n",
            " Loss S2:  0.05289446517098241\n",
            " Loss S01:  0.03306094355514792\n",
            " Loss S2:  0.05265001036593171\n",
            " Loss S01:  0.0331967818884202\n",
            " Loss S2:  0.05247662423385514\n",
            "\n",
            "Epoch: 19\n",
            "Loss S01:  0.03588367998600006\n",
            "Loss S2:  0.038914699107408524\n",
            "Loss S01:  0.033742445944385094\n",
            "Loss S2:  0.03709945590658621\n",
            "Loss S01:  0.033570136076637676\n",
            "Loss S2:  0.037350298393340337\n",
            "Loss S01:  0.033570891967223536\n",
            "Loss S2:  0.037331413958341844\n",
            "Loss S01:  0.03375338158774667\n",
            "Loss S2:  0.037568024654940864\n",
            "Loss S01:  0.03401041739419395\n",
            "Loss S2:  0.037781523138869046\n",
            "Loss S01:  0.033859911878578\n",
            "Loss S2:  0.03777179201362563\n",
            "Loss S01:  0.03397081587725962\n",
            "Loss S2:  0.03792594882174277\n",
            "Loss S01:  0.03376519256903802\n",
            "Loss S2:  0.037718073177484816\n",
            "Loss S01:  0.033710342166679244\n",
            "Loss S2:  0.03765466711023352\n",
            "Loss S01:  0.0336069145451973\n",
            "Loss S2:  0.03755828671821273\n",
            "Loss S01:  0.03360870398312538\n",
            "Loss S2:  0.03758080746676471\n",
            "Loss S01:  0.03367375359247046\n",
            "Loss S2:  0.03764558866743214\n",
            "Loss S01:  0.033733929770479676\n",
            "Loss S2:  0.037745575328137125\n",
            "Loss S01:  0.03374147910545481\n",
            "Loss S2:  0.03771635771114776\n",
            "Loss S01:  0.03376459331553898\n",
            "Loss S2:  0.03772024383509396\n",
            "Loss S01:  0.0337424115364596\n",
            "Loss S2:  0.03773379126876037\n",
            "Loss S01:  0.03381577558946191\n",
            "Loss S2:  0.03782235164391367\n",
            "Loss S01:  0.03387903520909462\n",
            "Loss S2:  0.03795640495765275\n",
            "Loss S01:  0.03384381298619415\n",
            "Loss S2:  0.03792897034533985\n",
            "Loss S01:  0.03375267361601194\n",
            "Loss S2:  0.03786049116013655\n",
            "Loss S01:  0.03377459309465512\n",
            "Loss S2:  0.037861625447657435\n",
            "Loss S01:  0.03378328750103847\n",
            "Loss S2:  0.03787097850671181\n",
            "Loss S01:  0.03378668964489714\n",
            "Loss S2:  0.0378566404473988\n",
            "Loss S01:  0.033809949748248976\n",
            "Loss S2:  0.037879372193358254\n",
            "Loss S01:  0.03386684357229457\n",
            "Loss S2:  0.03786833127181369\n",
            "Loss S01:  0.03387822220779927\n",
            "Loss S2:  0.037871890440868694\n",
            "Loss S01:  0.0338734055274747\n",
            "Loss S2:  0.03782059774627545\n",
            "Loss S01:  0.03387268922321525\n",
            "Loss S2:  0.0378304355482826\n",
            "Loss S01:  0.03387276103073584\n",
            "Loss S2:  0.037866669985436904\n",
            "Loss S01:  0.03387763789944673\n",
            "Loss S2:  0.03785628095219698\n",
            "Loss S01:  0.03381465338889234\n",
            "Loss S2:  0.03781591499541733\n",
            "Loss S01:  0.03380390881586855\n",
            "Loss S2:  0.037799060008143336\n",
            "Loss S01:  0.03379138090346695\n",
            "Loss S2:  0.03778358717699425\n",
            "Loss S01:  0.033796334671965446\n",
            "Loss S2:  0.03776900330421862\n",
            "Loss S01:  0.03379346830956107\n",
            "Loss S2:  0.037765057359495736\n",
            "Loss S01:  0.03379246633283154\n",
            "Loss S2:  0.037778229769990054\n",
            "Loss S01:  0.033754882656258714\n",
            "Loss S2:  0.03773152333184715\n",
            "Loss S01:  0.033749356191224\n",
            "Loss S2:  0.03772551192855585\n",
            "Loss S01:  0.0337401945286852\n",
            "Loss S2:  0.03769316546180669\n",
            "Loss S01:  0.033746222140931724\n",
            "Loss S2:  0.03769176320504964\n",
            "Loss S01:  0.033742215556892455\n",
            "Loss S2:  0.03768571441734794\n",
            "Loss S01:  0.03376679652483333\n",
            "Loss S2:  0.037679644766088906\n",
            "Loss S01:  0.033750418176023145\n",
            "Loss S2:  0.037663883034770834\n",
            "Loss S01:  0.03375659893581521\n",
            "Loss S2:  0.037664689790972775\n",
            "Loss S01:  0.03374497001028378\n",
            "Loss S2:  0.03765871680595658\n",
            "Loss S01:  0.03374006366458218\n",
            "Loss S2:  0.03767978394335366\n",
            "Loss S01:  0.03373197198404501\n",
            "Loss S2:  0.037652174240243155\n",
            "Loss S01:  0.03374537807771173\n",
            "Loss S2:  0.03766573246436778\n",
            "Loss S01:  0.03371367029572577\n",
            "Loss S2:  0.037628561019320834\n",
            "Validation: \n",
            " Loss S01:  0.029843706637620926\n",
            " Loss S2:  0.045857954770326614\n",
            " Loss S01:  0.032609829058249794\n",
            " Loss S2:  0.05104068773133414\n",
            " Loss S01:  0.03238731154763117\n",
            " Loss S2:  0.05161202589913112\n",
            " Loss S01:  0.03244785173628174\n",
            " Loss S2:  0.05137687060432356\n",
            " Loss S01:  0.03257795784300492\n",
            " Loss S2:  0.05117280785868197\n",
            "\n",
            "Epoch: 20\n",
            "Loss S01:  0.03417567163705826\n",
            "Loss S2:  0.03886784613132477\n",
            "Loss S01:  0.03332976810634136\n",
            "Loss S2:  0.03658013824712147\n",
            "Loss S01:  0.03323664498471078\n",
            "Loss S2:  0.03674795904329845\n",
            "Loss S01:  0.03310356883993072\n",
            "Loss S2:  0.036544972249577125\n",
            "Loss S01:  0.033178311931650814\n",
            "Loss S2:  0.036843505937878676\n",
            "Loss S01:  0.03334244910408469\n",
            "Loss S2:  0.03712429661376804\n",
            "Loss S01:  0.03321448493687833\n",
            "Loss S2:  0.03702383396811173\n",
            "Loss S01:  0.03328354034940122\n",
            "Loss S2:  0.03715145698105785\n",
            "Loss S01:  0.033155280682775706\n",
            "Loss S2:  0.03699657033531018\n",
            "Loss S01:  0.033058352645609405\n",
            "Loss S2:  0.03696391758109842\n",
            "Loss S01:  0.0329640785619469\n",
            "Loss S2:  0.03689168642579329\n",
            "Loss S01:  0.032985696921477445\n",
            "Loss S2:  0.03694370622234838\n",
            "Loss S01:  0.03301175731457462\n",
            "Loss S2:  0.03702916908424255\n",
            "Loss S01:  0.033048305916422195\n",
            "Loss S2:  0.037008958648293074\n",
            "Loss S01:  0.033093775074321326\n",
            "Loss S2:  0.03703846075697571\n",
            "Loss S01:  0.033150416681703355\n",
            "Loss S2:  0.037136688199363006\n",
            "Loss S01:  0.033124498004868906\n",
            "Loss S2:  0.03706850426167435\n",
            "Loss S01:  0.03320484434129202\n",
            "Loss S2:  0.03716656049354035\n",
            "Loss S01:  0.03324750664731416\n",
            "Loss S2:  0.037255066801994544\n",
            "Loss S01:  0.033199243715608305\n",
            "Loss S2:  0.03723007586848049\n",
            "Loss S01:  0.033132428294094046\n",
            "Loss S2:  0.03716996445584653\n",
            "Loss S01:  0.03313705587309401\n",
            "Loss S2:  0.03715034597222274\n",
            "Loss S01:  0.033122694212521904\n",
            "Loss S2:  0.03710659844505841\n",
            "Loss S01:  0.033136996105958376\n",
            "Loss S2:  0.03711857117744751\n",
            "Loss S01:  0.03317465846469293\n",
            "Loss S2:  0.037135941961865204\n",
            "Loss S01:  0.03316032213429768\n",
            "Loss S2:  0.03713322690580471\n",
            "Loss S01:  0.03316638049922912\n",
            "Loss S2:  0.03716384879245612\n",
            "Loss S01:  0.03315227104838924\n",
            "Loss S2:  0.03713925468745707\n",
            "Loss S01:  0.03318763373823989\n",
            "Loss S2:  0.037142339894992174\n",
            "Loss S01:  0.03321914786402507\n",
            "Loss S2:  0.03718373481555493\n",
            "Loss S01:  0.03324041001969002\n",
            "Loss S2:  0.03716856253602576\n",
            "Loss S01:  0.033200383527725454\n",
            "Loss S2:  0.03713028372484005\n",
            "Loss S01:  0.03316695593018955\n",
            "Loss S2:  0.03710330530676144\n",
            "Loss S01:  0.03316346217183188\n",
            "Loss S2:  0.03709654189263586\n",
            "Loss S01:  0.03315375087089028\n",
            "Loss S2:  0.03707981383826726\n",
            "Loss S01:  0.03315440889055233\n",
            "Loss S2:  0.03707680821503669\n",
            "Loss S01:  0.0331650915506639\n",
            "Loss S2:  0.03709831842094907\n",
            "Loss S01:  0.033144742511514065\n",
            "Loss S2:  0.03706379003842886\n",
            "Loss S01:  0.03313354847664282\n",
            "Loss S2:  0.03706429205502425\n",
            "Loss S01:  0.03310246771806494\n",
            "Loss S2:  0.0370273251191277\n",
            "Loss S01:  0.03310709018399591\n",
            "Loss S2:  0.03703727725512369\n",
            "Loss S01:  0.03313328978354043\n",
            "Loss S2:  0.03703424226664859\n",
            "Loss S01:  0.03314852292014556\n",
            "Loss S2:  0.037042743988742174\n",
            "Loss S01:  0.033113411595297244\n",
            "Loss S2:  0.03701247585172443\n",
            "Loss S01:  0.03310984475728201\n",
            "Loss S2:  0.037011287472888725\n",
            "Loss S01:  0.03310987889816253\n",
            "Loss S2:  0.03702139567806821\n",
            "Loss S01:  0.033112640320899174\n",
            "Loss S2:  0.03705038743622908\n",
            "Loss S01:  0.03310596687430536\n",
            "Loss S2:  0.0370347734912439\n",
            "Loss S01:  0.033114637366327575\n",
            "Loss S2:  0.037033747742242974\n",
            "Loss S01:  0.0330764046754468\n",
            "Loss S2:  0.03699635071660982\n",
            "Validation: \n",
            " Loss S01:  0.029189929366111755\n",
            " Loss S2:  0.046172719448804855\n",
            " Loss S01:  0.031780193958963664\n",
            " Loss S2:  0.050995281232254844\n",
            " Loss S01:  0.03158858831881023\n",
            " Loss S2:  0.051313094522168\n",
            " Loss S01:  0.0316327768148946\n",
            " Loss S2:  0.05100975216167872\n",
            " Loss S01:  0.031749277065197624\n",
            " Loss S2:  0.05087795835218312\n",
            "\n",
            "Epoch: 21\n",
            "Loss S01:  0.03415674716234207\n",
            "Loss S2:  0.03674440458416939\n",
            "Loss S01:  0.03192075003277172\n",
            "Loss S2:  0.0359764944084666\n",
            "Loss S01:  0.03210386128297874\n",
            "Loss S2:  0.036212866593684466\n",
            "Loss S01:  0.032209929559499986\n",
            "Loss S2:  0.03612362863796373\n",
            "Loss S01:  0.03243822727079799\n",
            "Loss S2:  0.03624388934453813\n",
            "Loss S01:  0.03255221306108961\n",
            "Loss S2:  0.036435667681051234\n",
            "Loss S01:  0.03253387757500664\n",
            "Loss S2:  0.036394449011957056\n",
            "Loss S01:  0.03272278551799311\n",
            "Loss S2:  0.036496941865959635\n",
            "Loss S01:  0.03256144395305051\n",
            "Loss S2:  0.036354070575332936\n",
            "Loss S01:  0.032470539335038635\n",
            "Loss S2:  0.036289239302277565\n",
            "Loss S01:  0.03241779959511639\n",
            "Loss S2:  0.03629362053874106\n",
            "Loss S01:  0.03247438498713949\n",
            "Loss S2:  0.03631163789546705\n",
            "Loss S01:  0.032481871822402496\n",
            "Loss S2:  0.03628214136197055\n",
            "Loss S01:  0.032531971851264246\n",
            "Loss S2:  0.03632594447383899\n",
            "Loss S01:  0.03258352358131967\n",
            "Loss S2:  0.03635512121972886\n",
            "Loss S01:  0.03259638502009657\n",
            "Loss S2:  0.03641635392922045\n",
            "Loss S01:  0.03257529909566322\n",
            "Loss S2:  0.03639837389129289\n",
            "Loss S01:  0.032604095738446505\n",
            "Loss S2:  0.036517314896074654\n",
            "Loss S01:  0.03265106215949546\n",
            "Loss S2:  0.036600961437376824\n",
            "Loss S01:  0.03261540592184866\n",
            "Loss S2:  0.0365416404464482\n",
            "Loss S01:  0.0325397207350725\n",
            "Loss S2:  0.03648086598337586\n",
            "Loss S01:  0.03253007736674982\n",
            "Loss S2:  0.036503263418143396\n",
            "Loss S01:  0.032507347827987976\n",
            "Loss S2:  0.03648947651299956\n",
            "Loss S01:  0.03250093414605438\n",
            "Loss S2:  0.036461220094651886\n",
            "Loss S01:  0.03252092077246098\n",
            "Loss S2:  0.03648316971493955\n",
            "Loss S01:  0.032536984758963626\n",
            "Loss S2:  0.036465506614798096\n",
            "Loss S01:  0.03255986635683825\n",
            "Loss S2:  0.03649164790508848\n",
            "Loss S01:  0.0325555087056877\n",
            "Loss S2:  0.03647017411585224\n",
            "Loss S01:  0.032573181956847365\n",
            "Loss S2:  0.03646078068113412\n",
            "Loss S01:  0.03256994195256856\n",
            "Loss S2:  0.036479770778790374\n",
            "Loss S01:  0.032576598895348585\n",
            "Loss S2:  0.03648031314841141\n",
            "Loss S01:  0.032568930992647\n",
            "Loss S2:  0.03645621611470195\n",
            "Loss S01:  0.03254628618395774\n",
            "Loss S2:  0.0364331445566776\n",
            "Loss S01:  0.032554182823285954\n",
            "Loss S2:  0.0364253830130907\n",
            "Loss S01:  0.032558990871845106\n",
            "Loss S2:  0.03640755589319464\n",
            "Loss S01:  0.03257188760805504\n",
            "Loss S2:  0.03641723636590857\n",
            "Loss S01:  0.03255895849245077\n",
            "Loss S2:  0.036396196297785254\n",
            "Loss S01:  0.03252298806133778\n",
            "Loss S2:  0.03635587755620801\n",
            "Loss S01:  0.032498472749091824\n",
            "Loss S2:  0.03635196382390076\n",
            "Loss S01:  0.032464376719825716\n",
            "Loss S2:  0.036310018490418754\n",
            "Loss S01:  0.032467253664113634\n",
            "Loss S2:  0.03629176989383531\n",
            "Loss S01:  0.03247791015228071\n",
            "Loss S2:  0.03628584958268488\n",
            "Loss S01:  0.0324698384860066\n",
            "Loss S2:  0.036275695974002826\n",
            "Loss S01:  0.032433501338875764\n",
            "Loss S2:  0.03625489861257115\n",
            "Loss S01:  0.03242339707421067\n",
            "Loss S2:  0.03624348129967308\n",
            "Loss S01:  0.0324195047836346\n",
            "Loss S2:  0.036257146876264044\n",
            "Loss S01:  0.03242571868099385\n",
            "Loss S2:  0.036276930838079875\n",
            "Loss S01:  0.03241538278157417\n",
            "Loss S2:  0.03626648783921056\n",
            "Loss S01:  0.03243110060552425\n",
            "Loss S2:  0.036284229723957374\n",
            "Loss S01:  0.032405154047393504\n",
            "Loss S2:  0.036245237311344526\n",
            "Validation: \n",
            " Loss S01:  0.028388142585754395\n",
            " Loss S2:  0.04629518464207649\n",
            " Loss S01:  0.0307755684923558\n",
            " Loss S2:  0.04917585672367187\n",
            " Loss S01:  0.03074789855901788\n",
            " Loss S2:  0.04951987915286204\n",
            " Loss S01:  0.030788147577741107\n",
            " Loss S2:  0.04919657666907936\n",
            " Loss S01:  0.030906286573520413\n",
            " Loss S2:  0.049062395491349844\n",
            "\n",
            "Epoch: 22\n",
            "Loss S01:  0.03197271376848221\n",
            "Loss S2:  0.03831242024898529\n",
            "Loss S01:  0.03137537583031438\n",
            "Loss S2:  0.035250348123637115\n",
            "Loss S01:  0.03145431132898444\n",
            "Loss S2:  0.03534347546242532\n",
            "Loss S01:  0.03133030938765695\n",
            "Loss S2:  0.0351510998462477\n",
            "Loss S01:  0.03166617171430006\n",
            "Loss S2:  0.03518379661368161\n",
            "Loss S01:  0.031972150044406164\n",
            "Loss S2:  0.0354712405333332\n",
            "Loss S01:  0.031935861395275\n",
            "Loss S2:  0.03540516009584802\n",
            "Loss S01:  0.03212701712905521\n",
            "Loss S2:  0.0357374552060181\n",
            "Loss S01:  0.03190731008847555\n",
            "Loss S2:  0.03557810357507364\n",
            "Loss S01:  0.03180537612310478\n",
            "Loss S2:  0.035496948982824336\n",
            "Loss S01:  0.03173243165901392\n",
            "Loss S2:  0.03543829145186608\n",
            "Loss S01:  0.03176269832964953\n",
            "Loss S2:  0.03549477661045285\n",
            "Loss S01:  0.03185598421380047\n",
            "Loss S2:  0.03552989543161609\n",
            "Loss S01:  0.03190632137462838\n",
            "Loss S2:  0.035550078746812944\n",
            "Loss S01:  0.03194080388292353\n",
            "Loss S2:  0.0355293361985303\n",
            "Loss S01:  0.03197063044305669\n",
            "Loss S2:  0.035595164430753286\n",
            "Loss S01:  0.03194613057125059\n",
            "Loss S2:  0.035548370077002865\n",
            "Loss S01:  0.031981872568963565\n",
            "Loss S2:  0.035630764559521313\n",
            "Loss S01:  0.03202849539068851\n",
            "Loss S2:  0.035718299036855854\n",
            "Loss S01:  0.031990027572003955\n",
            "Loss S2:  0.0356949674482433\n",
            "Loss S01:  0.03194459676927892\n",
            "Loss S2:  0.03562800254468894\n",
            "Loss S01:  0.03194678052200525\n",
            "Loss S2:  0.03567652919846123\n",
            "Loss S01:  0.03197860284560946\n",
            "Loss S2:  0.035693062528361026\n",
            "Loss S01:  0.031985857084522513\n",
            "Loss S2:  0.03567621148961447\n",
            "Loss S01:  0.032021333884710595\n",
            "Loss S2:  0.0357046708062724\n",
            "Loss S01:  0.03202707701143753\n",
            "Loss S2:  0.03568170041558277\n",
            "Loss S01:  0.03203587265421148\n",
            "Loss S2:  0.03569028069803998\n",
            "Loss S01:  0.03201062846035315\n",
            "Loss S2:  0.035649702062905936\n",
            "Loss S01:  0.0319948198399722\n",
            "Loss S2:  0.03565136622968942\n",
            "Loss S01:  0.03199788177213103\n",
            "Loss S2:  0.03569202000896136\n",
            "Loss S01:  0.03200216756582854\n",
            "Loss S2:  0.03566477554803869\n",
            "Loss S01:  0.03195804294641953\n",
            "Loss S2:  0.03563308258843383\n",
            "Loss S01:  0.031953896265302865\n",
            "Loss S2:  0.03561691014310838\n",
            "Loss S01:  0.03192937774217921\n",
            "Loss S2:  0.03560195223419868\n",
            "Loss S01:  0.031918048935114816\n",
            "Loss S2:  0.03558180144171386\n",
            "Loss S01:  0.031917884383361225\n",
            "Loss S2:  0.03558093841131936\n",
            "Loss S01:  0.03193816389382876\n",
            "Loss S2:  0.035612018082363125\n",
            "Loss S01:  0.03189384578312986\n",
            "Loss S2:  0.03558140043219466\n",
            "Loss S01:  0.03186123612828142\n",
            "Loss S2:  0.035582737855398124\n",
            "Loss S01:  0.03182428634113363\n",
            "Loss S2:  0.035518013893643306\n",
            "Loss S01:  0.03182968407627799\n",
            "Loss S2:  0.03550407490658195\n",
            "Loss S01:  0.031852887236397634\n",
            "Loss S2:  0.03551149758489898\n",
            "Loss S01:  0.03186760235338908\n",
            "Loss S2:  0.03552393184757997\n",
            "Loss S01:  0.031851000327556424\n",
            "Loss S2:  0.03552189704004822\n",
            "Loss S01:  0.03184135767574213\n",
            "Loss S2:  0.03550394270104481\n",
            "Loss S01:  0.03183033399102164\n",
            "Loss S2:  0.03550833939861696\n",
            "Loss S01:  0.03183927227401552\n",
            "Loss S2:  0.03552573984347429\n",
            "Loss S01:  0.031860101477672084\n",
            "Loss S2:  0.03552350099742286\n",
            "Loss S01:  0.031878701049114215\n",
            "Loss S2:  0.035543707638916996\n",
            "Loss S01:  0.03185164005080211\n",
            "Loss S2:  0.03550630897025591\n",
            "Validation: \n",
            " Loss S01:  0.027219027280807495\n",
            " Loss S2:  0.044404495507478714\n",
            " Loss S01:  0.030035876979430515\n",
            " Loss S2:  0.049019847597394674\n",
            " Loss S01:  0.03001275108900012\n",
            " Loss S2:  0.04932388572431192\n",
            " Loss S01:  0.03007008742968567\n",
            " Loss S2:  0.04899448498350675\n",
            " Loss S01:  0.03011665548439379\n",
            " Loss S2:  0.048826003792109315\n",
            "\n",
            "Epoch: 23\n",
            "Loss S01:  0.032308779656887054\n",
            "Loss S2:  0.03392689675092697\n",
            "Loss S01:  0.030953001400286503\n",
            "Loss S2:  0.03380750813944773\n",
            "Loss S01:  0.030883797666146642\n",
            "Loss S2:  0.03432285732456616\n",
            "Loss S01:  0.031048804701816653\n",
            "Loss S2:  0.03439518641079626\n",
            "Loss S01:  0.031129821544376816\n",
            "Loss S2:  0.03451103371817891\n",
            "Loss S01:  0.03145851004941791\n",
            "Loss S2:  0.03490698191465116\n",
            "Loss S01:  0.031433871168582164\n",
            "Loss S2:  0.03483567331902317\n",
            "Loss S01:  0.03152507422885425\n",
            "Loss S2:  0.03511473379084762\n",
            "Loss S01:  0.03133397225152563\n",
            "Loss S2:  0.03494822519061006\n",
            "Loss S01:  0.031251808996875206\n",
            "Loss S2:  0.034867413341999054\n",
            "Loss S01:  0.031118528622359334\n",
            "Loss S2:  0.034845433483767035\n",
            "Loss S01:  0.031139861423153063\n",
            "Loss S2:  0.03486299449326219\n",
            "Loss S01:  0.031201397045707898\n",
            "Loss S2:  0.03489739384717685\n",
            "Loss S01:  0.031233176987134775\n",
            "Loss S2:  0.034950765077280635\n",
            "Loss S01:  0.031283983841855476\n",
            "Loss S2:  0.03495628850415666\n",
            "Loss S01:  0.03130338128828845\n",
            "Loss S2:  0.03499206731601662\n",
            "Loss S01:  0.031341075203063326\n",
            "Loss S2:  0.03498269320228455\n",
            "Loss S01:  0.03140464752956092\n",
            "Loss S2:  0.03508368924216569\n",
            "Loss S01:  0.031442088049255024\n",
            "Loss S2:  0.035199694510405236\n",
            "Loss S01:  0.031407972222379364\n",
            "Loss S2:  0.03515979172710661\n",
            "Loss S01:  0.03133008331616423\n",
            "Loss S2:  0.035093180371902476\n",
            "Loss S01:  0.031344601902139696\n",
            "Loss S2:  0.03509689756267444\n",
            "Loss S01:  0.03132995657150832\n",
            "Loss S2:  0.03510567774296616\n",
            "Loss S01:  0.03134244569129758\n",
            "Loss S2:  0.035067429580600745\n",
            "Loss S01:  0.03139026360238489\n",
            "Loss S2:  0.03506909790004437\n",
            "Loss S01:  0.03138298881184532\n",
            "Loss S2:  0.03505484783554457\n",
            "Loss S01:  0.031382673666223715\n",
            "Loss S2:  0.03506126005016981\n",
            "Loss S01:  0.03137967489467336\n",
            "Loss S2:  0.03503881720654199\n",
            "Loss S01:  0.03135909012840312\n",
            "Loss S2:  0.035019847978688644\n",
            "Loss S01:  0.03136662838666914\n",
            "Loss S2:  0.035035671800682226\n",
            "Loss S01:  0.03138362260479666\n",
            "Loss S2:  0.035041818401445185\n",
            "Loss S01:  0.03135489900012491\n",
            "Loss S2:  0.03503594385801404\n",
            "Loss S01:  0.03134100367764817\n",
            "Loss S2:  0.03499215922530195\n",
            "Loss S01:  0.0313323785035992\n",
            "Loss S2:  0.034973884772318005\n",
            "Loss S01:  0.031348871252884616\n",
            "Loss S2:  0.03498273109061278\n",
            "Loss S01:  0.031340105766881565\n",
            "Loss S2:  0.035013024676644224\n",
            "Loss S01:  0.03136284056524656\n",
            "Loss S2:  0.03504623859150753\n",
            "Loss S01:  0.03132991391834063\n",
            "Loss S2:  0.03501024284974905\n",
            "Loss S01:  0.03129485198186608\n",
            "Loss S2:  0.03498506273712543\n",
            "Loss S01:  0.03126557750622635\n",
            "Loss S2:  0.034939851030669246\n",
            "Loss S01:  0.03127657126941883\n",
            "Loss S2:  0.034941239333583826\n",
            "Loss S01:  0.03129002513984839\n",
            "Loss S2:  0.03494437248276098\n",
            "Loss S01:  0.031302045955546115\n",
            "Loss S2:  0.03493981153622376\n",
            "Loss S01:  0.0312850585643833\n",
            "Loss S2:  0.03491550100980253\n",
            "Loss S01:  0.031293871168410425\n",
            "Loss S2:  0.034930136202583235\n",
            "Loss S01:  0.031289341683993055\n",
            "Loss S2:  0.03493701755389274\n",
            "Loss S01:  0.03129918207444752\n",
            "Loss S2:  0.034956733104237504\n",
            "Loss S01:  0.031305245090189\n",
            "Loss S2:  0.03494575648566593\n",
            "Loss S01:  0.031313638626878575\n",
            "Loss S2:  0.03495904883435635\n",
            "Loss S01:  0.03128141428812458\n",
            "Loss S2:  0.034928845996587195\n",
            "Validation: \n",
            " Loss S01:  0.028321633115410805\n",
            " Loss S2:  0.04581069201231003\n",
            " Loss S01:  0.03053053929692223\n",
            " Loss S2:  0.04957206689176105\n",
            " Loss S01:  0.03023838110995002\n",
            " Loss S2:  0.04997267855740175\n",
            " Loss S01:  0.03026138759050213\n",
            " Loss S2:  0.04953899973484337\n",
            " Loss S01:  0.03039286126969037\n",
            " Loss S2:  0.04936755770518456\n",
            "\n",
            "Epoch: 24\n",
            "Loss S01:  0.034929390996694565\n",
            "Loss S2:  0.03703014925122261\n",
            "Loss S01:  0.030704543841156094\n",
            "Loss S2:  0.03416132215749134\n",
            "Loss S01:  0.030715275910638627\n",
            "Loss S2:  0.0339346020704224\n",
            "Loss S01:  0.03069379030456466\n",
            "Loss S2:  0.03377314867271531\n",
            "Loss S01:  0.030801549599301523\n",
            "Loss S2:  0.03407401178123021\n",
            "Loss S01:  0.031007164212710717\n",
            "Loss S2:  0.03429359997458318\n",
            "Loss S01:  0.030921481824556334\n",
            "Loss S2:  0.03426974120198703\n",
            "Loss S01:  0.031073943069073517\n",
            "Loss S2:  0.034655741822551676\n",
            "Loss S01:  0.03084227569217299\n",
            "Loss S2:  0.03442118988360888\n",
            "Loss S01:  0.030723588459275582\n",
            "Loss S2:  0.03436474209385259\n",
            "Loss S01:  0.030668224254162005\n",
            "Loss S2:  0.03431892157117329\n",
            "Loss S01:  0.030797347141144512\n",
            "Loss S2:  0.03435761298615116\n",
            "Loss S01:  0.03082694267192163\n",
            "Loss S2:  0.03446789663131079\n",
            "Loss S01:  0.030821842475588085\n",
            "Loss S2:  0.0344503571258019\n",
            "Loss S01:  0.030862224215628408\n",
            "Loss S2:  0.03449426049776111\n",
            "Loss S01:  0.030872758465590854\n",
            "Loss S2:  0.03458076932572371\n",
            "Loss S01:  0.030848320168645485\n",
            "Loss S2:  0.03457186648749417\n",
            "Loss S01:  0.030905559800608814\n",
            "Loss S2:  0.034666057523579624\n",
            "Loss S01:  0.030942663344559748\n",
            "Loss S2:  0.03476274241976317\n",
            "Loss S01:  0.030902085328445385\n",
            "Loss S2:  0.03471621070349716\n",
            "Loss S01:  0.03086561491870465\n",
            "Loss S2:  0.034654333270085394\n",
            "Loss S01:  0.0308676914651812\n",
            "Loss S2:  0.03466859365420601\n",
            "Loss S01:  0.03086596463566722\n",
            "Loss S2:  0.03465225067984195\n",
            "Loss S01:  0.03086926072868176\n",
            "Loss S2:  0.03461870046340542\n",
            "Loss S01:  0.030906451734640787\n",
            "Loss S2:  0.03462360207285129\n",
            "Loss S01:  0.030927320912599088\n",
            "Loss S2:  0.03460959119507041\n",
            "Loss S01:  0.03093601348613642\n",
            "Loss S2:  0.03463656389621939\n",
            "Loss S01:  0.030912445418234243\n",
            "Loss S2:  0.034620715037153214\n",
            "Loss S01:  0.030923939359728977\n",
            "Loss S2:  0.03460798433773034\n",
            "Loss S01:  0.030948918671235188\n",
            "Loss S2:  0.03462386387320319\n",
            "Loss S01:  0.030956827363995618\n",
            "Loss S2:  0.034615709257630814\n",
            "Loss S01:  0.030919103239366478\n",
            "Loss S2:  0.03459943475207715\n",
            "Loss S01:  0.030904324574401818\n",
            "Loss S2:  0.03458145076287127\n",
            "Loss S01:  0.030908377154581137\n",
            "Loss S2:  0.034580189008772196\n",
            "Loss S01:  0.030913327369959123\n",
            "Loss S2:  0.03456284561333768\n",
            "Loss S01:  0.03090848820416676\n",
            "Loss S2:  0.034566471830774576\n",
            "Loss S01:  0.030932910988040248\n",
            "Loss S2:  0.03459844252718948\n",
            "Loss S01:  0.030903178380144575\n",
            "Loss S2:  0.03456526393738558\n",
            "Loss S01:  0.030873942333139146\n",
            "Loss S2:  0.034558247026848045\n",
            "Loss S01:  0.030850121446544556\n",
            "Loss S2:  0.03451719256522863\n",
            "Loss S01:  0.03087085761371396\n",
            "Loss S2:  0.03450139617068958\n",
            "Loss S01:  0.0308741810131102\n",
            "Loss S2:  0.03446505035652151\n",
            "Loss S01:  0.030898104459401263\n",
            "Loss S2:  0.034452028842443805\n",
            "Loss S01:  0.030866110883220995\n",
            "Loss S2:  0.03444679374017876\n",
            "Loss S01:  0.030845612435350344\n",
            "Loss S2:  0.03444622398318212\n",
            "Loss S01:  0.03084403768330482\n",
            "Loss S2:  0.034446310710384946\n",
            "Loss S01:  0.030839803755897503\n",
            "Loss S2:  0.03446954751383198\n",
            "Loss S01:  0.03082972010815093\n",
            "Loss S2:  0.034457138179941796\n",
            "Loss S01:  0.03085362608786556\n",
            "Loss S2:  0.034475856540015976\n",
            "Loss S01:  0.030829301498481308\n",
            "Loss S2:  0.03444545986529159\n",
            "Validation: \n",
            " Loss S01:  0.02808494120836258\n",
            " Loss S2:  0.04542522504925728\n",
            " Loss S01:  0.030910916803848176\n",
            " Loss S2:  0.049984625939811976\n",
            " Loss S01:  0.03068727668283916\n",
            " Loss S2:  0.05035693198442459\n",
            " Loss S01:  0.030780371003707903\n",
            " Loss S2:  0.04996224846996245\n",
            " Loss S01:  0.030906632220671502\n",
            " Loss S2:  0.04983710925336237\n",
            "\n",
            "Epoch: 25\n",
            "Loss S01:  0.03282678499817848\n",
            "Loss S2:  0.03826107084751129\n",
            "Loss S01:  0.030046333135529\n",
            "Loss S2:  0.03347041136161848\n",
            "Loss S01:  0.03022110346882116\n",
            "Loss S2:  0.03350021335340682\n",
            "Loss S01:  0.030253238014636502\n",
            "Loss S2:  0.033401925897886674\n",
            "Loss S01:  0.030479227851440267\n",
            "Loss S2:  0.03370835759290835\n",
            "Loss S01:  0.030624518243997704\n",
            "Loss S2:  0.03407448442543254\n",
            "Loss S01:  0.03056864403798932\n",
            "Loss S2:  0.03398147613176557\n",
            "Loss S01:  0.030778508554671853\n",
            "Loss S2:  0.03423893328388812\n",
            "Loss S01:  0.030555580501203185\n",
            "Loss S2:  0.034106114005417\n",
            "Loss S01:  0.03041835096511212\n",
            "Loss S2:  0.03400994827049774\n",
            "Loss S01:  0.030317965599865018\n",
            "Loss S2:  0.03394782806903419\n",
            "Loss S01:  0.0303189543889718\n",
            "Loss S2:  0.03393026916226288\n",
            "Loss S01:  0.030352497919778194\n",
            "Loss S2:  0.03398194147104567\n",
            "Loss S01:  0.03037749920466929\n",
            "Loss S2:  0.0339799536292562\n",
            "Loss S01:  0.03042101886466885\n",
            "Loss S2:  0.03397857245876857\n",
            "Loss S01:  0.03042173283236311\n",
            "Loss S2:  0.034080302342772484\n",
            "Loss S01:  0.03039963189396799\n",
            "Loss S2:  0.034061471116006006\n",
            "Loss S01:  0.030467607799852102\n",
            "Loss S2:  0.034159692897521264\n",
            "Loss S01:  0.030513000638527764\n",
            "Loss S2:  0.03425416001504627\n",
            "Loss S01:  0.030496152780989078\n",
            "Loss S2:  0.034180921390262575\n",
            "Loss S01:  0.030444738811892063\n",
            "Loss S2:  0.03407637053980163\n",
            "Loss S01:  0.030440724017848902\n",
            "Loss S2:  0.0341210982263512\n",
            "Loss S01:  0.03044685210644929\n",
            "Loss S2:  0.03414886600729836\n",
            "Loss S01:  0.030439816915240638\n",
            "Loss S2:  0.03412842357055449\n",
            "Loss S01:  0.030485506621633823\n",
            "Loss S2:  0.03414241209842605\n",
            "Loss S01:  0.030497139601771577\n",
            "Loss S2:  0.03414015617384854\n",
            "Loss S01:  0.03049202886377943\n",
            "Loss S2:  0.03414389389741923\n",
            "Loss S01:  0.030478994757266942\n",
            "Loss S2:  0.03411005540472555\n",
            "Loss S01:  0.03049321164413491\n",
            "Loss S2:  0.03410986472862471\n",
            "Loss S01:  0.030483390515049297\n",
            "Loss S2:  0.03412729730234318\n",
            "Loss S01:  0.030491003085955037\n",
            "Loss S2:  0.03414157109700168\n",
            "Loss S01:  0.030459967678430764\n",
            "Loss S2:  0.03411771194751815\n",
            "Loss S01:  0.030436986326196482\n",
            "Loss S2:  0.03409863390446266\n",
            "Loss S01:  0.030425554276458086\n",
            "Loss S2:  0.034079380052057275\n",
            "Loss S01:  0.03042552313737331\n",
            "Loss S2:  0.03405401471707303\n",
            "Loss S01:  0.03042222314283379\n",
            "Loss S2:  0.03405287714298295\n",
            "Loss S01:  0.030434584719776447\n",
            "Loss S2:  0.034045593013311025\n",
            "Loss S01:  0.030410692251395345\n",
            "Loss S2:  0.03401138202219479\n",
            "Loss S01:  0.030387717839379325\n",
            "Loss S2:  0.03401404486240677\n",
            "Loss S01:  0.030360703473277104\n",
            "Loss S2:  0.0339796614435399\n",
            "Loss S01:  0.030372604370080027\n",
            "Loss S2:  0.03396463101530016\n",
            "Loss S01:  0.03038684653521599\n",
            "Loss S2:  0.033959892567093065\n",
            "Loss S01:  0.030403298195318767\n",
            "Loss S2:  0.03395183097279129\n",
            "Loss S01:  0.03038786918253329\n",
            "Loss S2:  0.03393456757742817\n",
            "Loss S01:  0.030399755636006255\n",
            "Loss S2:  0.033940134831871034\n",
            "Loss S01:  0.030403331814214025\n",
            "Loss S2:  0.03394941833208908\n",
            "Loss S01:  0.030398898416576572\n",
            "Loss S2:  0.033964135172956676\n",
            "Loss S01:  0.030397057703314568\n",
            "Loss S2:  0.033959856150066775\n",
            "Loss S01:  0.0304022551109538\n",
            "Loss S2:  0.033964307347541044\n",
            "Loss S01:  0.030370494677038395\n",
            "Loss S2:  0.033917445992925746\n",
            "Validation: \n",
            " Loss S01:  0.025864800438284874\n",
            " Loss S2:  0.045611731708049774\n",
            " Loss S01:  0.028552206676630748\n",
            " Loss S2:  0.0493783973866985\n",
            " Loss S01:  0.028307516763849957\n",
            " Loss S2:  0.04980829630683108\n",
            " Loss S01:  0.02837734687768045\n",
            " Loss S2:  0.04957150063309513\n",
            " Loss S01:  0.02850466315853007\n",
            " Loss S2:  0.049355947860965026\n",
            "\n",
            "Epoch: 26\n",
            "Loss S01:  0.03164827451109886\n",
            "Loss S2:  0.03538646921515465\n",
            "Loss S01:  0.029149515892971645\n",
            "Loss S2:  0.03307077728889205\n",
            "Loss S01:  0.02976735858690171\n",
            "Loss S2:  0.03354458067388762\n",
            "Loss S01:  0.029841095209121704\n",
            "Loss S2:  0.03347427722427153\n",
            "Loss S01:  0.02998946743403993\n",
            "Loss S2:  0.033687060380854256\n",
            "Loss S01:  0.030097376336069667\n",
            "Loss S2:  0.03375558452863319\n",
            "Loss S01:  0.030090598328436007\n",
            "Loss S2:  0.03367182459743297\n",
            "Loss S01:  0.030173796609463826\n",
            "Loss S2:  0.033890839088970504\n",
            "Loss S01:  0.029981500089720444\n",
            "Loss S2:  0.033714847225281924\n",
            "Loss S01:  0.02996015186411339\n",
            "Loss S2:  0.03364675610766306\n",
            "Loss S01:  0.02992392661474129\n",
            "Loss S2:  0.03361815754508618\n",
            "Loss S01:  0.029949221021688736\n",
            "Loss S2:  0.033647638435165085\n",
            "Loss S01:  0.030035376979792413\n",
            "Loss S2:  0.033716947332886625\n",
            "Loss S01:  0.03006622934614429\n",
            "Loss S2:  0.033743253276320814\n",
            "Loss S01:  0.030090875355592857\n",
            "Loss S2:  0.03377557037305747\n",
            "Loss S01:  0.030133872040060183\n",
            "Loss S2:  0.03384713065002533\n",
            "Loss S01:  0.03009378796686297\n",
            "Loss S2:  0.03380344131302019\n",
            "Loss S01:  0.030141690691486436\n",
            "Loss S2:  0.03387235798900239\n",
            "Loss S01:  0.030179320458960796\n",
            "Loss S2:  0.033919286898733504\n",
            "Loss S01:  0.030169214858278556\n",
            "Loss S2:  0.033877742803455645\n",
            "Loss S01:  0.030147059519418436\n",
            "Loss S2:  0.03382238383353943\n",
            "Loss S01:  0.030125015375526594\n",
            "Loss S2:  0.03382547488391965\n",
            "Loss S01:  0.030155825399165778\n",
            "Loss S2:  0.03383594933408418\n",
            "Loss S01:  0.030126456461556546\n",
            "Loss S2:  0.03378085787226627\n",
            "Loss S01:  0.030149948539761093\n",
            "Loss S2:  0.03379387255464352\n",
            "Loss S01:  0.03014942212586859\n",
            "Loss S2:  0.03378134626019524\n",
            "Loss S01:  0.030151138189195216\n",
            "Loss S2:  0.033770316926284315\n",
            "Loss S01:  0.030119817607737114\n",
            "Loss S2:  0.03374435859165288\n",
            "Loss S01:  0.030100837038346033\n",
            "Loss S2:  0.03373395912084079\n",
            "Loss S01:  0.03011923950946413\n",
            "Loss S2:  0.03375719359650235\n",
            "Loss S01:  0.03013817978979543\n",
            "Loss S2:  0.03375424903839133\n",
            "Loss S01:  0.03011781240175583\n",
            "Loss S2:  0.03372456822125092\n",
            "Loss S01:  0.030079614133153378\n",
            "Loss S2:  0.03368591134816501\n",
            "Loss S01:  0.030065142338549622\n",
            "Loss S2:  0.03365525563903085\n",
            "Loss S01:  0.030067772126573623\n",
            "Loss S2:  0.033646065758164326\n",
            "Loss S01:  0.03006999647812626\n",
            "Loss S2:  0.03365854691300127\n",
            "Loss S01:  0.030073604327200854\n",
            "Loss S2:  0.033665493089406445\n",
            "Loss S01:  0.030043752246468537\n",
            "Loss S2:  0.03362555112539115\n",
            "Loss S01:  0.03002575156706681\n",
            "Loss S2:  0.03362682872889392\n",
            "Loss S01:  0.029993773569040896\n",
            "Loss S2:  0.03358066506931544\n",
            "Loss S01:  0.029982000710140736\n",
            "Loss S2:  0.03357951021161014\n",
            "Loss S01:  0.02998620575778821\n",
            "Loss S2:  0.03358300581773847\n",
            "Loss S01:  0.03000110191361638\n",
            "Loss S2:  0.03357979677387484\n",
            "Loss S01:  0.029988733252073938\n",
            "Loss S2:  0.03354523679254503\n",
            "Loss S01:  0.029979796611330135\n",
            "Loss S2:  0.033536415990619434\n",
            "Loss S01:  0.029978172634697278\n",
            "Loss S2:  0.03353056926569362\n",
            "Loss S01:  0.029976926877318906\n",
            "Loss S2:  0.033547406709084286\n",
            "Loss S01:  0.029964612986726365\n",
            "Loss S2:  0.033545007763110146\n",
            "Loss S01:  0.029980815292612928\n",
            "Loss S2:  0.033565761529203514\n",
            "Loss S01:  0.029952173187661073\n",
            "Loss S2:  0.033541157063999634\n",
            "Validation: \n",
            " Loss S01:  0.027612118050456047\n",
            " Loss S2:  0.043861836194992065\n",
            " Loss S01:  0.03011859394609928\n",
            " Loss S2:  0.047515845369725\n",
            " Loss S01:  0.029937496227098674\n",
            " Loss S2:  0.04791780652069464\n",
            " Loss S01:  0.029926678104723086\n",
            " Loss S2:  0.04760709246162508\n",
            " Loss S01:  0.030060839413860698\n",
            " Loss S2:  0.047445550193021324\n",
            "\n",
            "Epoch: 27\n",
            "Loss S01:  0.030666803941130638\n",
            "Loss S2:  0.03614425286650658\n",
            "Loss S01:  0.028965968638658524\n",
            "Loss S2:  0.032666917043653404\n",
            "Loss S01:  0.02904822579806759\n",
            "Loss S2:  0.032869853788898104\n",
            "Loss S01:  0.02918851507767554\n",
            "Loss S2:  0.03271082650509573\n",
            "Loss S01:  0.029321707467116962\n",
            "Loss S2:  0.033018110047389825\n",
            "Loss S01:  0.029572138818455676\n",
            "Loss S2:  0.033292396213201916\n",
            "Loss S01:  0.029450229048484662\n",
            "Loss S2:  0.033184362086849134\n",
            "Loss S01:  0.02955183966583769\n",
            "Loss S2:  0.033385450125370225\n",
            "Loss S01:  0.02937238559954696\n",
            "Loss S2:  0.033234828462203346\n",
            "Loss S01:  0.029307203387821112\n",
            "Loss S2:  0.03318474556391056\n",
            "Loss S01:  0.029278737004145537\n",
            "Loss S2:  0.033123666752535516\n",
            "Loss S01:  0.02930938969324301\n",
            "Loss S2:  0.033124164988597236\n",
            "Loss S01:  0.02940049562757173\n",
            "Loss S2:  0.03319359215145761\n",
            "Loss S01:  0.029458719853112717\n",
            "Loss S2:  0.033191844053168336\n",
            "Loss S01:  0.029487061215207933\n",
            "Loss S2:  0.03317983423405928\n",
            "Loss S01:  0.029513187768147483\n",
            "Loss S2:  0.03321647306053054\n",
            "Loss S01:  0.02951582068843501\n",
            "Loss S2:  0.03321146383094862\n",
            "Loss S01:  0.029564706456155804\n",
            "Loss S2:  0.0332894975439324\n",
            "Loss S01:  0.029611789729466754\n",
            "Loss S2:  0.03335200291312202\n",
            "Loss S01:  0.02959192787048392\n",
            "Loss S2:  0.033303552487601785\n",
            "Loss S01:  0.02952102003665409\n",
            "Loss S2:  0.03324699178523389\n",
            "Loss S01:  0.029526522524337066\n",
            "Loss S2:  0.03324632049136535\n",
            "Loss S01:  0.029541248497784948\n",
            "Loss S2:  0.033223289760394334\n",
            "Loss S01:  0.029565858578075577\n",
            "Loss S2:  0.03320887587254956\n",
            "Loss S01:  0.02960779283705836\n",
            "Loss S2:  0.03322643127400598\n",
            "Loss S01:  0.02963523469509119\n",
            "Loss S2:  0.03321988111829378\n",
            "Loss S01:  0.02963596558862034\n",
            "Loss S2:  0.03323258396766195\n",
            "Loss S01:  0.029608380241917507\n",
            "Loss S2:  0.033202866339760516\n",
            "Loss S01:  0.029588602648988313\n",
            "Loss S2:  0.03316539260177638\n",
            "Loss S01:  0.029591747831642833\n",
            "Loss S2:  0.03318381718523109\n",
            "Loss S01:  0.02961044580957224\n",
            "Loss S2:  0.033181107977399\n",
            "Loss S01:  0.029576071247554285\n",
            "Loss S2:  0.03315202932553276\n",
            "Loss S01:  0.029553016943630772\n",
            "Loss S2:  0.0331378259820823\n",
            "Loss S01:  0.029548369747589363\n",
            "Loss S2:  0.03313876260803545\n",
            "Loss S01:  0.029555284885192545\n",
            "Loss S2:  0.033127977368025835\n",
            "Loss S01:  0.029555153897684865\n",
            "Loss S2:  0.03315685519104839\n",
            "Loss S01:  0.02957867037948644\n",
            "Loss S2:  0.033187005885585194\n",
            "Loss S01:  0.029566745969122954\n",
            "Loss S2:  0.03314945281475702\n",
            "Loss S01:  0.02956019905139142\n",
            "Loss S2:  0.033151867958347944\n",
            "Loss S01:  0.029550344818998178\n",
            "Loss S2:  0.03313759620994558\n",
            "Loss S01:  0.02954802330481144\n",
            "Loss S2:  0.03313205035826066\n",
            "Loss S01:  0.029555734917702758\n",
            "Loss S2:  0.033147433586871826\n",
            "Loss S01:  0.029568574287333567\n",
            "Loss S2:  0.03316164865311138\n",
            "Loss S01:  0.029553101447750134\n",
            "Loss S2:  0.03315641822211289\n",
            "Loss S01:  0.029554050808557036\n",
            "Loss S2:  0.033141919882659736\n",
            "Loss S01:  0.02955543785097858\n",
            "Loss S2:  0.03314944327769681\n",
            "Loss S01:  0.02956153715429363\n",
            "Loss S2:  0.033173666818475514\n",
            "Loss S01:  0.029542006405896947\n",
            "Loss S2:  0.0331569100807237\n",
            "Loss S01:  0.02953867791239908\n",
            "Loss S2:  0.03316794609236618\n",
            "Loss S01:  0.029517562676774994\n",
            "Loss S2:  0.03313676675671594\n",
            "Validation: \n",
            " Loss S01:  0.02765587344765663\n",
            " Loss S2:  0.042904891073703766\n",
            " Loss S01:  0.02952289040244761\n",
            " Loss S2:  0.04736835119270143\n",
            " Loss S01:  0.029554717787882177\n",
            " Loss S2:  0.04778918841990029\n",
            " Loss S01:  0.029568158548142088\n",
            " Loss S2:  0.04743606670469534\n",
            " Loss S01:  0.029719184164279773\n",
            " Loss S2:  0.04734959605115431\n",
            "\n",
            "Epoch: 28\n",
            "Loss S01:  0.029829854145646095\n",
            "Loss S2:  0.03389286622405052\n",
            "Loss S01:  0.028432752259752968\n",
            "Loss S2:  0.03218507174063812\n",
            "Loss S01:  0.028656582569792158\n",
            "Loss S2:  0.03256168748651232\n",
            "Loss S01:  0.028719348592623588\n",
            "Loss S2:  0.032598344428885366\n",
            "Loss S01:  0.028886235069210935\n",
            "Loss S2:  0.032629392041665754\n",
            "Loss S01:  0.02914130987197745\n",
            "Loss S2:  0.032793138285770136\n",
            "Loss S01:  0.029077115209131945\n",
            "Loss S2:  0.03269027408639916\n",
            "Loss S01:  0.02924844842981285\n",
            "Loss S2:  0.03296040347449376\n",
            "Loss S01:  0.029123273367683094\n",
            "Loss S2:  0.032814171280205987\n",
            "Loss S01:  0.029036592774011275\n",
            "Loss S2:  0.032757692612134494\n",
            "Loss S01:  0.028945395520122926\n",
            "Loss S2:  0.032709786539325617\n",
            "Loss S01:  0.028955176069929794\n",
            "Loss S2:  0.03268567224343618\n",
            "Loss S01:  0.02898942947018245\n",
            "Loss S2:  0.032669568588295256\n",
            "Loss S01:  0.029031369650295673\n",
            "Loss S2:  0.03269224825763065\n",
            "Loss S01:  0.029090049141582024\n",
            "Loss S2:  0.03274733878355077\n",
            "Loss S01:  0.029142761356289813\n",
            "Loss S2:  0.03280423320089745\n",
            "Loss S01:  0.02915817827726743\n",
            "Loss S2:  0.03281563261280889\n",
            "Loss S01:  0.02921122380080279\n",
            "Loss S2:  0.03290547169091409\n",
            "Loss S01:  0.029264696549562458\n",
            "Loss S2:  0.03297888968160469\n",
            "Loss S01:  0.029252917393651934\n",
            "Loss S2:  0.03291476814378619\n",
            "Loss S01:  0.02919132061376797\n",
            "Loss S2:  0.0328479073571032\n",
            "Loss S01:  0.029185500296088757\n",
            "Loss S2:  0.03285639632440291\n",
            "Loss S01:  0.029203553317426557\n",
            "Loss S2:  0.032829608228816164\n",
            "Loss S01:  0.029203739476861893\n",
            "Loss S2:  0.032800820394060314\n",
            "Loss S01:  0.029217030845304248\n",
            "Loss S2:  0.03280593096993523\n",
            "Loss S01:  0.02920060189982572\n",
            "Loss S2:  0.03277416427089161\n",
            "Loss S01:  0.02921307160451266\n",
            "Loss S2:  0.03277594314252965\n",
            "Loss S01:  0.02920342169399631\n",
            "Loss S2:  0.032755341827594486\n",
            "Loss S01:  0.02919220292668962\n",
            "Loss S2:  0.03274022849455635\n",
            "Loss S01:  0.02918411010421838\n",
            "Loss S2:  0.03274888300762553\n",
            "Loss S01:  0.029185868979143936\n",
            "Loss S2:  0.03273259766053322\n",
            "Loss S01:  0.029163688499109153\n",
            "Loss S2:  0.03269945127572086\n",
            "Loss S01:  0.029140707122806076\n",
            "Loss S2:  0.032672220342023724\n",
            "Loss S01:  0.02913279196084266\n",
            "Loss S2:  0.03265836411732561\n",
            "Loss S01:  0.029121706223994754\n",
            "Loss S2:  0.03264813250766472\n",
            "Loss S01:  0.029127225316954813\n",
            "Loss S2:  0.03266588360750437\n",
            "Loss S01:  0.02914473828227566\n",
            "Loss S2:  0.03267450298897282\n",
            "Loss S01:  0.02912233361216247\n",
            "Loss S2:  0.03264118145117059\n",
            "Loss S01:  0.0291033104365188\n",
            "Loss S2:  0.03264382981213685\n",
            "Loss S01:  0.029088033024993395\n",
            "Loss S2:  0.03260954790045995\n",
            "Loss S01:  0.02909845420324297\n",
            "Loss S2:  0.03261435142906378\n",
            "Loss S01:  0.02910510654994026\n",
            "Loss S2:  0.03261783896716552\n",
            "Loss S01:  0.02911841702999242\n",
            "Loss S2:  0.03262572124104885\n",
            "Loss S01:  0.029115082013254377\n",
            "Loss S2:  0.03262167082550631\n",
            "Loss S01:  0.02912735239583619\n",
            "Loss S2:  0.03262473683795826\n",
            "Loss S01:  0.029129586680203743\n",
            "Loss S2:  0.03262310466850941\n",
            "Loss S01:  0.02914593175231376\n",
            "Loss S2:  0.032651247800107634\n",
            "Loss S01:  0.029150501391880072\n",
            "Loss S2:  0.032658605656394786\n",
            "Loss S01:  0.029163596199016064\n",
            "Loss S2:  0.03267490744916045\n",
            "Loss S01:  0.02914699412836805\n",
            "Loss S2:  0.032650728302186474\n",
            "Validation: \n",
            " Loss S01:  0.026166312396526337\n",
            " Loss S2:  0.043662652373313904\n",
            " Loss S01:  0.02793143450149468\n",
            " Loss S2:  0.047663681918666476\n",
            " Loss S01:  0.027890474285657812\n",
            " Loss S2:  0.04805589848902167\n",
            " Loss S01:  0.027969696452138853\n",
            " Loss S2:  0.0477250802834503\n",
            " Loss S01:  0.028086394876425648\n",
            " Loss S2:  0.04752248042711505\n",
            "\n",
            "Epoch: 29\n",
            "Loss S01:  0.029524143785238266\n",
            "Loss S2:  0.03480153903365135\n",
            "Loss S01:  0.02842441878535531\n",
            "Loss S2:  0.03139644759622487\n",
            "Loss S01:  0.02837471007591202\n",
            "Loss S2:  0.03176434320353326\n",
            "Loss S01:  0.028571016966335235\n",
            "Loss S2:  0.03178417694664771\n",
            "Loss S01:  0.028811335881672253\n",
            "Loss S2:  0.03223779397766765\n",
            "Loss S01:  0.02908671490263705\n",
            "Loss S2:  0.03255994409761008\n",
            "Loss S01:  0.02898644393340486\n",
            "Loss S2:  0.032476783775892415\n",
            "Loss S01:  0.029062234311246535\n",
            "Loss S2:  0.03268167156148964\n",
            "Loss S01:  0.028812069974747705\n",
            "Loss S2:  0.03246121922576869\n",
            "Loss S01:  0.02875923664196507\n",
            "Loss S2:  0.032365122257353184\n",
            "Loss S01:  0.028684130639280422\n",
            "Loss S2:  0.032346023191319836\n",
            "Loss S01:  0.028696194207211874\n",
            "Loss S2:  0.03230702979406258\n",
            "Loss S01:  0.028763871162760357\n",
            "Loss S2:  0.032349732250344654\n",
            "Loss S01:  0.028789044052134942\n",
            "Loss S2:  0.03238831764755358\n",
            "Loss S01:  0.028831092746439555\n",
            "Loss S2:  0.03239986264800772\n",
            "Loss S01:  0.02889944795543784\n",
            "Loss S2:  0.03248313703383041\n",
            "Loss S01:  0.028909074478082775\n",
            "Loss S2:  0.03249033288278195\n",
            "Loss S01:  0.02893715041379134\n",
            "Loss S2:  0.03255606588651562\n",
            "Loss S01:  0.02897663852868818\n",
            "Loss S2:  0.03259526884687539\n",
            "Loss S01:  0.02894751130754411\n",
            "Loss S2:  0.032573435455560684\n",
            "Loss S01:  0.028902311239447167\n",
            "Loss S2:  0.03251791251498965\n",
            "Loss S01:  0.028884744386396138\n",
            "Loss S2:  0.03248182940186482\n",
            "Loss S01:  0.028899768696111792\n",
            "Loss S2:  0.032499408838233795\n",
            "Loss S01:  0.028894623878585313\n",
            "Loss S2:  0.03245724496548568\n",
            "Loss S01:  0.0289211344994079\n",
            "Loss S2:  0.032454111964994446\n",
            "Loss S01:  0.02892316930530318\n",
            "Loss S2:  0.032424255849355724\n",
            "Loss S01:  0.028927580913764305\n",
            "Loss S2:  0.032436938035762176\n",
            "Loss S01:  0.028924597406629268\n",
            "Loss S2:  0.032415341709592684\n",
            "Loss S01:  0.028924905519362446\n",
            "Loss S2:  0.03240501780553433\n",
            "Loss S01:  0.028931007416490018\n",
            "Loss S2:  0.03242586471165988\n",
            "Loss S01:  0.028953275615442236\n",
            "Loss S2:  0.03240709579193909\n",
            "Loss S01:  0.028920566459512787\n",
            "Loss S2:  0.03239719215121683\n",
            "Loss S01:  0.028893822542974883\n",
            "Loss S2:  0.03236789847622593\n",
            "Loss S01:  0.028872918112490473\n",
            "Loss S2:  0.03235368782885125\n",
            "Loss S01:  0.028880827538559865\n",
            "Loss S2:  0.032367764843786215\n",
            "Loss S01:  0.028893201301495235\n",
            "Loss S2:  0.03237722790733701\n",
            "Loss S01:  0.028906629031260917\n",
            "Loss S2:  0.03240042667412857\n",
            "Loss S01:  0.028889436766786396\n",
            "Loss S2:  0.0323733416438665\n",
            "Loss S01:  0.028874987570124036\n",
            "Loss S2:  0.03237947831394791\n",
            "Loss S01:  0.02886357018366799\n",
            "Loss S2:  0.0323490409652138\n",
            "Loss S01:  0.028852617670027097\n",
            "Loss S2:  0.03232354367444491\n",
            "Loss S01:  0.028851191329695013\n",
            "Loss S2:  0.032314748161103024\n",
            "Loss S01:  0.028855329229011954\n",
            "Loss S2:  0.032321666152771746\n",
            "Loss S01:  0.028835439795234917\n",
            "Loss S2:  0.03230865204192508\n",
            "Loss S01:  0.02882228957704541\n",
            "Loss S2:  0.03228859671828698\n",
            "Loss S01:  0.0288051753831478\n",
            "Loss S2:  0.03227892428421789\n",
            "Loss S01:  0.028804939203853203\n",
            "Loss S2:  0.03229608543524256\n",
            "Loss S01:  0.02881293971241786\n",
            "Loss S2:  0.032289481953166096\n",
            "Loss S01:  0.028813464326973764\n",
            "Loss S2:  0.03230434903592305\n",
            "Loss S01:  0.02879696650441941\n",
            "Loss S2:  0.03227338315862138\n",
            "Validation: \n",
            " Loss S01:  0.025759749114513397\n",
            " Loss S2:  0.04366963356733322\n",
            " Loss S01:  0.027461605944803784\n",
            " Loss S2:  0.04665028463516917\n",
            " Loss S01:  0.027490819009338936\n",
            " Loss S2:  0.04707418400339964\n",
            " Loss S01:  0.02752563824541256\n",
            " Loss S2:  0.046679213704144365\n",
            " Loss S01:  0.02763489275066941\n",
            " Loss S2:  0.04652927815914154\n",
            "\n",
            "Epoch: 30\n",
            "Loss S01:  0.031083665788173676\n",
            "Loss S2:  0.033802058547735214\n",
            "Loss S01:  0.02810168012299321\n",
            "Loss S2:  0.031092115931890228\n",
            "Loss S01:  0.028172671794891357\n",
            "Loss S2:  0.031759098083490415\n",
            "Loss S01:  0.028273689590634837\n",
            "Loss S2:  0.03167750620313229\n",
            "Loss S01:  0.028472630806812425\n",
            "Loss S2:  0.03181236505326701\n",
            "Loss S01:  0.028725997745698572\n",
            "Loss S2:  0.03201271524178047\n",
            "Loss S01:  0.028640078044817097\n",
            "Loss S2:  0.03196708189293009\n",
            "Loss S01:  0.028776805034615625\n",
            "Loss S2:  0.032127745528246314\n",
            "Loss S01:  0.028623511807786092\n",
            "Loss S2:  0.03195703878170914\n",
            "Loss S01:  0.028491295505683502\n",
            "Loss S2:  0.03185049493561734\n",
            "Loss S01:  0.02840133688163639\n",
            "Loss S2:  0.03176146623964357\n",
            "Loss S01:  0.028454511300534814\n",
            "Loss S2:  0.031737721348936494\n",
            "Loss S01:  0.02850744680863274\n",
            "Loss S2:  0.03181523189318081\n",
            "Loss S01:  0.028543679342242598\n",
            "Loss S2:  0.03183779945127837\n",
            "Loss S01:  0.028557232191376652\n",
            "Loss S2:  0.03184134708651414\n",
            "Loss S01:  0.02860236963540908\n",
            "Loss S2:  0.03191904935813108\n",
            "Loss S01:  0.028602448957306997\n",
            "Loss S2:  0.03192045565069832\n",
            "Loss S01:  0.02865172042484172\n",
            "Loss S2:  0.03203696777161799\n",
            "Loss S01:  0.028687867773336602\n",
            "Loss S2:  0.03211820825358122\n",
            "Loss S01:  0.028647212818733062\n",
            "Loss S2:  0.03207376665625897\n",
            "Loss S01:  0.028612444307006414\n",
            "Loss S2:  0.03205291491316919\n",
            "Loss S01:  0.028601374936188566\n",
            "Loss S2:  0.03209586755783072\n",
            "Loss S01:  0.02861134233416745\n",
            "Loss S2:  0.03207510606937818\n",
            "Loss S01:  0.02862114564100385\n",
            "Loss S2:  0.03204438984555341\n",
            "Loss S01:  0.028664006005307944\n",
            "Loss S2:  0.03208322370911782\n",
            "Loss S01:  0.02869200503891445\n",
            "Loss S2:  0.03208894942297166\n",
            "Loss S01:  0.028704362111415898\n",
            "Loss S2:  0.032120066784122436\n",
            "Loss S01:  0.028699243681787126\n",
            "Loss S2:  0.032083028487558736\n",
            "Loss S01:  0.028685763511422265\n",
            "Loss S2:  0.032059818983502235\n",
            "Loss S01:  0.02869041205169409\n",
            "Loss S2:  0.032065440366153455\n",
            "Loss S01:  0.028657725773925006\n",
            "Loss S2:  0.032038450711391296\n",
            "Loss S01:  0.0286312721418127\n",
            "Loss S2:  0.032012024487090264\n",
            "Loss S01:  0.028592443122670657\n",
            "Loss S2:  0.031986905540400574\n",
            "Loss S01:  0.028607154495766517\n",
            "Loss S2:  0.03198981865835334\n",
            "Loss S01:  0.0285908296933709\n",
            "Loss S2:  0.03197557556161608\n",
            "Loss S01:  0.02858943570182364\n",
            "Loss S2:  0.031970281886239335\n",
            "Loss S01:  0.02857999800294225\n",
            "Loss S2:  0.031957346791210595\n",
            "Loss S01:  0.028563752082600747\n",
            "Loss S2:  0.031922443752300064\n",
            "Loss S01:  0.028554878742715194\n",
            "Loss S2:  0.0319242106794607\n",
            "Loss S01:  0.02854148368529804\n",
            "Loss S2:  0.031889528317181656\n",
            "Loss S01:  0.028548422146430635\n",
            "Loss S2:  0.03189135249007372\n",
            "Loss S01:  0.02856103397017558\n",
            "Loss S2:  0.03189687739486677\n",
            "Loss S01:  0.02858496417076882\n",
            "Loss S2:  0.031897456860811295\n",
            "Loss S01:  0.028550812023802035\n",
            "Loss S2:  0.03186457641336315\n",
            "Loss S01:  0.028545117829324437\n",
            "Loss S2:  0.03185705932663952\n",
            "Loss S01:  0.028542348999373135\n",
            "Loss S2:  0.03186083969530007\n",
            "Loss S01:  0.028544270738539105\n",
            "Loss S2:  0.03187805408543206\n",
            "Loss S01:  0.02855497125670662\n",
            "Loss S2:  0.03187309877428912\n",
            "Loss S01:  0.028563383269644575\n",
            "Loss S2:  0.03188757640222502\n",
            "Loss S01:  0.02853778876409754\n",
            "Loss S2:  0.03186200308383968\n",
            "Validation: \n",
            " Loss S01:  0.026850013062357903\n",
            " Loss S2:  0.04418081045150757\n",
            " Loss S01:  0.028334227701028187\n",
            " Loss S2:  0.04666957951017788\n",
            " Loss S01:  0.02814847135507479\n",
            " Loss S2:  0.047125581561065305\n",
            " Loss S01:  0.028148310930758227\n",
            " Loss S2:  0.046748807378968256\n",
            " Loss S01:  0.02830985494325926\n",
            " Loss S2:  0.04660825127804721\n",
            "\n",
            "Epoch: 31\n",
            "Loss S01:  0.02996339462697506\n",
            "Loss S2:  0.032963432371616364\n",
            "Loss S01:  0.027804422784935345\n",
            "Loss S2:  0.03051601993766698\n",
            "Loss S01:  0.02798920293294248\n",
            "Loss S2:  0.03134388112950893\n",
            "Loss S01:  0.028139314644279018\n",
            "Loss S2:  0.03137244626639351\n",
            "Loss S01:  0.02827408128395313\n",
            "Loss S2:  0.031566525650460545\n",
            "Loss S01:  0.028449467657243505\n",
            "Loss S2:  0.03157225864775041\n",
            "Loss S01:  0.02837586094487886\n",
            "Loss S2:  0.03150385383089058\n",
            "Loss S01:  0.028419391658734267\n",
            "Loss S2:  0.03165519150944663\n",
            "Loss S01:  0.028247724850604564\n",
            "Loss S2:  0.0314888508507499\n",
            "Loss S01:  0.028170346886247068\n",
            "Loss S2:  0.03148343309678219\n",
            "Loss S01:  0.028110883152573415\n",
            "Loss S2:  0.031471073295515364\n",
            "Loss S01:  0.0281360008091003\n",
            "Loss S2:  0.03149400539024993\n",
            "Loss S01:  0.0281690892817314\n",
            "Loss S2:  0.03153458099104156\n",
            "Loss S01:  0.02818006140585164\n",
            "Loss S2:  0.031553820273234646\n",
            "Loss S01:  0.02820549045312912\n",
            "Loss S2:  0.031557455222974436\n",
            "Loss S01:  0.02822530297994219\n",
            "Loss S2:  0.03161306628673677\n",
            "Loss S01:  0.028218071213605242\n",
            "Loss S2:  0.03158922639230023\n",
            "Loss S01:  0.028256144516213597\n",
            "Loss S2:  0.031659974460626204\n",
            "Loss S01:  0.02829341548211996\n",
            "Loss S2:  0.03173418344096255\n",
            "Loss S01:  0.02829325597011606\n",
            "Loss S2:  0.0317402058500426\n",
            "Loss S01:  0.028252943562556854\n",
            "Loss S2:  0.031713665782738085\n",
            "Loss S01:  0.028234102034950144\n",
            "Loss S2:  0.031718049136595136\n",
            "Loss S01:  0.02825323039691103\n",
            "Loss S2:  0.0317396970878764\n",
            "Loss S01:  0.028240980209532755\n",
            "Loss S2:  0.03169496143760877\n",
            "Loss S01:  0.028266708540471263\n",
            "Loss S2:  0.031678556963121246\n",
            "Loss S01:  0.028284725840704374\n",
            "Loss S2:  0.031645444225505054\n",
            "Loss S01:  0.028292052626210155\n",
            "Loss S2:  0.031644932497506856\n",
            "Loss S01:  0.028276409678903453\n",
            "Loss S2:  0.03160603952721256\n",
            "Loss S01:  0.02826657464389699\n",
            "Loss S2:  0.03158056976611716\n",
            "Loss S01:  0.028257676473853924\n",
            "Loss S2:  0.031607400542733186\n",
            "Loss S01:  0.02825074101430039\n",
            "Loss S2:  0.031595823952179414\n",
            "Loss S01:  0.028245290879076318\n",
            "Loss S2:  0.031582140684175725\n",
            "Loss S01:  0.028223819270544334\n",
            "Loss S2:  0.03156225273611946\n",
            "Loss S01:  0.028233836233345765\n",
            "Loss S2:  0.03156238722297\n",
            "Loss S01:  0.028214596932934176\n",
            "Loss S2:  0.0315481448311173\n",
            "Loss S01:  0.02822687878589175\n",
            "Loss S2:  0.03157220328189539\n",
            "Loss S01:  0.028264883797295866\n",
            "Loss S2:  0.031599481768101206\n",
            "Loss S01:  0.028247013416974692\n",
            "Loss S2:  0.03156420138646972\n",
            "Loss S01:  0.028243539874439477\n",
            "Loss S2:  0.03156289062279416\n",
            "Loss S01:  0.02822267427526014\n",
            "Loss S2:  0.03154396770707787\n",
            "Loss S01:  0.02822059504873586\n",
            "Loss S2:  0.0315536742755897\n",
            "Loss S01:  0.028235761688936076\n",
            "Loss S2:  0.03154580487695199\n",
            "Loss S01:  0.028235944023360268\n",
            "Loss S2:  0.03156838282464944\n",
            "Loss S01:  0.028218720821348256\n",
            "Loss S2:  0.03155615796570573\n",
            "Loss S01:  0.028224254892864464\n",
            "Loss S2:  0.03156323894402202\n",
            "Loss S01:  0.0282158827875842\n",
            "Loss S2:  0.03156859497488074\n",
            "Loss S01:  0.028212455107371873\n",
            "Loss S2:  0.03157442572189256\n",
            "Loss S01:  0.028201737213729546\n",
            "Loss S2:  0.031557039019598325\n",
            "Loss S01:  0.0282048987739733\n",
            "Loss S2:  0.0315740312335397\n",
            "Loss S01:  0.028182942355753447\n",
            "Loss S2:  0.03154358353518905\n",
            "Validation: \n",
            " Loss S01:  0.026360511779785156\n",
            " Loss S2:  0.044317856431007385\n",
            " Loss S01:  0.027927750100692112\n",
            " Loss S2:  0.0477924600598358\n",
            " Loss S01:  0.02775695165846406\n",
            " Loss S2:  0.04813584721669918\n",
            " Loss S01:  0.027740015358221334\n",
            " Loss S2:  0.0478058006675517\n",
            " Loss S01:  0.027815906100986915\n",
            " Loss S2:  0.04766073818376035\n",
            "\n",
            "Epoch: 32\n",
            "Loss S01:  0.029034418985247612\n",
            "Loss S2:  0.03425977751612663\n",
            "Loss S01:  0.027380162511359562\n",
            "Loss S2:  0.03097996081818234\n",
            "Loss S01:  0.02761543293793996\n",
            "Loss S2:  0.031219548324034327\n",
            "Loss S01:  0.027826754496462883\n",
            "Loss S2:  0.030998647453323487\n",
            "Loss S01:  0.027931749002962577\n",
            "Loss S2:  0.031135522101710483\n",
            "Loss S01:  0.02812338379376075\n",
            "Loss S2:  0.03128075541234484\n",
            "Loss S01:  0.02806817596686668\n",
            "Loss S2:  0.031230387140492925\n",
            "Loss S01:  0.02813194311735496\n",
            "Loss S2:  0.031459561299899935\n",
            "Loss S01:  0.02795314400192396\n",
            "Loss S2:  0.0313068313041219\n",
            "Loss S01:  0.02786877943755506\n",
            "Loss S2:  0.03126344144098706\n",
            "Loss S01:  0.02778232711745371\n",
            "Loss S2:  0.031205128491072372\n",
            "Loss S01:  0.02780714678066271\n",
            "Loss S2:  0.031228614551527006\n",
            "Loss S01:  0.0278634943745353\n",
            "Loss S2:  0.03123422396515519\n",
            "Loss S01:  0.027915640769682767\n",
            "Loss S2:  0.031257789567795416\n",
            "Loss S01:  0.027918381325847712\n",
            "Loss S2:  0.031240567340390057\n",
            "Loss S01:  0.027990694250294705\n",
            "Loss S2:  0.03133587859916371\n",
            "Loss S01:  0.02797686723957521\n",
            "Loss S2:  0.031373348017657024\n",
            "Loss S01:  0.02801756126185258\n",
            "Loss S2:  0.03142892903708226\n",
            "Loss S01:  0.028023022818713557\n",
            "Loss S2:  0.03148117419640992\n",
            "Loss S01:  0.02799043589861605\n",
            "Loss S2:  0.031427327767794666\n",
            "Loss S01:  0.027922016276574847\n",
            "Loss S2:  0.03135601344022585\n",
            "Loss S01:  0.027891328470048746\n",
            "Loss S2:  0.03135734220957869\n",
            "Loss S01:  0.027930293004639548\n",
            "Loss S2:  0.0313661158236578\n",
            "Loss S01:  0.027915277119193758\n",
            "Loss S2:  0.03136654541670502\n",
            "Loss S01:  0.027917120860758163\n",
            "Loss S2:  0.03135597028023722\n",
            "Loss S01:  0.02792185786146804\n",
            "Loss S2:  0.031312498423564004\n",
            "Loss S01:  0.027948843336653435\n",
            "Loss S2:  0.03133507940732656\n",
            "Loss S01:  0.027944470470516885\n",
            "Loss S2:  0.03131387183422092\n",
            "Loss S01:  0.027946930325646418\n",
            "Loss S2:  0.031286082704455404\n",
            "Loss S01:  0.027957700820210873\n",
            "Loss S2:  0.03130794594326789\n",
            "Loss S01:  0.027974410589004673\n",
            "Loss S2:  0.03129628680002848\n",
            "Loss S01:  0.02797245315704315\n",
            "Loss S2:  0.03127943182969017\n",
            "Loss S01:  0.02793480484888561\n",
            "Loss S2:  0.03125280507396315\n",
            "Loss S01:  0.027950637498335897\n",
            "Loss S2:  0.03126019658661501\n",
            "Loss S01:  0.02795036923129363\n",
            "Loss S2:  0.031252250637957425\n",
            "Loss S01:  0.027960430103701743\n",
            "Loss S2:  0.03129381024705888\n",
            "Loss S01:  0.02796161929119657\n",
            "Loss S2:  0.03133309918902092\n",
            "Loss S01:  0.02793871100258956\n",
            "Loss S2:  0.03131006417590171\n",
            "Loss S01:  0.027912367301506633\n",
            "Loss S2:  0.03130947033877135\n",
            "Loss S01:  0.027884428615650863\n",
            "Loss S2:  0.031273627310724514\n",
            "Loss S01:  0.027886647494170733\n",
            "Loss S2:  0.031267258062252674\n",
            "Loss S01:  0.02788145588189726\n",
            "Loss S2:  0.03126043355468561\n",
            "Loss S01:  0.027891339548374583\n",
            "Loss S2:  0.031257683482029945\n",
            "Loss S01:  0.02787506736797689\n",
            "Loss S2:  0.031248777179942053\n",
            "Loss S01:  0.027882684440333014\n",
            "Loss S2:  0.03125164093546857\n",
            "Loss S01:  0.02788352163843067\n",
            "Loss S2:  0.03126587096742146\n",
            "Loss S01:  0.027892959253648358\n",
            "Loss S2:  0.03128442354904338\n",
            "Loss S01:  0.027901070833111264\n",
            "Loss S2:  0.03127931617595841\n",
            "Loss S01:  0.02790187063032525\n",
            "Loss S2:  0.031291256540623376\n",
            "Loss S01:  0.027873349051672183\n",
            "Loss S2:  0.03126157035713526\n",
            "Validation: \n",
            " Loss S01:  0.02533610537648201\n",
            " Loss S2:  0.04319010302424431\n",
            " Loss S01:  0.02630159595892543\n",
            " Loss S2:  0.04587439057372865\n",
            " Loss S01:  0.026209205570744305\n",
            " Loss S2:  0.04625818424108552\n",
            " Loss S01:  0.026273193448537686\n",
            " Loss S2:  0.04593782328435632\n",
            " Loss S01:  0.026370543250699104\n",
            " Loss S2:  0.04573678427640303\n",
            "\n",
            "Epoch: 33\n",
            "Loss S01:  0.029485182836651802\n",
            "Loss S2:  0.031697988510131836\n",
            "Loss S01:  0.02715470709583976\n",
            "Loss S2:  0.03057286465032534\n",
            "Loss S01:  0.02707332887110256\n",
            "Loss S2:  0.030992081034041587\n",
            "Loss S01:  0.02729270180627223\n",
            "Loss S2:  0.031020281055281238\n",
            "Loss S01:  0.027385752284672202\n",
            "Loss S2:  0.031149858199968572\n",
            "Loss S01:  0.02768264919081155\n",
            "Loss S2:  0.03133190719082075\n",
            "Loss S01:  0.027648234312407306\n",
            "Loss S2:  0.031206460058933398\n",
            "Loss S01:  0.027759569788902577\n",
            "Loss S2:  0.031278091181122084\n",
            "Loss S01:  0.027611057952414326\n",
            "Loss S2:  0.031076018778998175\n",
            "Loss S01:  0.027462178165768528\n",
            "Loss S2:  0.031018192808215436\n",
            "Loss S01:  0.027385627297629223\n",
            "Loss S2:  0.030882231135031964\n",
            "Loss S01:  0.027413551234178716\n",
            "Loss S2:  0.030915648013621837\n",
            "Loss S01:  0.02748098235177107\n",
            "Loss S2:  0.030930775641040367\n",
            "Loss S01:  0.02754543541098824\n",
            "Loss S2:  0.03093752826613779\n",
            "Loss S01:  0.027570491062199817\n",
            "Loss S2:  0.030940243591230814\n",
            "Loss S01:  0.027618965257378603\n",
            "Loss S2:  0.030997736342498007\n",
            "Loss S01:  0.027590986121422755\n",
            "Loss S2:  0.030947948163895872\n",
            "Loss S01:  0.027650063497978345\n",
            "Loss S2:  0.031012077810384377\n",
            "Loss S01:  0.027676320783999744\n",
            "Loss S2:  0.031065191929511602\n",
            "Loss S01:  0.027657354133525443\n",
            "Loss S2:  0.0310030991981947\n",
            "Loss S01:  0.027601354435753466\n",
            "Loss S2:  0.03097746718619297\n",
            "Loss S01:  0.02758479952565019\n",
            "Loss S2:  0.030982854462700996\n",
            "Loss S01:  0.02760152796038945\n",
            "Loss S2:  0.030993304716852996\n",
            "Loss S01:  0.027585425329479305\n",
            "Loss S2:  0.03096178683625671\n",
            "Loss S01:  0.027607205072688365\n",
            "Loss S2:  0.030968531788446597\n",
            "Loss S01:  0.02756481771301701\n",
            "Loss S2:  0.030929684305036686\n",
            "Loss S01:  0.02758593539710931\n",
            "Loss S2:  0.030941808753257968\n",
            "Loss S01:  0.027558569682975098\n",
            "Loss S2:  0.03092342610316303\n",
            "Loss S01:  0.027563301760212808\n",
            "Loss S2:  0.03092139442302154\n",
            "Loss S01:  0.027573875103568294\n",
            "Loss S2:  0.030945201497204936\n",
            "Loss S01:  0.027596149253241248\n",
            "Loss S2:  0.030949146050551404\n",
            "Loss S01:  0.02759399565110444\n",
            "Loss S2:  0.030942767960321863\n",
            "Loss S01:  0.027577814469084933\n",
            "Loss S2:  0.030912902020498227\n",
            "Loss S01:  0.027577724335912133\n",
            "Loss S2:  0.030919906500378045\n",
            "Loss S01:  0.027573516914659233\n",
            "Loss S2:  0.0309164955873381\n",
            "Loss S01:  0.027595778561045982\n",
            "Loss S2:  0.030954727611140986\n",
            "Loss S01:  0.027610734373413624\n",
            "Loss S2:  0.03097706959373898\n",
            "Loss S01:  0.02758992848932904\n",
            "Loss S2:  0.030947251742419527\n",
            "Loss S01:  0.02757339869818975\n",
            "Loss S2:  0.030937181849257527\n",
            "Loss S01:  0.02754776638067897\n",
            "Loss S2:  0.030908581817432133\n",
            "Loss S01:  0.027557408042307805\n",
            "Loss S2:  0.030909361243433787\n",
            "Loss S01:  0.027570547060157262\n",
            "Loss S2:  0.030913801786745843\n",
            "Loss S01:  0.027589480686853164\n",
            "Loss S2:  0.030915776780870634\n",
            "Loss S01:  0.02757457698632697\n",
            "Loss S2:  0.030910467158510348\n",
            "Loss S01:  0.027576301191973577\n",
            "Loss S2:  0.030910371559885352\n",
            "Loss S01:  0.027556456691542115\n",
            "Loss S2:  0.030904952316914588\n",
            "Loss S01:  0.027568437373205017\n",
            "Loss S2:  0.030925783609522913\n",
            "Loss S01:  0.027570903202372498\n",
            "Loss S2:  0.03091721473985417\n",
            "Loss S01:  0.027578037638767098\n",
            "Loss S2:  0.030922681859959684\n",
            "Loss S01:  0.02754847336628772\n",
            "Loss S2:  0.03089640731496505\n",
            "Validation: \n",
            " Loss S01:  0.025540539994835854\n",
            " Loss S2:  0.04317277669906616\n",
            " Loss S01:  0.026697267911263874\n",
            " Loss S2:  0.04559842266497158\n",
            " Loss S01:  0.026449731346674082\n",
            " Loss S2:  0.0460142689507182\n",
            " Loss S01:  0.026485246804649712\n",
            " Loss S2:  0.04565285579835782\n",
            " Loss S01:  0.026612860001163717\n",
            " Loss S2:  0.04558171170913143\n",
            "\n",
            "Epoch: 34\n",
            "Loss S01:  0.029162468388676643\n",
            "Loss S2:  0.03306238353252411\n",
            "Loss S01:  0.02658581801436164\n",
            "Loss S2:  0.02957922440360893\n",
            "Loss S01:  0.026642970829492525\n",
            "Loss S2:  0.030115822684906778\n",
            "Loss S01:  0.02684524835598084\n",
            "Loss S2:  0.03021915214917352\n",
            "Loss S01:  0.02694193205637176\n",
            "Loss S2:  0.030389071374041277\n",
            "Loss S01:  0.02714501204443913\n",
            "Loss S2:  0.030472166909306656\n",
            "Loss S01:  0.027165000372734227\n",
            "Loss S2:  0.03043016571490491\n",
            "Loss S01:  0.02735761193637277\n",
            "Loss S2:  0.030605490253844733\n",
            "Loss S01:  0.0272000804313539\n",
            "Loss S2:  0.030504779039341727\n",
            "Loss S01:  0.02714042365550995\n",
            "Loss S2:  0.03044368719661629\n",
            "Loss S01:  0.02714440093772246\n",
            "Loss S2:  0.030442362927859373\n",
            "Loss S01:  0.027147680218960787\n",
            "Loss S2:  0.030512736640400714\n",
            "Loss S01:  0.027189792064595812\n",
            "Loss S2:  0.030572348805375335\n",
            "Loss S01:  0.027201412018132573\n",
            "Loss S2:  0.03056996064047322\n",
            "Loss S01:  0.027248073171110863\n",
            "Loss S2:  0.030584876947369135\n",
            "Loss S01:  0.02728977645223109\n",
            "Loss S2:  0.030646066717951503\n",
            "Loss S01:  0.027264263156963432\n",
            "Loss S2:  0.030622776146538508\n",
            "Loss S01:  0.027306900321565873\n",
            "Loss S2:  0.030686788936281763\n",
            "Loss S01:  0.027349753120080544\n",
            "Loss S2:  0.030788321994780175\n",
            "Loss S01:  0.02731186232446688\n",
            "Loss S2:  0.03070601995507772\n",
            "Loss S01:  0.027281692492502247\n",
            "Loss S2:  0.030685344720212976\n",
            "Loss S01:  0.0272874876380108\n",
            "Loss S2:  0.03069792730265884\n",
            "Loss S01:  0.027282423150148328\n",
            "Loss S2:  0.030710710473864325\n",
            "Loss S01:  0.027288725188413222\n",
            "Loss S2:  0.030687797532388657\n",
            "Loss S01:  0.02732434062696839\n",
            "Loss S2:  0.030708215319207595\n",
            "Loss S01:  0.02735441527607669\n",
            "Loss S2:  0.030706259062090718\n",
            "Loss S01:  0.02736945373230967\n",
            "Loss S2:  0.030724981100337716\n",
            "Loss S01:  0.02735342319051278\n",
            "Loss S2:  0.030709557475803964\n",
            "Loss S01:  0.02735082530068545\n",
            "Loss S2:  0.03072396297259687\n",
            "Loss S01:  0.027361772461599092\n",
            "Loss S2:  0.030750709269003768\n",
            "Loss S01:  0.027358518789624454\n",
            "Loss S2:  0.030736665962344388\n",
            "Loss S01:  0.02734482206907303\n",
            "Loss S2:  0.030711046141539355\n",
            "Loss S01:  0.027327049570013057\n",
            "Loss S2:  0.030690857165745485\n",
            "Loss S01:  0.02731589572869939\n",
            "Loss S2:  0.030704677099322623\n",
            "Loss S01:  0.02730403711631501\n",
            "Loss S2:  0.030670990987170126\n",
            "Loss S01:  0.02732953147967996\n",
            "Loss S2:  0.030703905906177994\n",
            "Loss S01:  0.02736630358854489\n",
            "Loss S2:  0.030721600077158857\n",
            "Loss S01:  0.02735755863697381\n",
            "Loss S2:  0.030699341625698172\n",
            "Loss S01:  0.027341550167190434\n",
            "Loss S2:  0.030692053098339108\n",
            "Loss S01:  0.02732254090287801\n",
            "Loss S2:  0.030659633424237866\n",
            "Loss S01:  0.027327957407494732\n",
            "Loss S2:  0.03066186775962016\n",
            "Loss S01:  0.02732852584435412\n",
            "Loss S2:  0.030653888853652052\n",
            "Loss S01:  0.027350054328239164\n",
            "Loss S2:  0.0306660071094996\n",
            "Loss S01:  0.02732916781786838\n",
            "Loss S2:  0.030657937936052645\n",
            "Loss S01:  0.027329697567217745\n",
            "Loss S2:  0.030651381150597616\n",
            "Loss S01:  0.027334417404396308\n",
            "Loss S2:  0.03065629287348222\n",
            "Loss S01:  0.02733319306871379\n",
            "Loss S2:  0.03065764693515032\n",
            "Loss S01:  0.02732387354356215\n",
            "Loss S2:  0.03064984815578805\n",
            "Loss S01:  0.027328830967481073\n",
            "Loss S2:  0.03066911387669693\n",
            "Loss S01:  0.027310557064055913\n",
            "Loss S2:  0.030648233129997603\n",
            "Validation: \n",
            " Loss S01:  0.025405284017324448\n",
            " Loss S2:  0.044842422008514404\n",
            " Loss S01:  0.027071295128691764\n",
            " Loss S2:  0.04644330857055528\n",
            " Loss S01:  0.027080516489904102\n",
            " Loss S2:  0.04678576521393729\n",
            " Loss S01:  0.02703069287856094\n",
            " Loss S2:  0.04644248170442269\n",
            " Loss S01:  0.027176307744266076\n",
            " Loss S2:  0.046351363951409305\n",
            "\n",
            "Epoch: 35\n",
            "Loss S01:  0.02903425134718418\n",
            "Loss S2:  0.0314512625336647\n",
            "Loss S01:  0.026426257396286183\n",
            "Loss S2:  0.029554962773214687\n",
            "Loss S01:  0.026505384328109876\n",
            "Loss S2:  0.030099095687979742\n",
            "Loss S01:  0.02661014690754875\n",
            "Loss S2:  0.029898994391964327\n",
            "Loss S01:  0.02685929039811216\n",
            "Loss S2:  0.030078767957847294\n",
            "Loss S01:  0.027096074220596577\n",
            "Loss S2:  0.03038381171577117\n",
            "Loss S01:  0.027083716431602102\n",
            "Loss S2:  0.030377471422562835\n",
            "Loss S01:  0.027130645765385156\n",
            "Loss S2:  0.03052510147992994\n",
            "Loss S01:  0.027011474686456316\n",
            "Loss S2:  0.030359488992411414\n",
            "Loss S01:  0.02704039975427664\n",
            "Loss S2:  0.030330291672886072\n",
            "Loss S01:  0.026991268859641388\n",
            "Loss S2:  0.03026561003805387\n",
            "Loss S01:  0.027002071441562327\n",
            "Loss S2:  0.03030514213684443\n",
            "Loss S01:  0.027036237692044787\n",
            "Loss S2:  0.030346440074365003\n",
            "Loss S01:  0.027063277230121707\n",
            "Loss S2:  0.030366992748534407\n",
            "Loss S01:  0.027111498810720783\n",
            "Loss S2:  0.030404917327753196\n",
            "Loss S01:  0.02716522616957197\n",
            "Loss S2:  0.030450746910461526\n",
            "Loss S01:  0.027182261419037113\n",
            "Loss S2:  0.030435009344671825\n",
            "Loss S01:  0.027230515956268672\n",
            "Loss S2:  0.0305300980958848\n",
            "Loss S01:  0.027256224304437637\n",
            "Loss S2:  0.030594060876669146\n",
            "Loss S01:  0.02721646327661906\n",
            "Loss S2:  0.030563775879082256\n",
            "Loss S01:  0.027178299080450736\n",
            "Loss S2:  0.030522722621165697\n",
            "Loss S01:  0.0271835379055326\n",
            "Loss S2:  0.030534840917205923\n",
            "Loss S01:  0.027181048114779848\n",
            "Loss S2:  0.030546389343174874\n",
            "Loss S01:  0.027147945460109485\n",
            "Loss S2:  0.03053016457464788\n",
            "Loss S01:  0.027182910289940004\n",
            "Loss S2:  0.030555444541561158\n",
            "Loss S01:  0.02717544798149293\n",
            "Loss S2:  0.030518954760762326\n",
            "Loss S01:  0.02717815215388934\n",
            "Loss S2:  0.030523894554525043\n",
            "Loss S01:  0.027155323916310755\n",
            "Loss S2:  0.030495633092808547\n",
            "Loss S01:  0.027147665003387528\n",
            "Loss S2:  0.03047696219299824\n",
            "Loss S01:  0.02714696765918912\n",
            "Loss S2:  0.030484633347422806\n",
            "Loss S01:  0.027162797569585007\n",
            "Loss S2:  0.03047727286642374\n",
            "Loss S01:  0.02714013583790451\n",
            "Loss S2:  0.03046798118512922\n",
            "Loss S01:  0.02711601574496131\n",
            "Loss S2:  0.03043814972507248\n",
            "Loss S01:  0.02712769786126664\n",
            "Loss S2:  0.030439911477740438\n",
            "Loss S01:  0.02712173420728302\n",
            "Loss S2:  0.030425080002621473\n",
            "Loss S01:  0.027119171367878586\n",
            "Loss S2:  0.030434279217042476\n",
            "Loss S01:  0.027141287004435823\n",
            "Loss S2:  0.030458238109987528\n",
            "Loss S01:  0.027136618201181573\n",
            "Loss S2:  0.03042255036251725\n",
            "Loss S01:  0.027132147800891106\n",
            "Loss S2:  0.030414173531094248\n",
            "Loss S01:  0.027113553336666672\n",
            "Loss S2:  0.03038233232296184\n",
            "Loss S01:  0.0271055842993637\n",
            "Loss S2:  0.030373721929299564\n",
            "Loss S01:  0.027113265600372694\n",
            "Loss S2:  0.03038992606774154\n",
            "Loss S01:  0.027132507210106294\n",
            "Loss S2:  0.030405135515572907\n",
            "Loss S01:  0.027124783006905125\n",
            "Loss S2:  0.030384826527912766\n",
            "Loss S01:  0.02712151406083653\n",
            "Loss S2:  0.03038230613127452\n",
            "Loss S01:  0.027112048532309925\n",
            "Loss S2:  0.030392852766716032\n",
            "Loss S01:  0.027116847012410194\n",
            "Loss S2:  0.03040779278959085\n",
            "Loss S01:  0.02711393141964818\n",
            "Loss S2:  0.030401209017337002\n",
            "Loss S01:  0.02711728969956758\n",
            "Loss S2:  0.030417705895747067\n",
            "Loss S01:  0.0270925962239079\n",
            "Loss S2:  0.030387133689950052\n",
            "Validation: \n",
            " Loss S01:  0.02532239444553852\n",
            " Loss S2:  0.04238147661089897\n",
            " Loss S01:  0.02664850643348126\n",
            " Loss S2:  0.04488267678590048\n",
            " Loss S01:  0.026443489895361227\n",
            " Loss S2:  0.045210714474683854\n",
            " Loss S01:  0.026428282108218945\n",
            " Loss S2:  0.04486162209364235\n",
            " Loss S01:  0.02654288863234314\n",
            " Loss S2:  0.04475463618650848\n",
            "\n",
            "Epoch: 36\n",
            "Loss S01:  0.028374556452035904\n",
            "Loss S2:  0.03220872953534126\n",
            "Loss S01:  0.02570523186163469\n",
            "Loss S2:  0.02922451411458579\n",
            "Loss S01:  0.026001844821231707\n",
            "Loss S2:  0.029765288212469647\n",
            "Loss S01:  0.02627961055165337\n",
            "Loss S2:  0.02965645576196332\n",
            "Loss S01:  0.026492355027940215\n",
            "Loss S2:  0.030036952800867035\n",
            "Loss S01:  0.026706211414991642\n",
            "Loss S2:  0.03019247894339702\n",
            "Loss S01:  0.026743901313328353\n",
            "Loss S2:  0.03011570476972666\n",
            "Loss S01:  0.026866289196719587\n",
            "Loss S2:  0.030333835579140087\n",
            "Loss S01:  0.02678597247067057\n",
            "Loss S2:  0.030126388771114527\n",
            "Loss S01:  0.026764927800376336\n",
            "Loss S2:  0.030061113535539134\n",
            "Loss S01:  0.026739499006088418\n",
            "Loss S2:  0.030054794937962353\n",
            "Loss S01:  0.026756991993065353\n",
            "Loss S2:  0.03008321036693749\n",
            "Loss S01:  0.026755678629087024\n",
            "Loss S2:  0.03010951679908047\n",
            "Loss S01:  0.026778663856951334\n",
            "Loss S2:  0.030168853685942316\n",
            "Loss S01:  0.02681806353284112\n",
            "Loss S2:  0.03015305848250575\n",
            "Loss S01:  0.026848107625711833\n",
            "Loss S2:  0.03018957000250453\n",
            "Loss S01:  0.02686330943327883\n",
            "Loss S2:  0.030208692729750776\n",
            "Loss S01:  0.026927134392467158\n",
            "Loss S2:  0.030280473941599415\n",
            "Loss S01:  0.026956386884767047\n",
            "Loss S2:  0.030323035459327435\n",
            "Loss S01:  0.02691443167938924\n",
            "Loss S2:  0.030254640738377397\n",
            "Loss S01:  0.026866112960808312\n",
            "Loss S2:  0.03021883769007168\n",
            "Loss S01:  0.026862157614663314\n",
            "Loss S2:  0.03026156511457893\n",
            "Loss S01:  0.026882125848558692\n",
            "Loss S2:  0.030288477575751992\n",
            "Loss S01:  0.026877040067663442\n",
            "Loss S2:  0.030252066073995647\n",
            "Loss S01:  0.02690668475980333\n",
            "Loss S2:  0.03024845227939212\n",
            "Loss S01:  0.026883960534555027\n",
            "Loss S2:  0.030208446404551604\n",
            "Loss S01:  0.026911783335423562\n",
            "Loss S2:  0.030229722752710412\n",
            "Loss S01:  0.026916500064114803\n",
            "Loss S2:  0.03023148176167504\n",
            "Loss S01:  0.02692594047755109\n",
            "Loss S2:  0.030229075642773266\n",
            "Loss S01:  0.02692043536128252\n",
            "Loss S2:  0.03024072358160699\n",
            "Loss S01:  0.02691573921925206\n",
            "Loss S2:  0.030222326795573646\n",
            "Loss S01:  0.026890420148274908\n",
            "Loss S2:  0.030220610805311004\n",
            "Loss S01:  0.026851934842138648\n",
            "Loss S2:  0.03019285977838381\n",
            "Loss S01:  0.026860793613550166\n",
            "Loss S2:  0.030202948260388347\n",
            "Loss S01:  0.026873309269023083\n",
            "Loss S2:  0.03017831027311139\n",
            "Loss S01:  0.02689297257410495\n",
            "Loss S2:  0.030200038625304175\n",
            "Loss S01:  0.026902506587165215\n",
            "Loss S2:  0.030221109967317607\n",
            "Loss S01:  0.026879543644398693\n",
            "Loss S2:  0.03018574570267991\n",
            "Loss S01:  0.02684490254095064\n",
            "Loss S2:  0.03018479762486429\n",
            "Loss S01:  0.026819074864658857\n",
            "Loss S2:  0.03015168706226684\n",
            "Loss S01:  0.026833540859848185\n",
            "Loss S2:  0.030146105396108437\n",
            "Loss S01:  0.026839741390116894\n",
            "Loss S2:  0.03014449760716617\n",
            "Loss S01:  0.026858434775698214\n",
            "Loss S2:  0.030159163578566066\n",
            "Loss S01:  0.026838906105011356\n",
            "Loss S2:  0.03014135918646982\n",
            "Loss S01:  0.026834655667141993\n",
            "Loss S2:  0.030121281937339137\n",
            "Loss S01:  0.026824604100372205\n",
            "Loss S2:  0.030119158319319432\n",
            "Loss S01:  0.026818452120118182\n",
            "Loss S2:  0.030128336699433027\n",
            "Loss S01:  0.026817663361154293\n",
            "Loss S2:  0.030122362002097624\n",
            "Loss S01:  0.026818640956078152\n",
            "Loss S2:  0.030129237940802148\n",
            "Loss S01:  0.02679921869120263\n",
            "Loss S2:  0.030103281838345188\n",
            "Validation: \n",
            " Loss S01:  0.025197459384799004\n",
            " Loss S2:  0.04430471360683441\n",
            " Loss S01:  0.02729031310549804\n",
            " Loss S2:  0.04632118805533364\n",
            " Loss S01:  0.027219784105332885\n",
            " Loss S2:  0.046662744770689706\n",
            " Loss S01:  0.027247650548815727\n",
            " Loss S2:  0.046194377614826455\n",
            " Loss S01:  0.027356721736766672\n",
            " Loss S2:  0.04606102485163712\n",
            "\n",
            "Epoch: 37\n",
            "Loss S01:  0.028450902551412582\n",
            "Loss S2:  0.031711407005786896\n",
            "Loss S01:  0.026308102025227112\n",
            "Loss S2:  0.02877214601771398\n",
            "Loss S01:  0.026386516434805735\n",
            "Loss S2:  0.029203810241250766\n",
            "Loss S01:  0.026309777652063677\n",
            "Loss S2:  0.029342531737300656\n",
            "Loss S01:  0.026366002312520655\n",
            "Loss S2:  0.029480010634515344\n",
            "Loss S01:  0.026533546948841975\n",
            "Loss S2:  0.02966932587179483\n",
            "Loss S01:  0.026478021787326844\n",
            "Loss S2:  0.02966912444986281\n",
            "Loss S01:  0.02657887239901113\n",
            "Loss S2:  0.029774998826250225\n",
            "Loss S01:  0.02645426342424787\n",
            "Loss S2:  0.029684821879606187\n",
            "Loss S01:  0.02646428512429798\n",
            "Loss S2:  0.02971368651468675\n",
            "Loss S01:  0.02646261940498163\n",
            "Loss S2:  0.029728105181072016\n",
            "Loss S01:  0.02649366405901608\n",
            "Loss S2:  0.02976524534526172\n",
            "Loss S01:  0.02651515892468208\n",
            "Loss S2:  0.029774351184033165\n",
            "Loss S01:  0.026558216519028177\n",
            "Loss S2:  0.029798456872920043\n",
            "Loss S01:  0.026594077058612033\n",
            "Loss S2:  0.029842722209844182\n",
            "Loss S01:  0.026611080467207542\n",
            "Loss S2:  0.029884720677571582\n",
            "Loss S01:  0.026577007777479865\n",
            "Loss S2:  0.02986341912692748\n",
            "Loss S01:  0.026619673102048404\n",
            "Loss S2:  0.029925337900020922\n",
            "Loss S01:  0.026655036400365567\n",
            "Loss S2:  0.030002968179998476\n",
            "Loss S01:  0.026604419808191154\n",
            "Loss S2:  0.029963506132130224\n",
            "Loss S01:  0.02659354967746272\n",
            "Loss S2:  0.02990806327938144\n",
            "Loss S01:  0.026602349380887517\n",
            "Loss S2:  0.029912358043953705\n",
            "Loss S01:  0.026618038249366423\n",
            "Loss S2:  0.029911349253619417\n",
            "Loss S01:  0.02660447804184703\n",
            "Loss S2:  0.029903667910403504\n",
            "Loss S01:  0.026648457490556964\n",
            "Loss S2:  0.029934907517554354\n",
            "Loss S01:  0.026676627031835428\n",
            "Loss S2:  0.02990630534718711\n",
            "Loss S01:  0.026686355642888737\n",
            "Loss S2:  0.02992812626891666\n",
            "Loss S01:  0.026659521662606083\n",
            "Loss S2:  0.029878464434639554\n",
            "Loss S01:  0.02665393098897145\n",
            "Loss S2:  0.029880576644397715\n",
            "Loss S01:  0.02665940209378287\n",
            "Loss S2:  0.029905060632634407\n",
            "Loss S01:  0.02666733271606341\n",
            "Loss S2:  0.029892066592641448\n",
            "Loss S01:  0.02666175951193\n",
            "Loss S2:  0.02989056329108128\n",
            "Loss S01:  0.026658258200666616\n",
            "Loss S2:  0.029871074181061667\n",
            "Loss S01:  0.026661255326560978\n",
            "Loss S2:  0.029872392971079874\n",
            "Loss S01:  0.02666450402756765\n",
            "Loss S2:  0.029857417481167575\n",
            "Loss S01:  0.026666802181689826\n",
            "Loss S2:  0.029873713399907124\n",
            "Loss S01:  0.02666420128371907\n",
            "Loss S2:  0.029883365505413664\n",
            "Loss S01:  0.026645593646401664\n",
            "Loss S2:  0.029867832531344216\n",
            "Loss S01:  0.026637482630416476\n",
            "Loss S2:  0.029870741484867618\n",
            "Loss S01:  0.026622903283180482\n",
            "Loss S2:  0.029839680989837403\n",
            "Loss S01:  0.026637822045872634\n",
            "Loss S2:  0.029826421457856075\n",
            "Loss S01:  0.02665180765056117\n",
            "Loss S2:  0.029819620768664238\n",
            "Loss S01:  0.026672829659279623\n",
            "Loss S2:  0.029824775871499985\n",
            "Loss S01:  0.026655899650328673\n",
            "Loss S2:  0.02980286015193313\n",
            "Loss S01:  0.026654394850337586\n",
            "Loss S2:  0.02980521771963897\n",
            "Loss S01:  0.026657212769278137\n",
            "Loss S2:  0.02981153966863113\n",
            "Loss S01:  0.026660999622591924\n",
            "Loss S2:  0.029815540537884593\n",
            "Loss S01:  0.026656685420756887\n",
            "Loss S2:  0.029809584777857594\n",
            "Loss S01:  0.02666746295229561\n",
            "Loss S2:  0.029839597386840475\n",
            "Loss S01:  0.02664952124649177\n",
            "Loss S2:  0.02982861097606161\n",
            "Validation: \n",
            " Loss S01:  0.025513922795653343\n",
            " Loss S2:  0.04267552122473717\n",
            " Loss S01:  0.02669348292762325\n",
            " Loss S2:  0.04550056691680636\n",
            " Loss S01:  0.026437727734446526\n",
            " Loss S2:  0.04580428678451515\n",
            " Loss S01:  0.02638838168416844\n",
            " Loss S2:  0.04538824690169976\n",
            " Loss S01:  0.026498945590890485\n",
            " Loss S2:  0.04527293449198758\n",
            "\n",
            "Epoch: 38\n",
            "Loss S01:  0.02824832685291767\n",
            "Loss S2:  0.032832808792591095\n",
            "Loss S01:  0.025739036161791195\n",
            "Loss S2:  0.02899906242435629\n",
            "Loss S01:  0.025986235500091596\n",
            "Loss S2:  0.029109934167492958\n",
            "Loss S01:  0.026188807622078928\n",
            "Loss S2:  0.029144800778838895\n",
            "Loss S01:  0.026214573396051804\n",
            "Loss S2:  0.029413395265980465\n",
            "Loss S01:  0.02632554700853778\n",
            "Loss S2:  0.029512369968727522\n",
            "Loss S01:  0.02626058471495988\n",
            "Loss S2:  0.02941419987282792\n",
            "Loss S01:  0.026333538421862562\n",
            "Loss S2:  0.029579849074214275\n",
            "Loss S01:  0.026166914053905158\n",
            "Loss S2:  0.02948760038908617\n",
            "Loss S01:  0.02618776005948638\n",
            "Loss S2:  0.029433451326830045\n",
            "Loss S01:  0.02619577739572171\n",
            "Loss S2:  0.029410991356662003\n",
            "Loss S01:  0.02623246180581617\n",
            "Loss S2:  0.02944135477950981\n",
            "Loss S01:  0.026262380413649495\n",
            "Loss S2:  0.029474678149154363\n",
            "Loss S01:  0.026283736376248243\n",
            "Loss S2:  0.029495270609969402\n",
            "Loss S01:  0.026298228030403454\n",
            "Loss S2:  0.02948580521112638\n",
            "Loss S01:  0.026352577355523772\n",
            "Loss S2:  0.029537884358143963\n",
            "Loss S01:  0.02634714870071559\n",
            "Loss S2:  0.02953817097250349\n",
            "Loss S01:  0.02640229002817681\n",
            "Loss S2:  0.029581696884325374\n",
            "Loss S01:  0.026443354487007496\n",
            "Loss S2:  0.02965911831809671\n",
            "Loss S01:  0.02643713348237944\n",
            "Loss S2:  0.029643193472430343\n",
            "Loss S01:  0.026386647767836776\n",
            "Loss S2:  0.029620996875623567\n",
            "Loss S01:  0.02635483771244779\n",
            "Loss S2:  0.029615394047227517\n",
            "Loss S01:  0.026375124812530716\n",
            "Loss S2:  0.029618526096729672\n",
            "Loss S01:  0.026366591695454213\n",
            "Loss S2:  0.0295990581636305\n",
            "Loss S01:  0.02643082011877501\n",
            "Loss S2:  0.029625472616543413\n",
            "Loss S01:  0.026436740948087664\n",
            "Loss S2:  0.02961775244944599\n",
            "Loss S01:  0.0264517810352689\n",
            "Loss S2:  0.029619313326889073\n",
            "Loss S01:  0.026453400621829878\n",
            "Loss S2:  0.029599948171045068\n",
            "Loss S01:  0.02645006337703547\n",
            "Loss S2:  0.029596379378088004\n",
            "Loss S01:  0.026460477506223413\n",
            "Loss S2:  0.029626519588875196\n",
            "Loss S01:  0.026464768348144138\n",
            "Loss S2:  0.02962909737273902\n",
            "Loss S01:  0.0264593158336506\n",
            "Loss S2:  0.029610341193208357\n",
            "Loss S01:  0.02642777891730965\n",
            "Loss S2:  0.02957668835500319\n",
            "Loss S01:  0.026415343697427623\n",
            "Loss S2:  0.02957900195435995\n",
            "Loss S01:  0.02641634130713877\n",
            "Loss S2:  0.02958268026583705\n",
            "Loss S01:  0.02642402273106609\n",
            "Loss S2:  0.029611124420947158\n",
            "Loss S01:  0.0264315017217827\n",
            "Loss S2:  0.0296259055074067\n",
            "Loss S01:  0.026407685668039516\n",
            "Loss S2:  0.029621077139141423\n",
            "Loss S01:  0.026399901381197563\n",
            "Loss S2:  0.02962458569292478\n",
            "Loss S01:  0.026380589470038632\n",
            "Loss S2:  0.029596220482798183\n",
            "Loss S01:  0.026381609956745496\n",
            "Loss S2:  0.02959422828476328\n",
            "Loss S01:  0.026401056398222916\n",
            "Loss S2:  0.029603121611867508\n",
            "Loss S01:  0.02641803162468867\n",
            "Loss S2:  0.029605020470297817\n",
            "Loss S01:  0.02639826052668753\n",
            "Loss S2:  0.029593704665931083\n",
            "Loss S01:  0.026402406267771104\n",
            "Loss S2:  0.029598245950512875\n",
            "Loss S01:  0.026397906260321253\n",
            "Loss S2:  0.029586389053720328\n",
            "Loss S01:  0.026392452657190424\n",
            "Loss S2:  0.029585489413323215\n",
            "Loss S01:  0.026382487880228178\n",
            "Loss S2:  0.029572046458752797\n",
            "Loss S01:  0.02638667823045888\n",
            "Loss S2:  0.02957677782712632\n",
            "Loss S01:  0.026374989930851397\n",
            "Loss S2:  0.029557640662382673\n",
            "Validation: \n",
            " Loss S01:  0.024440927430987358\n",
            " Loss S2:  0.0421997532248497\n",
            " Loss S01:  0.025755899115687327\n",
            " Loss S2:  0.04517728693428494\n",
            " Loss S01:  0.025692305323190806\n",
            " Loss S2:  0.04556350946062949\n",
            " Loss S01:  0.025735471794595483\n",
            " Loss S2:  0.045229503487954376\n",
            " Loss S01:  0.02584684759746363\n",
            " Loss S2:  0.04505296709176935\n",
            "\n",
            "Epoch: 39\n",
            "Loss S01:  0.025463156402111053\n",
            "Loss S2:  0.03220602497458458\n",
            "Loss S01:  0.025562655011361294\n",
            "Loss S2:  0.028715377842838116\n",
            "Loss S01:  0.02592530855465503\n",
            "Loss S2:  0.02902387428496565\n",
            "Loss S01:  0.025944886608950554\n",
            "Loss S2:  0.028857047579461528\n",
            "Loss S01:  0.026096582321858987\n",
            "Loss S2:  0.0292121210567108\n",
            "Loss S01:  0.026259934274004956\n",
            "Loss S2:  0.02926253892627417\n",
            "Loss S01:  0.026133508161931742\n",
            "Loss S2:  0.02917760250265481\n",
            "Loss S01:  0.026201286864742428\n",
            "Loss S2:  0.029380984210842093\n",
            "Loss S01:  0.0260068143453495\n",
            "Loss S2:  0.029273179141275675\n",
            "Loss S01:  0.02599496292052688\n",
            "Loss S2:  0.029276377818250394\n",
            "Loss S01:  0.02601614708800127\n",
            "Loss S2:  0.02926535367744394\n",
            "Loss S01:  0.026030894922646316\n",
            "Loss S2:  0.02931694305493488\n",
            "Loss S01:  0.026076528359173744\n",
            "Loss S2:  0.02934310611429786\n",
            "Loss S01:  0.026135456186669473\n",
            "Loss S2:  0.029325321643748355\n",
            "Loss S01:  0.026171299054267558\n",
            "Loss S2:  0.02937232347614799\n",
            "Loss S01:  0.026229370126365035\n",
            "Loss S2:  0.029403694048030487\n",
            "Loss S01:  0.02625100898862996\n",
            "Loss S2:  0.029381903223924755\n",
            "Loss S01:  0.026308674278140765\n",
            "Loss S2:  0.029456649128107998\n",
            "Loss S01:  0.026354183941997216\n",
            "Loss S2:  0.029534013262687467\n",
            "Loss S01:  0.02629756703192651\n",
            "Loss S2:  0.02950814013315745\n",
            "Loss S01:  0.02627589597148978\n",
            "Loss S2:  0.02944469588127599\n",
            "Loss S01:  0.026282840792389843\n",
            "Loss S2:  0.029452425926500023\n",
            "Loss S01:  0.026283166518780442\n",
            "Loss S2:  0.029455740561042977\n",
            "Loss S01:  0.026276499524841578\n",
            "Loss S2:  0.02944530284604727\n",
            "Loss S01:  0.026298575273316926\n",
            "Loss S2:  0.029452059479793572\n",
            "Loss S01:  0.02628728667964261\n",
            "Loss S2:  0.029411644878912256\n",
            "Loss S01:  0.02629896959867971\n",
            "Loss S2:  0.029414590576599384\n",
            "Loss S01:  0.02628570263071254\n",
            "Loss S2:  0.029393494679039255\n",
            "Loss S01:  0.026279248754590007\n",
            "Loss S2:  0.029383652740791175\n",
            "Loss S01:  0.02628444764391868\n",
            "Loss S2:  0.029404036515101126\n",
            "Loss S01:  0.026275783597779434\n",
            "Loss S2:  0.0293946740808479\n",
            "Loss S01:  0.026267650007147498\n",
            "Loss S2:  0.029395062200892778\n",
            "Loss S01:  0.026244905571282097\n",
            "Loss S2:  0.029381818680609125\n",
            "Loss S01:  0.02623889517613048\n",
            "Loss S2:  0.029376031859043503\n",
            "Loss S01:  0.02623983165604278\n",
            "Loss S2:  0.029366729901202263\n",
            "Loss S01:  0.026245386602404792\n",
            "Loss S2:  0.029388297078573804\n",
            "Loss S01:  0.02624225699525956\n",
            "Loss S2:  0.029394483601370015\n",
            "Loss S01:  0.026225755714141135\n",
            "Loss S2:  0.029370072380591916\n",
            "Loss S01:  0.02620692342871756\n",
            "Loss S2:  0.029373935084017557\n",
            "Loss S01:  0.02618696122332607\n",
            "Loss S2:  0.029356819065406804\n",
            "Loss S01:  0.02619852514895715\n",
            "Loss S2:  0.029346682544695468\n",
            "Loss S01:  0.026199284383524073\n",
            "Loss S2:  0.029349933145001277\n",
            "Loss S01:  0.02619927559676193\n",
            "Loss S2:  0.029347212405570047\n",
            "Loss S01:  0.026177801701765327\n",
            "Loss S2:  0.029334911164717443\n",
            "Loss S01:  0.026178473391297724\n",
            "Loss S2:  0.029329098908910677\n",
            "Loss S01:  0.026177868717855993\n",
            "Loss S2:  0.029335643373380214\n",
            "Loss S01:  0.02618118515339177\n",
            "Loss S2:  0.029344482403935425\n",
            "Loss S01:  0.026186968257251312\n",
            "Loss S2:  0.029323102945892927\n",
            "Loss S01:  0.02618624392718758\n",
            "Loss S2:  0.02932543174330757\n",
            "Loss S01:  0.026172723932203112\n",
            "Loss S2:  0.0293092617230175\n",
            "Validation: \n",
            " Loss S01:  0.023791955783963203\n",
            " Loss S2:  0.042635228484869\n",
            " Loss S01:  0.026200106456166224\n",
            " Loss S2:  0.04583423389565377\n",
            " Loss S01:  0.026055729698117185\n",
            " Loss S2:  0.04615420739098293\n",
            " Loss S01:  0.025984280727437286\n",
            " Loss S2:  0.045637539786393524\n",
            " Loss S01:  0.026054457680862626\n",
            " Loss S2:  0.04550610758640148\n",
            "\n",
            "Epoch: 40\n",
            "Loss S01:  0.02838856168091297\n",
            "Loss S2:  0.030288180336356163\n",
            "Loss S01:  0.02547729184681719\n",
            "Loss S2:  0.02837694724175063\n",
            "Loss S01:  0.025602383095593678\n",
            "Loss S2:  0.028557232004545983\n",
            "Loss S01:  0.02571918154435773\n",
            "Loss S2:  0.02854757515653487\n",
            "Loss S01:  0.025799871990230025\n",
            "Loss S2:  0.02879133488892055\n",
            "Loss S01:  0.02587861885481021\n",
            "Loss S2:  0.029026470457514126\n",
            "Loss S01:  0.025868790926503352\n",
            "Loss S2:  0.028992159933340353\n",
            "Loss S01:  0.025943338110203475\n",
            "Loss S2:  0.02915461844121906\n",
            "Loss S01:  0.025827121656434037\n",
            "Loss S2:  0.02900576545500461\n",
            "Loss S01:  0.02582493481727747\n",
            "Loss S2:  0.029057964396018248\n",
            "Loss S01:  0.025781023214654166\n",
            "Loss S2:  0.029043123992805432\n",
            "Loss S01:  0.025824418627061287\n",
            "Loss S2:  0.02909176744654909\n",
            "Loss S01:  0.025838525011396603\n",
            "Loss S2:  0.02916324075526935\n",
            "Loss S01:  0.0258626289921635\n",
            "Loss S2:  0.029177967643806042\n",
            "Loss S01:  0.02591776250736088\n",
            "Loss S2:  0.029177337647118468\n",
            "Loss S01:  0.025954814590760412\n",
            "Loss S2:  0.02922561060365857\n",
            "Loss S01:  0.025941252905111876\n",
            "Loss S2:  0.02921657868146156\n",
            "Loss S01:  0.02599934655192651\n",
            "Loss S2:  0.02925969177737222\n",
            "Loss S01:  0.0260359492173511\n",
            "Loss S2:  0.02930588137296682\n",
            "Loss S01:  0.026017826976966484\n",
            "Loss S2:  0.02926686288135526\n",
            "Loss S01:  0.02598392511185129\n",
            "Loss S2:  0.0292271218118976\n",
            "Loss S01:  0.02596173472525949\n",
            "Loss S2:  0.029214383910688178\n",
            "Loss S01:  0.02595908185509265\n",
            "Loss S2:  0.029212843947011423\n",
            "Loss S01:  0.025964921751579682\n",
            "Loss S2:  0.029189801827331125\n",
            "Loss S01:  0.02597495277423829\n",
            "Loss S2:  0.02918281403845524\n",
            "Loss S01:  0.025999764166030274\n",
            "Loss S2:  0.029178130310548254\n",
            "Loss S01:  0.02602507428790646\n",
            "Loss S2:  0.02916778046709139\n",
            "Loss S01:  0.026022737887017842\n",
            "Loss S2:  0.02914067709000568\n",
            "Loss S01:  0.025999733475550638\n",
            "Loss S2:  0.029134279448419703\n",
            "Loss S01:  0.02599178074100583\n",
            "Loss S2:  0.029151160801245586\n",
            "Loss S01:  0.025999286264626685\n",
            "Loss S2:  0.029152431546948677\n",
            "Loss S01:  0.02598935426167928\n",
            "Loss S2:  0.029149115678533864\n",
            "Loss S01:  0.025959725715103923\n",
            "Loss S2:  0.029118699854315255\n",
            "Loss S01:  0.025966375621649072\n",
            "Loss S2:  0.029120821550803602\n",
            "Loss S01:  0.025957230527685886\n",
            "Loss S2:  0.029104634868172257\n",
            "Loss S01:  0.025969596420470465\n",
            "Loss S2:  0.0291271712684054\n",
            "Loss S01:  0.025994606666947997\n",
            "Loss S2:  0.029149281567896503\n",
            "Loss S01:  0.025995189971679628\n",
            "Loss S2:  0.02913040701410359\n",
            "Loss S01:  0.025981096384601957\n",
            "Loss S2:  0.029140567363012494\n",
            "Loss S01:  0.025965190390148735\n",
            "Loss S2:  0.029117413179572584\n",
            "Loss S01:  0.025973048126162437\n",
            "Loss S2:  0.029122230167177847\n",
            "Loss S01:  0.0259759873197099\n",
            "Loss S2:  0.029129328704699694\n",
            "Loss S01:  0.02598959033658414\n",
            "Loss S2:  0.029144217700403263\n",
            "Loss S01:  0.025971337063821452\n",
            "Loss S2:  0.029120879109490776\n",
            "Loss S01:  0.0259611693436867\n",
            "Loss S2:  0.029120215584361363\n",
            "Loss S01:  0.02595595735437061\n",
            "Loss S2:  0.029115716879258135\n",
            "Loss S01:  0.025957430654292252\n",
            "Loss S2:  0.02911641798234684\n",
            "Loss S01:  0.025956859950553586\n",
            "Loss S2:  0.029102266198637126\n",
            "Loss S01:  0.025970329284636988\n",
            "Loss S2:  0.029115605530118\n",
            "Loss S01:  0.025951545313759393\n",
            "Loss S2:  0.029087621111673154\n",
            "Validation: \n",
            " Loss S01:  0.023970453068614006\n",
            " Loss S2:  0.042854879051446915\n",
            " Loss S01:  0.025305876863144693\n",
            " Loss S2:  0.045475040872891746\n",
            " Loss S01:  0.02522559450348703\n",
            " Loss S2:  0.045712399228316984\n",
            " Loss S01:  0.025208974715139044\n",
            " Loss S2:  0.04538412471530867\n",
            " Loss S01:  0.025250961314196938\n",
            " Loss S2:  0.04520709886227125\n",
            "\n",
            "Epoch: 41\n",
            "Loss S01:  0.026890430599451065\n",
            "Loss S2:  0.03146752715110779\n",
            "Loss S01:  0.025304389270869167\n",
            "Loss S2:  0.02829054387455637\n",
            "Loss S01:  0.02553354984238034\n",
            "Loss S2:  0.028588189965202696\n",
            "Loss S01:  0.0256701925348851\n",
            "Loss S2:  0.028597970703436483\n",
            "Loss S01:  0.025841437507329916\n",
            "Loss S2:  0.02886465687032153\n",
            "Loss S01:  0.025979024997236682\n",
            "Loss S2:  0.029018653191479983\n",
            "Loss S01:  0.02588191246766536\n",
            "Loss S2:  0.028961597896013103\n",
            "Loss S01:  0.0258708247836207\n",
            "Loss S2:  0.029115127425798228\n",
            "Loss S01:  0.02574816056423717\n",
            "Loss S2:  0.028950949877868466\n",
            "Loss S01:  0.025747519477710618\n",
            "Loss S2:  0.028915690094396308\n",
            "Loss S01:  0.02567446823831242\n",
            "Loss S2:  0.028858640351065314\n",
            "Loss S01:  0.02565389937041579\n",
            "Loss S2:  0.02885398755336667\n",
            "Loss S01:  0.025685674195324092\n",
            "Loss S2:  0.02889514866877686\n",
            "Loss S01:  0.025716976141770376\n",
            "Loss S2:  0.028884523896543123\n",
            "Loss S01:  0.025717300395910622\n",
            "Loss S2:  0.028904806426230896\n",
            "Loss S01:  0.02574804795301513\n",
            "Loss S2:  0.02893267781627889\n",
            "Loss S01:  0.02573524385413028\n",
            "Loss S2:  0.028950947169098795\n",
            "Loss S01:  0.025753718828074416\n",
            "Loss S2:  0.02902364194916006\n",
            "Loss S01:  0.025782965617614555\n",
            "Loss S2:  0.029090565397759168\n",
            "Loss S01:  0.025758154017372904\n",
            "Loss S2:  0.029033990264797086\n",
            "Loss S01:  0.025720961252922444\n",
            "Loss S2:  0.028982453307702175\n",
            "Loss S01:  0.02571666465798543\n",
            "Loss S2:  0.028996948664787258\n",
            "Loss S01:  0.025737720191883286\n",
            "Loss S2:  0.029003444907352394\n",
            "Loss S01:  0.025746064228348402\n",
            "Loss S2:  0.028995456504615356\n",
            "Loss S01:  0.02578686013106736\n",
            "Loss S2:  0.029000379740264405\n",
            "Loss S01:  0.025781224129268848\n",
            "Loss S2:  0.02898922451285727\n",
            "Loss S01:  0.025782733741733763\n",
            "Loss S2:  0.028999154523757224\n",
            "Loss S01:  0.02577857450475552\n",
            "Loss S2:  0.0289631173619485\n",
            "Loss S01:  0.025769665731229817\n",
            "Loss S2:  0.028954760055471995\n",
            "Loss S01:  0.025771976189361404\n",
            "Loss S2:  0.028979896263721883\n",
            "Loss S01:  0.02578014589573854\n",
            "Loss S2:  0.028970872692491524\n",
            "Loss S01:  0.025768746019990883\n",
            "Loss S2:  0.02895116210702531\n",
            "Loss S01:  0.025749051902802935\n",
            "Loss S2:  0.028922501330574352\n",
            "Loss S01:  0.02574638099468727\n",
            "Loss S2:  0.028916827490232503\n",
            "Loss S01:  0.0257442751653558\n",
            "Loss S2:  0.028906179433606585\n",
            "Loss S01:  0.025759756650000895\n",
            "Loss S2:  0.028930629425790916\n",
            "Loss S01:  0.0257734236748595\n",
            "Loss S2:  0.028941783207853084\n",
            "Loss S01:  0.025751702547716002\n",
            "Loss S2:  0.028916163530672657\n",
            "Loss S01:  0.025731456155578297\n",
            "Loss S2:  0.028916680691592648\n",
            "Loss S01:  0.02571500591990893\n",
            "Loss S2:  0.02887645313311416\n",
            "Loss S01:  0.02572998783534899\n",
            "Loss S2:  0.02887954020830907\n",
            "Loss S01:  0.02573756073259379\n",
            "Loss S2:  0.028889217887554147\n",
            "Loss S01:  0.025742895600348356\n",
            "Loss S2:  0.028886459812061803\n",
            "Loss S01:  0.02573442800087193\n",
            "Loss S2:  0.02886851441348512\n",
            "Loss S01:  0.025724147072657435\n",
            "Loss S2:  0.02886993802087107\n",
            "Loss S01:  0.025728020490768742\n",
            "Loss S2:  0.02887158884673055\n",
            "Loss S01:  0.025727186848989277\n",
            "Loss S2:  0.02886967706027362\n",
            "Loss S01:  0.02572334070162029\n",
            "Loss S2:  0.028862990122565293\n",
            "Loss S01:  0.025734180890472913\n",
            "Loss S2:  0.028858592408455584\n",
            "Loss S01:  0.025710423429574355\n",
            "Loss S2:  0.02882764043472328\n",
            "Validation: \n",
            " Loss S01:  0.02426805905997753\n",
            " Loss S2:  0.04186434671282768\n",
            " Loss S01:  0.02618806188305219\n",
            " Loss S2:  0.045154688968544916\n",
            " Loss S01:  0.02616306939503042\n",
            " Loss S2:  0.04533508120150101\n",
            " Loss S01:  0.02612385642333109\n",
            " Loss S2:  0.044884132557227965\n",
            " Loss S01:  0.02622092825671037\n",
            " Loss S2:  0.044814012116856046\n",
            "\n",
            "Epoch: 42\n",
            "Loss S01:  0.027959618717432022\n",
            "Loss S2:  0.03115268237888813\n",
            "Loss S01:  0.025075074454600162\n",
            "Loss S2:  0.027988314628601074\n",
            "Loss S01:  0.02502269617148808\n",
            "Loss S2:  0.02820489883777641\n",
            "Loss S01:  0.02517649687586292\n",
            "Loss S2:  0.028206733986735344\n",
            "Loss S01:  0.025281159207224846\n",
            "Loss S2:  0.028391229034197038\n",
            "Loss S01:  0.025449747767518547\n",
            "Loss S2:  0.028530470479060623\n",
            "Loss S01:  0.02541858977714523\n",
            "Loss S2:  0.028534284563826732\n",
            "Loss S01:  0.025497251465706756\n",
            "Loss S2:  0.02873224355805088\n",
            "Loss S01:  0.025386674030695434\n",
            "Loss S2:  0.028614118741250333\n",
            "Loss S01:  0.025381326654946412\n",
            "Loss S2:  0.028602450226361934\n",
            "Loss S01:  0.02532459764123553\n",
            "Loss S2:  0.02854906826621235\n",
            "Loss S01:  0.02539062604039639\n",
            "Loss S2:  0.028661990722825936\n",
            "Loss S01:  0.025404625872442545\n",
            "Loss S2:  0.02869498517271901\n",
            "Loss S01:  0.02541952999664172\n",
            "Loss S2:  0.028711531311273575\n",
            "Loss S01:  0.025482122581584235\n",
            "Loss S2:  0.02876058816645585\n",
            "Loss S01:  0.025512194964072563\n",
            "Loss S2:  0.02876827957120952\n",
            "Loss S01:  0.025555842342028706\n",
            "Loss S2:  0.028779925952046554\n",
            "Loss S01:  0.025604456979018902\n",
            "Loss S2:  0.028850579253065656\n",
            "Loss S01:  0.02564182262147329\n",
            "Loss S2:  0.028902424983062797\n",
            "Loss S01:  0.02562357596467927\n",
            "Loss S2:  0.028836782768134672\n",
            "Loss S01:  0.02557317844003587\n",
            "Loss S2:  0.02877876846425569\n",
            "Loss S01:  0.025562007801157038\n",
            "Loss S2:  0.028788526026064187\n",
            "Loss S01:  0.025556131025852122\n",
            "Loss S2:  0.028752354812298424\n",
            "Loss S01:  0.025557296261900946\n",
            "Loss S2:  0.028729714848326916\n",
            "Loss S01:  0.025587765816830994\n",
            "Loss S2:  0.028731595462041275\n",
            "Loss S01:  0.025607937412611043\n",
            "Loss S2:  0.028706396095068806\n",
            "Loss S01:  0.02562818006822889\n",
            "Loss S2:  0.0287087876492419\n",
            "Loss S01:  0.025613175100231082\n",
            "Loss S2:  0.028687773994084214\n",
            "Loss S01:  0.025613088346354902\n",
            "Loss S2:  0.028687587067537886\n",
            "Loss S01:  0.025600076323419912\n",
            "Loss S2:  0.028699458445521565\n",
            "Loss S01:  0.025608666272109924\n",
            "Loss S2:  0.02870746120884767\n",
            "Loss S01:  0.02559337634413572\n",
            "Loss S2:  0.02868416074699336\n",
            "Loss S01:  0.025574261003472723\n",
            "Loss S2:  0.028661973728012075\n",
            "Loss S01:  0.025574843447058942\n",
            "Loss S2:  0.028669286468643074\n",
            "Loss S01:  0.02558383146229902\n",
            "Loss S2:  0.02868228673366857\n",
            "Loss S01:  0.025601993911923505\n",
            "Loss S2:  0.02871189217621784\n",
            "Loss S01:  0.025624684865619998\n",
            "Loss S2:  0.028735737432403246\n",
            "Loss S01:  0.02559395368911346\n",
            "Loss S2:  0.028722364445420288\n",
            "Loss S01:  0.025571843379438707\n",
            "Loss S2:  0.028719680471800443\n",
            "Loss S01:  0.025557440412623804\n",
            "Loss S2:  0.028691861244952283\n",
            "Loss S01:  0.0255489082509963\n",
            "Loss S2:  0.028671906815093948\n",
            "Loss S01:  0.02555296626712429\n",
            "Loss S2:  0.028662314022145714\n",
            "Loss S01:  0.025565678045826115\n",
            "Loss S2:  0.028663566190032665\n",
            "Loss S01:  0.02554622228989883\n",
            "Loss S2:  0.028647422405897602\n",
            "Loss S01:  0.025543059258301512\n",
            "Loss S2:  0.028641366566660182\n",
            "Loss S01:  0.025538680955205947\n",
            "Loss S2:  0.028639387401304065\n",
            "Loss S01:  0.025535948037163813\n",
            "Loss S2:  0.028638972826059626\n",
            "Loss S01:  0.025530271026485793\n",
            "Loss S2:  0.028624360779826302\n",
            "Loss S01:  0.025536911790559298\n",
            "Loss S2:  0.02863533310824521\n",
            "Loss S01:  0.025531498893579495\n",
            "Loss S2:  0.028616196360219035\n",
            "Validation: \n",
            " Loss S01:  0.024403037503361702\n",
            " Loss S2:  0.04421539977192879\n",
            " Loss S01:  0.025876854679414203\n",
            " Loss S2:  0.046492748494659154\n",
            " Loss S01:  0.02590252181923971\n",
            " Loss S2:  0.04659309910564888\n",
            " Loss S01:  0.025936092631738693\n",
            " Loss S2:  0.04626331569962814\n",
            " Loss S01:  0.026039837411156407\n",
            " Loss S2:  0.04611616793238087\n",
            "\n",
            "Epoch: 43\n",
            "Loss S01:  0.025843147188425064\n",
            "Loss S2:  0.029118994250893593\n",
            "Loss S01:  0.02445904521102255\n",
            "Loss S2:  0.027742918580770493\n",
            "Loss S01:  0.02488645263725803\n",
            "Loss S2:  0.02823442823830105\n",
            "Loss S01:  0.024933168784745278\n",
            "Loss S2:  0.028172288811014544\n",
            "Loss S01:  0.025011992127430147\n",
            "Loss S2:  0.028245646506547928\n",
            "Loss S01:  0.025249043187382175\n",
            "Loss S2:  0.028441141487336626\n",
            "Loss S01:  0.025262129874747307\n",
            "Loss S2:  0.028426323544050825\n",
            "Loss S01:  0.0253941652850366\n",
            "Loss S2:  0.02853368891691658\n",
            "Loss S01:  0.025262021862062407\n",
            "Loss S2:  0.028433290620644886\n",
            "Loss S01:  0.025260759578956352\n",
            "Loss S2:  0.02844461649246923\n",
            "Loss S01:  0.025228851484042584\n",
            "Loss S2:  0.028406975108503114\n",
            "Loss S01:  0.025305970546764298\n",
            "Loss S2:  0.028487182314599957\n",
            "Loss S01:  0.02539577262768568\n",
            "Loss S2:  0.028522558310184597\n",
            "Loss S01:  0.025429533197338344\n",
            "Loss S2:  0.028508454703420172\n",
            "Loss S01:  0.025457197377550686\n",
            "Loss S2:  0.028493991795372455\n",
            "Loss S01:  0.025494768302764325\n",
            "Loss S2:  0.02853190694127651\n",
            "Loss S01:  0.025474921586165516\n",
            "Loss S2:  0.02851388220507536\n",
            "Loss S01:  0.025482646162398377\n",
            "Loss S2:  0.028591369643633128\n",
            "Loss S01:  0.025487938679713572\n",
            "Loss S2:  0.02867241753316716\n",
            "Loss S01:  0.025448105410131483\n",
            "Loss S2:  0.028611784839895384\n",
            "Loss S01:  0.025416114807721987\n",
            "Loss S2:  0.02857483044933917\n",
            "Loss S01:  0.02539224042535111\n",
            "Loss S2:  0.028569114204690355\n",
            "Loss S01:  0.025438560147630684\n",
            "Loss S2:  0.028565044266201253\n",
            "Loss S01:  0.025471539635743414\n",
            "Loss S2:  0.028553732183007968\n",
            "Loss S01:  0.02550410051903537\n",
            "Loss S2:  0.028561100502850108\n",
            "Loss S01:  0.025504643327567682\n",
            "Loss S2:  0.028554984683772006\n",
            "Loss S01:  0.025505107973784323\n",
            "Loss S2:  0.028577566232489443\n",
            "Loss S01:  0.02550492978678858\n",
            "Loss S2:  0.02857962762077796\n",
            "Loss S01:  0.025504145347032683\n",
            "Loss S2:  0.02857114566867886\n",
            "Loss S01:  0.0255203049860357\n",
            "Loss S2:  0.028596808071347447\n",
            "Loss S01:  0.025507751821332993\n",
            "Loss S2:  0.028581628229382428\n",
            "Loss S01:  0.025481223550975516\n",
            "Loss S2:  0.028552219816678208\n",
            "Loss S01:  0.025438167218617934\n",
            "Loss S2:  0.028531312797439062\n",
            "Loss S01:  0.02543618958837316\n",
            "Loss S2:  0.028524856744305965\n",
            "Loss S01:  0.025440183061221477\n",
            "Loss S2:  0.028515076046246937\n",
            "Loss S01:  0.02543651497262156\n",
            "Loss S2:  0.028532232017258973\n",
            "Loss S01:  0.025447114553932008\n",
            "Loss S2:  0.028543270763185215\n",
            "Loss S01:  0.025425895200063275\n",
            "Loss S2:  0.028527263451538318\n",
            "Loss S01:  0.025416996007359874\n",
            "Loss S2:  0.028518982412503772\n",
            "Loss S01:  0.02540849771855585\n",
            "Loss S2:  0.02849791168480578\n",
            "Loss S01:  0.02541371003406452\n",
            "Loss S2:  0.02850492648371586\n",
            "Loss S01:  0.025419039828498868\n",
            "Loss S2:  0.028484763264873602\n",
            "Loss S01:  0.02542852818293413\n",
            "Loss S2:  0.028494048854800696\n",
            "Loss S01:  0.02540068353014175\n",
            "Loss S2:  0.028476384237166624\n",
            "Loss S01:  0.025392475252857014\n",
            "Loss S2:  0.028479111573052784\n",
            "Loss S01:  0.02539780683518117\n",
            "Loss S2:  0.028489232670408395\n",
            "Loss S01:  0.025387775177164347\n",
            "Loss S2:  0.02849158077397729\n",
            "Loss S01:  0.025380933530739412\n",
            "Loss S2:  0.02848733666564502\n",
            "Loss S01:  0.025376134054452614\n",
            "Loss S2:  0.028488602706089834\n",
            "Loss S01:  0.025357037479183095\n",
            "Loss S2:  0.028461449163114223\n",
            "Validation: \n",
            " Loss S01:  0.02405574917793274\n",
            " Loss S2:  0.04205499589443207\n",
            " Loss S01:  0.025266911390991437\n",
            " Loss S2:  0.04379192384935561\n",
            " Loss S01:  0.025168663753968912\n",
            " Loss S2:  0.04411584193386683\n",
            " Loss S01:  0.02513206701298229\n",
            " Loss S2:  0.043696740367373484\n",
            " Loss S01:  0.025233079138913272\n",
            " Loss S2:  0.04359615125038006\n",
            "\n",
            "Epoch: 44\n",
            "Loss S01:  0.027427004650235176\n",
            "Loss S2:  0.030942635610699654\n",
            "Loss S01:  0.024717422540892254\n",
            "Loss S2:  0.02773455378006805\n",
            "Loss S01:  0.024631414030279433\n",
            "Loss S2:  0.027898135284582775\n",
            "Loss S01:  0.024752606607733235\n",
            "Loss S2:  0.027861436408373616\n",
            "Loss S01:  0.024943986180715444\n",
            "Loss S2:  0.028139661043518928\n",
            "Loss S01:  0.025096816175124225\n",
            "Loss S2:  0.028310893541749788\n",
            "Loss S01:  0.025046141604419616\n",
            "Loss S2:  0.028275644223465294\n",
            "Loss S01:  0.025160022316054558\n",
            "Loss S2:  0.028487906410870418\n",
            "Loss S01:  0.02502998468224649\n",
            "Loss S2:  0.028301700443765263\n",
            "Loss S01:  0.024992230808833144\n",
            "Loss S2:  0.028287968792758144\n",
            "Loss S01:  0.024952440992882935\n",
            "Loss S2:  0.02827012068638117\n",
            "Loss S01:  0.024993630008654552\n",
            "Loss S2:  0.02831465754288811\n",
            "Loss S01:  0.02504663802067603\n",
            "Loss S2:  0.0283188374081919\n",
            "Loss S01:  0.02507753573528683\n",
            "Loss S2:  0.028323009012992145\n",
            "Loss S01:  0.025164229385818995\n",
            "Loss S2:  0.028377401199958\n",
            "Loss S01:  0.025209892047754187\n",
            "Loss S2:  0.028423307784247082\n",
            "Loss S01:  0.02522088012293629\n",
            "Loss S2:  0.02843699983743407\n",
            "Loss S01:  0.02527812927177078\n",
            "Loss S2:  0.028505847413550344\n",
            "Loss S01:  0.02530554107786542\n",
            "Loss S2:  0.028564153012732116\n",
            "Loss S01:  0.025278283010056506\n",
            "Loss S2:  0.028497305762081246\n",
            "Loss S01:  0.02525164372292324\n",
            "Loss S2:  0.028450992922136438\n",
            "Loss S01:  0.025248223977490058\n",
            "Loss S2:  0.028451409491035046\n",
            "Loss S01:  0.02526670257767401\n",
            "Loss S2:  0.028453938030886434\n",
            "Loss S01:  0.02527012289763787\n",
            "Loss S2:  0.028433411176522056\n",
            "Loss S01:  0.02531767801934258\n",
            "Loss S2:  0.028455262555435484\n",
            "Loss S01:  0.02532053117703394\n",
            "Loss S2:  0.028433689788221365\n",
            "Loss S01:  0.02534082639691245\n",
            "Loss S2:  0.028448725575973704\n",
            "Loss S01:  0.025339858668316773\n",
            "Loss S2:  0.02844157499498327\n",
            "Loss S01:  0.025314611459011708\n",
            "Loss S2:  0.0284151585567146\n",
            "Loss S01:  0.025316583114288926\n",
            "Loss S2:  0.02843305486481624\n",
            "Loss S01:  0.02533882234455937\n",
            "Loss S2:  0.028428912540260344\n",
            "Loss S01:  0.025316973543243776\n",
            "Loss S2:  0.028397096201202492\n",
            "Loss S01:  0.025294600031411166\n",
            "Loss S2:  0.02837715680330899\n",
            "Loss S01:  0.025291606294289455\n",
            "Loss S2:  0.02837324229679619\n",
            "Loss S01:  0.0252883464338318\n",
            "Loss S2:  0.028369080843376623\n",
            "Loss S01:  0.025291879913448608\n",
            "Loss S2:  0.028397367434029566\n",
            "Loss S01:  0.025313903915584912\n",
            "Loss S2:  0.02841670100688109\n",
            "Loss S01:  0.025302614665256358\n",
            "Loss S2:  0.028394190614636376\n",
            "Loss S01:  0.025294705531181\n",
            "Loss S2:  0.028392589761046912\n",
            "Loss S01:  0.025278755118284386\n",
            "Loss S2:  0.0283537378101169\n",
            "Loss S01:  0.02528280297995654\n",
            "Loss S2:  0.02834021309367439\n",
            "Loss S01:  0.025288070291462027\n",
            "Loss S2:  0.028339616211081362\n",
            "Loss S01:  0.025309515586056507\n",
            "Loss S2:  0.028350038644182993\n",
            "Loss S01:  0.025288422149011818\n",
            "Loss S2:  0.02833939597805802\n",
            "Loss S01:  0.025293894707649745\n",
            "Loss S2:  0.02834131709318988\n",
            "Loss S01:  0.025297884455574324\n",
            "Loss S2:  0.028337109206190658\n",
            "Loss S01:  0.025302334104759575\n",
            "Loss S2:  0.02835090820012925\n",
            "Loss S01:  0.0252982336679778\n",
            "Loss S2:  0.02833837094358831\n",
            "Loss S01:  0.025311409404768517\n",
            "Loss S2:  0.028354240201893813\n",
            "Loss S01:  0.025294558898130405\n",
            "Loss S2:  0.02833916132704425\n",
            "Validation: \n",
            " Loss S01:  0.024170497432351112\n",
            " Loss S2:  0.04214685410261154\n",
            " Loss S01:  0.025513543969108946\n",
            " Loss S2:  0.04451750697834151\n",
            " Loss S01:  0.025349069750163614\n",
            " Loss S2:  0.04476878401346323\n",
            " Loss S01:  0.025262446799239176\n",
            " Loss S2:  0.04429984746164963\n",
            " Loss S01:  0.02531443271831966\n",
            " Loss S2:  0.04422853197212572\n",
            "\n",
            "Epoch: 45\n",
            "Loss S01:  0.02407987229526043\n",
            "Loss S2:  0.02876521460711956\n",
            "Loss S01:  0.02413530732420358\n",
            "Loss S2:  0.027340359139171513\n",
            "Loss S01:  0.024336620189604304\n",
            "Loss S2:  0.02788072886566321\n",
            "Loss S01:  0.024521079455171863\n",
            "Loss S2:  0.027821388335958604\n",
            "Loss S01:  0.024576733797425178\n",
            "Loss S2:  0.0280399451532015\n",
            "Loss S01:  0.02488905772128526\n",
            "Loss S2:  0.02812036985129702\n",
            "Loss S01:  0.02483772874244901\n",
            "Loss S2:  0.028083964296784558\n",
            "Loss S01:  0.02503035374930207\n",
            "Loss S2:  0.02827315651614901\n",
            "Loss S01:  0.024920321265120567\n",
            "Loss S2:  0.02813658929993341\n",
            "Loss S01:  0.024888276357899655\n",
            "Loss S2:  0.028138248679729607\n",
            "Loss S01:  0.02486682487743916\n",
            "Loss S2:  0.02808582967165673\n",
            "Loss S01:  0.024864164927789756\n",
            "Loss S2:  0.028063707590640127\n",
            "Loss S01:  0.024909228132652842\n",
            "Loss S2:  0.02808956991234594\n",
            "Loss S01:  0.024918808466719306\n",
            "Loss S2:  0.028070682517558565\n",
            "Loss S01:  0.024984695043758297\n",
            "Loss S2:  0.028109837798996173\n",
            "Loss S01:  0.025055990782597206\n",
            "Loss S2:  0.028157648574930944\n",
            "Loss S01:  0.02504612075060791\n",
            "Loss S2:  0.02815473901096338\n",
            "Loss S01:  0.025088050593322482\n",
            "Loss S2:  0.028210027535495005\n",
            "Loss S01:  0.025144539722009918\n",
            "Loss S2:  0.028277291046733356\n",
            "Loss S01:  0.025114426974460716\n",
            "Loss S2:  0.02825042172408229\n",
            "Loss S01:  0.02507527552508003\n",
            "Loss S2:  0.02820545174552137\n",
            "Loss S01:  0.025072295607125025\n",
            "Loss S2:  0.028216072653919035\n",
            "Loss S01:  0.025061911351270807\n",
            "Loss S2:  0.02821784734523674\n",
            "Loss S01:  0.025066314249005153\n",
            "Loss S2:  0.02822820446353196\n",
            "Loss S01:  0.025100437506909688\n",
            "Loss S2:  0.028256440867526898\n",
            "Loss S01:  0.025092323529589698\n",
            "Loss S2:  0.028217427439957976\n",
            "Loss S01:  0.02509864949232317\n",
            "Loss S2:  0.02822131714944182\n",
            "Loss S01:  0.02508334286950831\n",
            "Loss S2:  0.028203788711740962\n",
            "Loss S01:  0.02509018396991852\n",
            "Loss S2:  0.028204640897249413\n",
            "Loss S01:  0.025109932863527966\n",
            "Loss S2:  0.028217173459910854\n",
            "Loss S01:  0.025123439364110513\n",
            "Loss S2:  0.028206738572184035\n",
            "Loss S01:  0.025115067923854786\n",
            "Loss S2:  0.028181361274081027\n",
            "Loss S01:  0.025107789152182893\n",
            "Loss S2:  0.02816060851574687\n",
            "Loss S01:  0.02509527430050863\n",
            "Loss S2:  0.028160461441267652\n",
            "Loss S01:  0.025097532711289495\n",
            "Loss S2:  0.028147663287490694\n",
            "Loss S01:  0.02510253203093496\n",
            "Loss S2:  0.028181753553810963\n",
            "Loss S01:  0.025122603881392123\n",
            "Loss S2:  0.028189472405245098\n",
            "Loss S01:  0.025099537478423504\n",
            "Loss S2:  0.02817280921392203\n",
            "Loss S01:  0.025077603669382456\n",
            "Loss S2:  0.028151838284895175\n",
            "Loss S01:  0.025054964968158157\n",
            "Loss S2:  0.02812287776881014\n",
            "Loss S01:  0.02504111211886281\n",
            "Loss S2:  0.028116557254755585\n",
            "Loss S01:  0.025047938944431986\n",
            "Loss S2:  0.028108679909268144\n",
            "Loss S01:  0.02506375258385852\n",
            "Loss S2:  0.028107464101138988\n",
            "Loss S01:  0.025043982273430115\n",
            "Loss S2:  0.02807260431314704\n",
            "Loss S01:  0.025037907456763746\n",
            "Loss S2:  0.028066519732310388\n",
            "Loss S01:  0.025030253913476568\n",
            "Loss S2:  0.0280669688096398\n",
            "Loss S01:  0.02503555697952589\n",
            "Loss S2:  0.02807457665478459\n",
            "Loss S01:  0.025045778246442225\n",
            "Loss S2:  0.028069879535403727\n",
            "Loss S01:  0.025056523478080725\n",
            "Loss S2:  0.028084205939839138\n",
            "Loss S01:  0.025036495588643973\n",
            "Loss S2:  0.02805297095669991\n",
            "Validation: \n",
            " Loss S01:  0.023642653599381447\n",
            " Loss S2:  0.043786510825157166\n",
            " Loss S01:  0.02506881801500207\n",
            " Loss S2:  0.04526037403515407\n",
            " Loss S01:  0.024910514174801546\n",
            " Loss S2:  0.04547393549142814\n",
            " Loss S01:  0.02492929726350503\n",
            " Loss S2:  0.04502951408751675\n",
            " Loss S01:  0.024971946797989034\n",
            " Loss S2:  0.04491210236777494\n",
            "\n",
            "Epoch: 46\n",
            "Loss S01:  0.025353362783789635\n",
            "Loss S2:  0.030576461926102638\n",
            "Loss S01:  0.024172579022971066\n",
            "Loss S2:  0.027600226415829224\n",
            "Loss S01:  0.024446682827103706\n",
            "Loss S2:  0.02807629392260597\n",
            "Loss S01:  0.024568804750038732\n",
            "Loss S2:  0.027850835914573362\n",
            "Loss S01:  0.02457939542648269\n",
            "Loss S2:  0.027883924498427207\n",
            "Loss S01:  0.024744098275607706\n",
            "Loss S2:  0.027957243004850314\n",
            "Loss S01:  0.024704691633337834\n",
            "Loss S2:  0.02787571455367276\n",
            "Loss S01:  0.0247930340259008\n",
            "Loss S2:  0.02803451586252367\n",
            "Loss S01:  0.024628964241272137\n",
            "Loss S2:  0.027868121747433403\n",
            "Loss S01:  0.0246262369206646\n",
            "Loss S2:  0.027820630121853325\n",
            "Loss S01:  0.024700655371393307\n",
            "Loss S2:  0.027848843097834305\n",
            "Loss S01:  0.024742822590711956\n",
            "Loss S2:  0.027869923224857263\n",
            "Loss S01:  0.024819663148527304\n",
            "Loss S2:  0.027905393730510365\n",
            "Loss S01:  0.024827565030969737\n",
            "Loss S2:  0.02792743428517844\n",
            "Loss S01:  0.024878529422249353\n",
            "Loss S2:  0.027939022142202296\n",
            "Loss S01:  0.024892844555788482\n",
            "Loss S2:  0.027973829663747195\n",
            "Loss S01:  0.024869906964787046\n",
            "Loss S2:  0.027975577502424673\n",
            "Loss S01:  0.024889674789288586\n",
            "Loss S2:  0.02804644788174253\n",
            "Loss S01:  0.0249052324141916\n",
            "Loss S2:  0.02809621903078003\n",
            "Loss S01:  0.024873354584368734\n",
            "Loss S2:  0.028067541089279488\n",
            "Loss S01:  0.024846973713135243\n",
            "Loss S2:  0.028023150142523188\n",
            "Loss S01:  0.024878116121506803\n",
            "Loss S2:  0.028026170401847194\n",
            "Loss S01:  0.02487189191229203\n",
            "Loss S2:  0.028053778624035654\n",
            "Loss S01:  0.024880808435412714\n",
            "Loss S2:  0.028039594354245054\n",
            "Loss S01:  0.024900092669913384\n",
            "Loss S2:  0.02804996552432721\n",
            "Loss S01:  0.024916522464607342\n",
            "Loss S2:  0.028027882814347982\n",
            "Loss S01:  0.024924211774977688\n",
            "Loss S2:  0.02803986971320092\n",
            "Loss S01:  0.02491686301395242\n",
            "Loss S2:  0.028024169393021242\n",
            "Loss S01:  0.024919704616971287\n",
            "Loss S2:  0.028016112347090373\n",
            "Loss S01:  0.024924081734211994\n",
            "Loss S2:  0.028029723611381865\n",
            "Loss S01:  0.024929669824896066\n",
            "Loss S2:  0.028023939115611025\n",
            "Loss S01:  0.02492728758112219\n",
            "Loss S2:  0.028026397534649087\n",
            "Loss S01:  0.0249156987590489\n",
            "Loss S2:  0.02798645078230684\n",
            "Loss S01:  0.024922135328238105\n",
            "Loss S2:  0.027980974371744786\n",
            "Loss S01:  0.024927798019519998\n",
            "Loss S2:  0.027966607689770086\n",
            "Loss S01:  0.024932493129347123\n",
            "Loss S2:  0.02799304749848496\n",
            "Loss S01:  0.02493830464285001\n",
            "Loss S2:  0.027998893911282113\n",
            "Loss S01:  0.024919604907700958\n",
            "Loss S2:  0.027978171396126964\n",
            "Loss S01:  0.024900407498589964\n",
            "Loss S2:  0.02797130888860839\n",
            "Loss S01:  0.02487667943910717\n",
            "Loss S2:  0.027937459352109437\n",
            "Loss S01:  0.024888087653942834\n",
            "Loss S2:  0.027939308221054792\n",
            "Loss S01:  0.024892761038929005\n",
            "Loss S2:  0.027945017750282068\n",
            "Loss S01:  0.024911939270458143\n",
            "Loss S2:  0.027945640822098262\n",
            "Loss S01:  0.02489560539131928\n",
            "Loss S2:  0.027931559342553886\n",
            "Loss S01:  0.024886989184653138\n",
            "Loss S2:  0.027918650158374757\n",
            "Loss S01:  0.024883334118121214\n",
            "Loss S2:  0.027920874508878344\n",
            "Loss S01:  0.024873848584300785\n",
            "Loss S2:  0.027923138800063\n",
            "Loss S01:  0.024886161745417145\n",
            "Loss S2:  0.0279274570010207\n",
            "Loss S01:  0.024896313163322123\n",
            "Loss S2:  0.02793713238888\n",
            "Loss S01:  0.02488391460991197\n",
            "Loss S2:  0.027912260304430836\n",
            "Validation: \n",
            " Loss S01:  0.023524845018982887\n",
            " Loss S2:  0.043012671172618866\n",
            " Loss S01:  0.02478758494059245\n",
            " Loss S2:  0.04516572239143508\n",
            " Loss S01:  0.024683872692105247\n",
            " Loss S2:  0.04545706346994493\n",
            " Loss S01:  0.02468773356226624\n",
            " Loss S2:  0.045044116553713064\n",
            " Loss S01:  0.024708193692344206\n",
            " Loss S2:  0.04488069384738251\n",
            "\n",
            "Epoch: 47\n",
            "Loss S01:  0.02628500387072563\n",
            "Loss S2:  0.029411695897579193\n",
            "Loss S01:  0.024503251029686493\n",
            "Loss S2:  0.026908268474719742\n",
            "Loss S01:  0.024598832641329085\n",
            "Loss S2:  0.02729770044485728\n",
            "Loss S01:  0.024559121639017138\n",
            "Loss S2:  0.027307636074481473\n",
            "Loss S01:  0.024717828077150554\n",
            "Loss S2:  0.027653534328792154\n",
            "Loss S01:  0.024914846919915256\n",
            "Loss S2:  0.02782477274098817\n",
            "Loss S01:  0.02486645479182728\n",
            "Loss S2:  0.027766380306394375\n",
            "Loss S01:  0.02490360397373287\n",
            "Loss S2:  0.02790301529244638\n",
            "Loss S01:  0.024743791807580878\n",
            "Loss S2:  0.027780629671466203\n",
            "Loss S01:  0.024741166690876195\n",
            "Loss S2:  0.027749851898668885\n",
            "Loss S01:  0.02470261743753263\n",
            "Loss S2:  0.027685677356059007\n",
            "Loss S01:  0.024713269426478997\n",
            "Loss S2:  0.02770639311622929\n",
            "Loss S01:  0.02469726353267993\n",
            "Loss S2:  0.027708096398918098\n",
            "Loss S01:  0.024715747556272354\n",
            "Loss S2:  0.027688529299756952\n",
            "Loss S01:  0.02474898270311508\n",
            "Loss S2:  0.02769847664691455\n",
            "Loss S01:  0.02478205585321843\n",
            "Loss S2:  0.027737251131345106\n",
            "Loss S01:  0.024793912511989938\n",
            "Loss S2:  0.027741527717028345\n",
            "Loss S01:  0.024848291508809864\n",
            "Loss S2:  0.02783031512204318\n",
            "Loss S01:  0.02485268954084723\n",
            "Loss S2:  0.027893683795935542\n",
            "Loss S01:  0.02483192000401582\n",
            "Loss S2:  0.027881349354046178\n",
            "Loss S01:  0.024804987523019018\n",
            "Loss S2:  0.027853230700193354\n",
            "Loss S01:  0.024800367697508416\n",
            "Loss S2:  0.027867183424708966\n",
            "Loss S01:  0.024797159049036278\n",
            "Loss S2:  0.02786452738711467\n",
            "Loss S01:  0.024803840914071895\n",
            "Loss S2:  0.02783660064915042\n",
            "Loss S01:  0.024861067100679233\n",
            "Loss S2:  0.027856850316477513\n",
            "Loss S01:  0.024898529869033046\n",
            "Loss S2:  0.027845719740150935\n",
            "Loss S01:  0.024902582867963104\n",
            "Loss S2:  0.02786525086967196\n",
            "Loss S01:  0.024873688995728192\n",
            "Loss S2:  0.027845095245105755\n",
            "Loss S01:  0.024860065264686995\n",
            "Loss S2:  0.02785460715881446\n",
            "Loss S01:  0.02485614157957105\n",
            "Loss S2:  0.02786785511370377\n",
            "Loss S01:  0.024865274809523673\n",
            "Loss S2:  0.027866467845895363\n",
            "Loss S01:  0.024843262801982964\n",
            "Loss S2:  0.027837390894627265\n",
            "Loss S01:  0.024827502998653973\n",
            "Loss S2:  0.02781960560223581\n",
            "Loss S01:  0.024825030639542552\n",
            "Loss S2:  0.02783473609212302\n",
            "Loss S01:  0.024838393575940663\n",
            "Loss S2:  0.02782847289859025\n",
            "Loss S01:  0.02485526198165709\n",
            "Loss S2:  0.027846524295558957\n",
            "Loss S01:  0.02486217712307571\n",
            "Loss S2:  0.027857864523239413\n",
            "Loss S01:  0.02484707259686006\n",
            "Loss S2:  0.027847490210658455\n",
            "Loss S01:  0.02484208375723969\n",
            "Loss S2:  0.02784419481170772\n",
            "Loss S01:  0.024816959319860124\n",
            "Loss S2:  0.0278080329537163\n",
            "Loss S01:  0.024815071388112935\n",
            "Loss S2:  0.027805207571260947\n",
            "Loss S01:  0.024820011263201127\n",
            "Loss S2:  0.02779549201655852\n",
            "Loss S01:  0.024829702704012253\n",
            "Loss S2:  0.027797499587516036\n",
            "Loss S01:  0.024817507860970332\n",
            "Loss S2:  0.02778776568682592\n",
            "Loss S01:  0.02481289120082682\n",
            "Loss S2:  0.027784244214393655\n",
            "Loss S01:  0.024819931428375637\n",
            "Loss S2:  0.027786417705256763\n",
            "Loss S01:  0.024825899989027005\n",
            "Loss S2:  0.027799446806473223\n",
            "Loss S01:  0.024820496891714207\n",
            "Loss S2:  0.0277870944137596\n",
            "Loss S01:  0.024826878569478055\n",
            "Loss S2:  0.027794880335550297\n",
            "Loss S01:  0.024816929102854428\n",
            "Loss S2:  0.02777042522905072\n",
            "Validation: \n",
            " Loss S01:  0.024867944419384003\n",
            " Loss S2:  0.04234279692173004\n",
            " Loss S01:  0.025898429165993418\n",
            " Loss S2:  0.04431702977135068\n",
            " Loss S01:  0.02585651730073661\n",
            " Loss S2:  0.04457926513945184\n",
            " Loss S01:  0.025821254481790495\n",
            " Loss S2:  0.04429815868373777\n",
            " Loss S01:  0.02591895751287172\n",
            " Loss S2:  0.04414167498916755\n",
            "\n",
            "Epoch: 48\n",
            "Loss S01:  0.025555366650223732\n",
            "Loss S2:  0.029154669493436813\n",
            "Loss S01:  0.024399122392589397\n",
            "Loss S2:  0.027016575024886566\n",
            "Loss S01:  0.02444909184816338\n",
            "Loss S2:  0.027336399558754193\n",
            "Loss S01:  0.024541504380683744\n",
            "Loss S2:  0.027244467047914382\n",
            "Loss S01:  0.024602337081621333\n",
            "Loss S2:  0.027507812511630175\n",
            "Loss S01:  0.024819089282377093\n",
            "Loss S2:  0.027567103468612127\n",
            "Loss S01:  0.024803887319857956\n",
            "Loss S2:  0.027556240680764933\n",
            "Loss S01:  0.02482550129504271\n",
            "Loss S2:  0.02763342337918953\n",
            "Loss S01:  0.02467839511824243\n",
            "Loss S2:  0.02752681368571005\n",
            "Loss S01:  0.024627544845526036\n",
            "Loss S2:  0.027479941823652813\n",
            "Loss S01:  0.02457034065950625\n",
            "Loss S2:  0.027397144309217386\n",
            "Loss S01:  0.024518590739315695\n",
            "Loss S2:  0.027359249808632576\n",
            "Loss S01:  0.024546625569832226\n",
            "Loss S2:  0.027387896845163393\n",
            "Loss S01:  0.02452457946447926\n",
            "Loss S2:  0.027393776767012726\n",
            "Loss S01:  0.02455840451339035\n",
            "Loss S2:  0.027445071555198507\n",
            "Loss S01:  0.024583202755905146\n",
            "Loss S2:  0.027484184229709455\n",
            "Loss S01:  0.024563528678339462\n",
            "Loss S2:  0.027460868591847626\n",
            "Loss S01:  0.02460439956327628\n",
            "Loss S2:  0.0275377819412633\n",
            "Loss S01:  0.024600710081790694\n",
            "Loss S2:  0.027595944805533845\n",
            "Loss S01:  0.024585986061324\n",
            "Loss S2:  0.027577452147038196\n",
            "Loss S01:  0.024576498566204634\n",
            "Loss S2:  0.027561450071299252\n",
            "Loss S01:  0.024574207627519048\n",
            "Loss S2:  0.02757650386030075\n",
            "Loss S01:  0.024585517207624145\n",
            "Loss S2:  0.027575046051613886\n",
            "Loss S01:  0.024570824554214228\n",
            "Loss S2:  0.027551686536375578\n",
            "Loss S01:  0.024582540631850724\n",
            "Loss S2:  0.02755233949181697\n",
            "Loss S01:  0.024582602744142847\n",
            "Loss S2:  0.0275373577151403\n",
            "Loss S01:  0.024588999434791762\n",
            "Loss S2:  0.027565876350263526\n",
            "Loss S01:  0.024567794199084444\n",
            "Loss S2:  0.02754675478045571\n",
            "Loss S01:  0.02456412040147917\n",
            "Loss S2:  0.027547591640961978\n",
            "Loss S01:  0.024565491466895\n",
            "Loss S2:  0.027563070033475297\n",
            "Loss S01:  0.02457783613168322\n",
            "Loss S2:  0.0275586784022096\n",
            "Loss S01:  0.024564805048311256\n",
            "Loss S2:  0.027555630269828716\n",
            "Loss S01:  0.024548640579124478\n",
            "Loss S2:  0.027539146118260617\n",
            "Loss S01:  0.024550025162830093\n",
            "Loss S2:  0.027535724732180737\n",
            "Loss S01:  0.024558705733843212\n",
            "Loss S2:  0.027531444589413506\n",
            "Loss S01:  0.024569691060657515\n",
            "Loss S2:  0.027540191349990008\n",
            "Loss S01:  0.024588419155442152\n",
            "Loss S2:  0.027570022321680245\n",
            "Loss S01:  0.024586028778408417\n",
            "Loss S2:  0.027558692984503877\n",
            "Loss S01:  0.024563259188467124\n",
            "Loss S2:  0.02753214966359101\n",
            "Loss S01:  0.024540765134765365\n",
            "Loss S2:  0.027510470493941966\n",
            "Loss S01:  0.024546889054061764\n",
            "Loss S2:  0.027513475865683056\n",
            "Loss S01:  0.024544627589248393\n",
            "Loss S2:  0.0275025986397861\n",
            "Loss S01:  0.024550660409369547\n",
            "Loss S2:  0.027505909665675742\n",
            "Loss S01:  0.024537409865351398\n",
            "Loss S2:  0.027498896508164308\n",
            "Loss S01:  0.024541298869380605\n",
            "Loss S2:  0.027504712976768715\n",
            "Loss S01:  0.02454238486560908\n",
            "Loss S2:  0.027505034159563332\n",
            "Loss S01:  0.024534623370932138\n",
            "Loss S2:  0.0275062821449918\n",
            "Loss S01:  0.02453552577209574\n",
            "Loss S2:  0.027507279008911673\n",
            "Loss S01:  0.0245421868042242\n",
            "Loss S2:  0.027523434707379887\n",
            "Loss S01:  0.02452275351205814\n",
            "Loss S2:  0.027508812233836007\n",
            "Validation: \n",
            " Loss S01:  0.023588014766573906\n",
            " Loss S2:  0.04190932214260101\n",
            " Loss S01:  0.02487661831435703\n",
            " Loss S2:  0.04427227431109974\n",
            " Loss S01:  0.024891539045223374\n",
            " Loss S2:  0.04468147138633379\n",
            " Loss S01:  0.024829009639435125\n",
            " Loss S2:  0.04434032646603272\n",
            " Loss S01:  0.024879191323746868\n",
            " Loss S2:  0.04415251126075968\n",
            "\n",
            "Epoch: 49\n",
            "Loss S01:  0.024135207757353783\n",
            "Loss S2:  0.02769329957664013\n",
            "Loss S01:  0.02408223497596654\n",
            "Loss S2:  0.026726788079196758\n",
            "Loss S01:  0.024052597050155913\n",
            "Loss S2:  0.026847610870997112\n",
            "Loss S01:  0.024103327744430112\n",
            "Loss S2:  0.026872890550763376\n",
            "Loss S01:  0.024101874722940165\n",
            "Loss S2:  0.027105526180892455\n",
            "Loss S01:  0.024275228344634466\n",
            "Loss S2:  0.027286482244437815\n",
            "Loss S01:  0.02422272267400241\n",
            "Loss S2:  0.0273235736750677\n",
            "Loss S01:  0.02434853634888857\n",
            "Loss S2:  0.027550265035578902\n",
            "Loss S01:  0.024284128766552903\n",
            "Loss S2:  0.027445821988361853\n",
            "Loss S01:  0.024264685318365203\n",
            "Loss S2:  0.027417071061311186\n",
            "Loss S01:  0.02422728708548711\n",
            "Loss S2:  0.027320724146643487\n",
            "Loss S01:  0.02422720641904586\n",
            "Loss S2:  0.027292441241107544\n",
            "Loss S01:  0.024283544260485113\n",
            "Loss S2:  0.027327591507149137\n",
            "Loss S01:  0.024339016015065534\n",
            "Loss S2:  0.027338547959127497\n",
            "Loss S01:  0.02438727894414824\n",
            "Loss S2:  0.027366672622713636\n",
            "Loss S01:  0.02441960068678619\n",
            "Loss S2:  0.027415115583614008\n",
            "Loss S01:  0.024471221503263675\n",
            "Loss S2:  0.02742883846487688\n",
            "Loss S01:  0.024508599050299465\n",
            "Loss S2:  0.027521626141510512\n",
            "Loss S01:  0.02450209689321439\n",
            "Loss S2:  0.027571289136347193\n",
            "Loss S01:  0.024496777353720516\n",
            "Loss S2:  0.027545212763142212\n",
            "Loss S01:  0.02446466334979629\n",
            "Loss S2:  0.02750362893242148\n",
            "Loss S01:  0.024462347620683257\n",
            "Loss S2:  0.02749552756936347\n",
            "Loss S01:  0.02447260392838204\n",
            "Loss S2:  0.02750848123289611\n",
            "Loss S01:  0.024472654317364548\n",
            "Loss S2:  0.027486695307286788\n",
            "Loss S01:  0.024503958137141717\n",
            "Loss S2:  0.02749888724898649\n",
            "Loss S01:  0.0245240638292287\n",
            "Loss S2:  0.02748554564300049\n",
            "Loss S01:  0.02451936211936547\n",
            "Loss S2:  0.027511511640302067\n",
            "Loss S01:  0.024511512066495374\n",
            "Loss S2:  0.02749439073224789\n",
            "Loss S01:  0.02451941661298063\n",
            "Loss S2:  0.027475788930822097\n",
            "Loss S01:  0.02451646117384696\n",
            "Loss S2:  0.027481235538151665\n",
            "Loss S01:  0.024511792037970204\n",
            "Loss S2:  0.027474674411687345\n",
            "Loss S01:  0.024509584369479268\n",
            "Loss S2:  0.02746522218779544\n",
            "Loss S01:  0.02447903242159484\n",
            "Loss S2:  0.027431206586204956\n",
            "Loss S01:  0.024482841870755948\n",
            "Loss S2:  0.02743836264393301\n",
            "Loss S01:  0.024476734883100055\n",
            "Loss S2:  0.02741731265750973\n",
            "Loss S01:  0.02448407039470822\n",
            "Loss S2:  0.027442652997467933\n",
            "Loss S01:  0.02450306300806537\n",
            "Loss S2:  0.027456398083315\n",
            "Loss S01:  0.024476691001067263\n",
            "Loss S2:  0.027425406786871727\n",
            "Loss S01:  0.02446317674196142\n",
            "Loss S2:  0.02742150572087039\n",
            "Loss S01:  0.02444848398227826\n",
            "Loss S2:  0.02739128609523749\n",
            "Loss S01:  0.024454709704491564\n",
            "Loss S2:  0.027395517665177508\n",
            "Loss S01:  0.024472479991741715\n",
            "Loss S2:  0.02739704904257526\n",
            "Loss S01:  0.0244872041595237\n",
            "Loss S2:  0.02741337321132351\n",
            "Loss S01:  0.02448351443539916\n",
            "Loss S2:  0.02741178348141867\n",
            "Loss S01:  0.02448020932896067\n",
            "Loss S2:  0.027411167152952446\n",
            "Loss S01:  0.024487549207790726\n",
            "Loss S2:  0.02741927006374838\n",
            "Loss S01:  0.0244943200046936\n",
            "Loss S2:  0.02742802203428849\n",
            "Loss S01:  0.02450021987508057\n",
            "Loss S2:  0.027422623380607355\n",
            "Loss S01:  0.024519519961735316\n",
            "Loss S2:  0.027446175207095434\n",
            "Loss S01:  0.024503878118610673\n",
            "Loss S2:  0.027421222134756944\n",
            "Validation: \n",
            " Loss S01:  0.024112990126013756\n",
            " Loss S2:  0.04177917540073395\n",
            " Loss S01:  0.025116164503352984\n",
            " Loss S2:  0.04402438462490127\n",
            " Loss S01:  0.025019534977107512\n",
            " Loss S2:  0.044248113817558055\n",
            " Loss S01:  0.024965351874955365\n",
            " Loss S2:  0.04391144930583532\n",
            " Loss S01:  0.024995664042639143\n",
            " Loss S2:  0.04376368095845352\n",
            "\n",
            "Epoch: 50\n",
            "Loss S01:  0.023846270516514778\n",
            "Loss S2:  0.028141431510448456\n",
            "Loss S01:  0.02379108796065504\n",
            "Loss S2:  0.026845597916028717\n",
            "Loss S01:  0.023943602329208738\n",
            "Loss S2:  0.026941082839454924\n",
            "Loss S01:  0.02404730724951913\n",
            "Loss S2:  0.02699504173811405\n",
            "Loss S01:  0.024103308777983595\n",
            "Loss S2:  0.027014848980598333\n",
            "Loss S01:  0.024295334393779438\n",
            "Loss S2:  0.027216563731724142\n",
            "Loss S01:  0.024229296315156044\n",
            "Loss S2:  0.027168980845418134\n",
            "Loss S01:  0.024308536962514192\n",
            "Loss S2:  0.027292551892534107\n",
            "Loss S01:  0.024177181109050174\n",
            "Loss S2:  0.02719372908734245\n",
            "Loss S01:  0.02415497820046577\n",
            "Loss S2:  0.02719335644864119\n",
            "Loss S01:  0.024135061106321836\n",
            "Loss S2:  0.027194415689399926\n",
            "Loss S01:  0.024190926521613792\n",
            "Loss S2:  0.027209169575357222\n",
            "Loss S01:  0.024238843978805977\n",
            "Loss S2:  0.02723317798073134\n",
            "Loss S01:  0.02424290636909827\n",
            "Loss S2:  0.027232821149225452\n",
            "Loss S01:  0.02426625188466505\n",
            "Loss S2:  0.02723333904030898\n",
            "Loss S01:  0.02430872604349591\n",
            "Loss S2:  0.027255979274085025\n",
            "Loss S01:  0.02431870438158512\n",
            "Loss S2:  0.027227523256532896\n",
            "Loss S01:  0.02434843827627207\n",
            "Loss S2:  0.027284215449502592\n",
            "Loss S01:  0.024370074014884332\n",
            "Loss S2:  0.027334633772132806\n",
            "Loss S01:  0.024327937152763313\n",
            "Loss S2:  0.02730348421485012\n",
            "Loss S01:  0.024296360006972925\n",
            "Loss S2:  0.02726036894010074\n",
            "Loss S01:  0.02429915631799054\n",
            "Loss S2:  0.027251460002426287\n",
            "Loss S01:  0.024297921442135967\n",
            "Loss S2:  0.027262670379890577\n",
            "Loss S01:  0.024319230179701532\n",
            "Loss S2:  0.02725562559706824\n",
            "Loss S01:  0.024360694300336955\n",
            "Loss S2:  0.027296428788932528\n",
            "Loss S01:  0.0243657848081622\n",
            "Loss S2:  0.0272991332444418\n",
            "Loss S01:  0.024392685392635994\n",
            "Loss S2:  0.027315551636319508\n",
            "Loss S01:  0.024368868982659934\n",
            "Loss S2:  0.027305085837456132\n",
            "Loss S01:  0.024368348422052596\n",
            "Loss S2:  0.027301903316930096\n",
            "Loss S01:  0.024384233415690074\n",
            "Loss S2:  0.02734278267908752\n",
            "Loss S01:  0.024378649129275468\n",
            "Loss S2:  0.02733398061183798\n",
            "Loss S01:  0.02437549565358752\n",
            "Loss S2:  0.02731053589864176\n",
            "Loss S01:  0.024355245963017517\n",
            "Loss S2:  0.027283920056398413\n",
            "Loss S01:  0.024358960875841787\n",
            "Loss S2:  0.02728723524717946\n",
            "Loss S01:  0.024365213248998888\n",
            "Loss S2:  0.027274060213277416\n",
            "Loss S01:  0.024366983703398636\n",
            "Loss S2:  0.027293857972421537\n",
            "Loss S01:  0.024382100107762292\n",
            "Loss S2:  0.02731978566672168\n",
            "Loss S01:  0.02436622595188592\n",
            "Loss S2:  0.027305822553379194\n",
            "Loss S01:  0.024358128708374158\n",
            "Loss S2:  0.027299997780540482\n",
            "Loss S01:  0.02434215293554089\n",
            "Loss S2:  0.027275332647478185\n",
            "Loss S01:  0.024338756657746666\n",
            "Loss S2:  0.027267155170143394\n",
            "Loss S01:  0.024356649812410636\n",
            "Loss S2:  0.02727751505001473\n",
            "Loss S01:  0.024374381389088415\n",
            "Loss S2:  0.027292986101782803\n",
            "Loss S01:  0.024360887758139944\n",
            "Loss S2:  0.027289050365441637\n",
            "Loss S01:  0.024350777297975527\n",
            "Loss S2:  0.027269506919917877\n",
            "Loss S01:  0.024344923903963252\n",
            "Loss S2:  0.0272746239510053\n",
            "Loss S01:  0.024342917883667665\n",
            "Loss S2:  0.02729086636478756\n",
            "Loss S01:  0.024350752773241887\n",
            "Loss S2:  0.02728730712615887\n",
            "Loss S01:  0.0243527051526085\n",
            "Loss S2:  0.02729814724605569\n",
            "Loss S01:  0.024334930687717898\n",
            "Loss S2:  0.027272727731198012\n",
            "Validation: \n",
            " Loss S01:  0.02374941296875477\n",
            " Loss S2:  0.04027306288480759\n",
            " Loss S01:  0.025022176050004504\n",
            " Loss S2:  0.04301068594767934\n",
            " Loss S01:  0.025009394191750665\n",
            " Loss S2:  0.043433622434371856\n",
            " Loss S01:  0.025024046235885777\n",
            " Loss S2:  0.04303066652329242\n",
            " Loss S01:  0.02511641187708319\n",
            " Loss S2:  0.042942865349260374\n",
            "\n",
            "Epoch: 51\n",
            "Loss S01:  0.02332139201462269\n",
            "Loss S2:  0.027636803686618805\n",
            "Loss S01:  0.024217810143123974\n",
            "Loss S2:  0.026626257395202465\n",
            "Loss S01:  0.024118361639834586\n",
            "Loss S2:  0.02673641610003653\n",
            "Loss S01:  0.024061190625352245\n",
            "Loss S2:  0.026732914750614473\n",
            "Loss S01:  0.024173652298930214\n",
            "Loss S2:  0.026961541848211753\n",
            "Loss S01:  0.024256230532830835\n",
            "Loss S2:  0.026968384234636437\n",
            "Loss S01:  0.02418822324911102\n",
            "Loss S2:  0.02690984461395467\n",
            "Loss S01:  0.02434681801938675\n",
            "Loss S2:  0.02706208701809527\n",
            "Loss S01:  0.024248771093509817\n",
            "Loss S2:  0.027008838698635868\n",
            "Loss S01:  0.024255873778691657\n",
            "Loss S2:  0.0270081216694562\n",
            "Loss S01:  0.02420266435509271\n",
            "Loss S2:  0.026986001465137643\n",
            "Loss S01:  0.02417199107239375\n",
            "Loss S2:  0.02704137045185308\n",
            "Loss S01:  0.02413459263803545\n",
            "Loss S2:  0.027074227417426662\n",
            "Loss S01:  0.024128556322619205\n",
            "Loss S2:  0.027099248306214355\n",
            "Loss S01:  0.02418172358145528\n",
            "Loss S2:  0.027147807603609478\n",
            "Loss S01:  0.024224204584857485\n",
            "Loss S2:  0.027187221356673747\n",
            "Loss S01:  0.0242375785380787\n",
            "Loss S2:  0.02720086244137391\n",
            "Loss S01:  0.024276418055881533\n",
            "Loss S2:  0.02724041125317763\n",
            "Loss S01:  0.024284700796172762\n",
            "Loss S2:  0.027258924573906877\n",
            "Loss S01:  0.024262417137076716\n",
            "Loss S2:  0.027230894280821864\n",
            "Loss S01:  0.024229522340407417\n",
            "Loss S2:  0.02719436652624785\n",
            "Loss S01:  0.024235467898760927\n",
            "Loss S2:  0.027197159049917735\n",
            "Loss S01:  0.02422816570999935\n",
            "Loss S2:  0.027171765104340752\n",
            "Loss S01:  0.0242317592104276\n",
            "Loss S2:  0.02716742396290168\n",
            "Loss S01:  0.024252908598028772\n",
            "Loss S2:  0.027185393435517288\n",
            "Loss S01:  0.024253476507575863\n",
            "Loss S2:  0.02716341040345777\n",
            "Loss S01:  0.02425916029061851\n",
            "Loss S2:  0.027167466872325345\n",
            "Loss S01:  0.024253422696816965\n",
            "Loss S2:  0.027143245425446445\n",
            "Loss S01:  0.024246631302867496\n",
            "Loss S2:  0.027116481194875842\n",
            "Loss S01:  0.024247794236053305\n",
            "Loss S2:  0.02710811642744287\n",
            "Loss S01:  0.024250838286753906\n",
            "Loss S2:  0.027110604284659178\n",
            "Loss S01:  0.024246944521832313\n",
            "Loss S2:  0.027102806853950983\n",
            "Loss S01:  0.024220887612191686\n",
            "Loss S2:  0.02708431372761355\n",
            "Loss S01:  0.024234230807360566\n",
            "Loss S2:  0.02709479874219419\n",
            "Loss S01:  0.024247655137018723\n",
            "Loss S2:  0.027100026662136453\n",
            "Loss S01:  0.024267988688076322\n",
            "Loss S2:  0.027133587652292006\n",
            "Loss S01:  0.024280681999766594\n",
            "Loss S2:  0.027164533868473322\n",
            "Loss S01:  0.024268615820176517\n",
            "Loss S2:  0.0271577216343417\n",
            "Loss S01:  0.024259712415065353\n",
            "Loss S2:  0.027146361880688842\n",
            "Loss S01:  0.024238186900306234\n",
            "Loss S2:  0.027124321488353906\n",
            "Loss S01:  0.024242716518572145\n",
            "Loss S2:  0.027114728930176343\n",
            "Loss S01:  0.024273084462994206\n",
            "Loss S2:  0.027116978944363095\n",
            "Loss S01:  0.024290087648320084\n",
            "Loss S2:  0.02711630418383452\n",
            "Loss S01:  0.024275338964368242\n",
            "Loss S2:  0.027101805744792082\n",
            "Loss S01:  0.024277629128017394\n",
            "Loss S2:  0.027094840517694178\n",
            "Loss S01:  0.0242764682007206\n",
            "Loss S2:  0.02709073140523386\n",
            "Loss S01:  0.02427445056720954\n",
            "Loss S2:  0.027093222001143235\n",
            "Loss S01:  0.024270155635166067\n",
            "Loss S2:  0.027084943373744907\n",
            "Loss S01:  0.024281508918549563\n",
            "Loss S2:  0.02709574463415171\n",
            "Loss S01:  0.024268085224261837\n",
            "Loss S2:  0.02707361395408204\n",
            "Validation: \n",
            " Loss S01:  0.023510273545980453\n",
            " Loss S2:  0.041482094675302505\n",
            " Loss S01:  0.025010597847756885\n",
            " Loss S2:  0.04379704931662196\n",
            " Loss S01:  0.02488491361642756\n",
            " Loss S2:  0.043981120808095464\n",
            " Loss S01:  0.024872334643465575\n",
            " Loss S2:  0.043598327358238036\n",
            " Loss S01:  0.024987249002780442\n",
            " Loss S2:  0.043479297816017524\n",
            "\n",
            "Epoch: 52\n",
            "Loss S01:  0.024848658591508865\n",
            "Loss S2:  0.028599247336387634\n",
            "Loss S01:  0.02365526489236138\n",
            "Loss S2:  0.026556667279113422\n",
            "Loss S01:  0.023857788227143743\n",
            "Loss S2:  0.02696955008875756\n",
            "Loss S01:  0.023851966485381126\n",
            "Loss S2:  0.02685082277222987\n",
            "Loss S01:  0.023924381812898125\n",
            "Loss S2:  0.026952912622108693\n",
            "Loss S01:  0.024045990268681563\n",
            "Loss S2:  0.02702419836001069\n",
            "Loss S01:  0.024000339210033417\n",
            "Loss S2:  0.027062349845884275\n",
            "Loss S01:  0.024047001461747666\n",
            "Loss S2:  0.027136452338645155\n",
            "Loss S01:  0.023975790811725604\n",
            "Loss S2:  0.027005266053257166\n",
            "Loss S01:  0.02395252332828202\n",
            "Loss S2:  0.02697359326367195\n",
            "Loss S01:  0.02393821289412456\n",
            "Loss S2:  0.026970612014284228\n",
            "Loss S01:  0.023993694265415002\n",
            "Loss S2:  0.027006264692088507\n",
            "Loss S01:  0.02401934662633691\n",
            "Loss S2:  0.027057183182929174\n",
            "Loss S01:  0.02402452520229889\n",
            "Loss S2:  0.027052891661078875\n",
            "Loss S01:  0.02405956173513798\n",
            "Loss S2:  0.02707364341468676\n",
            "Loss S01:  0.02408565654857269\n",
            "Loss S2:  0.027108317401432835\n",
            "Loss S01:  0.024110483086627464\n",
            "Loss S2:  0.027124775492626686\n",
            "Loss S01:  0.024142476016579317\n",
            "Loss S2:  0.027204620167177323\n",
            "Loss S01:  0.02419100083328413\n",
            "Loss S2:  0.027233312390127235\n",
            "Loss S01:  0.024160869127012673\n",
            "Loss S2:  0.027191415614404604\n",
            "Loss S01:  0.024138392333812382\n",
            "Loss S2:  0.027133851752278223\n",
            "Loss S01:  0.024118124697163207\n",
            "Loss S2:  0.027124864292017655\n",
            "Loss S01:  0.024109820805793433\n",
            "Loss S2:  0.02711350737114298\n",
            "Loss S01:  0.024116728043594916\n",
            "Loss S2:  0.02708498611078634\n",
            "Loss S01:  0.02415769823915731\n",
            "Loss S2:  0.027095621861362853\n",
            "Loss S01:  0.024147478930384988\n",
            "Loss S2:  0.027096076011360878\n",
            "Loss S01:  0.024161408423406867\n",
            "Loss S2:  0.0271032873569663\n",
            "Loss S01:  0.02413464824899316\n",
            "Loss S2:  0.027070884039536173\n",
            "Loss S01:  0.02412917598207014\n",
            "Loss S2:  0.027057625227392357\n",
            "Loss S01:  0.02413257865612859\n",
            "Loss S2:  0.027073323451417827\n",
            "Loss S01:  0.024134389239292207\n",
            "Loss S2:  0.027088428299836938\n",
            "Loss S01:  0.024127737436311806\n",
            "Loss S2:  0.02707901864740818\n",
            "Loss S01:  0.024105604958497104\n",
            "Loss S2:  0.02705977815333928\n",
            "Loss S01:  0.024111216223807134\n",
            "Loss S2:  0.027035099417646487\n",
            "Loss S01:  0.02411563349259564\n",
            "Loss S2:  0.027021852690628196\n",
            "Loss S01:  0.024114450089569785\n",
            "Loss S2:  0.027043442435541385\n",
            "Loss S01:  0.024109756754623554\n",
            "Loss S2:  0.027041273723018466\n",
            "Loss S01:  0.02409577353121939\n",
            "Loss S2:  0.02700965755149682\n",
            "Loss S01:  0.024078990296116026\n",
            "Loss S2:  0.02700391847376279\n",
            "Loss S01:  0.02407272732661813\n",
            "Loss S2:  0.02698438484555163\n",
            "Loss S01:  0.02408173618526976\n",
            "Loss S2:  0.0269827052226239\n",
            "Loss S01:  0.024091591437657673\n",
            "Loss S2:  0.026985978150237216\n",
            "Loss S01:  0.024095788403389856\n",
            "Loss S2:  0.0269854766871966\n",
            "Loss S01:  0.024083026375820354\n",
            "Loss S2:  0.026969489423242635\n",
            "Loss S01:  0.02407101262149627\n",
            "Loss S2:  0.026960882453285918\n",
            "Loss S01:  0.024066003915046378\n",
            "Loss S2:  0.026968928615659672\n",
            "Loss S01:  0.024062415083906656\n",
            "Loss S2:  0.026971944573450504\n",
            "Loss S01:  0.02405347919483094\n",
            "Loss S2:  0.02696699711579173\n",
            "Loss S01:  0.024054151864842416\n",
            "Loss S2:  0.026982893067530187\n",
            "Loss S01:  0.024039354873346942\n",
            "Loss S2:  0.0269623348478023\n",
            "Validation: \n",
            " Loss S01:  0.0231956634670496\n",
            " Loss S2:  0.04160810261964798\n",
            " Loss S01:  0.024598974291057812\n",
            " Loss S2:  0.043498477942886804\n",
            " Loss S01:  0.024559179230070696\n",
            " Loss S2:  0.043743413941162386\n",
            " Loss S01:  0.02459967511965603\n",
            " Loss S2:  0.043364362577434444\n",
            " Loss S01:  0.024692929591293687\n",
            " Loss S2:  0.04322568451364835\n",
            "\n",
            "Epoch: 53\n",
            "Loss S01:  0.025244342163205147\n",
            "Loss S2:  0.027857495471835136\n",
            "Loss S01:  0.023453540592031044\n",
            "Loss S2:  0.02620339495214549\n",
            "Loss S01:  0.023593850788616\n",
            "Loss S2:  0.026574964147238506\n",
            "Loss S01:  0.023627823639300563\n",
            "Loss S2:  0.02656548854804808\n",
            "Loss S01:  0.023830039108671795\n",
            "Loss S2:  0.026830646731868024\n",
            "Loss S01:  0.023974047243302942\n",
            "Loss S2:  0.02696765652474235\n",
            "Loss S01:  0.023950683746914395\n",
            "Loss S2:  0.026964176537804915\n",
            "Loss S01:  0.02399465404975582\n",
            "Loss S2:  0.02707379906844925\n",
            "Loss S01:  0.023811694763508844\n",
            "Loss S2:  0.026960791376085928\n",
            "Loss S01:  0.02380369267948381\n",
            "Loss S2:  0.026925428535093318\n",
            "Loss S01:  0.023821253956544518\n",
            "Loss S2:  0.026913556076660013\n",
            "Loss S01:  0.023848664401485038\n",
            "Loss S2:  0.02693803996049069\n",
            "Loss S01:  0.023884849084063995\n",
            "Loss S2:  0.02696252254045699\n",
            "Loss S01:  0.023906469643684744\n",
            "Loss S2:  0.026966598652701342\n",
            "Loss S01:  0.023951347507800615\n",
            "Loss S2:  0.02695992850559823\n",
            "Loss S01:  0.02400227100742574\n",
            "Loss S2:  0.02700882852817608\n",
            "Loss S01:  0.023992955777215663\n",
            "Loss S2:  0.026991603514237433\n",
            "Loss S01:  0.023991033402189874\n",
            "Loss S2:  0.027051745854013147\n",
            "Loss S01:  0.023985866004426176\n",
            "Loss S2:  0.02705991418404474\n",
            "Loss S01:  0.023948890854316857\n",
            "Loss S2:  0.027019244526069203\n",
            "Loss S01:  0.023928023680154956\n",
            "Loss S2:  0.02699166084106882\n",
            "Loss S01:  0.023926174762477808\n",
            "Loss S2:  0.026980962763182924\n",
            "Loss S01:  0.02393928563803839\n",
            "Loss S2:  0.026984522599575206\n",
            "Loss S01:  0.023920577858046535\n",
            "Loss S2:  0.026937667119167584\n",
            "Loss S01:  0.023950125465625548\n",
            "Loss S2:  0.026943947810726048\n",
            "Loss S01:  0.023961076498090983\n",
            "Loss S2:  0.02692129911921651\n",
            "Loss S01:  0.02396200021364908\n",
            "Loss S2:  0.02692919779012258\n",
            "Loss S01:  0.023952328768383532\n",
            "Loss S2:  0.026917554135692075\n",
            "Loss S01:  0.023954933694832266\n",
            "Loss S2:  0.026909590874437336\n",
            "Loss S01:  0.02395972064168183\n",
            "Loss S2:  0.02691411267313146\n",
            "Loss S01:  0.02396057794903996\n",
            "Loss S2:  0.026912846692228237\n",
            "Loss S01:  0.02395001017275922\n",
            "Loss S2:  0.026897950684861355\n",
            "Loss S01:  0.023934181860859893\n",
            "Loss S2:  0.026879579519324955\n",
            "Loss S01:  0.023930541827734502\n",
            "Loss S2:  0.026857945848582733\n",
            "Loss S01:  0.023934939739641205\n",
            "Loss S2:  0.026851417059816224\n",
            "Loss S01:  0.023937352282921133\n",
            "Loss S2:  0.026864345055444966\n",
            "Loss S01:  0.02394772524277259\n",
            "Loss S2:  0.026871788864981105\n",
            "Loss S01:  0.02393022365366031\n",
            "Loss S2:  0.026855968747217702\n",
            "Loss S01:  0.02391177988818937\n",
            "Loss S2:  0.026845336287939954\n",
            "Loss S01:  0.023897737278924573\n",
            "Loss S2:  0.026815852274179763\n",
            "Loss S01:  0.02389690362177139\n",
            "Loss S2:  0.026802554694085646\n",
            "Loss S01:  0.023907380545661397\n",
            "Loss S2:  0.026793658289233552\n",
            "Loss S01:  0.023914962122106212\n",
            "Loss S2:  0.02679013950648472\n",
            "Loss S01:  0.023899309838454296\n",
            "Loss S2:  0.026785912783060164\n",
            "Loss S01:  0.023903488470853863\n",
            "Loss S2:  0.026780780569529858\n",
            "Loss S01:  0.02390263126357696\n",
            "Loss S2:  0.02678693676503693\n",
            "Loss S01:  0.023898377715571583\n",
            "Loss S2:  0.02679176792227653\n",
            "Loss S01:  0.023897380142961083\n",
            "Loss S2:  0.026784842090400416\n",
            "Loss S01:  0.023903411400665115\n",
            "Loss S2:  0.02679987376838489\n",
            "Loss S01:  0.023897069603083333\n",
            "Loss S2:  0.026788146479734334\n",
            "Validation: \n",
            " Loss S01:  0.024055713787674904\n",
            " Loss S2:  0.04141126573085785\n",
            " Loss S01:  0.025330579351811183\n",
            " Loss S2:  0.04445265002903484\n",
            " Loss S01:  0.02518535668893558\n",
            " Loss S2:  0.04469821883774385\n",
            " Loss S01:  0.02513342866765671\n",
            " Loss S2:  0.04433548248937873\n",
            " Loss S01:  0.025222167349707933\n",
            " Loss S2:  0.04420068896847007\n",
            "\n",
            "Epoch: 54\n",
            "Loss S01:  0.024864740669727325\n",
            "Loss S2:  0.028899144381284714\n",
            "Loss S01:  0.023488539186390964\n",
            "Loss S2:  0.026213164525953205\n",
            "Loss S01:  0.02364639033164297\n",
            "Loss S2:  0.026441268711572603\n",
            "Loss S01:  0.023748889505382505\n",
            "Loss S2:  0.026507420164923513\n",
            "Loss S01:  0.023759145425950607\n",
            "Loss S2:  0.02663336094559693\n",
            "Loss S01:  0.023826940825172498\n",
            "Loss S2:  0.02667586005055437\n",
            "Loss S01:  0.02373011052974912\n",
            "Loss S2:  0.026538205531532646\n",
            "Loss S01:  0.023774837943869576\n",
            "Loss S2:  0.02664769923603031\n",
            "Loss S01:  0.02375061207531411\n",
            "Loss S2:  0.02657986124172623\n",
            "Loss S01:  0.023728673888759298\n",
            "Loss S2:  0.02661839913535904\n",
            "Loss S01:  0.023673891965853105\n",
            "Loss S2:  0.026595994461290906\n",
            "Loss S01:  0.0236960154463042\n",
            "Loss S2:  0.026598289692858317\n",
            "Loss S01:  0.02371417447801464\n",
            "Loss S2:  0.026617605808722085\n",
            "Loss S01:  0.023757946769927294\n",
            "Loss S2:  0.026641042177913753\n",
            "Loss S01:  0.023787626890954396\n",
            "Loss S2:  0.02662999116888283\n",
            "Loss S01:  0.023809109240869025\n",
            "Loss S2:  0.026672564237224344\n",
            "Loss S01:  0.023829594485878205\n",
            "Loss S2:  0.02669986803663073\n",
            "Loss S01:  0.023840218597249677\n",
            "Loss S2:  0.026737924741461264\n",
            "Loss S01:  0.023846372776762558\n",
            "Loss S2:  0.02681086765015652\n",
            "Loss S01:  0.02384075846191476\n",
            "Loss S2:  0.026774201103295956\n",
            "Loss S01:  0.023807888350157597\n",
            "Loss S2:  0.026726774741612856\n",
            "Loss S01:  0.023794370398843457\n",
            "Loss S2:  0.026729233201010533\n",
            "Loss S01:  0.023813640496512346\n",
            "Loss S2:  0.026721863059234297\n",
            "Loss S01:  0.02381066299039564\n",
            "Loss S2:  0.026695699596301817\n",
            "Loss S01:  0.023835089343650213\n",
            "Loss S2:  0.026703733494853082\n",
            "Loss S01:  0.023835289139909097\n",
            "Loss S2:  0.026680313147218578\n",
            "Loss S01:  0.023845984097356082\n",
            "Loss S2:  0.02667676533261935\n",
            "Loss S01:  0.023838046896941547\n",
            "Loss S2:  0.026661332772281776\n",
            "Loss S01:  0.02383573759625603\n",
            "Loss S2:  0.026652501677278947\n",
            "Loss S01:  0.023842923733633\n",
            "Loss S2:  0.026654404866480335\n",
            "Loss S01:  0.023841254456445228\n",
            "Loss S2:  0.026652897302791526\n",
            "Loss S01:  0.0238350235716323\n",
            "Loss S2:  0.026643275226379513\n",
            "Loss S01:  0.023808277655984755\n",
            "Loss S2:  0.026632921364030733\n",
            "Loss S01:  0.023820291428047365\n",
            "Loss S2:  0.026637823481304047\n",
            "Loss S01:  0.02381076072165169\n",
            "Loss S2:  0.026635804538590466\n",
            "Loss S01:  0.023832721885113296\n",
            "Loss S2:  0.0266720726290065\n",
            "Loss S01:  0.023838150198893866\n",
            "Loss S2:  0.026677050609709152\n",
            "Loss S01:  0.023827315526550027\n",
            "Loss S2:  0.02665418909104526\n",
            "Loss S01:  0.023811684848051372\n",
            "Loss S2:  0.026659163243071302\n",
            "Loss S01:  0.023781576000935282\n",
            "Loss S2:  0.026646159289171325\n",
            "Loss S01:  0.023784906684683447\n",
            "Loss S2:  0.026636659523049496\n",
            "Loss S01:  0.023791538397362342\n",
            "Loss S2:  0.026632871138897254\n",
            "Loss S01:  0.023798092137889453\n",
            "Loss S2:  0.02663471297562264\n",
            "Loss S01:  0.023785957288679833\n",
            "Loss S2:  0.026618555793214562\n",
            "Loss S01:  0.023785339398077286\n",
            "Loss S2:  0.02661935895123855\n",
            "Loss S01:  0.02378783576983438\n",
            "Loss S2:  0.02662106538955494\n",
            "Loss S01:  0.02379749686641057\n",
            "Loss S2:  0.02663543849876413\n",
            "Loss S01:  0.023805964520624237\n",
            "Loss S2:  0.02664262165499341\n",
            "Loss S01:  0.02381144025268153\n",
            "Loss S2:  0.026650269006586125\n",
            "Loss S01:  0.023793816255394163\n",
            "Loss S2:  0.026629696973806242\n",
            "Validation: \n",
            " Loss S01:  0.023115959018468857\n",
            " Loss S2:  0.04178609699010849\n",
            " Loss S01:  0.024347508060080663\n",
            " Loss S2:  0.044379735099417825\n",
            " Loss S01:  0.024160043041153653\n",
            " Loss S2:  0.04458727042486028\n",
            " Loss S01:  0.024052938141050886\n",
            " Loss S2:  0.04415503400759619\n",
            " Loss S01:  0.0241174064806582\n",
            " Loss S2:  0.04398503205106582\n",
            "\n",
            "Epoch: 55\n",
            "Loss S01:  0.02438139170408249\n",
            "Loss S2:  0.027372704818844795\n",
            "Loss S01:  0.023203839124603706\n",
            "Loss S2:  0.02621776746077971\n",
            "Loss S01:  0.023373092569056012\n",
            "Loss S2:  0.02640682343570959\n",
            "Loss S01:  0.023377622207326275\n",
            "Loss S2:  0.026402378034207127\n",
            "Loss S01:  0.02349237207232452\n",
            "Loss S2:  0.02649697134407555\n",
            "Loss S01:  0.02372782461929555\n",
            "Loss S2:  0.026616819385512202\n",
            "Loss S01:  0.023724475386934202\n",
            "Loss S2:  0.02654806558103835\n",
            "Loss S01:  0.02372458508946526\n",
            "Loss S2:  0.026622400238690242\n",
            "Loss S01:  0.023562108499952304\n",
            "Loss S2:  0.026444490240127953\n",
            "Loss S01:  0.0235441145333615\n",
            "Loss S2:  0.026426800414101108\n",
            "Loss S01:  0.023496590388735923\n",
            "Loss S2:  0.026411999551819103\n",
            "Loss S01:  0.023543931812316447\n",
            "Loss S2:  0.026497589877328358\n",
            "Loss S01:  0.023552458918045374\n",
            "Loss S2:  0.02652103730962296\n",
            "Loss S01:  0.023583599686167623\n",
            "Loss S2:  0.02653948803439395\n",
            "Loss S01:  0.023639290428436394\n",
            "Loss S2:  0.026576361467018195\n",
            "Loss S01:  0.023682821147291866\n",
            "Loss S2:  0.026638291467795307\n",
            "Loss S01:  0.02369989433921642\n",
            "Loss S2:  0.026659635190637954\n",
            "Loss S01:  0.0237197831752356\n",
            "Loss S2:  0.02670967376284432\n",
            "Loss S01:  0.023744861332915763\n",
            "Loss S2:  0.02673722026632965\n",
            "Loss S01:  0.023737315829667746\n",
            "Loss S2:  0.02671814394909986\n",
            "Loss S01:  0.0237029200299314\n",
            "Loss S2:  0.02666860172960592\n",
            "Loss S01:  0.023687127174288742\n",
            "Loss S2:  0.026663159889773736\n",
            "Loss S01:  0.023696839152013555\n",
            "Loss S2:  0.026670629462033375\n",
            "Loss S01:  0.023663185036930687\n",
            "Loss S2:  0.026620262177475602\n",
            "Loss S01:  0.02369116811676886\n",
            "Loss S2:  0.026642724601126804\n",
            "Loss S01:  0.023691501193846838\n",
            "Loss S2:  0.026607540692109986\n",
            "Loss S01:  0.023694376607507582\n",
            "Loss S2:  0.026619542918214396\n",
            "Loss S01:  0.0236973697835119\n",
            "Loss S2:  0.026618420974629832\n",
            "Loss S01:  0.02369851210018706\n",
            "Loss S2:  0.026598886258608505\n",
            "Loss S01:  0.023699386147750204\n",
            "Loss S2:  0.026596192078492075\n",
            "Loss S01:  0.023693274335310703\n",
            "Loss S2:  0.026582699894063498\n",
            "Loss S01:  0.023675953136858833\n",
            "Loss S2:  0.02656213544500794\n",
            "Loss S01:  0.023656201963762627\n",
            "Loss S2:  0.026540558669147462\n",
            "Loss S01:  0.023656459418308337\n",
            "Loss S2:  0.02654100694550128\n",
            "Loss S01:  0.023659589719518887\n",
            "Loss S2:  0.02654843942925902\n",
            "Loss S01:  0.023656019229877027\n",
            "Loss S2:  0.026575327370623576\n",
            "Loss S01:  0.023673071863950124\n",
            "Loss S2:  0.02660293455578779\n",
            "Loss S01:  0.023673650223731673\n",
            "Loss S2:  0.026592627268113858\n",
            "Loss S01:  0.023670519479694643\n",
            "Loss S2:  0.02659792255541784\n",
            "Loss S01:  0.023664993817544045\n",
            "Loss S2:  0.026578934377302296\n",
            "Loss S01:  0.023681390175869933\n",
            "Loss S2:  0.026586756741911396\n",
            "Loss S01:  0.0237017086867941\n",
            "Loss S2:  0.026586476495883762\n",
            "Loss S01:  0.02369999808575648\n",
            "Loss S2:  0.026595214439330363\n",
            "Loss S01:  0.02368068410701652\n",
            "Loss S2:  0.026574767322177954\n",
            "Loss S01:  0.02367931647889333\n",
            "Loss S2:  0.026566784710638107\n",
            "Loss S01:  0.0236701355821674\n",
            "Loss S2:  0.02656099059414573\n",
            "Loss S01:  0.023683378222416642\n",
            "Loss S2:  0.026577740261197864\n",
            "Loss S01:  0.023675976669111323\n",
            "Loss S2:  0.026568597447593753\n",
            "Loss S01:  0.023688335192395594\n",
            "Loss S2:  0.026570036875668782\n",
            "Loss S01:  0.02366638563709929\n",
            "Loss S2:  0.02654397476259657\n",
            "Validation: \n",
            " Loss S01:  0.023547250777482986\n",
            " Loss S2:  0.040283434092998505\n",
            " Loss S01:  0.024768002508651642\n",
            " Loss S2:  0.0429161742684387\n",
            " Loss S01:  0.024554108201367098\n",
            " Loss S2:  0.04300135446757805\n",
            " Loss S01:  0.02449851391501114\n",
            " Loss S2:  0.042606416295786376\n",
            " Loss S01:  0.02457520303626855\n",
            " Loss S2:  0.04246453430365633\n",
            "\n",
            "Epoch: 56\n",
            "Loss S01:  0.024831393733620644\n",
            "Loss S2:  0.029104841873049736\n",
            "Loss S01:  0.0234629711644216\n",
            "Loss S2:  0.026465145701711826\n",
            "Loss S01:  0.023497068899728003\n",
            "Loss S2:  0.02645697630941868\n",
            "Loss S01:  0.02346082290093745\n",
            "Loss S2:  0.026261351101340785\n",
            "Loss S01:  0.02356235251375815\n",
            "Loss S2:  0.02641678510642633\n",
            "Loss S01:  0.02373360820552882\n",
            "Loss S2:  0.026497576750961004\n",
            "Loss S01:  0.023729028027565754\n",
            "Loss S2:  0.02650442539302052\n",
            "Loss S01:  0.023723322766977296\n",
            "Loss S2:  0.02663673421131893\n",
            "Loss S01:  0.02357900871630804\n",
            "Loss S2:  0.026496476045361272\n",
            "Loss S01:  0.023543100415186567\n",
            "Loss S2:  0.02652846256291473\n",
            "Loss S01:  0.023486301081605477\n",
            "Loss S2:  0.02645477032897496\n",
            "Loss S01:  0.023523574174792918\n",
            "Loss S2:  0.026506251412200497\n",
            "Loss S01:  0.023549665504496945\n",
            "Loss S2:  0.026577260129707903\n",
            "Loss S01:  0.02355405921471938\n",
            "Loss S2:  0.026566994525776565\n",
            "Loss S01:  0.023598616678558344\n",
            "Loss S2:  0.026591298058100624\n",
            "Loss S01:  0.023668752588479725\n",
            "Loss S2:  0.02663217277716327\n",
            "Loss S01:  0.02368208386513017\n",
            "Loss S2:  0.026613722445431705\n",
            "Loss S01:  0.023732827802063428\n",
            "Loss S2:  0.02667253736776915\n",
            "Loss S01:  0.023744759762155416\n",
            "Loss S2:  0.026703680119379448\n",
            "Loss S01:  0.02370823800797862\n",
            "Loss S2:  0.026663127701475983\n",
            "Loss S01:  0.02368878148755624\n",
            "Loss S2:  0.026637848290563815\n",
            "Loss S01:  0.023666674747921845\n",
            "Loss S2:  0.0266426675003993\n",
            "Loss S01:  0.02366181623982898\n",
            "Loss S2:  0.026653375788444306\n",
            "Loss S01:  0.023680279434565858\n",
            "Loss S2:  0.026635044637374032\n",
            "Loss S01:  0.02371395228592934\n",
            "Loss S2:  0.02666397848750051\n",
            "Loss S01:  0.023715326280529754\n",
            "Loss S2:  0.026645711441320254\n",
            "Loss S01:  0.023718739784825808\n",
            "Loss S2:  0.026635930091999042\n",
            "Loss S01:  0.023701796548722856\n",
            "Loss S2:  0.026616500986128274\n",
            "Loss S01:  0.023721998935280323\n",
            "Loss S2:  0.02661422106480471\n",
            "Loss S01:  0.023698503958042134\n",
            "Loss S2:  0.02660776582575336\n",
            "Loss S01:  0.0236898606946302\n",
            "Loss S2:  0.02659000702004298\n",
            "Loss S01:  0.02367792421670397\n",
            "Loss S2:  0.026566976819103555\n",
            "Loss S01:  0.02365876016471059\n",
            "Loss S2:  0.02654239920046285\n",
            "Loss S01:  0.02365619890121715\n",
            "Loss S2:  0.026543954634909543\n",
            "Loss S01:  0.02366361303850353\n",
            "Loss S2:  0.02653863119873658\n",
            "Loss S01:  0.02366192352313262\n",
            "Loss S2:  0.02655697228689479\n",
            "Loss S01:  0.02366439419749536\n",
            "Loss S2:  0.02657016047851861\n",
            "Loss S01:  0.023649520730233256\n",
            "Loss S2:  0.02655065023513496\n",
            "Loss S01:  0.023634229246478067\n",
            "Loss S2:  0.026554058078588464\n",
            "Loss S01:  0.023610078431951724\n",
            "Loss S2:  0.026511042428862715\n",
            "Loss S01:  0.023623097539319658\n",
            "Loss S2:  0.026517083882319364\n",
            "Loss S01:  0.023627813230212007\n",
            "Loss S2:  0.026503572843225624\n",
            "Loss S01:  0.023637547584653185\n",
            "Loss S2:  0.026500248175441794\n",
            "Loss S01:  0.0236192204429835\n",
            "Loss S2:  0.026483100927553706\n",
            "Loss S01:  0.023615694902583854\n",
            "Loss S2:  0.02649365242173612\n",
            "Loss S01:  0.02361735112618184\n",
            "Loss S2:  0.026486796126628662\n",
            "Loss S01:  0.02361442603235519\n",
            "Loss S2:  0.026491739429581967\n",
            "Loss S01:  0.023617183093590594\n",
            "Loss S2:  0.026486329556365682\n",
            "Loss S01:  0.023622248693526162\n",
            "Loss S2:  0.026490215409601305\n",
            "Loss S01:  0.023609111977024137\n",
            "Loss S2:  0.026471064492392442\n",
            "Validation: \n",
            " Loss S01:  0.02303348481655121\n",
            " Loss S2:  0.041627224534749985\n",
            " Loss S01:  0.024171598255634308\n",
            " Loss S2:  0.0433111996168182\n",
            " Loss S01:  0.02414168567373985\n",
            " Loss S2:  0.04349661164167451\n",
            " Loss S01:  0.024090840801840922\n",
            " Loss S2:  0.04310781398757559\n",
            " Loss S01:  0.02411257498610167\n",
            " Loss S2:  0.04300219027532472\n",
            "\n",
            "Epoch: 57\n",
            "Loss S01:  0.026659555733203888\n",
            "Loss S2:  0.02834318019449711\n",
            "Loss S01:  0.023303561759265987\n",
            "Loss S2:  0.025767107409509746\n",
            "Loss S01:  0.0232953658948342\n",
            "Loss S2:  0.02609250596946194\n",
            "Loss S01:  0.023423493990013675\n",
            "Loss S2:  0.02617703402234662\n",
            "Loss S01:  0.02350866117673676\n",
            "Loss S2:  0.02632690611772421\n",
            "Loss S01:  0.023575140966796408\n",
            "Loss S2:  0.02638704673040147\n",
            "Loss S01:  0.02352619940628771\n",
            "Loss S2:  0.02637363579429564\n",
            "Loss S01:  0.023567487498824025\n",
            "Loss S2:  0.026441561215570276\n",
            "Loss S01:  0.023429870927407417\n",
            "Loss S2:  0.026244265360780703\n",
            "Loss S01:  0.023410687591512123\n",
            "Loss S2:  0.026260174544794217\n",
            "Loss S01:  0.02334258943279781\n",
            "Loss S2:  0.02627569350349431\n",
            "Loss S01:  0.023360267231190526\n",
            "Loss S2:  0.026279997114125674\n",
            "Loss S01:  0.023391254277022416\n",
            "Loss S2:  0.0263019388556973\n",
            "Loss S01:  0.02340858359998874\n",
            "Loss S2:  0.02627755040373966\n",
            "Loss S01:  0.02346339463817735\n",
            "Loss S2:  0.026270128350625647\n",
            "Loss S01:  0.02350627424947868\n",
            "Loss S2:  0.026297595944033553\n",
            "Loss S01:  0.023483951963623118\n",
            "Loss S2:  0.026258535023709263\n",
            "Loss S01:  0.023510342540099607\n",
            "Loss S2:  0.026298873703817876\n",
            "Loss S01:  0.023511798671595956\n",
            "Loss S2:  0.026354294445221596\n",
            "Loss S01:  0.02350196761142521\n",
            "Loss S2:  0.0263099658902715\n",
            "Loss S01:  0.02349780038443964\n",
            "Loss S2:  0.02628463389935778\n",
            "Loss S01:  0.023503930550686555\n",
            "Loss S2:  0.026284955680299708\n",
            "Loss S01:  0.02349719389160564\n",
            "Loss S2:  0.026285226697026334\n",
            "Loss S01:  0.023496110121280084\n",
            "Loss S2:  0.026273371571134695\n",
            "Loss S01:  0.023541565497571998\n",
            "Loss S2:  0.02629763105134499\n",
            "Loss S01:  0.023532368153689868\n",
            "Loss S2:  0.02627900872839637\n",
            "Loss S01:  0.023529560665575024\n",
            "Loss S2:  0.026306082362530334\n",
            "Loss S01:  0.02352255650167096\n",
            "Loss S2:  0.026291592864532753\n",
            "Loss S01:  0.023516639258407614\n",
            "Loss S2:  0.026292527876661767\n",
            "Loss S01:  0.023521391207614716\n",
            "Loss S2:  0.026298790818073906\n",
            "Loss S01:  0.023520426626666837\n",
            "Loss S2:  0.026285653364569245\n",
            "Loss S01:  0.02351261179451007\n",
            "Loss S2:  0.02627182132609404\n",
            "Loss S01:  0.023489393172002285\n",
            "Loss S2:  0.026257781706448657\n",
            "Loss S01:  0.023506309454444672\n",
            "Loss S2:  0.026263899876091776\n",
            "Loss S01:  0.02350715913774331\n",
            "Loss S2:  0.02625260657775612\n",
            "Loss S01:  0.02351916521426789\n",
            "Loss S2:  0.02628228502438279\n",
            "Loss S01:  0.023533991442534073\n",
            "Loss S2:  0.026302632775126733\n",
            "Loss S01:  0.023523268217065265\n",
            "Loss S2:  0.026292899437750768\n",
            "Loss S01:  0.023511205044553035\n",
            "Loss S2:  0.026277498921423448\n",
            "Loss S01:  0.0234886172758725\n",
            "Loss S2:  0.02625981037078611\n",
            "Loss S01:  0.023492576266427586\n",
            "Loss S2:  0.026268616667077724\n",
            "Loss S01:  0.023501475525163386\n",
            "Loss S2:  0.026272393160311554\n",
            "Loss S01:  0.023500311270249995\n",
            "Loss S2:  0.026280140557333014\n",
            "Loss S01:  0.023485393792805703\n",
            "Loss S2:  0.026256460674850404\n",
            "Loss S01:  0.023485854686218863\n",
            "Loss S2:  0.02625743594958263\n",
            "Loss S01:  0.023494406398840068\n",
            "Loss S2:  0.02627453639971179\n",
            "Loss S01:  0.023490422346716073\n",
            "Loss S2:  0.026279188191554552\n",
            "Loss S01:  0.023483170326917793\n",
            "Loss S2:  0.0262628914175393\n",
            "Loss S01:  0.023486446434333764\n",
            "Loss S2:  0.02627329987990757\n",
            "Loss S01:  0.023470583400269876\n",
            "Loss S2:  0.026260720474869315\n",
            "Validation: \n",
            " Loss S01:  0.02334870956838131\n",
            " Loss S2:  0.04003081098198891\n",
            " Loss S01:  0.024196601428446315\n",
            " Loss S2:  0.04267676564909163\n",
            " Loss S01:  0.02401523991692357\n",
            " Loss S2:  0.042860339327556334\n",
            " Loss S01:  0.02391964960537973\n",
            " Loss S2:  0.0424443389915052\n",
            " Loss S01:  0.023966920757551254\n",
            " Loss S2:  0.042381444922935815\n",
            "\n",
            "Epoch: 58\n",
            "Loss S01:  0.02357962541282177\n",
            "Loss S2:  0.02915474772453308\n",
            "Loss S01:  0.022539080713282932\n",
            "Loss S2:  0.025660591877319595\n",
            "Loss S01:  0.022703302935475393\n",
            "Loss S2:  0.025910169329671634\n",
            "Loss S01:  0.02304219202168526\n",
            "Loss S2:  0.025980326617437023\n",
            "Loss S01:  0.023084056586390587\n",
            "Loss S2:  0.026027059909410594\n",
            "Loss S01:  0.023253058612931008\n",
            "Loss S2:  0.02609894431981386\n",
            "Loss S01:  0.023215042396647033\n",
            "Loss S2:  0.026011316655356376\n",
            "Loss S01:  0.023265498231204464\n",
            "Loss S2:  0.026164916566979717\n",
            "Loss S01:  0.023166171424550776\n",
            "Loss S2:  0.026066157529935425\n",
            "Loss S01:  0.02316830116887014\n",
            "Loss S2:  0.026050099632242224\n",
            "Loss S01:  0.023136455399712715\n",
            "Loss S2:  0.02604989220600317\n",
            "Loss S01:  0.02316195596647155\n",
            "Loss S2:  0.026053307628309406\n",
            "Loss S01:  0.02317159313493031\n",
            "Loss S2:  0.026101846740511823\n",
            "Loss S01:  0.02321578680274596\n",
            "Loss S2:  0.026116507326942363\n",
            "Loss S01:  0.023246640471596244\n",
            "Loss S2:  0.02614809827654497\n",
            "Loss S01:  0.02329476507047549\n",
            "Loss S2:  0.026218544284732925\n",
            "Loss S01:  0.023326774978119393\n",
            "Loss S2:  0.026212810058586347\n",
            "Loss S01:  0.023359902170405053\n",
            "Loss S2:  0.026286604699858447\n",
            "Loss S01:  0.023386461042814492\n",
            "Loss S2:  0.026340822786998355\n",
            "Loss S01:  0.023379858349162248\n",
            "Loss S2:  0.026309492875924285\n",
            "Loss S01:  0.023352308995762273\n",
            "Loss S2:  0.026268047164773465\n",
            "Loss S01:  0.023360041671948974\n",
            "Loss S2:  0.02627622939081271\n",
            "Loss S01:  0.023355175180537667\n",
            "Loss S2:  0.026275096486461647\n",
            "Loss S01:  0.023346717126699754\n",
            "Loss S2:  0.026250116317432164\n",
            "Loss S01:  0.023392065107451433\n",
            "Loss S2:  0.02624996711357006\n",
            "Loss S01:  0.023388420665762813\n",
            "Loss S2:  0.026220345090466193\n",
            "Loss S01:  0.023387960568641338\n",
            "Loss S2:  0.026216927009466963\n",
            "Loss S01:  0.023369510091043925\n",
            "Loss S2:  0.026199007929196216\n",
            "Loss S01:  0.02335713709641606\n",
            "Loss S2:  0.02619641011057163\n",
            "Loss S01:  0.023368767568284703\n",
            "Loss S2:  0.026213296436259838\n",
            "Loss S01:  0.023381053343217238\n",
            "Loss S2:  0.026219521410936534\n",
            "Loss S01:  0.02336294574130003\n",
            "Loss S2:  0.026196404123660835\n",
            "Loss S01:  0.02334551379734482\n",
            "Loss S2:  0.026183886790062037\n",
            "Loss S01:  0.023359184105545733\n",
            "Loss S2:  0.02620199035706837\n",
            "Loss S01:  0.02336857440032043\n",
            "Loss S2:  0.026193787739817\n",
            "Loss S01:  0.0233906400986971\n",
            "Loss S2:  0.026233235055692177\n",
            "Loss S01:  0.02340111491422574\n",
            "Loss S2:  0.026248067318154836\n",
            "Loss S01:  0.023392442046511527\n",
            "Loss S2:  0.02623020414252969\n",
            "Loss S01:  0.023382600577758365\n",
            "Loss S2:  0.026222276029585226\n",
            "Loss S01:  0.02337937566268322\n",
            "Loss S2:  0.02621078174894728\n",
            "Loss S01:  0.023384228093256975\n",
            "Loss S2:  0.026210512928757583\n",
            "Loss S01:  0.02338562187922262\n",
            "Loss S2:  0.026210003113260813\n",
            "Loss S01:  0.02338823212881411\n",
            "Loss S2:  0.026210277148847626\n",
            "Loss S01:  0.023379269209441896\n",
            "Loss S2:  0.02619623322376934\n",
            "Loss S01:  0.023371598714975274\n",
            "Loss S2:  0.02618981481375608\n",
            "Loss S01:  0.02336504357864878\n",
            "Loss S2:  0.026187810588405032\n",
            "Loss S01:  0.023374099805627883\n",
            "Loss S2:  0.026194912189549065\n",
            "Loss S01:  0.02336488903011739\n",
            "Loss S2:  0.02618803270161152\n",
            "Loss S01:  0.0233664724999059\n",
            "Loss S2:  0.026193202908873062\n",
            "Loss S01:  0.023349656755685077\n",
            "Loss S2:  0.026177676663925595\n",
            "Validation: \n",
            " Loss S01:  0.023358825594186783\n",
            " Loss S2:  0.04023732244968414\n",
            " Loss S01:  0.024774289202122463\n",
            " Loss S2:  0.04345245712569782\n",
            " Loss S01:  0.02458956996660407\n",
            " Loss S2:  0.04367764194200679\n",
            " Loss S01:  0.024513509246658106\n",
            " Loss S2:  0.043334427427073\n",
            " Loss S01:  0.024633190330163934\n",
            " Loss S2:  0.043218481825825605\n",
            "\n",
            "Epoch: 59\n",
            "Loss S01:  0.023822743445634842\n",
            "Loss S2:  0.026088878512382507\n",
            "Loss S01:  0.023249709809368305\n",
            "Loss S2:  0.02553381415253336\n",
            "Loss S01:  0.023033238237812406\n",
            "Loss S2:  0.0258214059507563\n",
            "Loss S01:  0.02304134776275004\n",
            "Loss S2:  0.025832811190235998\n",
            "Loss S01:  0.023089388567136555\n",
            "Loss S2:  0.02593189119020613\n",
            "Loss S01:  0.023283177329336897\n",
            "Loss S2:  0.025955930973092716\n",
            "Loss S01:  0.0232526888979263\n",
            "Loss S2:  0.025917818922488414\n",
            "Loss S01:  0.023328256622796327\n",
            "Loss S2:  0.02600357054509747\n",
            "Loss S01:  0.02317183639532254\n",
            "Loss S2:  0.025869279295022106\n",
            "Loss S01:  0.023164658633234737\n",
            "Loss S2:  0.025886732921168044\n",
            "Loss S01:  0.023132550502472586\n",
            "Loss S2:  0.025849669895107202\n",
            "Loss S01:  0.02317769310294508\n",
            "Loss S2:  0.02589501155255077\n",
            "Loss S01:  0.023220788364198584\n",
            "Loss S2:  0.02600205746625573\n",
            "Loss S01:  0.023239921821096472\n",
            "Loss S2:  0.026009184628270055\n",
            "Loss S01:  0.02330986224114895\n",
            "Loss S2:  0.02603655268854283\n",
            "Loss S01:  0.0233387304778328\n",
            "Loss S2:  0.026065411564234076\n",
            "Loss S01:  0.023351381561770943\n",
            "Loss S2:  0.02608310670724937\n",
            "Loss S01:  0.023345743028218287\n",
            "Loss S2:  0.026129875244975787\n",
            "Loss S01:  0.023360240309195625\n",
            "Loss S2:  0.02619588279386581\n",
            "Loss S01:  0.023347273712772974\n",
            "Loss S2:  0.026179666245717027\n",
            "Loss S01:  0.02332082425999404\n",
            "Loss S2:  0.026150016049247476\n",
            "Loss S01:  0.0233142666553999\n",
            "Loss S2:  0.026147667362721047\n",
            "Loss S01:  0.02331994393865717\n",
            "Loss S2:  0.026164154442052497\n",
            "Loss S01:  0.02332069869133043\n",
            "Loss S2:  0.02616266303119205\n",
            "Loss S01:  0.02336533548488162\n",
            "Loss S2:  0.026192277863127066\n",
            "Loss S01:  0.023352095627214804\n",
            "Loss S2:  0.0261694950930982\n",
            "Loss S01:  0.023352995914517692\n",
            "Loss S2:  0.026159637682568068\n",
            "Loss S01:  0.023340447266273393\n",
            "Loss S2:  0.026139787841602004\n",
            "Loss S01:  0.023341235595347617\n",
            "Loss S2:  0.026139151510447795\n",
            "Loss S01:  0.02333285580181174\n",
            "Loss S2:  0.026148287861054297\n",
            "Loss S01:  0.02333219523891262\n",
            "Loss S2:  0.026153174046414634\n",
            "Loss S01:  0.02333798732644492\n",
            "Loss S2:  0.026148288106467948\n",
            "Loss S01:  0.023320262062122517\n",
            "Loss S2:  0.026122912555654472\n",
            "Loss S01:  0.023327294022430105\n",
            "Loss S2:  0.026134342782401968\n",
            "Loss S01:  0.023332046230167935\n",
            "Loss S2:  0.026127165516625403\n",
            "Loss S01:  0.023337844170184217\n",
            "Loss S2:  0.026139178464555334\n",
            "Loss S01:  0.023361176319440978\n",
            "Loss S2:  0.026152374336387643\n",
            "Loss S01:  0.02334262100311945\n",
            "Loss S2:  0.026130994505718392\n",
            "Loss S01:  0.023330452414400623\n",
            "Loss S2:  0.026127436860730956\n",
            "Loss S01:  0.023307078296456802\n",
            "Loss S2:  0.026093181184566844\n",
            "Loss S01:  0.023303618408423706\n",
            "Loss S2:  0.026096932766517797\n",
            "Loss S01:  0.023310419983273586\n",
            "Loss S2:  0.026107849580419326\n",
            "Loss S01:  0.023311982285275597\n",
            "Loss S2:  0.026107834404303455\n",
            "Loss S01:  0.02329807113121391\n",
            "Loss S2:  0.026095247289420833\n",
            "Loss S01:  0.023293910358025104\n",
            "Loss S2:  0.026090790409924222\n",
            "Loss S01:  0.02329069265306921\n",
            "Loss S2:  0.026090135587226525\n",
            "Loss S01:  0.023281323939658816\n",
            "Loss S2:  0.02609136893359415\n",
            "Loss S01:  0.023274539762241826\n",
            "Loss S2:  0.02608292933846102\n",
            "Loss S01:  0.023275589543295998\n",
            "Loss S2:  0.026078380260949573\n",
            "Loss S01:  0.02325930043129353\n",
            "Loss S2:  0.026044389149534483\n",
            "Validation: \n",
            " Loss S01:  0.023283228278160095\n",
            " Loss S2:  0.03930584341287613\n",
            " Loss S01:  0.024407019040414264\n",
            " Loss S2:  0.04253229002157847\n",
            " Loss S01:  0.02436901906096354\n",
            " Loss S2:  0.04268816486001015\n",
            " Loss S01:  0.024335571763212563\n",
            " Loss S2:  0.04228386154673139\n",
            " Loss S01:  0.024478265164810935\n",
            " Loss S2:  0.04214140676237919\n",
            "\n",
            "Epoch: 60\n",
            "Loss S01:  0.02415410988032818\n",
            "Loss S2:  0.027702756226062775\n",
            "Loss S01:  0.022730217569253662\n",
            "Loss S2:  0.025423710149797527\n",
            "Loss S01:  0.023096951079510507\n",
            "Loss S2:  0.025562074833682606\n",
            "Loss S01:  0.023170247133220395\n",
            "Loss S2:  0.025644133227967447\n",
            "Loss S01:  0.023155429786661776\n",
            "Loss S2:  0.025731184724264028\n",
            "Loss S01:  0.023178017241697685\n",
            "Loss S2:  0.02568401670192971\n",
            "Loss S01:  0.023217439223996928\n",
            "Loss S2:  0.025714996102891984\n",
            "Loss S01:  0.02326658832460222\n",
            "Loss S2:  0.025874503192977166\n",
            "Loss S01:  0.023131989908439142\n",
            "Loss S2:  0.025773604273980046\n",
            "Loss S01:  0.023118817810337623\n",
            "Loss S2:  0.025793288517129292\n",
            "Loss S01:  0.023107606327474706\n",
            "Loss S2:  0.025763043783383795\n",
            "Loss S01:  0.02313208955902237\n",
            "Loss S2:  0.02579290965186046\n",
            "Loss S01:  0.02314643999022886\n",
            "Loss S2:  0.025798699410616858\n",
            "Loss S01:  0.023140065826987494\n",
            "Loss S2:  0.02577581941967702\n",
            "Loss S01:  0.023160330026496388\n",
            "Loss S2:  0.025805039027481214\n",
            "Loss S01:  0.023175474836040807\n",
            "Loss S2:  0.02583639162117677\n",
            "Loss S01:  0.02316949025999685\n",
            "Loss S2:  0.025838734917574047\n",
            "Loss S01:  0.023208689075290112\n",
            "Loss S2:  0.02589033157374078\n",
            "Loss S01:  0.023217488810913997\n",
            "Loss S2:  0.02593874793437963\n",
            "Loss S01:  0.023177277755128775\n",
            "Loss S2:  0.025900325898802718\n",
            "Loss S01:  0.023154586840847237\n",
            "Loss S2:  0.025871331557928035\n",
            "Loss S01:  0.023169169202446938\n",
            "Loss S2:  0.025870931206791885\n",
            "Loss S01:  0.023164216884121097\n",
            "Loss S2:  0.025885939353663998\n",
            "Loss S01:  0.023164645208295805\n",
            "Loss S2:  0.025864633961350886\n",
            "Loss S01:  0.023199711791027137\n",
            "Loss S2:  0.025893000715810233\n",
            "Loss S01:  0.02321814680360703\n",
            "Loss S2:  0.02589054270628914\n",
            "Loss S01:  0.02322268077097405\n",
            "Loss S2:  0.025904121562496\n",
            "Loss S01:  0.0232159749182058\n",
            "Loss S2:  0.025896076284816345\n",
            "Loss S01:  0.023203278119506784\n",
            "Loss S2:  0.02588480796492609\n",
            "Loss S01:  0.023192699921807062\n",
            "Loss S2:  0.02588885770374557\n",
            "Loss S01:  0.023189574696643407\n",
            "Loss S2:  0.02587153000193973\n",
            "Loss S01:  0.02318576158554416\n",
            "Loss S2:  0.025860480766539788\n",
            "Loss S01:  0.02315670970212262\n",
            "Loss S2:  0.02583092309511339\n",
            "Loss S01:  0.02316262033905868\n",
            "Loss S2:  0.025848777295059307\n",
            "Loss S01:  0.023156431113039295\n",
            "Loss S2:  0.025863832900644746\n",
            "Loss S01:  0.023164059563494814\n",
            "Loss S2:  0.02589391572055993\n",
            "Loss S01:  0.02317303134814689\n",
            "Loss S2:  0.025912189231850104\n",
            "Loss S01:  0.023159764912933353\n",
            "Loss S2:  0.025901606170877292\n",
            "Loss S01:  0.023148454813741323\n",
            "Loss S2:  0.025902038909436213\n",
            "Loss S01:  0.023125168288607733\n",
            "Loss S2:  0.02588287603748424\n",
            "Loss S01:  0.023130295282587445\n",
            "Loss S2:  0.025900931666579627\n",
            "Loss S01:  0.02313395542457446\n",
            "Loss S2:  0.0259177353224035\n",
            "Loss S01:  0.023145106592328418\n",
            "Loss S2:  0.025929299141158685\n",
            "Loss S01:  0.02314241661791735\n",
            "Loss S2:  0.02591710847968845\n",
            "Loss S01:  0.02313809391626425\n",
            "Loss S2:  0.02589489972797516\n",
            "Loss S01:  0.023140141063115814\n",
            "Loss S2:  0.025894050542141275\n",
            "Loss S01:  0.02313992631603477\n",
            "Loss S2:  0.025914481843047957\n",
            "Loss S01:  0.023139780153957394\n",
            "Loss S2:  0.02590563974784185\n",
            "Loss S01:  0.023138114136959313\n",
            "Loss S2:  0.025916241364952432\n",
            "Loss S01:  0.023122317159582545\n",
            "Loss S2:  0.025900953993686835\n",
            "Validation: \n",
            " Loss S01:  0.022371262311935425\n",
            " Loss S2:  0.04071877896785736\n",
            " Loss S01:  0.023684004232996984\n",
            " Loss S2:  0.04259228830536207\n",
            " Loss S01:  0.023710883563248124\n",
            " Loss S2:  0.04274044767385576\n",
            " Loss S01:  0.023691511361813935\n",
            " Loss S2:  0.04232432778741493\n",
            " Loss S01:  0.023744282025245974\n",
            " Loss S2:  0.0422177045709557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create ensamble model\n",
        "\n",
        "1.8 In this step you will create a new network class that takes s1, and s2 as perimeters. This class should initiate a new network that ensembles both s1 and s2, and have a classifier for cross-entropy. In the forward method pass the input x from both s1 and s2 and then concatenate there outputs along axis 1. Then pass this concatinated output through classifier of appropriate shape. "
      ],
      "metadata": {
        "id": "25tcDBBIu2H2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, s1,s2):\n",
        "        super(Net, self).__init__()\n",
        "        self.s1 = s1\n",
        "        self.s2 = s2\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.s1(x)\n",
        "        out2 = self.s2(x)\n",
        "        out = torch.cat((out1,out2),1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "net = Net(s01,s2)\n",
        "net.to(device)\n",
        "summary(net, (3, 32, 32))"
      ],
      "metadata": {
        "id": "EpzuTHRovW1T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "235e439c-a593-4758-c5a1-1323fa0111a3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 256, 1, 1]               0\n",
            "           Linear-31                  [-1, 256]          65,792\n",
            "              VGG-32                  [-1, 256]               0\n",
            "           Conv2d-33           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-34           [-1, 32, 32, 32]              64\n",
            "             ReLU-35           [-1, 32, 32, 32]               0\n",
            "           Conv2d-36           [-1, 32, 32, 32]           9,248\n",
            "      BatchNorm2d-37           [-1, 32, 32, 32]              64\n",
            "             ReLU-38           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-39           [-1, 32, 16, 16]               0\n",
            "           Conv2d-40           [-1, 64, 16, 16]          18,496\n",
            "      BatchNorm2d-41           [-1, 64, 16, 16]             128\n",
            "             ReLU-42           [-1, 64, 16, 16]               0\n",
            "           Conv2d-43           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-44           [-1, 64, 16, 16]             128\n",
            "             ReLU-45           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-46             [-1, 64, 8, 8]               0\n",
            "           Conv2d-47            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-48            [-1, 128, 8, 8]             256\n",
            "             ReLU-49            [-1, 128, 8, 8]               0\n",
            "           Conv2d-50            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-51            [-1, 128, 8, 8]             256\n",
            "             ReLU-52            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-53            [-1, 128, 4, 4]               0\n",
            "           Conv2d-54            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-55            [-1, 256, 4, 4]             512\n",
            "             ReLU-56            [-1, 256, 4, 4]               0\n",
            "           Conv2d-57            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-58            [-1, 256, 4, 4]             512\n",
            "             ReLU-59            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-60            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-61            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-62            [-1, 256, 1, 1]               0\n",
            "           Linear-63                  [-1, 256]          65,792\n",
            "              VGG-64                  [-1, 256]               0\n",
            "           Linear-65                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 2,485,066\n",
            "Trainable params: 2,485,066\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 5.88\n",
            "Params size (MB): 9.48\n",
            "Estimated Total Size (MB): 15.37\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train The Ensambled network\n",
        "1.9 In this step you will freez all the conv layers in the ensambled network and then finetune it on orignal dataset. "
      ],
      "metadata": {
        "id": "qNFSWlD5wvZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in net.modules():\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        m.weight.requires_grad = False\n",
        "        if m.bias is not None:\n",
        "            m.bias.requires_grad = False\n",
        "net = net.to(device)\n",
        "summary(net, (3, 32, 32))"
      ],
      "metadata": {
        "id": "tbZ9_90YxCwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1577f977-1ad6-4bae-f9bc-e34eec5e4a5f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 256, 1, 1]               0\n",
            "           Linear-31                  [-1, 256]          65,792\n",
            "              VGG-32                  [-1, 256]               0\n",
            "           Conv2d-33           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-34           [-1, 32, 32, 32]              64\n",
            "             ReLU-35           [-1, 32, 32, 32]               0\n",
            "           Conv2d-36           [-1, 32, 32, 32]           9,248\n",
            "      BatchNorm2d-37           [-1, 32, 32, 32]              64\n",
            "             ReLU-38           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-39           [-1, 32, 16, 16]               0\n",
            "           Conv2d-40           [-1, 64, 16, 16]          18,496\n",
            "      BatchNorm2d-41           [-1, 64, 16, 16]             128\n",
            "             ReLU-42           [-1, 64, 16, 16]               0\n",
            "           Conv2d-43           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-44           [-1, 64, 16, 16]             128\n",
            "             ReLU-45           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-46             [-1, 64, 8, 8]               0\n",
            "           Conv2d-47            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-48            [-1, 128, 8, 8]             256\n",
            "             ReLU-49            [-1, 128, 8, 8]               0\n",
            "           Conv2d-50            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-51            [-1, 128, 8, 8]             256\n",
            "             ReLU-52            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-53            [-1, 128, 4, 4]               0\n",
            "           Conv2d-54            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-55            [-1, 256, 4, 4]             512\n",
            "             ReLU-56            [-1, 256, 4, 4]               0\n",
            "           Conv2d-57            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-58            [-1, 256, 4, 4]             512\n",
            "             ReLU-59            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-60            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-61            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-62            [-1, 256, 1, 1]               0\n",
            "           Linear-63                  [-1, 256]          65,792\n",
            "              VGG-64                  [-1, 256]               0\n",
            "           Linear-65                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 2,485,066\n",
            "Trainable params: 140,554\n",
            "Non-trainable params: 2,344,512\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 5.88\n",
            "Params size (MB): 9.48\n",
            "Estimated Total Size (MB): 15.37\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        net.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if(batch_idx % 200 == 0):\n",
        "          print(\"Accuracy : \",100.*correct/total,\" Loss : \", train_loss/(batch_idx+1))\n",
        "def test(epoch):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\"Accuracy : \",100.*correct/total,\" Loss : \", test_loss/(batch_idx+1))"
      ],
      "metadata": {
        "id": "2SzjCW-6xHIA"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "    train(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test(epoch)"
      ],
      "metadata": {
        "id": "gxz8dPNXxMKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1663562-ebd1-4eb4-ed34-bb96649d6b68"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Accuracy :  3.0  Loss :  2.5537290573120117\n",
            "Accuracy :  77.01492537313433  Loss :  1.0113611778809657\n",
            "Accuracy :  81.46633416458853  Loss :  0.7432798603526375\n",
            "Validation: \n",
            "Accuracy :  90.0  Loss :  0.4220707416534424\n",
            "Accuracy :  85.19047619047619  Loss :  0.45214904560929253\n",
            "Accuracy :  84.46341463414635  Loss :  0.46110903816979104\n",
            "Accuracy :  84.62295081967213  Loss :  0.4537209167343671\n",
            "Accuracy :  84.62962962962963  Loss :  0.4543304390009539\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy :  87.0  Loss :  0.3931857943534851\n",
            "Accuracy :  85.99004975124379  Loss :  0.4110661900755185\n",
            "Accuracy :  86.09725685785536  Loss :  0.405594103391331\n",
            "Validation: \n",
            "Accuracy :  89.0  Loss :  0.38721492886543274\n",
            "Accuracy :  85.23809523809524  Loss :  0.43797185875120614\n",
            "Accuracy :  84.8048780487805  Loss :  0.4464068303747875\n",
            "Accuracy :  85.0  Loss :  0.4388218849408822\n",
            "Accuracy :  84.93827160493827  Loss :  0.43908868454120775\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy :  86.0  Loss :  0.3703875243663788\n",
            "Accuracy :  86.34825870646766  Loss :  0.39538248876730603\n",
            "Accuracy :  86.44638403990025  Loss :  0.3920614146755223\n",
            "Validation: \n",
            "Accuracy :  89.0  Loss :  0.37400686740875244\n",
            "Accuracy :  85.14285714285714  Loss :  0.4360109241235824\n",
            "Accuracy :  84.6829268292683  Loss :  0.44404799763749286\n",
            "Accuracy :  84.93442622950819  Loss :  0.43571721286070153\n",
            "Accuracy :  84.92592592592592  Loss :  0.4360661697976383\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy :  88.0  Loss :  0.3456130623817444\n",
            "Accuracy :  86.51741293532338  Loss :  0.3934552865983242\n",
            "Accuracy :  86.6708229426434  Loss :  0.3890968321490466\n",
            "Validation: \n",
            "Accuracy :  89.0  Loss :  0.3667614459991455\n",
            "Accuracy :  85.38095238095238  Loss :  0.4332822610934575\n",
            "Accuracy :  84.82926829268293  Loss :  0.4406048739101829\n",
            "Accuracy :  85.18032786885246  Loss :  0.4329089891226565\n",
            "Accuracy :  85.17283950617283  Loss :  0.43290916287604675\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy :  86.0  Loss :  0.36285117268562317\n",
            "Accuracy :  86.69651741293532  Loss :  0.38908926675568767\n",
            "Accuracy :  86.68079800498754  Loss :  0.38695668813742307\n",
            "Validation: \n",
            "Accuracy :  89.0  Loss :  0.35659676790237427\n",
            "Accuracy :  85.42857142857143  Loss :  0.4319282656624204\n",
            "Accuracy :  84.7560975609756  Loss :  0.43936381252800544\n",
            "Accuracy :  85.11475409836065  Loss :  0.4316603472975434\n",
            "Accuracy :  85.20987654320987  Loss :  0.4315134706320586\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy :  89.0  Loss :  0.34395045042037964\n",
            "Accuracy :  86.55721393034825  Loss :  0.38644681544742776\n",
            "Accuracy :  86.70573566084788  Loss :  0.3836758098474464\n",
            "Validation: \n",
            "Accuracy :  89.0  Loss :  0.3532673120498657\n",
            "Accuracy :  85.47619047619048  Loss :  0.4301165079786664\n",
            "Accuracy :  84.90243902439025  Loss :  0.43706592226900703\n",
            "Accuracy :  85.21311475409836  Loss :  0.4296402254554092\n",
            "Accuracy :  85.30864197530865  Loss :  0.4294137183898761\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy :  88.0  Loss :  0.3545823395252228\n",
            "Accuracy :  86.71144278606965  Loss :  0.38175436187146317\n",
            "Accuracy :  86.7780548628429  Loss :  0.3817182868867741\n",
            "Validation: \n",
            "Accuracy :  89.0  Loss :  0.3516291677951813\n",
            "Accuracy :  85.42857142857143  Loss :  0.4288004878021422\n",
            "Accuracy :  84.85365853658537  Loss :  0.435896090617994\n",
            "Accuracy :  85.24590163934427  Loss :  0.42832510344317704\n",
            "Accuracy :  85.34567901234568  Loss :  0.428077463750486\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy :  89.0  Loss :  0.35162121057510376\n",
            "Accuracy :  86.94527363184079  Loss :  0.3790210873185106\n",
            "Accuracy :  87.0349127182045  Loss :  0.3769686314233223\n",
            "Validation: \n",
            "Accuracy :  89.0  Loss :  0.34609881043434143\n",
            "Accuracy :  85.33333333333333  Loss :  0.4283263761372793\n",
            "Accuracy :  84.78048780487805  Loss :  0.43490716851339106\n",
            "Accuracy :  85.21311475409836  Loss :  0.427292633496347\n",
            "Accuracy :  85.33333333333333  Loss :  0.4274348449191929\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy :  90.0  Loss :  0.2940535545349121\n",
            "Accuracy :  86.93034825870647  Loss :  0.376705704785105\n",
            "Accuracy :  86.93017456359102  Loss :  0.375060561291892\n",
            "Validation: \n",
            "Accuracy :  89.0  Loss :  0.3433424234390259\n",
            "Accuracy :  85.57142857142857  Loss :  0.42753368545146214\n",
            "Accuracy :  84.97560975609755  Loss :  0.43378826503346607\n",
            "Accuracy :  85.36065573770492  Loss :  0.42632307551923343\n",
            "Accuracy :  85.48148148148148  Loss :  0.4259017372940793\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy :  89.0  Loss :  0.3424116373062134\n",
            "Accuracy :  87.06965174129353  Loss :  0.3758697783324256\n",
            "Accuracy :  87.05236907730674  Loss :  0.3752762345304513\n",
            "Validation: \n",
            "Accuracy :  89.0  Loss :  0.3411214053630829\n",
            "Accuracy :  85.71428571428571  Loss :  0.42690961204823996\n",
            "Accuracy :  85.07317073170732  Loss :  0.43311822232676717\n",
            "Accuracy :  85.37704918032787  Loss :  0.4262538476557028\n",
            "Accuracy :  85.55555555555556  Loss :  0.4260880176299884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_name = 'ss1student.pt'\n",
        "path = F\"/content/gdrive/MyDrive/{model_save_name}\" \n",
        "torch.save(s01.state_dict(), path)\n",
        "#s01.load_state_dict(torch.load(path))\n",
        "model_save_name = 'ss2student.pt'\n",
        "path = F\"/content/gdrive/MyDrive/{model_save_name}\" \n",
        "torch.save(s2.state_dict(), path)\n",
        "#s2.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "_TF6UDYswobN"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Create** **4** **More Students.**\n",
        "\n"
      ],
      "metadata": {
        "id": "42aMmjCt8EOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "    'VGGS1': [32, 32, 'M', 64, 64, 'M', 128, 128, 'M', 256, 256, 'M','M'],\n",
        "    'VGGS33': [32, 32, 'M', 64, 64, 'M', 128, 'M','M','M'],\n",
        "    'VGGS2':[32,'M', 32, 'M', 64, 64, 'M', 128,128,'M',128,128, 'M'],\n",
        "    \n",
        "\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(128, 128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "s11 = VGG('VGGS2')\n",
        "s11 = s11.to(device)\n",
        "summary(s11, (3, 32, 32))\n",
        "s22 = VGG('VGGS2')\n",
        "s22 = s22.to(device)\n",
        "summary(s22, (3,32,32))\n",
        "s33 = VGG('VGGS2')\n",
        "s33 = s33.to(device)\n",
        "summary(s33, (3, 32, 32))\n",
        "s44 = VGG('VGGS2')\n",
        "s44 = s44.to(device)\n",
        "summary(s44, (3, 32, 32))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6SbVIr-0iQ3",
        "outputId": "8dba02be-4ef3-4ece-9184-7a0b0459cab6"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "================================================================\n",
            "Total params: 600,096\n",
            "Trainable params: 600,096\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.33\n",
            "Params size (MB): 2.29\n",
            "Estimated Total Size (MB): 3.64\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "================================================================\n",
            "Total params: 600,096\n",
            "Trainable params: 600,096\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.33\n",
            "Params size (MB): 2.29\n",
            "Estimated Total Size (MB): 3.64\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "================================================================\n",
            "Total params: 600,096\n",
            "Trainable params: 600,096\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.33\n",
            "Params size (MB): 2.29\n",
            "Estimated Total Size (MB): 3.64\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "================================================================\n",
            "Total params: 600,096\n",
            "Trainable params: 600,096\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.33\n",
            "Params size (MB): 2.29\n",
            "Estimated Total Size (MB): 3.64\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TA_WOH = nn.Sequential(*list(s01.children())[:-1],nn.Flatten())\n",
        "#TA_WOH = nn.Sequential(*list(s1.children())[:],nn.Flatten())\n",
        "summary(s1, (3, 32, 32))\n",
        "summary(TA_WOH, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq3XMEdH0kYB",
        "outputId": "e3adfa80-8579-4baf-d14f-f6011fb40f6c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "           Conv2d-29            [-1, 512, 2, 2]       1,180,160\n",
            "      BatchNorm2d-30            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-31            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-32            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-33            [-1, 512, 1, 1]               0\n",
            "           Linear-34                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 2,360,490\n",
            "Trainable params: 8,074\n",
            "Non-trainable params: 2,352,416\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.98\n",
            "Params size (MB): 9.00\n",
            "Estimated Total Size (MB): 12.00\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 256, 1, 1]               0\n",
            "          Flatten-31                  [-1, 256]               0\n",
            "================================================================\n",
            "Total params: 1,174,176\n",
            "Trainable params: 1,920\n",
            "Non-trainable params: 1,172,256\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.94\n",
            "Params size (MB): 4.48\n",
            "Estimated Total Size (MB): 7.43\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TA2_WOH = nn.Sequential(*list(s2.children())[:-1],nn.Flatten())\n",
        "summary(s2, (3, 32, 32))\n",
        "summary(TA2_WOH, (3, 32, 32))"
      ],
      "metadata": {
        "id": "buiyd5tGEY4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be636efb-6bd5-4ba5-c2c5-8d02784e817f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 256, 1, 1]               0\n",
            "           Linear-31                  [-1, 256]          65,792\n",
            "================================================================\n",
            "Total params: 1,239,968\n",
            "Trainable params: 67,712\n",
            "Non-trainable params: 1,172,256\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.94\n",
            "Params size (MB): 4.73\n",
            "Estimated Total Size (MB): 7.68\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 256, 1, 1]               0\n",
            "          Flatten-31                  [-1, 256]               0\n",
            "================================================================\n",
            "Total params: 1,174,176\n",
            "Trainable params: 1,920\n",
            "Non-trainable params: 1,172,256\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.94\n",
            "Params size (MB): 4.48\n",
            "Estimated Total Size (MB): 7.43\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TA_WOH.eval()\n",
        "TA2_WOH.eval()\n",
        "# s1.eval()\n",
        "# s2.eval()\n",
        "TADenseTrain = None\n",
        "TADenseTest = None\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs1 = TA_WOH(inputs)\n",
        "        outputs2 = TA2_WOH(inputs)\n",
        "        if(TADenseTrain == None):\n",
        "            TADenseTrain = torch.cat((outputs1,outputs2),1) \n",
        "        else:\n",
        "            totalOUTPUT = torch.cat((outputs1,outputs2),1)         \n",
        "            TADenseTrain = torch.cat((TADenseTrain,totalOUTPUT))\n",
        "           \n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs1 = TA_WOH(inputs)\n",
        "        outputs2 = TA2_WOH(inputs)\n",
        "        if(TADenseTest == None):\n",
        "            TADenseTest = torch.cat((outputs1,outputs2),1)\n",
        "        else:\n",
        "            totalOUTPUT = torch.cat((outputs1,outputs2),1)         \n",
        "            TADenseTest = torch.cat((TADenseTest,totalOUTPUT))\n",
        "           "
      ],
      "metadata": {
        "id": "0CQM3bJZFE70"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(TADenseTrain.shape)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFUqixLL0tYh",
        "outputId": "e69ba9b3-89b3-4399-cff3-f95fa64f65dc"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50000, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer1 = optim.Adam(s11.parameters(), lr=0.0001)\n",
        "optimizer2 = optim.Adam(s22.parameters(), lr=0.0001)\n",
        "optimizer3 = optim.Adam(s33.parameters(), lr=0.0001)\n",
        "optimizer4 = optim.Adam(s44.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def train4(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    s11.train()\n",
        "    train_loss1 = 0\n",
        "    train_loss2 = 0\n",
        "    train_loss3 = 0\n",
        "    train_loss4= 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        #code here\n",
        "        targets = TADenseTrain[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "        targets.to(device)\n",
        "        s11.zero_grad()\n",
        "        s22.zero_grad()\n",
        "        s33.zero_grad()\n",
        "        s44.zero_grad()\n",
        "        output1 = s11(inputs)\n",
        "        output2 = s22(inputs)\n",
        "        output3 = s33(inputs)\n",
        "        output4 = s44(inputs)\n",
        "        \n",
        "        loss1 = criterion(output1, targets[:,:128])\n",
        "        loss2 = criterion(output2, targets[:,128:256])\n",
        "        loss3 = criterion(output3, targets[:,256:384])\n",
        "        loss4 = criterion(output4, targets[:,384:512])\n",
        "        loss1.backward()\n",
        "        loss2.backward()\n",
        "        loss3.backward()\n",
        "        loss4.backward()\n",
        "        optimizer1.step()\n",
        "        optimizer2.step()\n",
        "        optimizer3.step()\n",
        "        optimizer4.step()\n",
        "      \n",
        "\n",
        "        train_loss1 += loss1.item()\n",
        "        train_loss2 += loss2.item()\n",
        "        train_loss3 += loss3.item()\n",
        "        train_loss4 += loss4.item()\n",
        "        if(batch_idx % 10 == 0):\n",
        "          print(\"Loss S11: \", train_loss1/(batch_idx+1))\n",
        "          print(\"Loss S22: \", train_loss2/(batch_idx+1))\n",
        "          print(\"Loss S33: \", train_loss3/(batch_idx+1))\n",
        "          print(\"Loss S44: \", train_loss4/(batch_idx+1))\n",
        "def test4(epoch):\n",
        "    s11.eval()\n",
        "    \n",
        "    test_loss1 = 0\n",
        "    test_loss2 = 0\n",
        "    test_loss3 = 0\n",
        "    test_loss4= 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            #code here\n",
        "            targets = TADenseTest[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "            targets.to(device)\n",
        "            output1 = s11(inputs)\n",
        "            output2 = s22(inputs)\n",
        "            output3 = s33(inputs)\n",
        "            output4 = s44(inputs)\n",
        "            loss1 = criterion(output1, targets[:,:128])\n",
        "            loss2 = criterion(output2, targets[:,128:256])\n",
        "            loss3 = criterion(output3, targets[:,256:384])\n",
        "            loss4 = criterion(output4, targets[:,384:512])\n",
        "            test_loss1 += loss1.item()\n",
        "            test_loss2 += loss2.item()\n",
        "            test_loss3 += loss3.item()\n",
        "            test_loss4 += loss4.item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\" Loss S11: \", test_loss1/(batch_idx+1))\n",
        "              print(\" Loss S22: \", test_loss2/(batch_idx+1))\n",
        "              print(\" Loss S33: \", test_loss3/(batch_idx+1))\n",
        "              print(\" Loss S44: \", test_loss4/(batch_idx+1))"
      ],
      "metadata": {
        "id": "e_zhZ-fv0vse"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+60):\n",
        "    train4(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test4(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5b7RheK0x0Y",
        "outputId": "551fedab-a87d-4cad-ebd9-99f1a15b490f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Loss S44:  0.038045919593124145\n",
            "Loss S11:  0.03550996528677294\n",
            "Loss S22:  0.03459072780920152\n",
            "Loss S33:  0.03335490805964418\n",
            "Loss S44:  0.03803264848725447\n",
            "Loss S11:  0.03550535775270887\n",
            "Loss S22:  0.03458388079396728\n",
            "Loss S33:  0.0333355626659721\n",
            "Loss S44:  0.03801008209225994\n",
            "Loss S11:  0.03550015966192194\n",
            "Loss S22:  0.03457298889057028\n",
            "Loss S33:  0.03333010542536936\n",
            "Loss S44:  0.03798855385603793\n",
            "Loss S11:  0.03551538717903813\n",
            "Loss S22:  0.03457497727879432\n",
            "Loss S33:  0.03334873858467806\n",
            "Loss S44:  0.037988819213759184\n",
            "Loss S11:  0.03551572950820018\n",
            "Loss S22:  0.034557567966116434\n",
            "Loss S33:  0.03335978848204388\n",
            "Loss S44:  0.038003743468807014\n",
            "Loss S11:  0.03550020582732968\n",
            "Loss S22:  0.03455046248042359\n",
            "Loss S33:  0.03336406770260186\n",
            "Loss S44:  0.03796598616475365\n",
            "Loss S11:  0.03547392822215407\n",
            "Loss S22:  0.034546763799947706\n",
            "Loss S33:  0.033366304008275505\n",
            "Loss S44:  0.0379292443101331\n",
            "Loss S11:  0.03545024431289157\n",
            "Loss S22:  0.034494344688132596\n",
            "Loss S33:  0.033354196964246234\n",
            "Loss S44:  0.03788087797134429\n",
            "Loss S11:  0.03545595558262674\n",
            "Loss S22:  0.034480883012946106\n",
            "Loss S33:  0.03333755564455528\n",
            "Loss S44:  0.03787300644670044\n",
            "Loss S11:  0.035459620298006526\n",
            "Loss S22:  0.0344623037434915\n",
            "Loss S33:  0.03333256458025199\n",
            "Loss S44:  0.03788109041224721\n",
            "Loss S11:  0.03545925791448341\n",
            "Loss S22:  0.034473355368416836\n",
            "Loss S33:  0.03335743989450348\n",
            "Loss S44:  0.037898533267462335\n",
            "Loss S11:  0.03544752121300128\n",
            "Loss S22:  0.034482737812980696\n",
            "Loss S33:  0.03333471794104908\n",
            "Loss S44:  0.03788217141567416\n",
            "Loss S11:  0.03545915610378697\n",
            "Loss S22:  0.03448712039324972\n",
            "Loss S33:  0.03333254808944369\n",
            "Loss S44:  0.03786449479002531\n",
            "Loss S11:  0.035442167554389344\n",
            "Loss S22:  0.03447228436360338\n",
            "Loss S33:  0.03335249439261837\n",
            "Loss S44:  0.03786209086885473\n",
            "Loss S11:  0.035448851758804086\n",
            "Loss S22:  0.03448567377835706\n",
            "Loss S33:  0.03335184605545698\n",
            "Loss S44:  0.037870436001754895\n",
            "Loss S11:  0.03544872282211315\n",
            "Loss S22:  0.03447612374230049\n",
            "Loss S33:  0.03335967108670567\n",
            "Loss S44:  0.03787837345983572\n",
            "Loss S11:  0.03544744084397498\n",
            "Loss S22:  0.03446148995410751\n",
            "Loss S33:  0.0333741356903327\n",
            "Loss S44:  0.03788682821510735\n",
            "Loss S11:  0.035426718970028546\n",
            "Loss S22:  0.03444098912240167\n",
            "Loss S33:  0.03335621809613438\n",
            "Loss S44:  0.03787808155151589\n",
            "Validation: \n",
            " Loss S11:  0.03256680443882942\n",
            " Loss S22:  0.04637949913740158\n",
            " Loss S33:  0.04064879193902016\n",
            " Loss S44:  0.04638452082872391\n",
            " Loss S11:  0.03374792351609185\n",
            " Loss S22:  0.045481707545973006\n",
            " Loss S33:  0.04489859122605551\n",
            " Loss S44:  0.04795909868109794\n",
            " Loss S11:  0.03369956976938539\n",
            " Loss S22:  0.045341223387456525\n",
            " Loss S33:  0.045795449759902025\n",
            " Loss S44:  0.04759693318387357\n",
            " Loss S11:  0.033523983948054864\n",
            " Loss S22:  0.04516966738661782\n",
            " Loss S33:  0.04593333711878198\n",
            " Loss S44:  0.04731755080770274\n",
            " Loss S11:  0.03353931681241518\n",
            " Loss S22:  0.04517605983548694\n",
            " Loss S33:  0.045965156299464495\n",
            " Loss S44:  0.04729721451431145\n",
            "\n",
            "Epoch: 39\n",
            "Loss S11:  0.04008784517645836\n",
            "Loss S22:  0.03805726394057274\n",
            "Loss S33:  0.03914225101470947\n",
            "Loss S44:  0.04089057818055153\n",
            "Loss S11:  0.03537777709690007\n",
            "Loss S22:  0.03491737558083101\n",
            "Loss S33:  0.03381588411602107\n",
            "Loss S44:  0.03784403611313213\n",
            "Loss S11:  0.03562796151354199\n",
            "Loss S22:  0.03488505871168205\n",
            "Loss S33:  0.033624691799992605\n",
            "Loss S44:  0.03781525897128241\n",
            "Loss S11:  0.035633801572745843\n",
            "Loss S22:  0.034890731796622276\n",
            "Loss S33:  0.033564045304252256\n",
            "Loss S44:  0.03766073166362701\n",
            "Loss S11:  0.03565363313366727\n",
            "Loss S22:  0.03477895119023032\n",
            "Loss S33:  0.033676931043950524\n",
            "Loss S44:  0.037820691106522954\n",
            "Loss S11:  0.035712033729342854\n",
            "Loss S22:  0.034719478521569105\n",
            "Loss S33:  0.0336298905313015\n",
            "Loss S44:  0.03791370250138582\n",
            "Loss S11:  0.03561863333719675\n",
            "Loss S22:  0.03466549701988697\n",
            "Loss S33:  0.03338738472857436\n",
            "Loss S44:  0.03771968265293074\n",
            "Loss S11:  0.0356569708338086\n",
            "Loss S22:  0.03483141914114986\n",
            "Loss S33:  0.03335294851534803\n",
            "Loss S44:  0.037797538957125704\n",
            "Loss S11:  0.03558083462678356\n",
            "Loss S22:  0.03474543117170716\n",
            "Loss S33:  0.03323108157901852\n",
            "Loss S44:  0.037679602288537554\n",
            "Loss S11:  0.035445996856951455\n",
            "Loss S22:  0.034740860133871926\n",
            "Loss S33:  0.033253954395979314\n",
            "Loss S44:  0.03760970731849199\n",
            "Loss S11:  0.0353155554004825\n",
            "Loss S22:  0.034643702596278476\n",
            "Loss S33:  0.03317675916570248\n",
            "Loss S44:  0.03748275258458487\n",
            "Loss S11:  0.03528647973865002\n",
            "Loss S22:  0.03450044878834003\n",
            "Loss S33:  0.03313717647111631\n",
            "Loss S44:  0.037369399021069206\n",
            "Loss S11:  0.03529526521105412\n",
            "Loss S22:  0.03448257298201076\n",
            "Loss S33:  0.03327654984063846\n",
            "Loss S44:  0.03739052256646235\n",
            "Loss S11:  0.03534319288744271\n",
            "Loss S22:  0.034495641242798046\n",
            "Loss S33:  0.0333481538358308\n",
            "Loss S44:  0.03753800906297815\n",
            "Loss S11:  0.03534806873781461\n",
            "Loss S22:  0.034415699898245486\n",
            "Loss S33:  0.033328618206982075\n",
            "Loss S44:  0.03760079830779251\n",
            "Loss S11:  0.035307843287456904\n",
            "Loss S22:  0.03440794011960361\n",
            "Loss S33:  0.03329935541610844\n",
            "Loss S44:  0.03766759312310756\n",
            "Loss S11:  0.0351852298662159\n",
            "Loss S22:  0.03435454692298211\n",
            "Loss S33:  0.03323899713917548\n",
            "Loss S44:  0.037607995874207954\n",
            "Loss S11:  0.03522995025006651\n",
            "Loss S22:  0.0343864474619864\n",
            "Loss S33:  0.03326347553067737\n",
            "Loss S44:  0.03764511209133773\n",
            "Loss S11:  0.03518796104999537\n",
            "Loss S22:  0.03440653130370938\n",
            "Loss S33:  0.03328031773848757\n",
            "Loss S44:  0.03764669710348324\n",
            "Loss S11:  0.03514783488128198\n",
            "Loss S22:  0.03445094608316559\n",
            "Loss S33:  0.03330730492765991\n",
            "Loss S44:  0.03760931697425418\n",
            "Loss S11:  0.035121552590558776\n",
            "Loss S22:  0.034399161541565734\n",
            "Loss S33:  0.033329892069546146\n",
            "Loss S44:  0.03758854452353805\n",
            "Loss S11:  0.03516686322847249\n",
            "Loss S22:  0.03437129804484935\n",
            "Loss S33:  0.03332832144017186\n",
            "Loss S44:  0.037568048048782124\n",
            "Loss S11:  0.0351677700304068\n",
            "Loss S22:  0.034363342086653904\n",
            "Loss S33:  0.03337246391129979\n",
            "Loss S44:  0.03756254565864127\n",
            "Loss S11:  0.035149791023947975\n",
            "Loss S22:  0.034328836617080166\n",
            "Loss S33:  0.03335270503318155\n",
            "Loss S44:  0.03752040896903385\n",
            "Loss S11:  0.03523355748705349\n",
            "Loss S22:  0.034384473903236054\n",
            "Loss S33:  0.03340654483227314\n",
            "Loss S44:  0.037561222261660324\n",
            "Loss S11:  0.03524124588208844\n",
            "Loss S22:  0.03437101099625289\n",
            "Loss S33:  0.03340911615892236\n",
            "Loss S44:  0.03756473520659356\n",
            "Loss S11:  0.035244648269880774\n",
            "Loss S22:  0.034368788937670515\n",
            "Loss S33:  0.03341887747133829\n",
            "Loss S44:  0.037563510481767724\n",
            "Loss S11:  0.0352621254720811\n",
            "Loss S22:  0.03435251367625495\n",
            "Loss S33:  0.03338640870533306\n",
            "Loss S44:  0.0375227450609647\n",
            "Loss S11:  0.03524946921183545\n",
            "Loss S22:  0.03435860372072225\n",
            "Loss S33:  0.033359184960612624\n",
            "Loss S44:  0.037517853465686916\n",
            "Loss S11:  0.03526273542611869\n",
            "Loss S22:  0.03436333274539189\n",
            "Loss S33:  0.0333856956975669\n",
            "Loss S44:  0.03754336660875078\n",
            "Loss S11:  0.035229782750242175\n",
            "Loss S22:  0.03437632207829889\n",
            "Loss S33:  0.03336124786754383\n",
            "Loss S44:  0.03755518040338228\n",
            "Loss S11:  0.03520625122969557\n",
            "Loss S22:  0.034341462357778255\n",
            "Loss S33:  0.03333624028316265\n",
            "Loss S44:  0.03757293079156201\n",
            "Loss S11:  0.03517551603509444\n",
            "Loss S22:  0.03431779984674907\n",
            "Loss S33:  0.03330819601790742\n",
            "Loss S44:  0.037552039803374224\n",
            "Loss S11:  0.03515820792141818\n",
            "Loss S22:  0.03431315630044102\n",
            "Loss S33:  0.03330637550606108\n",
            "Loss S44:  0.037536907277078424\n",
            "Loss S11:  0.03516036186290801\n",
            "Loss S22:  0.03430166799039785\n",
            "Loss S33:  0.03330099471787379\n",
            "Loss S44:  0.0375356882635799\n",
            "Loss S11:  0.03518421030430882\n",
            "Loss S22:  0.03429488554872848\n",
            "Loss S33:  0.033328126367722824\n",
            "Loss S44:  0.03755762078865641\n",
            "Loss S11:  0.03518687742259199\n",
            "Loss S22:  0.03431799385046529\n",
            "Loss S33:  0.03332412861535258\n",
            "Loss S44:  0.037566486952460044\n",
            "Loss S11:  0.03515975953975938\n",
            "Loss S22:  0.03430765980196931\n",
            "Loss S33:  0.033310401035005835\n",
            "Loss S44:  0.03753575002849905\n",
            "Loss S11:  0.03511685022981617\n",
            "Loss S22:  0.03430295215914725\n",
            "Loss S33:  0.03329004096527269\n",
            "Loss S44:  0.037489925078519684\n",
            "Loss S11:  0.035090375382005407\n",
            "Loss S22:  0.03425986668490388\n",
            "Loss S33:  0.03325592727421799\n",
            "Loss S44:  0.0374636413038844\n",
            "Loss S11:  0.0350980893046853\n",
            "Loss S22:  0.034230379912621366\n",
            "Loss S33:  0.0332510519559098\n",
            "Loss S44:  0.03745690264049314\n",
            "Loss S11:  0.03508215919680839\n",
            "Loss S22:  0.03421172991376869\n",
            "Loss S33:  0.033246780072685576\n",
            "Loss S44:  0.037437194980989116\n",
            "Loss S11:  0.03509933896132716\n",
            "Loss S22:  0.034229571614724154\n",
            "Loss S33:  0.0332640839780453\n",
            "Loss S44:  0.03745701131267106\n",
            "Loss S11:  0.035082470686142515\n",
            "Loss S22:  0.03421615402099704\n",
            "Loss S33:  0.033221921985083\n",
            "Loss S44:  0.03743149722189472\n",
            "Loss S11:  0.035100512899238774\n",
            "Loss S22:  0.034228618140596384\n",
            "Loss S33:  0.03322769215757074\n",
            "Loss S44:  0.03745498143288554\n",
            "Loss S11:  0.03509760088805613\n",
            "Loss S22:  0.03422753539291559\n",
            "Loss S33:  0.03322098227403645\n",
            "Loss S44:  0.037470677731240666\n",
            "Loss S11:  0.035086116723215245\n",
            "Loss S22:  0.03422754695093554\n",
            "Loss S33:  0.03321958632246791\n",
            "Loss S44:  0.037481765078942325\n",
            "Loss S11:  0.035073109390312954\n",
            "Loss S22:  0.03421741476452528\n",
            "Loss S33:  0.03321535634608532\n",
            "Loss S44:  0.03748453952640991\n",
            "Loss S11:  0.03507823705394402\n",
            "Loss S22:  0.03420047098364131\n",
            "Loss S33:  0.03322728391419205\n",
            "Loss S44:  0.0374836492547746\n",
            "Loss S11:  0.03506954894410617\n",
            "Loss S22:  0.034180153544438346\n",
            "Loss S33:  0.03320756105444339\n",
            "Loss S44:  0.03747250126815133\n",
            "Validation: \n",
            " Loss S11:  0.032587938010692596\n",
            " Loss S22:  0.04483870416879654\n",
            " Loss S33:  0.04254525154829025\n",
            " Loss S44:  0.04735194146633148\n",
            " Loss S11:  0.03430744207331112\n",
            " Loss S22:  0.044507671679769246\n",
            " Loss S33:  0.045873675495386124\n",
            " Loss S44:  0.04864986435998054\n",
            " Loss S11:  0.03417954234996947\n",
            " Loss S22:  0.044424146504663836\n",
            " Loss S33:  0.046953759996629346\n",
            " Loss S44:  0.0484785774677265\n",
            " Loss S11:  0.03406109544830244\n",
            " Loss S22:  0.044300782265233214\n",
            " Loss S33:  0.04698532276221963\n",
            " Loss S44:  0.04816517844551899\n",
            " Loss S11:  0.033980407280686464\n",
            " Loss S22:  0.04429638468556934\n",
            " Loss S33:  0.046989813860919744\n",
            " Loss S44:  0.04810836915800601\n",
            "\n",
            "Epoch: 40\n",
            "Loss S11:  0.0384768508374691\n",
            "Loss S22:  0.037004269659519196\n",
            "Loss S33:  0.03855200111865997\n",
            "Loss S44:  0.04256301000714302\n",
            "Loss S11:  0.03479278595610098\n",
            "Loss S22:  0.03434673629023812\n",
            "Loss S33:  0.03313666954636574\n",
            "Loss S44:  0.037322544916109604\n",
            "Loss S11:  0.035049199348404295\n",
            "Loss S22:  0.03415337080756823\n",
            "Loss S33:  0.03332583748158954\n",
            "Loss S44:  0.03744748146051452\n",
            "Loss S11:  0.03518605688887258\n",
            "Loss S22:  0.034045075817454244\n",
            "Loss S33:  0.0332993098324345\n",
            "Loss S44:  0.03767040516099622\n",
            "Loss S11:  0.03516234339373868\n",
            "Loss S22:  0.03399219509305024\n",
            "Loss S33:  0.033211366341608324\n",
            "Loss S44:  0.037857499155329495\n",
            "Loss S11:  0.035163601385612114\n",
            "Loss S22:  0.03403456509113312\n",
            "Loss S33:  0.03336950508402843\n",
            "Loss S44:  0.03781480641633857\n",
            "Loss S11:  0.03518596291542053\n",
            "Loss S22:  0.03406021023382906\n",
            "Loss S33:  0.03321309235008037\n",
            "Loss S44:  0.037603941975069825\n",
            "Loss S11:  0.03518050688673073\n",
            "Loss S22:  0.03420526901601066\n",
            "Loss S33:  0.03331356521853259\n",
            "Loss S44:  0.03754969303246955\n",
            "Loss S11:  0.03509960682303817\n",
            "Loss S22:  0.034158931079286116\n",
            "Loss S33:  0.033144395545492936\n",
            "Loss S44:  0.0373743084937702\n",
            "Loss S11:  0.03507867694965431\n",
            "Loss S22:  0.03420223780795113\n",
            "Loss S33:  0.033115152535693984\n",
            "Loss S44:  0.037289035557718064\n",
            "Loss S11:  0.03497623984176334\n",
            "Loss S22:  0.03408592127927459\n",
            "Loss S33:  0.032995087915275355\n",
            "Loss S44:  0.037225022764489205\n",
            "Loss S11:  0.03496415448282753\n",
            "Loss S22:  0.03405688149300781\n",
            "Loss S33:  0.0329593493863269\n",
            "Loss S44:  0.03713984448496286\n",
            "Loss S11:  0.034981108539976366\n",
            "Loss S22:  0.034092525651385965\n",
            "Loss S33:  0.03310503162566788\n",
            "Loss S44:  0.03719292029984726\n",
            "Loss S11:  0.03501625539635429\n",
            "Loss S22:  0.03410784502065819\n",
            "Loss S33:  0.033117411103639895\n",
            "Loss S44:  0.0372692825457522\n",
            "Loss S11:  0.03506148252504092\n",
            "Loss S22:  0.03408136167266267\n",
            "Loss S33:  0.033089701852477185\n",
            "Loss S44:  0.03732836624303608\n",
            "Loss S11:  0.035074490905794876\n",
            "Loss S22:  0.03412334466760127\n",
            "Loss S33:  0.03314274886240628\n",
            "Loss S44:  0.03731205499428787\n",
            "Loss S11:  0.03501280902751854\n",
            "Loss S22:  0.03409470886177158\n",
            "Loss S33:  0.03304597414937449\n",
            "Loss S44:  0.037247456911002626\n",
            "Loss S11:  0.03504074625235203\n",
            "Loss S22:  0.034085050225257874\n",
            "Loss S33:  0.03304443478976425\n",
            "Loss S44:  0.0373076459948431\n",
            "Loss S11:  0.035059707301015354\n",
            "Loss S22:  0.03410204024522344\n",
            "Loss S33:  0.03311658055825128\n",
            "Loss S44:  0.03735861330401173\n",
            "Loss S11:  0.03502220214778528\n",
            "Loss S22:  0.03411014676484138\n",
            "Loss S33:  0.0331147793894975\n",
            "Loss S44:  0.0373517180456541\n",
            "Loss S11:  0.03498475228561394\n",
            "Loss S22:  0.034059571055927086\n",
            "Loss S33:  0.03311183894822253\n",
            "Loss S44:  0.03734777511945411\n",
            "Loss S11:  0.03505590913808459\n",
            "Loss S22:  0.03408237447812094\n",
            "Loss S33:  0.03311168196747936\n",
            "Loss S44:  0.03736059189337125\n",
            "Loss S11:  0.035080939843171864\n",
            "Loss S22:  0.03408693513786631\n",
            "Loss S33:  0.033110278157087475\n",
            "Loss S44:  0.03735475274165292\n",
            "Loss S11:  0.03510498915883628\n",
            "Loss S22:  0.03409596052004661\n",
            "Loss S33:  0.03309292444612557\n",
            "Loss S44:  0.0373316104987483\n",
            "Loss S11:  0.035182838656657466\n",
            "Loss S22:  0.034148494144940277\n",
            "Loss S33:  0.03313377929302667\n",
            "Loss S44:  0.03739421531805359\n",
            "Loss S11:  0.03516042755686667\n",
            "Loss S22:  0.03415260809766819\n",
            "Loss S33:  0.033120702521734505\n",
            "Loss S44:  0.03737141842029959\n",
            "Loss S11:  0.035160357318104914\n",
            "Loss S22:  0.0341497206082746\n",
            "Loss S33:  0.03311856861772208\n",
            "Loss S44:  0.037407380996193465\n",
            "Loss S11:  0.035171084319038586\n",
            "Loss S22:  0.03416368123350108\n",
            "Loss S33:  0.03311638079748602\n",
            "Loss S44:  0.03742787639208386\n",
            "Loss S11:  0.03516459452432458\n",
            "Loss S22:  0.034147022842194265\n",
            "Loss S33:  0.03311269519539066\n",
            "Loss S44:  0.03742618480982305\n",
            "Loss S11:  0.03517012041433366\n",
            "Loss S22:  0.03415145256470159\n",
            "Loss S33:  0.03313193246210154\n",
            "Loss S44:  0.03746518030557845\n",
            "Loss S11:  0.035135789907404354\n",
            "Loss S22:  0.03412283028157249\n",
            "Loss S33:  0.03313409670123032\n",
            "Loss S44:  0.03746430085742988\n",
            "Loss S11:  0.035109794857369744\n",
            "Loss S22:  0.03408448450818322\n",
            "Loss S33:  0.033096012171250064\n",
            "Loss S44:  0.03743873219158488\n",
            "Loss S11:  0.035113038245669775\n",
            "Loss S22:  0.034082724437163994\n",
            "Loss S33:  0.0330786205178295\n",
            "Loss S44:  0.037416403585133894\n",
            "Loss S11:  0.03509573046444406\n",
            "Loss S22:  0.03405872274336498\n",
            "Loss S33:  0.03305615187591657\n",
            "Loss S44:  0.0373977998727398\n",
            "Loss S11:  0.035081009743762506\n",
            "Loss S22:  0.03406159125836539\n",
            "Loss S33:  0.033045469294370446\n",
            "Loss S44:  0.03741143746547335\n",
            "Loss S11:  0.03509569802769908\n",
            "Loss S22:  0.03406476400835052\n",
            "Loss S33:  0.03307314802948226\n",
            "Loss S44:  0.03744752608515598\n",
            "Loss S11:  0.03510686974982806\n",
            "Loss S22:  0.03407236818131317\n",
            "Loss S33:  0.03306344244371161\n",
            "Loss S44:  0.037463215744726545\n",
            "Loss S11:  0.03509009312185637\n",
            "Loss S22:  0.03406451104304219\n",
            "Loss S33:  0.03306966935688595\n",
            "Loss S44:  0.03743263989727131\n",
            "Loss S11:  0.03506909421305331\n",
            "Loss S22:  0.03405274147671351\n",
            "Loss S33:  0.03303642044934075\n",
            "Loss S44:  0.03738599367381081\n",
            "Loss S11:  0.0350180473512091\n",
            "Loss S22:  0.03402957268764296\n",
            "Loss S33:  0.033011873817199944\n",
            "Loss S44:  0.037353512118844426\n",
            "Loss S11:  0.03501899010894304\n",
            "Loss S22:  0.03401385837157914\n",
            "Loss S33:  0.03301846819402571\n",
            "Loss S44:  0.03735020916509509\n",
            "Loss S11:  0.03500495832458755\n",
            "Loss S22:  0.03399918540641484\n",
            "Loss S33:  0.03301782499083347\n",
            "Loss S44:  0.03735081939402868\n",
            "Loss S11:  0.035000526525451846\n",
            "Loss S22:  0.03399985999470652\n",
            "Loss S33:  0.03303821689743469\n",
            "Loss S44:  0.03737139869083135\n",
            "Loss S11:  0.034994382709437624\n",
            "Loss S22:  0.0339912276827404\n",
            "Loss S33:  0.03300890906901896\n",
            "Loss S44:  0.03735014699451331\n",
            "Loss S11:  0.03499736965902133\n",
            "Loss S22:  0.03400221362270736\n",
            "Loss S33:  0.03300468906625058\n",
            "Loss S44:  0.03737062600160402\n",
            "Loss S11:  0.034993176652064875\n",
            "Loss S22:  0.03400567111594317\n",
            "Loss S33:  0.03302841451836795\n",
            "Loss S44:  0.03736834171870331\n",
            "Loss S11:  0.03498758380897262\n",
            "Loss S22:  0.0340153129646098\n",
            "Loss S33:  0.03304595167702165\n",
            "Loss S44:  0.03738707489861325\n",
            "Loss S11:  0.034982779614959554\n",
            "Loss S22:  0.034001871354229385\n",
            "Loss S33:  0.03304739236720935\n",
            "Loss S44:  0.037398143234463\n",
            "Loss S11:  0.0349885851121556\n",
            "Loss S22:  0.033977233605331544\n",
            "Loss S33:  0.03305785319614931\n",
            "Loss S44:  0.03739033260706061\n",
            "Loss S11:  0.03496982692503759\n",
            "Loss S22:  0.033957359591563464\n",
            "Loss S33:  0.033042356918154325\n",
            "Loss S44:  0.03737615488181289\n",
            "Validation: \n",
            " Loss S11:  0.03333253413438797\n",
            " Loss S22:  0.04565566033124924\n",
            " Loss S33:  0.04221430793404579\n",
            " Loss S44:  0.04552973806858063\n",
            " Loss S11:  0.03380244686490014\n",
            " Loss S22:  0.04376124005232539\n",
            " Loss S33:  0.045194242326986225\n",
            " Loss S44:  0.04759493913678896\n",
            " Loss S11:  0.03371090714524432\n",
            " Loss S22:  0.04353782016693092\n",
            " Loss S33:  0.046197085936621925\n",
            " Loss S44:  0.04743351542004725\n",
            " Loss S11:  0.03359541660327403\n",
            " Loss S22:  0.04343277325884241\n",
            " Loss S33:  0.04621225241266313\n",
            " Loss S44:  0.04713163921823267\n",
            " Loss S11:  0.03356424132706942\n",
            " Loss S22:  0.043466867626081276\n",
            " Loss S33:  0.04616155415589427\n",
            " Loss S44:  0.04709878260338748\n",
            "\n",
            "Epoch: 41\n",
            "Loss S11:  0.03720592334866524\n",
            "Loss S22:  0.033724989742040634\n",
            "Loss S33:  0.0367017537355423\n",
            "Loss S44:  0.03903387859463692\n",
            "Loss S11:  0.034442242912270805\n",
            "Loss S22:  0.03364227983084592\n",
            "Loss S33:  0.03305691819299351\n",
            "Loss S44:  0.03746197745203972\n",
            "Loss S11:  0.03497998540600141\n",
            "Loss S22:  0.033971693366765976\n",
            "Loss S33:  0.03285707276137102\n",
            "Loss S44:  0.03753603817451568\n",
            "Loss S11:  0.03491546153541534\n",
            "Loss S22:  0.033971298846506306\n",
            "Loss S33:  0.03265600573391684\n",
            "Loss S44:  0.037346733914267634\n",
            "Loss S11:  0.03466979750409359\n",
            "Loss S22:  0.03412434286097201\n",
            "Loss S33:  0.03259626480682594\n",
            "Loss S44:  0.037318068851784965\n",
            "Loss S11:  0.03481053677844066\n",
            "Loss S22:  0.03425587896330684\n",
            "Loss S33:  0.03269085540052723\n",
            "Loss S44:  0.03748729525535714\n",
            "Loss S11:  0.034848150232287704\n",
            "Loss S22:  0.034175596948041294\n",
            "Loss S33:  0.03261765920114322\n",
            "Loss S44:  0.03730546909033275\n",
            "Loss S11:  0.03489898501987189\n",
            "Loss S22:  0.03417973855221775\n",
            "Loss S33:  0.03270358642117238\n",
            "Loss S44:  0.03720938309397496\n",
            "Loss S11:  0.03488031034300357\n",
            "Loss S22:  0.034099629746727\n",
            "Loss S33:  0.03257416676224014\n",
            "Loss S44:  0.03706343393818832\n",
            "Loss S11:  0.03482437260694556\n",
            "Loss S22:  0.034098234170904525\n",
            "Loss S33:  0.032548444611685615\n",
            "Loss S44:  0.036992261288585244\n",
            "Loss S11:  0.034708634443064726\n",
            "Loss S22:  0.03396607826620635\n",
            "Loss S33:  0.03243529182480703\n",
            "Loss S44:  0.036866040845023526\n",
            "Loss S11:  0.03466551715659129\n",
            "Loss S22:  0.03389059906607276\n",
            "Loss S33:  0.03239452107264115\n",
            "Loss S44:  0.03678433862221134\n",
            "Loss S11:  0.034721873993100213\n",
            "Loss S22:  0.03393956410798652\n",
            "Loss S33:  0.03251550345073554\n",
            "Loss S44:  0.03685105816761324\n",
            "Loss S11:  0.03471563044110782\n",
            "Loss S22:  0.03394668442659251\n",
            "Loss S33:  0.03258604627415424\n",
            "Loss S44:  0.036931945963670276\n",
            "Loss S11:  0.03473410028832179\n",
            "Loss S22:  0.033919439208528676\n",
            "Loss S33:  0.032597709750981196\n",
            "Loss S44:  0.03697798792140704\n",
            "Loss S11:  0.034696425872528\n",
            "Loss S22:  0.03390472282390326\n",
            "Loss S33:  0.0325961367331988\n",
            "Loss S44:  0.03696649033107505\n",
            "Loss S11:  0.03465424785795419\n",
            "Loss S22:  0.033899535153130565\n",
            "Loss S33:  0.032584856594016096\n",
            "Loss S44:  0.03693445378578968\n",
            "Loss S11:  0.03469141042241228\n",
            "Loss S22:  0.03393287755200389\n",
            "Loss S33:  0.032577652070257396\n",
            "Loss S44:  0.036958269818484435\n",
            "Loss S11:  0.0346956810301674\n",
            "Loss S22:  0.03397075990369307\n",
            "Loss S33:  0.03262121401192075\n",
            "Loss S44:  0.0369837768993325\n",
            "Loss S11:  0.034683735085485494\n",
            "Loss S22:  0.03393156543686128\n",
            "Loss S33:  0.03262541842944335\n",
            "Loss S44:  0.03701389822816349\n",
            "Loss S11:  0.03466008962201538\n",
            "Loss S22:  0.033857990732759385\n",
            "Loss S33:  0.032634757032886666\n",
            "Loss S44:  0.03700443861348119\n",
            "Loss S11:  0.03470100028996501\n",
            "Loss S22:  0.03387240141241754\n",
            "Loss S33:  0.032633111931348296\n",
            "Loss S44:  0.03697877246621661\n",
            "Loss S11:  0.0347181539645427\n",
            "Loss S22:  0.033838546594680705\n",
            "Loss S33:  0.03267029855754311\n",
            "Loss S44:  0.036970852052464205\n",
            "Loss S11:  0.034685426637594836\n",
            "Loss S22:  0.03382330013689024\n",
            "Loss S33:  0.03263428274267938\n",
            "Loss S44:  0.03696412651182769\n",
            "Loss S11:  0.03475328817716278\n",
            "Loss S22:  0.033862259486280535\n",
            "Loss S33:  0.03268223670910256\n",
            "Loss S44:  0.037007076348877545\n",
            "Loss S11:  0.034723031494128274\n",
            "Loss S22:  0.03385281095288664\n",
            "Loss S33:  0.03266431164901808\n",
            "Loss S44:  0.036983235186314675\n",
            "Loss S11:  0.03474056703605871\n",
            "Loss S22:  0.033850695780867364\n",
            "Loss S33:  0.03267314147304073\n",
            "Loss S44:  0.0369979297435375\n",
            "Loss S11:  0.034736826369793215\n",
            "Loss S22:  0.033831536247501515\n",
            "Loss S33:  0.03267601514544434\n",
            "Loss S44:  0.03698839592262827\n",
            "Loss S11:  0.0347233471656185\n",
            "Loss S22:  0.03381365002050094\n",
            "Loss S33:  0.0326557336739692\n",
            "Loss S44:  0.03700282966910308\n",
            "Loss S11:  0.03472921960183845\n",
            "Loss S22:  0.033819404299470154\n",
            "Loss S33:  0.03267748325525485\n",
            "Loss S44:  0.03702814838628179\n",
            "Loss S11:  0.034724534418050235\n",
            "Loss S22:  0.033801495212059086\n",
            "Loss S33:  0.032678168644491225\n",
            "Loss S44:  0.03701279342075519\n",
            "Loss S11:  0.03472435672638692\n",
            "Loss S22:  0.03378166736274287\n",
            "Loss S33:  0.03267329712053971\n",
            "Loss S44:  0.03704196542884759\n",
            "Loss S11:  0.03473923964037145\n",
            "Loss S22:  0.033791413931089025\n",
            "Loss S33:  0.03266690545526806\n",
            "Loss S44:  0.03703195377720108\n",
            "Loss S11:  0.034732019011032546\n",
            "Loss S22:  0.03378365382979824\n",
            "Loss S33:  0.03264582804277584\n",
            "Loss S44:  0.03703781067649763\n",
            "Loss S11:  0.034715188306666186\n",
            "Loss S22:  0.03377142470450171\n",
            "Loss S33:  0.03263616672228159\n",
            "Loss S44:  0.03704881555567389\n",
            "Loss S11:  0.03472403103108929\n",
            "Loss S22:  0.03374110357544021\n",
            "Loss S33:  0.032644948776163946\n",
            "Loss S44:  0.037046117586033295\n",
            "Loss S11:  0.03474319301586897\n",
            "Loss S22:  0.03372869215430976\n",
            "Loss S33:  0.03266230266324536\n",
            "Loss S44:  0.03706383005486301\n",
            "Loss S11:  0.03471907204634578\n",
            "Loss S22:  0.033721246272124694\n",
            "Loss S33:  0.0326644565679153\n",
            "Loss S44:  0.03702731396390421\n",
            "Loss S11:  0.034695724465363606\n",
            "Loss S22:  0.033725429595181634\n",
            "Loss S33:  0.03263558727002207\n",
            "Loss S44:  0.03699326249245271\n",
            "Loss S11:  0.03466563045864215\n",
            "Loss S22:  0.033684962598221076\n",
            "Loss S33:  0.03259752470704601\n",
            "Loss S44:  0.036975748846521765\n",
            "Loss S11:  0.03465916241456446\n",
            "Loss S22:  0.03367151572204142\n",
            "Loss S33:  0.032591026968455075\n",
            "Loss S44:  0.03696919709183926\n",
            "Loss S11:  0.03465020595857117\n",
            "Loss S22:  0.0336591466273342\n",
            "Loss S33:  0.032590761526947766\n",
            "Loss S44:  0.03695586856699338\n",
            "Loss S11:  0.03465433690847382\n",
            "Loss S22:  0.033662637850521863\n",
            "Loss S33:  0.03260573111209054\n",
            "Loss S44:  0.03696251627845323\n",
            "Loss S11:  0.0346550412363699\n",
            "Loss S22:  0.033672916087086405\n",
            "Loss S33:  0.03259514106260375\n",
            "Loss S44:  0.03694632793765887\n",
            "Loss S11:  0.03466991612649694\n",
            "Loss S22:  0.033677435161169965\n",
            "Loss S33:  0.03259322557454747\n",
            "Loss S44:  0.036931578331793787\n",
            "Loss S11:  0.03467190274328588\n",
            "Loss S22:  0.03367069428848586\n",
            "Loss S33:  0.03260542754653817\n",
            "Loss S44:  0.03692210263297986\n",
            "Loss S11:  0.034673199150509273\n",
            "Loss S22:  0.03367185822956733\n",
            "Loss S33:  0.032610839802505134\n",
            "Loss S44:  0.03695375180587075\n",
            "Loss S11:  0.03466909316283376\n",
            "Loss S22:  0.0336562284563061\n",
            "Loss S33:  0.032609510965123295\n",
            "Loss S44:  0.036953706120293105\n",
            "Loss S11:  0.03466727500181188\n",
            "Loss S22:  0.03364684817665828\n",
            "Loss S33:  0.03260447090712382\n",
            "Loss S44:  0.03696353815635376\n",
            "Loss S11:  0.034660789205136947\n",
            "Loss S22:  0.033648425728355795\n",
            "Loss S33:  0.03258761158876895\n",
            "Loss S44:  0.03695733712458805\n",
            "Validation: \n",
            " Loss S11:  0.03298163041472435\n",
            " Loss S22:  0.04711994528770447\n",
            " Loss S33:  0.041627466678619385\n",
            " Loss S44:  0.046310413628816605\n",
            " Loss S11:  0.033639944735027495\n",
            " Loss S22:  0.04549031882059006\n",
            " Loss S33:  0.04435620173102334\n",
            " Loss S44:  0.047703515560854046\n",
            " Loss S11:  0.033573517575860023\n",
            " Loss S22:  0.045230880107094605\n",
            " Loss S33:  0.04529047139534136\n",
            " Loss S44:  0.04752862716956836\n",
            " Loss S11:  0.03351094207314194\n",
            " Loss S22:  0.04497650216837398\n",
            " Loss S33:  0.04528488005038168\n",
            " Loss S44:  0.04720692434271828\n",
            " Loss S11:  0.033482624470819664\n",
            " Loss S22:  0.04497676250743277\n",
            " Loss S33:  0.045244990169634056\n",
            " Loss S44:  0.04717997148816968\n",
            "\n",
            "Epoch: 42\n",
            "Loss S11:  0.03556462749838829\n",
            "Loss S22:  0.03683479130268097\n",
            "Loss S33:  0.03509366884827614\n",
            "Loss S44:  0.04088101163506508\n",
            "Loss S11:  0.03466026823629032\n",
            "Loss S22:  0.03316012075678869\n",
            "Loss S33:  0.03252224742688916\n",
            "Loss S44:  0.03680213608525016\n",
            "Loss S11:  0.034782307665972484\n",
            "Loss S22:  0.03321360974084763\n",
            "Loss S33:  0.03233563021889755\n",
            "Loss S44:  0.03712836706212589\n",
            "Loss S11:  0.03478692712322358\n",
            "Loss S22:  0.03349529579281807\n",
            "Loss S33:  0.032582609223261956\n",
            "Loss S44:  0.0373337653375441\n",
            "Loss S11:  0.03459966219053036\n",
            "Loss S22:  0.03359702129553004\n",
            "Loss S33:  0.0326301326929796\n",
            "Loss S44:  0.03731169242684434\n",
            "Loss S11:  0.034648739020614064\n",
            "Loss S22:  0.03362455018156884\n",
            "Loss S33:  0.03266622622807821\n",
            "Loss S44:  0.037497447840138975\n",
            "Loss S11:  0.03473357483744621\n",
            "Loss S22:  0.03361437521630623\n",
            "Loss S33:  0.032495258864564974\n",
            "Loss S44:  0.037315167120245636\n",
            "Loss S11:  0.03474086223983429\n",
            "Loss S22:  0.03378838075088783\n",
            "Loss S33:  0.032546847481542907\n",
            "Loss S44:  0.03724810181998871\n",
            "Loss S11:  0.03463500315024529\n",
            "Loss S22:  0.033836073575564375\n",
            "Loss S33:  0.03239887373314963\n",
            "Loss S44:  0.0370954612247002\n",
            "Loss S11:  0.03465486588058891\n",
            "Loss S22:  0.033866132149002054\n",
            "Loss S33:  0.03248288355522103\n",
            "Loss S44:  0.03708452653590139\n",
            "Loss S11:  0.0345648741441788\n",
            "Loss S22:  0.03375531550459933\n",
            "Loss S33:  0.03239173705846366\n",
            "Loss S44:  0.03697384805372446\n",
            "Loss S11:  0.03453404437918384\n",
            "Loss S22:  0.03365602711769375\n",
            "Loss S33:  0.03244191982053422\n",
            "Loss S44:  0.036981450444137726\n",
            "Loss S11:  0.03454828938115234\n",
            "Loss S22:  0.033613963945468596\n",
            "Loss S33:  0.032492333068704804\n",
            "Loss S44:  0.03695362519996225\n",
            "Loss S11:  0.03455055701254888\n",
            "Loss S22:  0.03361025417067168\n",
            "Loss S33:  0.03252260620129927\n",
            "Loss S44:  0.037032895757041814\n",
            "Loss S11:  0.03448392109985047\n",
            "Loss S22:  0.03351241569464088\n",
            "Loss S33:  0.032511726576597136\n",
            "Loss S44:  0.037020591186716204\n",
            "Loss S11:  0.034499451555064974\n",
            "Loss S22:  0.033535286118060545\n",
            "Loss S33:  0.032520274551498966\n",
            "Loss S44:  0.037044233661021615\n",
            "Loss S11:  0.03444807210601635\n",
            "Loss S22:  0.03354185060684725\n",
            "Loss S33:  0.03246206951021038\n",
            "Loss S44:  0.03696876951718923\n",
            "Loss S11:  0.034473521275478494\n",
            "Loss S22:  0.033598237022844674\n",
            "Loss S33:  0.032484137131805306\n",
            "Loss S44:  0.036952558708818334\n",
            "Loss S11:  0.03450921780908305\n",
            "Loss S22:  0.033591460004366566\n",
            "Loss S33:  0.03252044940645194\n",
            "Loss S44:  0.03695381243419911\n",
            "Loss S11:  0.03447834310653322\n",
            "Loss S22:  0.03357064565009784\n",
            "Loss S33:  0.03250473311551266\n",
            "Loss S44:  0.03693519013870449\n",
            "Loss S11:  0.034445042522689005\n",
            "Loss S22:  0.033547386082250684\n",
            "Loss S33:  0.03251736290493415\n",
            "Loss S44:  0.03690772926184668\n",
            "Loss S11:  0.034472148441731645\n",
            "Loss S22:  0.03355199291913713\n",
            "Loss S33:  0.03251743698007123\n",
            "Loss S44:  0.036918912895059136\n",
            "Loss S11:  0.034459716459205245\n",
            "Loss S22:  0.033558830222730186\n",
            "Loss S33:  0.032538797133244\n",
            "Loss S44:  0.0369247908775623\n",
            "Loss S11:  0.0344261188559976\n",
            "Loss S22:  0.03358225905017936\n",
            "Loss S33:  0.032536135091410054\n",
            "Loss S44:  0.03689755251010259\n",
            "Loss S11:  0.03450266700006125\n",
            "Loss S22:  0.033622527657331765\n",
            "Loss S33:  0.032566795164247764\n",
            "Loss S44:  0.036919345793264044\n",
            "Loss S11:  0.03450447758771034\n",
            "Loss S22:  0.03362000736582327\n",
            "Loss S33:  0.03256047394616433\n",
            "Loss S44:  0.03689122766968739\n",
            "Loss S11:  0.03453435988604337\n",
            "Loss S22:  0.03362299262107104\n",
            "Loss S33:  0.03256451390626559\n",
            "Loss S44:  0.03688428098500003\n",
            "Loss S11:  0.03454587473860526\n",
            "Loss S22:  0.033614655269303004\n",
            "Loss S33:  0.03256145188399347\n",
            "Loss S44:  0.036876019049174674\n",
            "Loss S11:  0.034543184649689765\n",
            "Loss S22:  0.03362921168663111\n",
            "Loss S33:  0.03254796719688962\n",
            "Loss S44:  0.03687760504327211\n",
            "Loss S11:  0.034546434968914776\n",
            "Loss S22:  0.03362630460654542\n",
            "Loss S33:  0.03255772340364268\n",
            "Loss S44:  0.036892331381331606\n",
            "Loss S11:  0.03450152302080413\n",
            "Loss S22:  0.03360852471048452\n",
            "Loss S33:  0.03254941764265992\n",
            "Loss S44:  0.03688369508201498\n",
            "Loss S11:  0.03446707255850843\n",
            "Loss S22:  0.033567264334708934\n",
            "Loss S33:  0.03251389065499858\n",
            "Loss S44:  0.03686572581385876\n",
            "Loss S11:  0.03445117864351592\n",
            "Loss S22:  0.03355152884097857\n",
            "Loss S33:  0.03249700906515307\n",
            "Loss S44:  0.03684326858953152\n",
            "Loss S11:  0.03444577028181438\n",
            "Loss S22:  0.03355095085893333\n",
            "Loss S33:  0.032490194766723136\n",
            "Loss S44:  0.03684386795561119\n",
            "Loss S11:  0.03444114240580116\n",
            "Loss S22:  0.03354019141288971\n",
            "Loss S33:  0.03248828745111167\n",
            "Loss S44:  0.03684619811972565\n",
            "Loss S11:  0.03441857129993432\n",
            "Loss S22:  0.03354336958686341\n",
            "Loss S33:  0.03249951424952756\n",
            "Loss S44:  0.036876263017328374\n",
            "Loss S11:  0.0344368265245182\n",
            "Loss S22:  0.03356219331251452\n",
            "Loss S33:  0.03252451333354055\n",
            "Loss S44:  0.03688434596041893\n",
            "Loss S11:  0.03441627624361823\n",
            "Loss S22:  0.03355486877262592\n",
            "Loss S33:  0.03251791287544924\n",
            "Loss S44:  0.03685503066429552\n",
            "Loss S11:  0.034400471454373804\n",
            "Loss S22:  0.0335461733241876\n",
            "Loss S33:  0.03247619317135629\n",
            "Loss S44:  0.03680739424672965\n",
            "Loss S11:  0.03438136895732654\n",
            "Loss S22:  0.033516687586370025\n",
            "Loss S33:  0.032457465105844886\n",
            "Loss S44:  0.03678914378671085\n",
            "Loss S11:  0.034400202840232194\n",
            "Loss S22:  0.03350320166706147\n",
            "Loss S33:  0.03244545456124214\n",
            "Loss S44:  0.03679443137752742\n",
            "Loss S11:  0.03439003242283987\n",
            "Loss S22:  0.03349570336314303\n",
            "Loss S33:  0.03245792630833286\n",
            "Loss S44:  0.036785658204207455\n",
            "Loss S11:  0.03439773915162987\n",
            "Loss S22:  0.033507746398767496\n",
            "Loss S33:  0.03249564208532739\n",
            "Loss S44:  0.03680924066915365\n",
            "Loss S11:  0.03437431058533092\n",
            "Loss S22:  0.03350362451768668\n",
            "Loss S33:  0.032470403347173035\n",
            "Loss S44:  0.036761446404346455\n",
            "Loss S11:  0.03439752486811068\n",
            "Loss S22:  0.03351014716521142\n",
            "Loss S33:  0.0324844961136985\n",
            "Loss S44:  0.03676486577914686\n",
            "Loss S11:  0.034406515730425156\n",
            "Loss S22:  0.03348761685440387\n",
            "Loss S33:  0.03248361121458383\n",
            "Loss S44:  0.03676205804104816\n",
            "Loss S11:  0.0344033025229378\n",
            "Loss S22:  0.033469863070292226\n",
            "Loss S33:  0.03248706303633862\n",
            "Loss S44:  0.036767840086444084\n",
            "Loss S11:  0.03440541669235994\n",
            "Loss S22:  0.033467918970560825\n",
            "Loss S33:  0.03248746109821867\n",
            "Loss S44:  0.03676167938802905\n",
            "Loss S11:  0.03441114083567553\n",
            "Loss S22:  0.03346151992941968\n",
            "Loss S33:  0.03250780959541981\n",
            "Loss S44:  0.03677222281384617\n",
            "Loss S11:  0.03440184000113224\n",
            "Loss S22:  0.03344868691808823\n",
            "Loss S33:  0.03247395775787335\n",
            "Loss S44:  0.036745110925006284\n",
            "Validation: \n",
            " Loss S11:  0.031906381249427795\n",
            " Loss S22:  0.046277254819869995\n",
            " Loss S33:  0.04170497506856918\n",
            " Loss S44:  0.046656835824251175\n",
            " Loss S11:  0.03338321165314743\n",
            " Loss S22:  0.04504464886018208\n",
            " Loss S33:  0.04472543281458673\n",
            " Loss S44:  0.04811430917609306\n",
            " Loss S11:  0.03335049848367528\n",
            " Loss S22:  0.044766505589572396\n",
            " Loss S33:  0.04562036120673505\n",
            " Loss S44:  0.04800012707710266\n",
            " Loss S11:  0.03324625781569324\n",
            " Loss S22:  0.04452806582949201\n",
            " Loss S33:  0.045555936203139726\n",
            " Loss S44:  0.0475945629057337\n",
            " Loss S11:  0.03315290746589502\n",
            " Loss S22:  0.04448166977108261\n",
            " Loss S33:  0.04550757571870898\n",
            " Loss S44:  0.04755850830747758\n",
            "\n",
            "Epoch: 43\n",
            "Loss S11:  0.0368049293756485\n",
            "Loss S22:  0.03510262817144394\n",
            "Loss S33:  0.03476202115416527\n",
            "Loss S44:  0.04136337339878082\n",
            "Loss S11:  0.03419522805647417\n",
            "Loss S22:  0.032933872362429444\n",
            "Loss S33:  0.03231441839174791\n",
            "Loss S44:  0.036138285967436706\n",
            "Loss S11:  0.03452367033986818\n",
            "Loss S22:  0.033434672902027764\n",
            "Loss S33:  0.03219105729034969\n",
            "Loss S44:  0.03621489998130571\n",
            "Loss S11:  0.03441651471920552\n",
            "Loss S22:  0.033565630355188926\n",
            "Loss S33:  0.032129259659878666\n",
            "Loss S44:  0.03645852676803066\n",
            "Loss S11:  0.03426800341140933\n",
            "Loss S22:  0.03352834497828309\n",
            "Loss S33:  0.032157361916289096\n",
            "Loss S44:  0.03659181233222892\n",
            "Loss S11:  0.034196801994945486\n",
            "Loss S22:  0.03351666661454182\n",
            "Loss S33:  0.03232185593714901\n",
            "Loss S44:  0.036625651971382255\n",
            "Loss S11:  0.03422828690438974\n",
            "Loss S22:  0.033562403569211724\n",
            "Loss S33:  0.03223379744125194\n",
            "Loss S44:  0.03646378872580216\n",
            "Loss S11:  0.034285940702112626\n",
            "Loss S22:  0.0336476225196056\n",
            "Loss S33:  0.03240994531923617\n",
            "Loss S44:  0.03657280438592736\n",
            "Loss S11:  0.03415587411066632\n",
            "Loss S22:  0.033574418636199865\n",
            "Loss S33:  0.032242405170827736\n",
            "Loss S44:  0.036432666451106835\n",
            "Loss S11:  0.034126074554828495\n",
            "Loss S22:  0.03359672199975658\n",
            "Loss S33:  0.03218979144898745\n",
            "Loss S44:  0.036478728219703\n",
            "Loss S11:  0.03404183837004227\n",
            "Loss S22:  0.03356396425330993\n",
            "Loss S33:  0.03211868447389933\n",
            "Loss S44:  0.03641860585401554\n",
            "Loss S11:  0.034123777067876074\n",
            "Loss S22:  0.03349446117676593\n",
            "Loss S33:  0.03211848077003483\n",
            "Loss S44:  0.03631047354088173\n",
            "Loss S11:  0.03418338057115551\n",
            "Loss S22:  0.033481983668055416\n",
            "Loss S33:  0.03221214518012587\n",
            "Loss S44:  0.03631466171465629\n",
            "Loss S11:  0.034179844914843105\n",
            "Loss S22:  0.03352520708705633\n",
            "Loss S33:  0.03229955343515364\n",
            "Loss S44:  0.0364487691291871\n",
            "Loss S11:  0.03420008419094779\n",
            "Loss S22:  0.03347108838088969\n",
            "Loss S33:  0.03227940373492579\n",
            "Loss S44:  0.03643347241037281\n",
            "Loss S11:  0.03420617003363884\n",
            "Loss S22:  0.03345352241041644\n",
            "Loss S33:  0.03230033444845124\n",
            "Loss S44:  0.0364138711959321\n",
            "Loss S11:  0.03415132182077592\n",
            "Loss S22:  0.03341594022577223\n",
            "Loss S33:  0.03226505641056144\n",
            "Loss S44:  0.03640322183600123\n",
            "Loss S11:  0.03420070555518594\n",
            "Loss S22:  0.03342602836589018\n",
            "Loss S33:  0.032289204214922866\n",
            "Loss S44:  0.036471383853091136\n",
            "Loss S11:  0.03422662843204006\n",
            "Loss S22:  0.03342431892759234\n",
            "Loss S33:  0.03231824550633602\n",
            "Loss S44:  0.03647003086150022\n",
            "Loss S11:  0.0342306660790562\n",
            "Loss S22:  0.03343438897379406\n",
            "Loss S33:  0.03229897479034219\n",
            "Loss S44:  0.036474744413847696\n",
            "Loss S11:  0.03421189099439045\n",
            "Loss S22:  0.03338894225768189\n",
            "Loss S33:  0.032314176221762726\n",
            "Loss S44:  0.03644740383200978\n",
            "Loss S11:  0.03424094995192442\n",
            "Loss S22:  0.0334208358732445\n",
            "Loss S33:  0.032322480515422414\n",
            "Loss S44:  0.036416649182825854\n",
            "Loss S11:  0.03426960173033481\n",
            "Loss S22:  0.03342905832766408\n",
            "Loss S33:  0.03236714238090213\n",
            "Loss S44:  0.03640382391362708\n",
            "Loss S11:  0.03425486870432571\n",
            "Loss S22:  0.033411256122318184\n",
            "Loss S33:  0.03235900126300849\n",
            "Loss S44:  0.03639552743855493\n",
            "Loss S11:  0.03431732873760813\n",
            "Loss S22:  0.03345824969754674\n",
            "Loss S33:  0.03238862555056687\n",
            "Loss S44:  0.03642957322144904\n",
            "Loss S11:  0.03428574363548917\n",
            "Loss S22:  0.03344921625230417\n",
            "Loss S33:  0.03240007828490667\n",
            "Loss S44:  0.03642952078545236\n",
            "Loss S11:  0.03427872895280977\n",
            "Loss S22:  0.03345583566960476\n",
            "Loss S33:  0.03241212165047382\n",
            "Loss S44:  0.036467014073297895\n",
            "Loss S11:  0.03426580793633232\n",
            "Loss S22:  0.03345380269843274\n",
            "Loss S33:  0.032407595160867456\n",
            "Loss S44:  0.036501951185759586\n",
            "Loss S11:  0.034259818951695414\n",
            "Loss S22:  0.03343775140592212\n",
            "Loss S33:  0.03239881511101519\n",
            "Loss S44:  0.0365134314551048\n",
            "Loss S11:  0.0342674778639963\n",
            "Loss S22:  0.03345435950585043\n",
            "Loss S33:  0.032415340824174306\n",
            "Loss S44:  0.03655704845677536\n",
            "Loss S11:  0.03424960174953621\n",
            "Loss S22:  0.03345708616473944\n",
            "Loss S33:  0.0324174782378828\n",
            "Loss S44:  0.03659436214356327\n",
            "Loss S11:  0.03424671742094866\n",
            "Loss S22:  0.03343898449632132\n",
            "Loss S33:  0.032404206034359055\n",
            "Loss S44:  0.03663047106032203\n",
            "Loss S11:  0.034241213338490215\n",
            "Loss S22:  0.03344018485199811\n",
            "Loss S33:  0.03240007679688968\n",
            "Loss S44:  0.03663876208235913\n",
            "Loss S11:  0.034241843417151814\n",
            "Loss S22:  0.033427509864696565\n",
            "Loss S33:  0.03239486326556371\n",
            "Loss S44:  0.036629713373962126\n",
            "Loss S11:  0.03425799305837525\n",
            "Loss S22:  0.033426359858462186\n",
            "Loss S33:  0.03239910156517959\n",
            "Loss S44:  0.03663265547794331\n",
            "Loss S11:  0.0342701967922371\n",
            "Loss S22:  0.03342330911093288\n",
            "Loss S33:  0.032406773312054465\n",
            "Loss S44:  0.036659825020111524\n",
            "Loss S11:  0.03428145546538348\n",
            "Loss S22:  0.03342625411573044\n",
            "Loss S33:  0.032406412694510334\n",
            "Loss S44:  0.03666060607766841\n",
            "Loss S11:  0.03423772956623703\n",
            "Loss S22:  0.03340073395771479\n",
            "Loss S33:  0.032406830763559775\n",
            "Loss S44:  0.03660178848233506\n",
            "Loss S11:  0.03420664660945496\n",
            "Loss S22:  0.03340178392240851\n",
            "Loss S33:  0.032400978460243056\n",
            "Loss S44:  0.03657856871058622\n",
            "Loss S11:  0.03416583322636459\n",
            "Loss S22:  0.033360105074580065\n",
            "Loss S33:  0.03237247661880367\n",
            "Loss S44:  0.036562454851005996\n",
            "Loss S11:  0.03417936241964895\n",
            "Loss S22:  0.03334688065466738\n",
            "Loss S33:  0.03235072451655258\n",
            "Loss S44:  0.036557912482808055\n",
            "Loss S11:  0.0341734260710414\n",
            "Loss S22:  0.03335816429479279\n",
            "Loss S33:  0.032355594328647694\n",
            "Loss S44:  0.036553965269649116\n",
            "Loss S11:  0.034198167873214655\n",
            "Loss S22:  0.033391882265053\n",
            "Loss S33:  0.03235488988441413\n",
            "Loss S44:  0.03656328914602975\n",
            "Loss S11:  0.034206089907241534\n",
            "Loss S22:  0.03338842993718842\n",
            "Loss S33:  0.032331313583524486\n",
            "Loss S44:  0.03655368084489884\n",
            "Loss S11:  0.034212868314346216\n",
            "Loss S22:  0.03338138722812508\n",
            "Loss S33:  0.032342240286150487\n",
            "Loss S44:  0.03656676515531378\n",
            "Loss S11:  0.03421820679931445\n",
            "Loss S22:  0.033367119642788184\n",
            "Loss S33:  0.03233925895373071\n",
            "Loss S44:  0.03657469469764809\n",
            "Loss S11:  0.03421497072240665\n",
            "Loss S22:  0.033374577910192875\n",
            "Loss S33:  0.03233293815639169\n",
            "Loss S44:  0.0365962357444127\n",
            "Loss S11:  0.034226234828274214\n",
            "Loss S22:  0.03336397646639631\n",
            "Loss S33:  0.0323452550182297\n",
            "Loss S44:  0.03662060099137816\n",
            "Loss S11:  0.034231572448742614\n",
            "Loss S22:  0.033348602988213114\n",
            "Loss S33:  0.032360694938476764\n",
            "Loss S44:  0.036620276377937154\n",
            "Loss S11:  0.034234014178518124\n",
            "Loss S22:  0.03334057187308726\n",
            "Loss S33:  0.0323414911692412\n",
            "Loss S44:  0.03659716389633245\n",
            "Validation: \n",
            " Loss S11:  0.03212570771574974\n",
            " Loss S22:  0.045264728367328644\n",
            " Loss S33:  0.04257776215672493\n",
            " Loss S44:  0.04607764258980751\n",
            " Loss S11:  0.03326453321746418\n",
            " Loss S22:  0.04447239477719579\n",
            " Loss S33:  0.045185324868985584\n",
            " Loss S44:  0.04787740625795864\n",
            " Loss S11:  0.033186761026338836\n",
            " Loss S22:  0.044160848074569936\n",
            " Loss S33:  0.046129964473770886\n",
            " Loss S44:  0.04778749759240848\n",
            " Loss S11:  0.03306248722994914\n",
            " Loss S22:  0.04405000274542902\n",
            " Loss S33:  0.04615984686085435\n",
            " Loss S44:  0.04751516903033022\n",
            " Loss S11:  0.033000351240237556\n",
            " Loss S22:  0.044004041195651634\n",
            " Loss S33:  0.046140656887013236\n",
            " Loss S44:  0.04745038741348702\n",
            "\n",
            "Epoch: 44\n",
            "Loss S11:  0.036652177572250366\n",
            "Loss S22:  0.03466558828949928\n",
            "Loss S33:  0.03555336594581604\n",
            "Loss S44:  0.04014410078525543\n",
            "Loss S11:  0.03378104384649883\n",
            "Loss S22:  0.03279236619445411\n",
            "Loss S33:  0.032201086594299835\n",
            "Loss S44:  0.03605449165810238\n",
            "Loss S11:  0.03402168463383402\n",
            "Loss S22:  0.03319753112182731\n",
            "Loss S33:  0.032050098602970443\n",
            "Loss S44:  0.03636476876480239\n",
            "Loss S11:  0.03395356958912265\n",
            "Loss S22:  0.0333246803211589\n",
            "Loss S33:  0.03207249326571342\n",
            "Loss S44:  0.036713371233594035\n",
            "Loss S11:  0.033947609455847155\n",
            "Loss S22:  0.03325255964769096\n",
            "Loss S33:  0.03223798292257437\n",
            "Loss S44:  0.036741342668126266\n",
            "Loss S11:  0.03397074714303017\n",
            "Loss S22:  0.033213844632401186\n",
            "Loss S33:  0.03237629597823994\n",
            "Loss S44:  0.03679189597274743\n",
            "Loss S11:  0.033937863394862315\n",
            "Loss S22:  0.0331788495732624\n",
            "Loss S33:  0.032174122077031214\n",
            "Loss S44:  0.03667431129295318\n",
            "Loss S11:  0.033965671933452846\n",
            "Loss S22:  0.03333385351678016\n",
            "Loss S33:  0.03224210737561676\n",
            "Loss S44:  0.03659483764163205\n",
            "Loss S11:  0.03395855468180445\n",
            "Loss S22:  0.03319829656386081\n",
            "Loss S33:  0.03208196275856024\n",
            "Loss S44:  0.03642990874747435\n",
            "Loss S11:  0.033932849928572935\n",
            "Loss S22:  0.0331984756739585\n",
            "Loss S33:  0.03202277898870327\n",
            "Loss S44:  0.03632494289387059\n",
            "Loss S11:  0.0338670003532183\n",
            "Loss S22:  0.0331569451396123\n",
            "Loss S33:  0.031897961779838745\n",
            "Loss S44:  0.036195167939704245\n",
            "Loss S11:  0.03382908851579503\n",
            "Loss S22:  0.033144583287942515\n",
            "Loss S33:  0.03189422158306247\n",
            "Loss S44:  0.03606577336721055\n",
            "Loss S11:  0.03380641228834952\n",
            "Loss S22:  0.03318839127675068\n",
            "Loss S33:  0.03200599446523288\n",
            "Loss S44:  0.036148513413288376\n",
            "Loss S11:  0.03380567178073275\n",
            "Loss S22:  0.0332166898057206\n",
            "Loss S33:  0.032061852088183844\n",
            "Loss S44:  0.036216363780029855\n",
            "Loss S11:  0.03381381132342714\n",
            "Loss S22:  0.033165951926551814\n",
            "Loss S33:  0.03206417370085598\n",
            "Loss S44:  0.03618391628330904\n",
            "Loss S11:  0.03379157730779111\n",
            "Loss S22:  0.03317537742191987\n",
            "Loss S33:  0.03203955186617295\n",
            "Loss S44:  0.03621071992775068\n",
            "Loss S11:  0.03374287788033115\n",
            "Loss S22:  0.03311246043499212\n",
            "Loss S33:  0.032009316295940685\n",
            "Loss S44:  0.036146725429603774\n",
            "Loss S11:  0.033773758541248\n",
            "Loss S22:  0.03312908329766745\n",
            "Loss S33:  0.03203001252391882\n",
            "Loss S44:  0.03620399317938333\n",
            "Loss S11:  0.03378458948836801\n",
            "Loss S22:  0.03314062420957984\n",
            "Loss S33:  0.03207396059322752\n",
            "Loss S44:  0.03628032369467106\n",
            "Loss S11:  0.033803907442467376\n",
            "Loss S22:  0.03316038709465434\n",
            "Loss S33:  0.03210354611004523\n",
            "Loss S44:  0.0363406093196251\n",
            "Loss S11:  0.03379076528386097\n",
            "Loss S22:  0.03310150779153577\n",
            "Loss S33:  0.032112763323519956\n",
            "Loss S44:  0.03633817934908381\n",
            "Loss S11:  0.03383507528364376\n",
            "Loss S22:  0.03313613515258965\n",
            "Loss S33:  0.03212403157346339\n",
            "Loss S44:  0.036354245070668194\n",
            "Loss S11:  0.033853712723468224\n",
            "Loss S22:  0.033124957183097825\n",
            "Loss S33:  0.032154010440227136\n",
            "Loss S44:  0.03633149161226879\n",
            "Loss S11:  0.03384685811477822\n",
            "Loss S22:  0.033125766169973266\n",
            "Loss S33:  0.032142535206817445\n",
            "Loss S44:  0.03631841011538908\n",
            "Loss S11:  0.0339257127655616\n",
            "Loss S22:  0.03316754337662978\n",
            "Loss S33:  0.03217355562258063\n",
            "Loss S44:  0.03637543913456167\n",
            "Loss S11:  0.03390856586634163\n",
            "Loss S22:  0.033144340579550104\n",
            "Loss S33:  0.03220121152847412\n",
            "Loss S44:  0.036391743187944726\n",
            "Loss S11:  0.0339244459309683\n",
            "Loss S22:  0.03313616397023429\n",
            "Loss S33:  0.03218634868347554\n",
            "Loss S44:  0.03641220959353036\n",
            "Loss S11:  0.033939627394410074\n",
            "Loss S22:  0.03315079600526178\n",
            "Loss S33:  0.032200829534897946\n",
            "Loss S44:  0.03643858066426652\n",
            "Loss S11:  0.0339521482401259\n",
            "Loss S22:  0.03317067408901092\n",
            "Loss S33:  0.03221930263835764\n",
            "Loss S44:  0.036484488965512596\n",
            "Loss S11:  0.03395507430267293\n",
            "Loss S22:  0.033180100376048856\n",
            "Loss S33:  0.032226728896299996\n",
            "Loss S44:  0.03651499284168904\n",
            "Loss S11:  0.03396838927561065\n",
            "Loss S22:  0.033194565248251755\n",
            "Loss S33:  0.03222565744010317\n",
            "Loss S44:  0.03652176048693269\n",
            "Loss S11:  0.03395382898868664\n",
            "Loss S22:  0.03315229580357335\n",
            "Loss S33:  0.03220098553722503\n",
            "Loss S44:  0.036525865118363664\n",
            "Loss S11:  0.033946701264548525\n",
            "Loss S22:  0.03311769193694042\n",
            "Loss S33:  0.03218116761530485\n",
            "Loss S44:  0.03649908434773717\n",
            "Loss S11:  0.0339239417347393\n",
            "Loss S22:  0.033103104962142936\n",
            "Loss S33:  0.03217278344393497\n",
            "Loss S44:  0.03649542438502218\n",
            "Loss S11:  0.033938619015940474\n",
            "Loss S22:  0.033099023729038377\n",
            "Loss S33:  0.032188133191852625\n",
            "Loss S44:  0.036499413757249057\n",
            "Loss S11:  0.03395600112797188\n",
            "Loss S22:  0.033084726953438545\n",
            "Loss S33:  0.032194934360831554\n",
            "Loss S44:  0.036512287025820156\n",
            "Loss S11:  0.03394547300903421\n",
            "Loss S22:  0.033072373558898714\n",
            "Loss S33:  0.03219671103680233\n",
            "Loss S44:  0.03651068238360895\n",
            "Loss S11:  0.033942152035605876\n",
            "Loss S22:  0.033073929331774984\n",
            "Loss S33:  0.03218385911574261\n",
            "Loss S44:  0.03647172170367363\n",
            "Loss S11:  0.03392684004047099\n",
            "Loss S22:  0.03308210288739111\n",
            "Loss S33:  0.03215038723598315\n",
            "Loss S44:  0.036425541709922744\n",
            "Loss S11:  0.03389432108806222\n",
            "Loss S22:  0.03304074869950867\n",
            "Loss S33:  0.03212036884120663\n",
            "Loss S44:  0.036392567212433766\n",
            "Loss S11:  0.03388606055530527\n",
            "Loss S22:  0.033032814129035075\n",
            "Loss S33:  0.03211503631047477\n",
            "Loss S44:  0.036388062697805076\n",
            "Loss S11:  0.0338977607932404\n",
            "Loss S22:  0.033023517823567355\n",
            "Loss S33:  0.032122532100645584\n",
            "Loss S44:  0.03639284617872569\n",
            "Loss S11:  0.03391434256244017\n",
            "Loss S22:  0.03302396487028752\n",
            "Loss S33:  0.03212864616899054\n",
            "Loss S44:  0.036406297860512256\n",
            "Loss S11:  0.03391329782070666\n",
            "Loss S22:  0.03302203614930266\n",
            "Loss S33:  0.032113399045442484\n",
            "Loss S44:  0.036395032040930406\n",
            "Loss S11:  0.03393770417287236\n",
            "Loss S22:  0.03301536540612748\n",
            "Loss S33:  0.032122208004027535\n",
            "Loss S44:  0.036384522462952165\n",
            "Loss S11:  0.03395212704352952\n",
            "Loss S22:  0.033019422262312303\n",
            "Loss S33:  0.03213474989565143\n",
            "Loss S44:  0.03639229717034591\n",
            "Loss S11:  0.033951152406972294\n",
            "Loss S22:  0.03301483534131029\n",
            "Loss S33:  0.03214274569861025\n",
            "Loss S44:  0.0364063294026637\n",
            "Loss S11:  0.0339509688175408\n",
            "Loss S22:  0.032993502570850076\n",
            "Loss S33:  0.032134862063005726\n",
            "Loss S44:  0.03640817806931438\n",
            "Loss S11:  0.03396010250162929\n",
            "Loss S22:  0.032986560600215334\n",
            "Loss S33:  0.03215202296892348\n",
            "Loss S44:  0.036426823458434884\n",
            "Loss S11:  0.03396009299486815\n",
            "Loss S22:  0.03297372301945618\n",
            "Loss S33:  0.03214574303257975\n",
            "Loss S44:  0.0364072532440458\n",
            "Validation: \n",
            " Loss S11:  0.03181701526045799\n",
            " Loss S22:  0.04512709379196167\n",
            " Loss S33:  0.04114626720547676\n",
            " Loss S44:  0.044266317039728165\n",
            " Loss S11:  0.03300434661408266\n",
            " Loss S22:  0.044162626954771224\n",
            " Loss S33:  0.044180265139965785\n",
            " Loss S44:  0.04648274679978689\n",
            " Loss S11:  0.032941426354937436\n",
            " Loss S22:  0.044000887925305016\n",
            " Loss S33:  0.0449423178485254\n",
            " Loss S44:  0.04638570592534251\n",
            " Loss S11:  0.032830727607256076\n",
            " Loss S22:  0.043903422709859787\n",
            " Loss S33:  0.04492833385946321\n",
            " Loss S44:  0.04610523031871827\n",
            " Loss S11:  0.032789475755927\n",
            " Loss S22:  0.043819718430807564\n",
            " Loss S33:  0.0449221641469149\n",
            " Loss S44:  0.046018740599170146\n",
            "\n",
            "Epoch: 45\n",
            "Loss S11:  0.03513041511178017\n",
            "Loss S22:  0.034987922757864\n",
            "Loss S33:  0.03489815071225166\n",
            "Loss S44:  0.038136690855026245\n",
            "Loss S11:  0.032896017994393005\n",
            "Loss S22:  0.03169484301046892\n",
            "Loss S33:  0.0316187148405747\n",
            "Loss S44:  0.03564314239404418\n",
            "Loss S11:  0.03344070760621911\n",
            "Loss S22:  0.03257247025058383\n",
            "Loss S33:  0.03166911254326502\n",
            "Loss S44:  0.03572613745927811\n",
            "Loss S11:  0.03374499671401516\n",
            "Loss S22:  0.032902496957009836\n",
            "Loss S33:  0.03195522941889301\n",
            "Loss S44:  0.036075895592089624\n",
            "Loss S11:  0.033722486710403024\n",
            "Loss S22:  0.03291805261155454\n",
            "Loss S33:  0.031946251032555974\n",
            "Loss S44:  0.03617776912160036\n",
            "Loss S11:  0.0337961553504654\n",
            "Loss S22:  0.03287375097473463\n",
            "Loss S33:  0.03201031951489402\n",
            "Loss S44:  0.0362220962404036\n",
            "Loss S11:  0.03379965622405537\n",
            "Loss S22:  0.03288067776526584\n",
            "Loss S33:  0.03190357909827936\n",
            "Loss S44:  0.03621362204678723\n",
            "Loss S11:  0.03389912054047618\n",
            "Loss S22:  0.033005618634568135\n",
            "Loss S33:  0.031924835130782196\n",
            "Loss S44:  0.036151288680627315\n",
            "Loss S11:  0.033761282095018726\n",
            "Loss S22:  0.032966930534184716\n",
            "Loss S33:  0.031748355354791806\n",
            "Loss S44:  0.0360321530405386\n",
            "Loss S11:  0.03379437337619263\n",
            "Loss S22:  0.03299225680530071\n",
            "Loss S33:  0.031721591703839355\n",
            "Loss S44:  0.03598826161616451\n",
            "Loss S11:  0.033763421699404716\n",
            "Loss S22:  0.03289831944252595\n",
            "Loss S33:  0.03158109161154468\n",
            "Loss S44:  0.0359013070638227\n",
            "Loss S11:  0.03367255990569656\n",
            "Loss S22:  0.03286583223254294\n",
            "Loss S33:  0.03158727697692476\n",
            "Loss S44:  0.035876592755451936\n",
            "Loss S11:  0.03375173441696266\n",
            "Loss S22:  0.03288280362001628\n",
            "Loss S33:  0.03171099209773146\n",
            "Loss S44:  0.035894069497373476\n",
            "Loss S11:  0.033772805990039845\n",
            "Loss S22:  0.032868800059188415\n",
            "Loss S33:  0.03177885347195254\n",
            "Loss S44:  0.035970748508818276\n",
            "Loss S11:  0.03377694040801085\n",
            "Loss S22:  0.032800364208982344\n",
            "Loss S33:  0.031704293537541485\n",
            "Loss S44:  0.0359779408611093\n",
            "Loss S11:  0.03375740640408156\n",
            "Loss S22:  0.032775532787308\n",
            "Loss S33:  0.03169066350389001\n",
            "Loss S44:  0.03597907829314273\n",
            "Loss S11:  0.03369347164580911\n",
            "Loss S22:  0.032749509485149236\n",
            "Loss S33:  0.03164292843028996\n",
            "Loss S44:  0.035968520036487844\n",
            "Loss S11:  0.03373229395794241\n",
            "Loss S22:  0.03274729437254674\n",
            "Loss S33:  0.03165114594743266\n",
            "Loss S44:  0.036014637239930924\n",
            "Loss S11:  0.03372905120377053\n",
            "Loss S22:  0.032775552737531743\n",
            "Loss S33:  0.0316627289976369\n",
            "Loss S44:  0.03610456476595191\n",
            "Loss S11:  0.03370081815693079\n",
            "Loss S22:  0.03277294251931275\n",
            "Loss S33:  0.03169504286848126\n",
            "Loss S44:  0.03612462556994086\n",
            "Loss S11:  0.03371361280740494\n",
            "Loss S22:  0.03273492076651967\n",
            "Loss S33:  0.031726401169501726\n",
            "Loss S44:  0.0361234476113349\n",
            "Loss S11:  0.03371923890900556\n",
            "Loss S22:  0.03276765195573393\n",
            "Loss S33:  0.03174397584174481\n",
            "Loss S44:  0.0361481857225652\n",
            "Loss S11:  0.03370441520814173\n",
            "Loss S22:  0.03276327792639376\n",
            "Loss S33:  0.03176911808213227\n",
            "Loss S44:  0.03614750830667321\n",
            "Loss S11:  0.033719954157481975\n",
            "Loss S22:  0.03275491361752217\n",
            "Loss S33:  0.03178113204672997\n",
            "Loss S44:  0.03610063515990585\n",
            "Loss S11:  0.033771380326127115\n",
            "Loss S22:  0.032780543441161575\n",
            "Loss S33:  0.03181223265239312\n",
            "Loss S44:  0.03612912431575698\n",
            "Loss S11:  0.033773329606153576\n",
            "Loss S22:  0.03278673151545078\n",
            "Loss S33:  0.03182979282065929\n",
            "Loss S44:  0.03610395127350828\n",
            "Loss S11:  0.03376019511985596\n",
            "Loss S22:  0.032785415720779774\n",
            "Loss S33:  0.031832154119437225\n",
            "Loss S44:  0.03610359836669489\n",
            "Loss S11:  0.03377032221545812\n",
            "Loss S22:  0.032810310362339896\n",
            "Loss S33:  0.031825800092952716\n",
            "Loss S44:  0.0361065748749074\n",
            "Loss S11:  0.03376563402469472\n",
            "Loss S22:  0.03283086253389769\n",
            "Loss S33:  0.031821659882138635\n",
            "Loss S44:  0.036131545470437544\n",
            "Loss S11:  0.03377160858170888\n",
            "Loss S22:  0.032833118414141466\n",
            "Loss S33:  0.03182075193785515\n",
            "Loss S44:  0.036137930389583316\n",
            "Loss S11:  0.03374362084705172\n",
            "Loss S22:  0.03284199076064203\n",
            "Loss S33:  0.0318187090422822\n",
            "Loss S44:  0.03614168460063166\n",
            "Loss S11:  0.03373189881396064\n",
            "Loss S22:  0.032803735926364014\n",
            "Loss S33:  0.03179293442697195\n",
            "Loss S44:  0.03615240062140767\n",
            "Loss S11:  0.033720869083549376\n",
            "Loss S22:  0.03276457368306282\n",
            "Loss S33:  0.03175934098335999\n",
            "Loss S44:  0.03612321555451998\n",
            "Loss S11:  0.03370009439229245\n",
            "Loss S22:  0.032746561963302494\n",
            "Loss S33:  0.03173544818361\n",
            "Loss S44:  0.036116664733996565\n",
            "Loss S11:  0.033706431591178664\n",
            "Loss S22:  0.0327588079874117\n",
            "Loss S33:  0.03174026152037106\n",
            "Loss S44:  0.036123494003131945\n",
            "Loss S11:  0.03372298633186226\n",
            "Loss S22:  0.03275835385646915\n",
            "Loss S33:  0.03176298090110817\n",
            "Loss S44:  0.03614164193152872\n",
            "Loss S11:  0.033729861296940375\n",
            "Loss S22:  0.03276697959018216\n",
            "Loss S33:  0.03178455104272286\n",
            "Loss S44:  0.03616205075048839\n",
            "Loss S11:  0.03371560023319368\n",
            "Loss S22:  0.03274866010280793\n",
            "Loss S33:  0.031789001775277916\n",
            "Loss S44:  0.03612472909198938\n",
            "Loss S11:  0.03370158508263548\n",
            "Loss S22:  0.03275213310900792\n",
            "Loss S33:  0.03176283933163628\n",
            "Loss S44:  0.03609367106060969\n",
            "Loss S11:  0.033681923151968995\n",
            "Loss S22:  0.03273432657999151\n",
            "Loss S33:  0.031730617856240026\n",
            "Loss S44:  0.03608004099039166\n",
            "Loss S11:  0.03369117119904617\n",
            "Loss S22:  0.03272353257621613\n",
            "Loss S33:  0.031727941010026565\n",
            "Loss S44:  0.03607440991629091\n",
            "Loss S11:  0.03369595245016317\n",
            "Loss S22:  0.03271806134939774\n",
            "Loss S33:  0.031726680732266456\n",
            "Loss S44:  0.036067324633870974\n",
            "Loss S11:  0.03371860947451087\n",
            "Loss S22:  0.03271993946045142\n",
            "Loss S33:  0.0317256591125721\n",
            "Loss S44:  0.036063147500829854\n",
            "Loss S11:  0.033714365278934656\n",
            "Loss S22:  0.032729220829402765\n",
            "Loss S33:  0.03171123459658601\n",
            "Loss S44:  0.03605154194715681\n",
            "Loss S11:  0.03373164950939668\n",
            "Loss S22:  0.03273467687454894\n",
            "Loss S33:  0.031710250269359745\n",
            "Loss S44:  0.036039832168493144\n",
            "Loss S11:  0.03372268963960348\n",
            "Loss S22:  0.03271451854289503\n",
            "Loss S33:  0.03170577163277338\n",
            "Loss S44:  0.03604200457588268\n",
            "Loss S11:  0.03370292751429655\n",
            "Loss S22:  0.03272156659796543\n",
            "Loss S33:  0.03172881101256079\n",
            "Loss S44:  0.03604581304815741\n",
            "Loss S11:  0.03369658521817495\n",
            "Loss S22:  0.03271302827208791\n",
            "Loss S33:  0.0317253465635698\n",
            "Loss S44:  0.03604980722670849\n",
            "Loss S11:  0.03367812826594419\n",
            "Loss S22:  0.03270454806219515\n",
            "Loss S33:  0.031745715306882055\n",
            "Loss S44:  0.036040433347534984\n",
            "Loss S11:  0.03366826039543584\n",
            "Loss S22:  0.03268511308065014\n",
            "Loss S33:  0.03171961471180445\n",
            "Loss S44:  0.03603291070722271\n",
            "Validation: \n",
            " Loss S11:  0.031545113772153854\n",
            " Loss S22:  0.04433561861515045\n",
            " Loss S33:  0.04146125167608261\n",
            " Loss S44:  0.044616296887397766\n",
            " Loss S11:  0.03293532949118387\n",
            " Loss S22:  0.04432406276464462\n",
            " Loss S33:  0.0436081417969295\n",
            " Loss S44:  0.04701073272597222\n",
            " Loss S11:  0.032816549172488656\n",
            " Loss S22:  0.04410626793779978\n",
            " Loss S33:  0.04434203111180445\n",
            " Loss S44:  0.04689153665449561\n",
            " Loss S11:  0.0326900736841022\n",
            " Loss S22:  0.04390394247946192\n",
            " Loss S33:  0.04432919096262729\n",
            " Loss S44:  0.04652219619907317\n",
            " Loss S11:  0.03266184126245387\n",
            " Loss S22:  0.0437704209284282\n",
            " Loss S33:  0.0443053255662506\n",
            " Loss S44:  0.04643717297801265\n",
            "\n",
            "Epoch: 46\n",
            "Loss S11:  0.035992927849292755\n",
            "Loss S22:  0.0360414944589138\n",
            "Loss S33:  0.034994449466466904\n",
            "Loss S44:  0.037247028201818466\n",
            "Loss S11:  0.033506602895530785\n",
            "Loss S22:  0.032605226744305\n",
            "Loss S33:  0.03213872709734873\n",
            "Loss S44:  0.03611912883140824\n",
            "Loss S11:  0.033731366374662945\n",
            "Loss S22:  0.032997021274197666\n",
            "Loss S33:  0.031771812587976456\n",
            "Loss S44:  0.036319331576426826\n",
            "Loss S11:  0.03379891223965153\n",
            "Loss S22:  0.033139097294019114\n",
            "Loss S33:  0.03164298093367007\n",
            "Loss S44:  0.03634372605912147\n",
            "Loss S11:  0.03363909458786976\n",
            "Loss S22:  0.032979973296566704\n",
            "Loss S33:  0.03162844519971347\n",
            "Loss S44:  0.03612737757403676\n",
            "Loss S11:  0.033663767298647\n",
            "Loss S22:  0.03285146446204653\n",
            "Loss S33:  0.031824499003443064\n",
            "Loss S44:  0.03612909049672239\n",
            "Loss S11:  0.03366191285189058\n",
            "Loss S22:  0.032742126218852446\n",
            "Loss S33:  0.03173532034652155\n",
            "Loss S44:  0.0360617534425415\n",
            "Loss S11:  0.03364622495858602\n",
            "Loss S22:  0.03286961531660087\n",
            "Loss S33:  0.03180747786143296\n",
            "Loss S44:  0.036116776541924814\n",
            "Loss S11:  0.033598429097989456\n",
            "Loss S22:  0.032855124707207264\n",
            "Loss S33:  0.03167414180014604\n",
            "Loss S44:  0.03593280791868398\n",
            "Loss S11:  0.03358648658044391\n",
            "Loss S22:  0.03279743097968154\n",
            "Loss S33:  0.03167779901279853\n",
            "Loss S44:  0.03592769007433902\n",
            "Loss S11:  0.033463510730774096\n",
            "Loss S22:  0.03269932347137739\n",
            "Loss S33:  0.03153178888824907\n",
            "Loss S44:  0.03582631822417278\n",
            "Loss S11:  0.03349861636892095\n",
            "Loss S22:  0.03268325046913044\n",
            "Loss S33:  0.03148932906018721\n",
            "Loss S44:  0.03579205579988591\n",
            "Loss S11:  0.03347487595270981\n",
            "Loss S22:  0.03273713054551073\n",
            "Loss S33:  0.03152706421726991\n",
            "Loss S44:  0.035826374399514235\n",
            "Loss S11:  0.03349503608889707\n",
            "Loss S22:  0.032766986112216956\n",
            "Loss S33:  0.0315440348313965\n",
            "Loss S44:  0.03586286886502768\n",
            "Loss S11:  0.033475389268170015\n",
            "Loss S22:  0.0327188936161234\n",
            "Loss S33:  0.03151732088720545\n",
            "Loss S44:  0.03585706128084913\n",
            "Loss S11:  0.03351441210764923\n",
            "Loss S22:  0.032707276945279924\n",
            "Loss S33:  0.031540258275653356\n",
            "Loss S44:  0.035898426362615546\n",
            "Loss S11:  0.03344041431866447\n",
            "Loss S22:  0.032651631150001324\n",
            "Loss S33:  0.03152263495400085\n",
            "Loss S44:  0.03584923382316317\n",
            "Loss S11:  0.03352385165703575\n",
            "Loss S22:  0.032697627549631555\n",
            "Loss S33:  0.0315507431759646\n",
            "Loss S44:  0.03591715104398672\n",
            "Loss S11:  0.03353335487126316\n",
            "Loss S22:  0.03273473294112234\n",
            "Loss S33:  0.03159020156779671\n",
            "Loss S44:  0.03593165627088995\n",
            "Loss S11:  0.0335067292548599\n",
            "Loss S22:  0.03272568341832199\n",
            "Loss S33:  0.031596132637006454\n",
            "Loss S44:  0.03589843036070544\n",
            "Loss S11:  0.03346291215932785\n",
            "Loss S22:  0.03271644012029491\n",
            "Loss S33:  0.03160335968679456\n",
            "Loss S44:  0.03589559913571201\n",
            "Loss S11:  0.03348532177825675\n",
            "Loss S22:  0.032719422996891616\n",
            "Loss S33:  0.031604788861992236\n",
            "Loss S44:  0.03590896222475581\n",
            "Loss S11:  0.033468504364673905\n",
            "Loss S22:  0.0327072783006667\n",
            "Loss S33:  0.0316169109886588\n",
            "Loss S44:  0.03587102507371708\n",
            "Loss S11:  0.03345618704212951\n",
            "Loss S22:  0.03271592097171457\n",
            "Loss S33:  0.03162555709416732\n",
            "Loss S44:  0.035821269600938406\n",
            "Loss S11:  0.03349623451187156\n",
            "Loss S22:  0.03273402448140734\n",
            "Loss S33:  0.03165698154245917\n",
            "Loss S44:  0.0358392123859453\n",
            "Loss S11:  0.033501802338843804\n",
            "Loss S22:  0.032717178319673614\n",
            "Loss S33:  0.03168511421853803\n",
            "Loss S44:  0.03580180047992691\n",
            "Loss S11:  0.03351511721324418\n",
            "Loss S22:  0.032706533440913274\n",
            "Loss S33:  0.031693712048146916\n",
            "Loss S44:  0.03580282313099766\n",
            "Loss S11:  0.03353252367037469\n",
            "Loss S22:  0.03271919579633487\n",
            "Loss S33:  0.03168614463695081\n",
            "Loss S44:  0.03580517325948965\n",
            "Loss S11:  0.03354935300307766\n",
            "Loss S22:  0.0327163284438997\n",
            "Loss S33:  0.031678086358828475\n",
            "Loss S44:  0.03579902525633255\n",
            "Loss S11:  0.03356027260064259\n",
            "Loss S22:  0.032731222654484804\n",
            "Loss S33:  0.03168507563622342\n",
            "Loss S44:  0.035859083673593514\n",
            "Loss S11:  0.033564722518192175\n",
            "Loss S22:  0.032739311469056676\n",
            "Loss S33:  0.03168709507441006\n",
            "Loss S44:  0.03584920174358691\n",
            "Loss S11:  0.03356046125388605\n",
            "Loss S22:  0.03272417112825576\n",
            "Loss S33:  0.0316701644007416\n",
            "Loss S44:  0.03584738653093289\n",
            "Loss S11:  0.0335534585993795\n",
            "Loss S22:  0.03271352146343093\n",
            "Loss S33:  0.03166435158744779\n",
            "Loss S44:  0.035845910962032754\n",
            "Loss S11:  0.033544716371780435\n",
            "Loss S22:  0.03270015749641415\n",
            "Loss S33:  0.03164445104645818\n",
            "Loss S44:  0.03583694674817457\n",
            "Loss S11:  0.0335510981616037\n",
            "Loss S22:  0.032702785154635254\n",
            "Loss S33:  0.03165052633983823\n",
            "Loss S44:  0.03583394306714584\n",
            "Loss S11:  0.033553166553760186\n",
            "Loss S22:  0.03271253040309815\n",
            "Loss S33:  0.0316880125382602\n",
            "Loss S44:  0.03585864345614727\n",
            "Loss S11:  0.033549413673187554\n",
            "Loss S22:  0.03271569042283412\n",
            "Loss S33:  0.0316844473997972\n",
            "Loss S44:  0.0358566368034837\n",
            "Loss S11:  0.03353652401232334\n",
            "Loss S22:  0.03269779527022672\n",
            "Loss S33:  0.031684246627146986\n",
            "Loss S44:  0.0358375646876839\n",
            "Loss S11:  0.033513960298981876\n",
            "Loss S22:  0.032681818132403645\n",
            "Loss S33:  0.031671187588627255\n",
            "Loss S44:  0.035803934740112835\n",
            "Loss S11:  0.03347582875481804\n",
            "Loss S22:  0.032637377633996635\n",
            "Loss S33:  0.03164383123064285\n",
            "Loss S44:  0.035778666062809314\n",
            "Loss S11:  0.03348632381202425\n",
            "Loss S22:  0.032630063872087625\n",
            "Loss S33:  0.03164050857213667\n",
            "Loss S44:  0.035770893905153894\n",
            "Loss S11:  0.033474095714099504\n",
            "Loss S22:  0.032620860961630686\n",
            "Loss S33:  0.031632531552129126\n",
            "Loss S44:  0.03577385371473874\n",
            "Loss S11:  0.03347273152808254\n",
            "Loss S22:  0.032632861113852676\n",
            "Loss S33:  0.03164768101749256\n",
            "Loss S44:  0.03577636279643573\n",
            "Loss S11:  0.03347895502300799\n",
            "Loss S22:  0.03262531670488918\n",
            "Loss S33:  0.0316299440429963\n",
            "Loss S44:  0.0357567128615147\n",
            "Loss S11:  0.03350585915756469\n",
            "Loss S22:  0.03262766628883053\n",
            "Loss S33:  0.03162760158278504\n",
            "Loss S44:  0.03573702414289894\n",
            "Loss S11:  0.0335128630063354\n",
            "Loss S22:  0.0326034593542505\n",
            "Loss S33:  0.0316397551438893\n",
            "Loss S44:  0.035737673981226736\n",
            "Loss S11:  0.03350548387187939\n",
            "Loss S22:  0.03259597488006258\n",
            "Loss S33:  0.03165852502715691\n",
            "Loss S44:  0.035744654228850196\n",
            "Loss S11:  0.03349683643763612\n",
            "Loss S22:  0.03260120455246822\n",
            "Loss S33:  0.031657885048229986\n",
            "Loss S44:  0.035756439674495384\n",
            "Loss S11:  0.03349100315862039\n",
            "Loss S22:  0.032589104901728165\n",
            "Loss S33:  0.03166829806262639\n",
            "Loss S44:  0.03576123313261913\n",
            "Loss S11:  0.03347781745910402\n",
            "Loss S22:  0.03257043751870906\n",
            "Loss S33:  0.031652713202076625\n",
            "Loss S44:  0.035746328600980114\n",
            "Validation: \n",
            " Loss S11:  0.03181297332048416\n",
            " Loss S22:  0.04628521949052811\n",
            " Loss S33:  0.040864817798137665\n",
            " Loss S44:  0.04448924958705902\n",
            " Loss S11:  0.032522093948154224\n",
            " Loss S22:  0.04423139936157635\n",
            " Loss S33:  0.044086570895853494\n",
            " Loss S44:  0.04657876367370287\n",
            " Loss S11:  0.03239278231815594\n",
            " Loss S22:  0.04404264633975378\n",
            " Loss S33:  0.04502835788014459\n",
            " Loss S44:  0.04653940976756375\n",
            " Loss S11:  0.03225936948275957\n",
            " Loss S22:  0.043835210507033304\n",
            " Loss S33:  0.045065389854497595\n",
            " Loss S44:  0.046250854786790786\n",
            " Loss S11:  0.032256474925412074\n",
            " Loss S22:  0.043811666422788005\n",
            " Loss S33:  0.045050265474451914\n",
            " Loss S44:  0.0462013953997765\n",
            "\n",
            "Epoch: 47\n",
            "Loss S11:  0.0359705314040184\n",
            "Loss S22:  0.03491245210170746\n",
            "Loss S33:  0.03701602295041084\n",
            "Loss S44:  0.03579031303524971\n",
            "Loss S11:  0.03333112326535312\n",
            "Loss S22:  0.032490989193320274\n",
            "Loss S33:  0.03222686865112998\n",
            "Loss S44:  0.03542243011973121\n",
            "Loss S11:  0.0336594232136295\n",
            "Loss S22:  0.03258655300097806\n",
            "Loss S33:  0.03172835734273706\n",
            "Loss S44:  0.035986323796567465\n",
            "Loss S11:  0.033673891737576456\n",
            "Loss S22:  0.03244324881703623\n",
            "Loss S33:  0.031639036151670644\n",
            "Loss S44:  0.03609496847756447\n",
            "Loss S11:  0.03340184820316187\n",
            "Loss S22:  0.03238302764550942\n",
            "Loss S33:  0.03155213031100064\n",
            "Loss S44:  0.03603510976564593\n",
            "Loss S11:  0.03347696455232069\n",
            "Loss S22:  0.032438456099115165\n",
            "Loss S33:  0.031645523347690996\n",
            "Loss S44:  0.036142481746626834\n",
            "Loss S11:  0.03357614274518412\n",
            "Loss S22:  0.032497459350795045\n",
            "Loss S33:  0.031516660401811364\n",
            "Loss S44:  0.03607220166042203\n",
            "Loss S11:  0.03359444693885219\n",
            "Loss S22:  0.03253454940629677\n",
            "Loss S33:  0.03161317117931977\n",
            "Loss S44:  0.036018467442670336\n",
            "Loss S11:  0.033466371903081\n",
            "Loss S22:  0.03248743241491877\n",
            "Loss S33:  0.03143300750741252\n",
            "Loss S44:  0.035901769167847104\n",
            "Loss S11:  0.033402490971998856\n",
            "Loss S22:  0.032464111203348245\n",
            "Loss S33:  0.0314050020220188\n",
            "Loss S44:  0.03578449642428985\n",
            "Loss S11:  0.033263972693003054\n",
            "Loss S22:  0.032440661609467895\n",
            "Loss S33:  0.03134827052086297\n",
            "Loss S44:  0.03565988106892841\n",
            "Loss S11:  0.033266868257710525\n",
            "Loss S22:  0.032408606781213135\n",
            "Loss S33:  0.031406739793784985\n",
            "Loss S44:  0.035635403265152965\n",
            "Loss S11:  0.033262179271618196\n",
            "Loss S22:  0.03238317441226037\n",
            "Loss S33:  0.0314966280259623\n",
            "Loss S44:  0.03562938097156276\n",
            "Loss S11:  0.033223469663439816\n",
            "Loss S22:  0.03238739870943641\n",
            "Loss S33:  0.031568284637031666\n",
            "Loss S44:  0.03569872263563043\n",
            "Loss S11:  0.033234633809180125\n",
            "Loss S22:  0.03236428497636572\n",
            "Loss S33:  0.03154753914750214\n",
            "Loss S44:  0.03569843729706944\n",
            "Loss S11:  0.03319780923278127\n",
            "Loss S22:  0.03239209424482276\n",
            "Loss S33:  0.03154470621059272\n",
            "Loss S44:  0.03571131575038496\n",
            "Loss S11:  0.03313114583168341\n",
            "Loss S22:  0.032374515539463265\n",
            "Loss S33:  0.03151927097058444\n",
            "Loss S44:  0.03566343461088142\n",
            "Loss S11:  0.03323205990705922\n",
            "Loss S22:  0.03238477653752991\n",
            "Loss S33:  0.03152578448255857\n",
            "Loss S44:  0.0356992290665706\n",
            "Loss S11:  0.0332640519890502\n",
            "Loss S22:  0.032434961238865696\n",
            "Loss S33:  0.031519730045403566\n",
            "Loss S44:  0.03575685440217922\n",
            "Loss S11:  0.033272260049094705\n",
            "Loss S22:  0.03241646151346062\n",
            "Loss S33:  0.03152365957177122\n",
            "Loss S44:  0.03576229784774219\n",
            "Loss S11:  0.03325922060665207\n",
            "Loss S22:  0.03241568807493988\n",
            "Loss S33:  0.03151683040445124\n",
            "Loss S44:  0.03574097531833755\n",
            "Loss S11:  0.033280472796392666\n",
            "Loss S22:  0.032462937624957326\n",
            "Loss S33:  0.03151207984870927\n",
            "Loss S44:  0.03576439393957927\n",
            "Loss S11:  0.033280694258833364\n",
            "Loss S22:  0.03246059840991756\n",
            "Loss S33:  0.03155902334872414\n",
            "Loss S44:  0.03577178896181454\n",
            "Loss S11:  0.03330536060906076\n",
            "Loss S22:  0.03248776267262506\n",
            "Loss S33:  0.03155317564708588\n",
            "Loss S44:  0.03578661936153839\n",
            "Loss S11:  0.03336141910790408\n",
            "Loss S22:  0.03250155453396289\n",
            "Loss S33:  0.03157101318054674\n",
            "Loss S44:  0.03583832259031005\n",
            "Loss S11:  0.03333814477778051\n",
            "Loss S22:  0.03249095710536161\n",
            "Loss S33:  0.03156608729011034\n",
            "Loss S44:  0.035819238200190055\n",
            "Loss S11:  0.03333959263888584\n",
            "Loss S22:  0.032483460488675654\n",
            "Loss S33:  0.03156768621955338\n",
            "Loss S44:  0.03581824805230702\n",
            "Loss S11:  0.033357942435629256\n",
            "Loss S22:  0.03248974175030895\n",
            "Loss S33:  0.03156207519071348\n",
            "Loss S44:  0.03581517452985818\n",
            "Loss S11:  0.03333869012357713\n",
            "Loss S22:  0.032480654707741904\n",
            "Loss S33:  0.03155234374530163\n",
            "Loss S44:  0.03580938049615278\n",
            "Loss S11:  0.03333297240647049\n",
            "Loss S22:  0.03249085880815983\n",
            "Loss S33:  0.03156631047552599\n",
            "Loss S44:  0.03583136597116993\n",
            "Loss S11:  0.03333820023286184\n",
            "Loss S22:  0.03248879599363305\n",
            "Loss S33:  0.031577136132408215\n",
            "Loss S44:  0.03581754639868522\n",
            "Loss S11:  0.033318045713798025\n",
            "Loss S22:  0.03246133707439784\n",
            "Loss S33:  0.031562475050377306\n",
            "Loss S44:  0.035834235339829774\n",
            "Loss S11:  0.033311877649196214\n",
            "Loss S22:  0.03243442761749493\n",
            "Loss S33:  0.03154345864076109\n",
            "Loss S44:  0.03583103633627899\n",
            "Loss S11:  0.033309182382872456\n",
            "Loss S22:  0.03242369092370628\n",
            "Loss S33:  0.03152616075000914\n",
            "Loss S44:  0.035843973619204994\n",
            "Loss S11:  0.033301188689697524\n",
            "Loss S22:  0.0323982282030967\n",
            "Loss S33:  0.03151151989627619\n",
            "Loss S44:  0.03584419416476729\n",
            "Loss S11:  0.03331983016745487\n",
            "Loss S22:  0.032388206306006494\n",
            "Loss S33:  0.031515285187000225\n",
            "Loss S44:  0.03587683032869104\n",
            "Loss S11:  0.03332991460002856\n",
            "Loss S22:  0.03240276286971866\n",
            "Loss S33:  0.03150381248475277\n",
            "Loss S44:  0.03586515905516134\n",
            "Loss S11:  0.03332216540637685\n",
            "Loss S22:  0.03239824281970285\n",
            "Loss S33:  0.0314993299092003\n",
            "Loss S44:  0.035827223417371754\n",
            "Loss S11:  0.03330819099515755\n",
            "Loss S22:  0.03239899266051652\n",
            "Loss S33:  0.03149150365724025\n",
            "Loss S44:  0.03579564623808454\n",
            "Loss S11:  0.03326841195106811\n",
            "Loss S22:  0.032362083030288175\n",
            "Loss S33:  0.03147326950507853\n",
            "Loss S44:  0.035764867506559245\n",
            "Loss S11:  0.033270305157302324\n",
            "Loss S22:  0.03234677481235114\n",
            "Loss S33:  0.03146341390573324\n",
            "Loss S44:  0.03574665006426653\n",
            "Loss S11:  0.033269674959082673\n",
            "Loss S22:  0.03235497075493318\n",
            "Loss S33:  0.03145480155038428\n",
            "Loss S44:  0.035745709495496575\n",
            "Loss S11:  0.033280010434694925\n",
            "Loss S22:  0.03236395458639942\n",
            "Loss S33:  0.03145637496367911\n",
            "Loss S44:  0.03575241221048628\n",
            "Loss S11:  0.033278621301020384\n",
            "Loss S22:  0.03237115304372565\n",
            "Loss S33:  0.031424453330772935\n",
            "Loss S44:  0.035747807857574\n",
            "Loss S11:  0.03328029544023024\n",
            "Loss S22:  0.032377650538911354\n",
            "Loss S33:  0.031427940936619735\n",
            "Loss S44:  0.03573762022735986\n",
            "Loss S11:  0.03328409977257252\n",
            "Loss S22:  0.0323813033730056\n",
            "Loss S33:  0.03143270326955884\n",
            "Loss S44:  0.03574451942517197\n",
            "Loss S11:  0.033283924122306636\n",
            "Loss S22:  0.03237948832940512\n",
            "Loss S33:  0.03144229181240153\n",
            "Loss S44:  0.03575097440202557\n",
            "Loss S11:  0.03329237259434034\n",
            "Loss S22:  0.03237845022982726\n",
            "Loss S33:  0.03145190541515811\n",
            "Loss S44:  0.03575633427883536\n",
            "Loss S11:  0.0332849548777213\n",
            "Loss S22:  0.03237655052524099\n",
            "Loss S33:  0.03146244578156426\n",
            "Loss S44:  0.03576495065092421\n",
            "Loss S11:  0.033274504149336435\n",
            "Loss S22:  0.03235732415755509\n",
            "Loss S33:  0.0314342144444251\n",
            "Loss S44:  0.03574932746346395\n",
            "Validation: \n",
            " Loss S11:  0.031148459762334824\n",
            " Loss S22:  0.04547591134905815\n",
            " Loss S33:  0.04088371992111206\n",
            " Loss S44:  0.044963348656892776\n",
            " Loss S11:  0.03258847356552169\n",
            " Loss S22:  0.043828883518775306\n",
            " Loss S33:  0.04524143430448714\n",
            " Loss S44:  0.047715408638829275\n",
            " Loss S11:  0.03250090941423323\n",
            " Loss S22:  0.043493107415554\n",
            " Loss S33:  0.04611797321860383\n",
            " Loss S44:  0.04751131256542555\n",
            " Loss S11:  0.03232198807059741\n",
            " Loss S22:  0.043348237383561056\n",
            " Loss S33:  0.04615946987369021\n",
            " Loss S44:  0.04719481372930964\n",
            " Loss S11:  0.03224118297666679\n",
            " Loss S22:  0.04328290235113214\n",
            " Loss S33:  0.046164396000497133\n",
            " Loss S44:  0.04712114598096153\n",
            "\n",
            "Epoch: 48\n",
            "Loss S11:  0.03488311171531677\n",
            "Loss S22:  0.03404371812939644\n",
            "Loss S33:  0.0337078794836998\n",
            "Loss S44:  0.03574252501130104\n",
            "Loss S11:  0.03276392665099014\n",
            "Loss S22:  0.0320188355716792\n",
            "Loss S33:  0.03127650069919499\n",
            "Loss S44:  0.03500944206660444\n",
            "Loss S11:  0.03270904719829559\n",
            "Loss S22:  0.0321170717832588\n",
            "Loss S33:  0.03107329715220701\n",
            "Loss S44:  0.035458416278873174\n",
            "Loss S11:  0.032939332507310376\n",
            "Loss S22:  0.03218333146745159\n",
            "Loss S33:  0.031246431171894073\n",
            "Loss S44:  0.03558459301148691\n",
            "Loss S11:  0.032930524747182686\n",
            "Loss S22:  0.032273482258726914\n",
            "Loss S33:  0.031294061870473185\n",
            "Loss S44:  0.03577005954050436\n",
            "Loss S11:  0.03308132184925033\n",
            "Loss S22:  0.03236300794079023\n",
            "Loss S33:  0.031475575166005715\n",
            "Loss S44:  0.035753750698823555\n",
            "Loss S11:  0.03313529531120277\n",
            "Loss S22:  0.0324493803877811\n",
            "Loss S33:  0.031347738243028764\n",
            "Loss S44:  0.035597520773528055\n",
            "Loss S11:  0.03326884079986895\n",
            "Loss S22:  0.03255485840351649\n",
            "Loss S33:  0.03142621861377232\n",
            "Loss S44:  0.03563063240177195\n",
            "Loss S11:  0.03319302201271057\n",
            "Loss S22:  0.03247130586317292\n",
            "Loss S33:  0.03129895879990525\n",
            "Loss S44:  0.035442691664268944\n",
            "Loss S11:  0.03318005251687962\n",
            "Loss S22:  0.032510687136551836\n",
            "Loss S33:  0.03125932888424659\n",
            "Loss S44:  0.03539381229451725\n",
            "Loss S11:  0.03307599445233251\n",
            "Loss S22:  0.03242252092107688\n",
            "Loss S33:  0.03116635334595005\n",
            "Loss S44:  0.035306990994970394\n",
            "Loss S11:  0.03306102999360175\n",
            "Loss S22:  0.03239869648540342\n",
            "Loss S33:  0.031172381814669917\n",
            "Loss S44:  0.03526189545723232\n",
            "Loss S11:  0.033088446679440414\n",
            "Loss S22:  0.03239239792316413\n",
            "Loss S33:  0.03126212475775195\n",
            "Loss S44:  0.03526852503111047\n",
            "Loss S11:  0.03310595107271926\n",
            "Loss S22:  0.03242936767353356\n",
            "Loss S33:  0.03132512082011645\n",
            "Loss S44:  0.035389264704275676\n",
            "Loss S11:  0.03314191686874586\n",
            "Loss S22:  0.03239238361253383\n",
            "Loss S33:  0.031351119081707714\n",
            "Loss S44:  0.0353791697465993\n",
            "Loss S11:  0.03314152445076712\n",
            "Loss S22:  0.03240984785586398\n",
            "Loss S33:  0.03133126226136621\n",
            "Loss S44:  0.03537953202150121\n",
            "Loss S11:  0.03307804058057181\n",
            "Loss S22:  0.032349518644883765\n",
            "Loss S33:  0.03130509326037783\n",
            "Loss S44:  0.03530980838659387\n",
            "Loss S11:  0.033148627379658624\n",
            "Loss S22:  0.032353827599109264\n",
            "Loss S33:  0.0313252199070844\n",
            "Loss S44:  0.03536931532081108\n",
            "Loss S11:  0.033155272822847685\n",
            "Loss S22:  0.03238457896185843\n",
            "Loss S33:  0.03137508662448404\n",
            "Loss S44:  0.03541449189844711\n",
            "Loss S11:  0.033155025894807275\n",
            "Loss S22:  0.032376513401953334\n",
            "Loss S33:  0.03140005803818166\n",
            "Loss S44:  0.03545148963235436\n",
            "Loss S11:  0.033142423062627\n",
            "Loss S22:  0.032347880924741425\n",
            "Loss S33:  0.031412072906001884\n",
            "Loss S44:  0.03547851391025444\n",
            "Loss S11:  0.03319003583942827\n",
            "Loss S22:  0.032375511329320934\n",
            "Loss S33:  0.03137626405363964\n",
            "Loss S44:  0.0354900920313399\n",
            "Loss S11:  0.03319548072116407\n",
            "Loss S22:  0.03239105061134871\n",
            "Loss S33:  0.03139404223481724\n",
            "Loss S44:  0.035512925037161795\n",
            "Loss S11:  0.03319619735501287\n",
            "Loss S22:  0.03237234901040147\n",
            "Loss S33:  0.031356089228352944\n",
            "Loss S44:  0.03550842526929203\n",
            "Loss S11:  0.033268331901722924\n",
            "Loss S22:  0.03240612197157258\n",
            "Loss S33:  0.0314026705039611\n",
            "Loss S44:  0.035581375986214\n",
            "Loss S11:  0.03324161640617002\n",
            "Loss S22:  0.03239497699704303\n",
            "Loss S33:  0.031410267836306675\n",
            "Loss S44:  0.035602307859882415\n",
            "Loss S11:  0.03321157579249578\n",
            "Loss S22:  0.03238636014761824\n",
            "Loss S33:  0.031422909327793395\n",
            "Loss S44:  0.03560849256328239\n",
            "Loss S11:  0.03321117135898873\n",
            "Loss S22:  0.03236514547791648\n",
            "Loss S33:  0.03143507482078463\n",
            "Loss S44:  0.03559641248625583\n",
            "Loss S11:  0.033218554608868536\n",
            "Loss S22:  0.03237202312147702\n",
            "Loss S33:  0.031433379231186526\n",
            "Loss S44:  0.03557996186709489\n",
            "Loss S11:  0.033209882356559285\n",
            "Loss S22:  0.03235728284696123\n",
            "Loss S33:  0.031425152774068085\n",
            "Loss S44:  0.03559125556620126\n",
            "Loss S11:  0.03320311647886256\n",
            "Loss S22:  0.032345288727073575\n",
            "Loss S33:  0.03141123910381945\n",
            "Loss S44:  0.03562211762631058\n",
            "Loss S11:  0.03316893787317912\n",
            "Loss S22:  0.032297427487526675\n",
            "Loss S33:  0.03139522227898863\n",
            "Loss S44:  0.035604371147236254\n",
            "Loss S11:  0.03317294432273906\n",
            "Loss S22:  0.032273153881788995\n",
            "Loss S33:  0.031365743214467605\n",
            "Loss S44:  0.03558756953041502\n",
            "Loss S11:  0.03316410024878302\n",
            "Loss S22:  0.03225759093810064\n",
            "Loss S33:  0.031354027206579003\n",
            "Loss S44:  0.03557933236717097\n",
            "Loss S11:  0.03316118481331382\n",
            "Loss S22:  0.03225932152848964\n",
            "Loss S33:  0.03134949570876762\n",
            "Loss S44:  0.035564271094075395\n",
            "Loss S11:  0.03318004103701169\n",
            "Loss S22:  0.03224947734966747\n",
            "Loss S33:  0.03136577958670946\n",
            "Loss S44:  0.035605821164160376\n",
            "Loss S11:  0.03318700601973692\n",
            "Loss S22:  0.032255845825469066\n",
            "Loss S33:  0.03137047819463005\n",
            "Loss S44:  0.03560480925349978\n",
            "Loss S11:  0.03315722282644552\n",
            "Loss S22:  0.032246613157084686\n",
            "Loss S33:  0.03136808070733219\n",
            "Loss S44:  0.035568887633936744\n",
            "Loss S11:  0.03313701010732044\n",
            "Loss S22:  0.03225404854247889\n",
            "Loss S33:  0.03136949992574888\n",
            "Loss S44:  0.03554868548521845\n",
            "Loss S11:  0.03310219568612478\n",
            "Loss S22:  0.03221737341407467\n",
            "Loss S33:  0.03134749865497622\n",
            "Loss S44:  0.035532368904413164\n",
            "Loss S11:  0.0330996510068749\n",
            "Loss S22:  0.03219597579033446\n",
            "Loss S33:  0.03133423001670332\n",
            "Loss S44:  0.03551830647703715\n",
            "Loss S11:  0.03309191734849536\n",
            "Loss S22:  0.03218915393245191\n",
            "Loss S33:  0.03134051380438816\n",
            "Loss S44:  0.03552498659368269\n",
            "Loss S11:  0.03309779895155419\n",
            "Loss S22:  0.03218852202109373\n",
            "Loss S33:  0.03134840567133914\n",
            "Loss S44:  0.035531085811891634\n",
            "Loss S11:  0.0330994342369505\n",
            "Loss S22:  0.03219416956233591\n",
            "Loss S33:  0.03133330579302565\n",
            "Loss S44:  0.03549658476424743\n",
            "Loss S11:  0.03311372247193\n",
            "Loss S22:  0.03219673456940927\n",
            "Loss S33:  0.031352793227653115\n",
            "Loss S44:  0.035498680817195614\n",
            "Loss S11:  0.0331292316656221\n",
            "Loss S22:  0.03220432444151384\n",
            "Loss S33:  0.03136065567197266\n",
            "Loss S44:  0.03549828329546885\n",
            "Loss S11:  0.03312901585663865\n",
            "Loss S22:  0.0322076322878519\n",
            "Loss S33:  0.03137481179888362\n",
            "Loss S44:  0.03549198514969773\n",
            "Loss S11:  0.033134939101566174\n",
            "Loss S22:  0.03220621639597694\n",
            "Loss S33:  0.03137627246763929\n",
            "Loss S44:  0.03549128776760238\n",
            "Loss S11:  0.03313384360000646\n",
            "Loss S22:  0.03220727991335977\n",
            "Loss S33:  0.03138055201240364\n",
            "Loss S44:  0.03547852557237703\n",
            "Loss S11:  0.03312556712666257\n",
            "Loss S22:  0.032190369377614526\n",
            "Loss S33:  0.031363566669724625\n",
            "Loss S44:  0.03545427255241424\n",
            "Validation: \n",
            " Loss S11:  0.03172101825475693\n",
            " Loss S22:  0.04506201669573784\n",
            " Loss S33:  0.04093054682016373\n",
            " Loss S44:  0.044291239231824875\n",
            " Loss S11:  0.03247002361431008\n",
            " Loss S22:  0.044426265571798594\n",
            " Loss S33:  0.04417407299791064\n",
            " Loss S44:  0.0469617424976258\n",
            " Loss S11:  0.032301458734564664\n",
            " Loss S22:  0.044282258283801196\n",
            " Loss S33:  0.0450877668108882\n",
            " Loss S44:  0.04696272595263109\n",
            " Loss S11:  0.032186431840794984\n",
            " Loss S22:  0.044058920052207885\n",
            " Loss S33:  0.04509625309070603\n",
            " Loss S44:  0.04668173914561506\n",
            " Loss S11:  0.03213879799493301\n",
            " Loss S22:  0.04397183031211665\n",
            " Loss S33:  0.04508676913417416\n",
            " Loss S44:  0.04667276139428586\n",
            "\n",
            "Epoch: 49\n",
            "Loss S11:  0.03543354943394661\n",
            "Loss S22:  0.035913992673158646\n",
            "Loss S33:  0.03391626849770546\n",
            "Loss S44:  0.04000809043645859\n",
            "Loss S11:  0.033111689442938026\n",
            "Loss S22:  0.03208314796740359\n",
            "Loss S33:  0.03159520063887943\n",
            "Loss S44:  0.03575052964416417\n",
            "Loss S11:  0.03314934777362006\n",
            "Loss S22:  0.03254465820888678\n",
            "Loss S33:  0.03174496583995365\n",
            "Loss S44:  0.03563640018304189\n",
            "Loss S11:  0.03329525659641912\n",
            "Loss S22:  0.032515515242853475\n",
            "Loss S33:  0.031717371856493336\n",
            "Loss S44:  0.03554577224196926\n",
            "Loss S11:  0.033172583616361384\n",
            "Loss S22:  0.03246167025006399\n",
            "Loss S33:  0.03166436735631489\n",
            "Loss S44:  0.035531200377679456\n",
            "Loss S11:  0.033186385700223496\n",
            "Loss S22:  0.03240569045438486\n",
            "Loss S33:  0.03174564578369552\n",
            "Loss S44:  0.03548193240866942\n",
            "Loss S11:  0.03322069911805332\n",
            "Loss S22:  0.032410601062364264\n",
            "Loss S33:  0.03152372483469424\n",
            "Loss S44:  0.03524048321071218\n",
            "Loss S11:  0.033194786567293424\n",
            "Loss S22:  0.03256490797748868\n",
            "Loss S33:  0.03162604544154355\n",
            "Loss S44:  0.03531600109918017\n",
            "Loss S11:  0.033102945804044055\n",
            "Loss S22:  0.0324445431707082\n",
            "Loss S33:  0.03145860107960524\n",
            "Loss S44:  0.03524521099012575\n",
            "Loss S11:  0.03308834321796894\n",
            "Loss S22:  0.03240069147240329\n",
            "Loss S33:  0.03136002174118063\n",
            "Loss S44:  0.03510566710770785\n",
            "Loss S11:  0.0329927811860153\n",
            "Loss S22:  0.03238202065303184\n",
            "Loss S33:  0.031238460249387392\n",
            "Loss S44:  0.03505065775301197\n",
            "Loss S11:  0.03301023252241246\n",
            "Loss S22:  0.03234485832152066\n",
            "Loss S33:  0.031211574788431864\n",
            "Loss S44:  0.03499541867960681\n",
            "Loss S11:  0.0330307902075535\n",
            "Loss S22:  0.03233444658377446\n",
            "Loss S33:  0.031234608362775203\n",
            "Loss S44:  0.03503090512654013\n",
            "Loss S11:  0.03304871079291551\n",
            "Loss S22:  0.032320517278809587\n",
            "Loss S33:  0.031248059479221133\n",
            "Loss S44:  0.035118407591153644\n",
            "Loss S11:  0.03302262887252983\n",
            "Loss S22:  0.03224017245179795\n",
            "Loss S33:  0.031236346890317634\n",
            "Loss S44:  0.035129046001544234\n",
            "Loss S11:  0.032969739386774846\n",
            "Loss S22:  0.03223161654707217\n",
            "Loss S33:  0.031234420749624042\n",
            "Loss S44:  0.035131921165234205\n",
            "Loss S11:  0.03290758943705825\n",
            "Loss S22:  0.03221897610827633\n",
            "Loss S33:  0.031189400795268718\n",
            "Loss S44:  0.03510651170559551\n",
            "Loss S11:  0.03297484485290901\n",
            "Loss S22:  0.03227286809804844\n",
            "Loss S33:  0.031209386133572513\n",
            "Loss S44:  0.03520160521331586\n",
            "Loss S11:  0.033017398964469605\n",
            "Loss S22:  0.03227547492358566\n",
            "Loss S33:  0.03121891079563133\n",
            "Loss S44:  0.03523353052979016\n",
            "Loss S11:  0.03300794724043439\n",
            "Loss S22:  0.032299596396962386\n",
            "Loss S33:  0.031234991507536454\n",
            "Loss S44:  0.035226757184685216\n",
            "Loss S11:  0.033000805132572926\n",
            "Loss S22:  0.03225714177708721\n",
            "Loss S33:  0.0312411440860721\n",
            "Loss S44:  0.03523699329489499\n",
            "Loss S11:  0.03302192945757183\n",
            "Loss S22:  0.03226542193883968\n",
            "Loss S33:  0.031264313871826606\n",
            "Loss S44:  0.035228061820799704\n",
            "Loss S11:  0.03300266647163559\n",
            "Loss S22:  0.032281632269669444\n",
            "Loss S33:  0.03130825925028432\n",
            "Loss S44:  0.03522306491886329\n",
            "Loss S11:  0.03298211647641091\n",
            "Loss S22:  0.03226623455167333\n",
            "Loss S33:  0.03128864833867395\n",
            "Loss S44:  0.03518025608515585\n",
            "Loss S11:  0.03302949357051325\n",
            "Loss S22:  0.03229017946018965\n",
            "Loss S33:  0.031324542023210604\n",
            "Loss S44:  0.035212552932349976\n",
            "Loss S11:  0.03300079298150017\n",
            "Loss S22:  0.03227182624646392\n",
            "Loss S33:  0.03130241760249394\n",
            "Loss S44:  0.0351692019809646\n",
            "Loss S11:  0.03299521160279882\n",
            "Loss S22:  0.03228524127454137\n",
            "Loss S33:  0.03130274229608048\n",
            "Loss S44:  0.035185295881496534\n",
            "Loss S11:  0.03300849010143773\n",
            "Loss S22:  0.03227281499640308\n",
            "Loss S33:  0.03129829357745902\n",
            "Loss S44:  0.03518662295850661\n",
            "Loss S11:  0.03300843462082839\n",
            "Loss S22:  0.032284150039280014\n",
            "Loss S33:  0.031307924724452436\n",
            "Loss S44:  0.035202926276682533\n",
            "Loss S11:  0.03301070915387873\n",
            "Loss S22:  0.03228097087687643\n",
            "Loss S33:  0.031307269342218065\n",
            "Loss S44:  0.03524492448675878\n",
            "Loss S11:  0.03300227487824882\n",
            "Loss S22:  0.03227031512489153\n",
            "Loss S33:  0.031301328076971724\n",
            "Loss S44:  0.03524966787336475\n",
            "Loss S11:  0.0329857623066837\n",
            "Loss S22:  0.03222886802946064\n",
            "Loss S33:  0.03127448462021696\n",
            "Loss S44:  0.03525098672561899\n",
            "Loss S11:  0.032990476917301385\n",
            "Loss S22:  0.03220773044381743\n",
            "Loss S33:  0.03126231510760814\n",
            "Loss S44:  0.03526772531416743\n",
            "Loss S11:  0.032975565760292314\n",
            "Loss S22:  0.03221313054377939\n",
            "Loss S33:  0.03125243791550668\n",
            "Loss S44:  0.035253293604063846\n",
            "Loss S11:  0.03297534297135743\n",
            "Loss S22:  0.032200624953267164\n",
            "Loss S33:  0.03125176831664753\n",
            "Loss S44:  0.035238552833101615\n",
            "Loss S11:  0.032988202007238\n",
            "Loss S22:  0.03220267893688122\n",
            "Loss S33:  0.0312742273830026\n",
            "Loss S44:  0.035266574371362006\n",
            "Loss S11:  0.032976870541138334\n",
            "Loss S22:  0.032197998418702314\n",
            "Loss S33:  0.031283271661508114\n",
            "Loss S44:  0.03526481829707477\n",
            "Loss S11:  0.0329550413911876\n",
            "Loss S22:  0.0321756813855907\n",
            "Loss S33:  0.03127207802332476\n",
            "Loss S44:  0.03523529860670515\n",
            "Loss S11:  0.03294250233162419\n",
            "Loss S22:  0.03217375918909946\n",
            "Loss S33:  0.03126196502622344\n",
            "Loss S44:  0.035215709265565935\n",
            "Loss S11:  0.03291894751779564\n",
            "Loss S22:  0.03214143299500046\n",
            "Loss S33:  0.03124064691078937\n",
            "Loss S44:  0.03519408200936549\n",
            "Loss S11:  0.0329202476405815\n",
            "Loss S22:  0.03211503321690452\n",
            "Loss S33:  0.031246468016037025\n",
            "Loss S44:  0.03519404725057823\n",
            "Loss S11:  0.03290303912095345\n",
            "Loss S22:  0.03209419277045704\n",
            "Loss S33:  0.03124627672643412\n",
            "Loss S44:  0.035196235000072025\n",
            "Loss S11:  0.03292380123799601\n",
            "Loss S22:  0.0320916895327687\n",
            "Loss S33:  0.03125396980561157\n",
            "Loss S44:  0.03520045840931335\n",
            "Loss S11:  0.0329222527576779\n",
            "Loss S22:  0.03208460389980031\n",
            "Loss S33:  0.03124436952623165\n",
            "Loss S44:  0.035191919213796156\n",
            "Loss S11:  0.032933062740734646\n",
            "Loss S22:  0.032080024611753945\n",
            "Loss S33:  0.031253683450473406\n",
            "Loss S44:  0.035190302202828615\n",
            "Loss S11:  0.03293768641953986\n",
            "Loss S22:  0.03206539980472456\n",
            "Loss S33:  0.031269269204489934\n",
            "Loss S44:  0.035211929469111225\n",
            "Loss S11:  0.03293574340803116\n",
            "Loss S22:  0.03206244387427535\n",
            "Loss S33:  0.03127856426538976\n",
            "Loss S44:  0.03523214603420711\n",
            "Loss S11:  0.03292936566354995\n",
            "Loss S22:  0.03205286914328481\n",
            "Loss S33:  0.031262787545372726\n",
            "Loss S44:  0.03522749306068552\n",
            "Loss S11:  0.03294013052590176\n",
            "Loss S22:  0.032038476624043966\n",
            "Loss S33:  0.031271868158836626\n",
            "Loss S44:  0.035237634612591995\n",
            "Loss S11:  0.032925609317124495\n",
            "Loss S22:  0.032025854788007416\n",
            "Loss S33:  0.03126081774986203\n",
            "Loss S44:  0.03522599783636888\n",
            "Validation: \n",
            " Loss S11:  0.032117076218128204\n",
            " Loss S22:  0.046642668545246124\n",
            " Loss S33:  0.03988000005483627\n",
            " Loss S44:  0.044216006994247437\n",
            " Loss S11:  0.03262403722675074\n",
            " Loss S22:  0.044415840258200966\n",
            " Loss S33:  0.0432539073129495\n",
            " Loss S44:  0.04636018670031002\n",
            " Loss S11:  0.03247463943936476\n",
            " Loss S22:  0.0441918580270395\n",
            " Loss S33:  0.04399975498275059\n",
            " Loss S44:  0.04603903677041938\n",
            " Loss S11:  0.032368983432162005\n",
            " Loss S22:  0.043998725406947686\n",
            " Loss S33:  0.04411408500593217\n",
            " Loss S44:  0.04577238674535126\n",
            " Loss S11:  0.03235520735199069\n",
            " Loss S22:  0.04389792082854259\n",
            " Loss S33:  0.0441127933193872\n",
            " Loss S44:  0.04571984915269746\n",
            "\n",
            "Epoch: 50\n",
            "Loss S11:  0.03294391557574272\n",
            "Loss S22:  0.0327267162501812\n",
            "Loss S33:  0.034927792847156525\n",
            "Loss S44:  0.04021719470620155\n",
            "Loss S11:  0.032418005507100715\n",
            "Loss S22:  0.031581791964444245\n",
            "Loss S33:  0.031186134300448677\n",
            "Loss S44:  0.03525032018395988\n",
            "Loss S11:  0.03282980097546464\n",
            "Loss S22:  0.03158394123117129\n",
            "Loss S33:  0.030995509099392665\n",
            "Loss S44:  0.03518824234959625\n",
            "Loss S11:  0.032736162084244916\n",
            "Loss S22:  0.031471743278445735\n",
            "Loss S33:  0.030982057654088544\n",
            "Loss S44:  0.03533078772166083\n",
            "Loss S11:  0.032425235729755426\n",
            "Loss S22:  0.03140521472001948\n",
            "Loss S33:  0.0309450556410522\n",
            "Loss S44:  0.03539474004107278\n",
            "Loss S11:  0.03242971897855693\n",
            "Loss S22:  0.03138554910672646\n",
            "Loss S33:  0.030997566404003724\n",
            "Loss S44:  0.03522187686872249\n",
            "Loss S11:  0.03255899922281015\n",
            "Loss S22:  0.03154764669474031\n",
            "Loss S33:  0.030889499077542883\n",
            "Loss S44:  0.03516459632970271\n",
            "Loss S11:  0.032609394108745415\n",
            "Loss S22:  0.0316404566221254\n",
            "Loss S33:  0.030978019598504186\n",
            "Loss S44:  0.03510996901338369\n",
            "Loss S11:  0.03261025923729679\n",
            "Loss S22:  0.0316695358466219\n",
            "Loss S33:  0.030956776743685757\n",
            "Loss S44:  0.03511389225353429\n",
            "Loss S11:  0.032594076347547574\n",
            "Loss S22:  0.03174133351134075\n",
            "Loss S33:  0.03093834260253461\n",
            "Loss S44:  0.035045650879760366\n",
            "Loss S11:  0.03252601424361219\n",
            "Loss S22:  0.031704557838268796\n",
            "Loss S33:  0.030853651058260757\n",
            "Loss S44:  0.03496176461772163\n",
            "Loss S11:  0.032581861726604065\n",
            "Loss S22:  0.03177477395883552\n",
            "Loss S33:  0.03089601465979138\n",
            "Loss S44:  0.03489046481748422\n",
            "Loss S11:  0.03263913784637924\n",
            "Loss S22:  0.03177924897552522\n",
            "Loss S33:  0.03097485092060625\n",
            "Loss S44:  0.03496750679698365\n",
            "Loss S11:  0.03270756242839434\n",
            "Loss S22:  0.031864547234682634\n",
            "Loss S33:  0.03100892163471866\n",
            "Loss S44:  0.03509355974845759\n",
            "Loss S11:  0.032742528376955515\n",
            "Loss S22:  0.031821761465241724\n",
            "Loss S33:  0.03104499561038423\n",
            "Loss S44:  0.035099919740084216\n",
            "Loss S11:  0.032727897956671304\n",
            "Loss S22:  0.031833970055773556\n",
            "Loss S33:  0.03108333893712388\n",
            "Loss S44:  0.035131019830012954\n",
            "Loss S11:  0.03265357995116563\n",
            "Loss S22:  0.03180862046917033\n",
            "Loss S33:  0.03104992227061935\n",
            "Loss S44:  0.03509469500378422\n",
            "Loss S11:  0.032742527308084114\n",
            "Loss S22:  0.031864304444071845\n",
            "Loss S33:  0.031056648675809827\n",
            "Loss S44:  0.03513102639706163\n",
            "Loss S11:  0.032783014370509275\n",
            "Loss S22:  0.031884845160976956\n",
            "Loss S33:  0.031083207176123534\n",
            "Loss S44:  0.035155559905408494\n",
            "Loss S11:  0.03275147230603308\n",
            "Loss S22:  0.03188204893071926\n",
            "Loss S33:  0.0310915540806286\n",
            "Loss S44:  0.035127499116418874\n",
            "Loss S11:  0.0327048793583367\n",
            "Loss S22:  0.03182422537449284\n",
            "Loss S33:  0.031083304453548507\n",
            "Loss S44:  0.03509815983400119\n",
            "Loss S11:  0.032736389646174216\n",
            "Loss S22:  0.03184454029169975\n",
            "Loss S33:  0.03107598972158127\n",
            "Loss S44:  0.03510017497950538\n",
            "Loss S11:  0.03272708593045964\n",
            "Loss S22:  0.031855912626261626\n",
            "Loss S33:  0.031094105079837515\n",
            "Loss S44:  0.03507040126662179\n",
            "Loss S11:  0.032713566676943334\n",
            "Loss S22:  0.031834380902769244\n",
            "Loss S33:  0.031058355190214656\n",
            "Loss S44:  0.035055669653531796\n",
            "Loss S11:  0.032782300997571824\n",
            "Loss S22:  0.031894387831888256\n",
            "Loss S33:  0.03109579908959104\n",
            "Loss S44:  0.03510573661704528\n",
            "Loss S11:  0.03276465189053243\n",
            "Loss S22:  0.03189948810435149\n",
            "Loss S33:  0.03106178631226855\n",
            "Loss S44:  0.03506151344107679\n",
            "Loss S11:  0.03277056114712437\n",
            "Loss S22:  0.031912485658551544\n",
            "Loss S33:  0.03104766315541505\n",
            "Loss S44:  0.03503929490359122\n",
            "Loss S11:  0.03281788021408544\n",
            "Loss S22:  0.03192404061693327\n",
            "Loss S33:  0.031053330275020476\n",
            "Loss S44:  0.035039654331508595\n",
            "Loss S11:  0.032815704460245856\n",
            "Loss S22:  0.03195638454840701\n",
            "Loss S33:  0.031048277903387978\n",
            "Loss S44:  0.03502736628400261\n",
            "Loss S11:  0.03284018391365653\n",
            "Loss S22:  0.031961217625034635\n",
            "Loss S33:  0.031074871939584563\n",
            "Loss S44:  0.03503704768096663\n",
            "Loss S11:  0.032824644818902016\n",
            "Loss S22:  0.031968552161292775\n",
            "Loss S33:  0.031066859116239406\n",
            "Loss S44:  0.0350435240831585\n",
            "Loss S11:  0.032811621293856785\n",
            "Loss S22:  0.03193987680185838\n",
            "Loss S33:  0.031035748882356946\n",
            "Loss S44:  0.03504438302404819\n",
            "Loss S11:  0.032788844274304736\n",
            "Loss S22:  0.031896883720456626\n",
            "Loss S33:  0.031028122487376412\n",
            "Loss S44:  0.03502756913665485\n",
            "Loss S11:  0.032791634875256846\n",
            "Loss S22:  0.03190137996841953\n",
            "Loss S33:  0.031018140351574228\n",
            "Loss S44:  0.03501815001657542\n",
            "Loss S11:  0.03279356280467377\n",
            "Loss S22:  0.031902078970654967\n",
            "Loss S33:  0.03102219151196417\n",
            "Loss S44:  0.03502828567193226\n",
            "Loss S11:  0.03280086058093781\n",
            "Loss S22:  0.03192097251635161\n",
            "Loss S33:  0.031050253638260045\n",
            "Loss S44:  0.03506159413595315\n",
            "Loss S11:  0.03282986632406382\n",
            "Loss S22:  0.03191760663778993\n",
            "Loss S33:  0.03105363401473394\n",
            "Loss S44:  0.03508573715314475\n",
            "Loss S11:  0.032810457276727635\n",
            "Loss S22:  0.03190140927215471\n",
            "Loss S33:  0.031057254981119036\n",
            "Loss S44:  0.035063405227508504\n",
            "Loss S11:  0.03279301797936907\n",
            "Loss S22:  0.031883173541525214\n",
            "Loss S33:  0.0310315524919687\n",
            "Loss S44:  0.0350317847804995\n",
            "Loss S11:  0.03276096572599295\n",
            "Loss S22:  0.03184848925684724\n",
            "Loss S33:  0.030997909514990915\n",
            "Loss S44:  0.035002604627129064\n",
            "Loss S11:  0.0327716002384772\n",
            "Loss S22:  0.031839708135721095\n",
            "Loss S33:  0.03100146422455287\n",
            "Loss S44:  0.035007548437191366\n",
            "Loss S11:  0.03276668993842283\n",
            "Loss S22:  0.031833705675863\n",
            "Loss S33:  0.031017053731855395\n",
            "Loss S44:  0.035019577152283814\n",
            "Loss S11:  0.032775458837737664\n",
            "Loss S22:  0.031839783283530106\n",
            "Loss S33:  0.031031027352194037\n",
            "Loss S44:  0.035023799099718304\n",
            "Loss S11:  0.03277358663400198\n",
            "Loss S22:  0.031842382196898805\n",
            "Loss S33:  0.031006271425816134\n",
            "Loss S44:  0.03499813368781675\n",
            "Loss S11:  0.03279752438875283\n",
            "Loss S22:  0.03184757638900053\n",
            "Loss S33:  0.03101218894309873\n",
            "Loss S44:  0.03499701565507453\n",
            "Loss S11:  0.032800290390783034\n",
            "Loss S22:  0.03183939263736726\n",
            "Loss S33:  0.031011321720256245\n",
            "Loss S44:  0.03500677627447307\n",
            "Loss S11:  0.03279807475555098\n",
            "Loss S22:  0.03183961789772206\n",
            "Loss S33:  0.031025629409805036\n",
            "Loss S44:  0.03503052059989301\n",
            "Loss S11:  0.032798936772466716\n",
            "Loss S22:  0.03182612630813492\n",
            "Loss S33:  0.03102263478827198\n",
            "Loss S44:  0.03502947651640751\n",
            "Loss S11:  0.03280615932433992\n",
            "Loss S22:  0.03182064150527832\n",
            "Loss S33:  0.031036359050820375\n",
            "Loss S44:  0.035032398824477395\n",
            "Loss S11:  0.032788476772044925\n",
            "Loss S22:  0.03179013531070863\n",
            "Loss S33:  0.031017322870931167\n",
            "Loss S44:  0.03501420903214976\n",
            "Validation: \n",
            " Loss S11:  0.031460799276828766\n",
            " Loss S22:  0.0456513911485672\n",
            " Loss S33:  0.04018426686525345\n",
            " Loss S44:  0.044829972088336945\n",
            " Loss S11:  0.03204498262632461\n",
            " Loss S22:  0.04453380547818683\n",
            " Loss S33:  0.04392584821298009\n",
            " Loss S44:  0.04694432623329617\n",
            " Loss S11:  0.032017908581509824\n",
            " Loss S22:  0.04406163750625238\n",
            " Loss S33:  0.04485886762054955\n",
            " Loss S44:  0.046772480101847064\n",
            " Loss S11:  0.03190578712669552\n",
            " Loss S22:  0.0438814213896384\n",
            " Loss S33:  0.04496006612650684\n",
            " Loss S44:  0.046436172467274744\n",
            " Loss S11:  0.03188712959304268\n",
            " Loss S22:  0.04383463619483842\n",
            " Loss S33:  0.04490728202609368\n",
            " Loss S44:  0.04634044921876472\n",
            "\n",
            "Epoch: 51\n",
            "Loss S11:  0.03588787466287613\n",
            "Loss S22:  0.033637575805187225\n",
            "Loss S33:  0.036765847355127335\n",
            "Loss S44:  0.03828522190451622\n",
            "Loss S11:  0.03329631669277495\n",
            "Loss S22:  0.0324853773821484\n",
            "Loss S33:  0.03136098977516998\n",
            "Loss S44:  0.0349537484686483\n",
            "Loss S11:  0.03324806415254161\n",
            "Loss S22:  0.03243467921302432\n",
            "Loss S33:  0.03126518392846698\n",
            "Loss S44:  0.03544353418761775\n",
            "Loss S11:  0.03300504025913054\n",
            "Loss S22:  0.032102242953354315\n",
            "Loss S33:  0.03119715952104138\n",
            "Loss S44:  0.03534696665742705\n",
            "Loss S11:  0.032687031777530184\n",
            "Loss S22:  0.03199679254576927\n",
            "Loss S33:  0.031074904632277606\n",
            "Loss S44:  0.03527859902781684\n",
            "Loss S11:  0.032752591751369776\n",
            "Loss S22:  0.03190462159759858\n",
            "Loss S33:  0.03109309913627073\n",
            "Loss S44:  0.03531409650310582\n",
            "Loss S11:  0.03284204726824995\n",
            "Loss S22:  0.031904406021120116\n",
            "Loss S33:  0.031037574420209792\n",
            "Loss S44:  0.0352096935947899\n",
            "Loss S11:  0.032783344833993575\n",
            "Loss S22:  0.031922931769784064\n",
            "Loss S33:  0.031093755369664917\n",
            "Loss S44:  0.03523747678059087\n",
            "Loss S11:  0.032717243365851446\n",
            "Loss S22:  0.03192806374971514\n",
            "Loss S33:  0.03100828683854621\n",
            "Loss S44:  0.03506577109205134\n",
            "Loss S11:  0.03268267396469038\n",
            "Loss S22:  0.03193362417456868\n",
            "Loss S33:  0.030918428715277505\n",
            "Loss S44:  0.034978858778601164\n",
            "Loss S11:  0.03257570649949041\n",
            "Loss S22:  0.03186571935542149\n",
            "Loss S33:  0.030835001130062756\n",
            "Loss S44:  0.03492736449409829\n",
            "Loss S11:  0.03256637816157964\n",
            "Loss S22:  0.031812217892021745\n",
            "Loss S33:  0.030781379573651263\n",
            "Loss S44:  0.03483010298228479\n",
            "Loss S11:  0.03260792564015743\n",
            "Loss S22:  0.031840736785334\n",
            "Loss S33:  0.03083700959162771\n",
            "Loss S44:  0.034864766507848234\n",
            "Loss S11:  0.03262930298010812\n",
            "Loss S22:  0.03182260972227304\n",
            "Loss S33:  0.030844518559352132\n",
            "Loss S44:  0.03496535554641986\n",
            "Loss S11:  0.03260884904269631\n",
            "Loss S22:  0.03178221252845957\n",
            "Loss S33:  0.03081213910106226\n",
            "Loss S44:  0.03496281673193823\n",
            "Loss S11:  0.032553870218577766\n",
            "Loss S22:  0.03177165210444406\n",
            "Loss S33:  0.03079271711261067\n",
            "Loss S44:  0.03491092011924611\n",
            "Loss S11:  0.03252700856244712\n",
            "Loss S22:  0.031741035604699055\n",
            "Loss S33:  0.03078280999007062\n",
            "Loss S44:  0.034875138830509245\n",
            "Loss S11:  0.03256880802412828\n",
            "Loss S22:  0.031754518156511743\n",
            "Loss S33:  0.030788769956394943\n",
            "Loss S44:  0.03489246297823755\n",
            "Loss S11:  0.0326193702607853\n",
            "Loss S22:  0.031761448799493566\n",
            "Loss S33:  0.030827230717266462\n",
            "Loss S44:  0.03491761644489199\n",
            "Loss S11:  0.03261687884507067\n",
            "Loss S22:  0.031725267777268175\n",
            "Loss S33:  0.030826504815233316\n",
            "Loss S44:  0.03491311381858681\n",
            "Loss S11:  0.03257522795034285\n",
            "Loss S22:  0.031693737323752684\n",
            "Loss S33:  0.030867265704185215\n",
            "Loss S44:  0.03490642213554525\n",
            "Loss S11:  0.03260360536346504\n",
            "Loss S22:  0.031707903582126044\n",
            "Loss S33:  0.030858426968350793\n",
            "Loss S44:  0.03488089050656246\n",
            "Loss S11:  0.0326062743025263\n",
            "Loss S22:  0.031722712317877766\n",
            "Loss S33:  0.03090758235678414\n",
            "Loss S44:  0.034911955441285045\n",
            "Loss S11:  0.03260521701178251\n",
            "Loss S22:  0.03171537288371877\n",
            "Loss S33:  0.030883779958922625\n",
            "Loss S44:  0.034884840637058406\n",
            "Loss S11:  0.03266024744114935\n",
            "Loss S22:  0.03174012748686852\n",
            "Loss S33:  0.030913982540369034\n",
            "Loss S44:  0.0349203439359843\n",
            "Loss S11:  0.032645838401944516\n",
            "Loss S22:  0.031729793866198376\n",
            "Loss S33:  0.030913521962396176\n",
            "Loss S44:  0.03491261966556192\n",
            "Loss S11:  0.03267003710014153\n",
            "Loss S22:  0.0317308116147573\n",
            "Loss S33:  0.03094481030957224\n",
            "Loss S44:  0.03492419510166307\n",
            "Loss S11:  0.03270163752715966\n",
            "Loss S22:  0.03173527198524053\n",
            "Loss S33:  0.030930725087813787\n",
            "Loss S44:  0.03490783341531384\n",
            "Loss S11:  0.032702784899131684\n",
            "Loss S22:  0.03174249520293334\n",
            "Loss S33:  0.030920691682138478\n",
            "Loss S44:  0.03490873764921974\n",
            "Loss S11:  0.03269782595182817\n",
            "Loss S22:  0.031745246761955344\n",
            "Loss S33:  0.030926612785079636\n",
            "Loss S44:  0.03492904933892779\n",
            "Loss S11:  0.03269300408092052\n",
            "Loss S22:  0.03172887749722432\n",
            "Loss S33:  0.030916194224179384\n",
            "Loss S44:  0.034925319644254306\n",
            "Loss S11:  0.03267785725917464\n",
            "Loss S22:  0.03171460214295571\n",
            "Loss S33:  0.03089945070397624\n",
            "Loss S44:  0.03493138806972281\n",
            "Loss S11:  0.03266651553574752\n",
            "Loss S22:  0.03170243686651144\n",
            "Loss S33:  0.030883340060571644\n",
            "Loss S44:  0.03492880657292044\n",
            "Loss S11:  0.03267110002387506\n",
            "Loss S22:  0.03171824316108695\n",
            "Loss S33:  0.030873930745749675\n",
            "Loss S44:  0.034936825623820195\n",
            "Loss S11:  0.03266527846215233\n",
            "Loss S22:  0.03171717661741542\n",
            "Loss S33:  0.03088150124264952\n",
            "Loss S44:  0.034941377642610905\n",
            "Loss S11:  0.03266751714730025\n",
            "Loss S22:  0.03171036795758114\n",
            "Loss S33:  0.030892729424895383\n",
            "Loss S44:  0.034968145748871006\n",
            "Loss S11:  0.032675961677470035\n",
            "Loss S22:  0.03170370753245671\n",
            "Loss S33:  0.0308993070275417\n",
            "Loss S44:  0.034974217048518545\n",
            "Loss S11:  0.03265621261595877\n",
            "Loss S22:  0.031684369837860216\n",
            "Loss S33:  0.030877378164235792\n",
            "Loss S44:  0.03493653841296618\n",
            "Loss S11:  0.03265504804965392\n",
            "Loss S22:  0.03168112709336081\n",
            "Loss S33:  0.030863239368780705\n",
            "Loss S44:  0.03491873604067042\n",
            "Loss S11:  0.03263020161491678\n",
            "Loss S22:  0.03165864231317397\n",
            "Loss S33:  0.03084255029421176\n",
            "Loss S44:  0.03490071049641313\n",
            "Loss S11:  0.03262912275320723\n",
            "Loss S22:  0.03164145439175745\n",
            "Loss S33:  0.030831552758589946\n",
            "Loss S44:  0.03488913809699459\n",
            "Loss S11:  0.03262914352825958\n",
            "Loss S22:  0.03164086141453607\n",
            "Loss S33:  0.030831150574623233\n",
            "Loss S44:  0.034895246277434115\n",
            "Loss S11:  0.032627341729055674\n",
            "Loss S22:  0.031647035493703464\n",
            "Loss S33:  0.03083192828750667\n",
            "Loss S44:  0.03489671631178635\n",
            "Loss S11:  0.03261146339611636\n",
            "Loss S22:  0.03164042389828478\n",
            "Loss S33:  0.030808128822776393\n",
            "Loss S44:  0.034874333587330344\n",
            "Loss S11:  0.03262038606212658\n",
            "Loss S22:  0.031644640617123264\n",
            "Loss S33:  0.030813602967158196\n",
            "Loss S44:  0.03486912603365456\n",
            "Loss S11:  0.032613858002384595\n",
            "Loss S22:  0.031641620795702985\n",
            "Loss S33:  0.03081097626881034\n",
            "Loss S44:  0.03486901137176612\n",
            "Loss S11:  0.03262276934568897\n",
            "Loss S22:  0.03163242107762303\n",
            "Loss S33:  0.03081513157037618\n",
            "Loss S44:  0.03488894288151357\n",
            "Loss S11:  0.032628507244612255\n",
            "Loss S22:  0.03162308605565767\n",
            "Loss S33:  0.03082213296125902\n",
            "Loss S44:  0.03488987281269961\n",
            "Loss S11:  0.03263442849899156\n",
            "Loss S22:  0.0316160405732366\n",
            "Loss S33:  0.030839872370297845\n",
            "Loss S44:  0.03489289115610961\n",
            "Loss S11:  0.03261824415014135\n",
            "Loss S22:  0.03159419476773491\n",
            "Loss S33:  0.03082361317746144\n",
            "Loss S44:  0.034899085703212956\n",
            "Validation: \n",
            " Loss S11:  0.031054480001330376\n",
            " Loss S22:  0.04498277232050896\n",
            " Loss S33:  0.03980766609311104\n",
            " Loss S44:  0.04406745731830597\n",
            " Loss S11:  0.03165766029130845\n",
            " Loss S22:  0.04335781027163778\n",
            " Loss S33:  0.04326532585989861\n",
            " Loss S44:  0.04648194231447719\n",
            " Loss S11:  0.03157152594407884\n",
            " Loss S22:  0.043136793574908884\n",
            " Loss S33:  0.0441629748518874\n",
            " Loss S44:  0.04616930408448708\n",
            " Loss S11:  0.03146299557974104\n",
            " Loss S22:  0.04305363153336478\n",
            " Loss S33:  0.044235907311810825\n",
            " Loss S44:  0.04593822109650393\n",
            " Loss S11:  0.03143556133188583\n",
            " Loss S22:  0.04303054318383888\n",
            " Loss S33:  0.0442222979977543\n",
            " Loss S44:  0.045911707903867886\n",
            "\n",
            "Epoch: 52\n",
            "Loss S11:  0.034570761024951935\n",
            "Loss S22:  0.030763830989599228\n",
            "Loss S33:  0.03429228067398071\n",
            "Loss S44:  0.03666157275438309\n",
            "Loss S11:  0.032498261467977005\n",
            "Loss S22:  0.03099395741115917\n",
            "Loss S33:  0.031125561249527065\n",
            "Loss S44:  0.034462463280016724\n",
            "Loss S11:  0.03249836934819108\n",
            "Loss S22:  0.03137062516595636\n",
            "Loss S33:  0.03084214794493857\n",
            "Loss S44:  0.034711861095967744\n",
            "Loss S11:  0.0325425255202478\n",
            "Loss S22:  0.0314923144516445\n",
            "Loss S33:  0.030855791463005926\n",
            "Loss S44:  0.034799192881872575\n",
            "Loss S11:  0.03244020003916287\n",
            "Loss S22:  0.03152495268278006\n",
            "Loss S33:  0.030746860475074952\n",
            "Loss S44:  0.03485636962804853\n",
            "Loss S11:  0.03257769211104103\n",
            "Loss S22:  0.0315476590219666\n",
            "Loss S33:  0.030795554100882774\n",
            "Loss S44:  0.03491203059606692\n",
            "Loss S11:  0.03271466505820634\n",
            "Loss S22:  0.03158162500648225\n",
            "Loss S33:  0.03074668711204021\n",
            "Loss S44:  0.034846515898577506\n",
            "Loss S11:  0.03272771452304343\n",
            "Loss S22:  0.03169710863329155\n",
            "Loss S33:  0.03087779347010901\n",
            "Loss S44:  0.03489941930477048\n",
            "Loss S11:  0.032630001206272914\n",
            "Loss S22:  0.0317116158519998\n",
            "Loss S33:  0.030765718033100353\n",
            "Loss S44:  0.03472495384882262\n",
            "Loss S11:  0.03263300585632141\n",
            "Loss S22:  0.03162943139917903\n",
            "Loss S33:  0.030745857753432713\n",
            "Loss S44:  0.03473342940784418\n",
            "Loss S11:  0.03253326231226473\n",
            "Loss S22:  0.03155667532124732\n",
            "Loss S33:  0.030628795154614023\n",
            "Loss S44:  0.0345752078412783\n",
            "Loss S11:  0.03255492369938004\n",
            "Loss S22:  0.03157709272125283\n",
            "Loss S33:  0.030605250180841568\n",
            "Loss S44:  0.03451197305778125\n",
            "Loss S11:  0.032548050834866595\n",
            "Loss S22:  0.03157693998934316\n",
            "Loss S33:  0.03063639641300706\n",
            "Loss S44:  0.034544438831816036\n",
            "Loss S11:  0.03251535348764813\n",
            "Loss S22:  0.03156549878816568\n",
            "Loss S33:  0.03067388038598854\n",
            "Loss S44:  0.0346237899351666\n",
            "Loss S11:  0.03251511390908813\n",
            "Loss S22:  0.03152326958135088\n",
            "Loss S33:  0.030672573900603232\n",
            "Loss S44:  0.03461427040768008\n",
            "Loss S11:  0.0324826306040516\n",
            "Loss S22:  0.03152054411340628\n",
            "Loss S33:  0.030673732993421177\n",
            "Loss S44:  0.03458615530602979\n",
            "Loss S11:  0.03244847173785201\n",
            "Loss S22:  0.031534729340339296\n",
            "Loss S33:  0.030629197956982608\n",
            "Loss S44:  0.03457215641226087\n",
            "Loss S11:  0.03254854749909967\n",
            "Loss S22:  0.03158320939680289\n",
            "Loss S33:  0.03063654821170004\n",
            "Loss S44:  0.03463002039413703\n",
            "Loss S11:  0.03261033927208811\n",
            "Loss S22:  0.03158366755268521\n",
            "Loss S33:  0.03066675823943391\n",
            "Loss S44:  0.034705106909927085\n",
            "Loss S11:  0.03258539274022841\n",
            "Loss S22:  0.03156671122067574\n",
            "Loss S33:  0.03065128863364926\n",
            "Loss S44:  0.03469776945354427\n",
            "Loss S11:  0.03256070285813132\n",
            "Loss S22:  0.031526069096590746\n",
            "Loss S33:  0.030659373710626988\n",
            "Loss S44:  0.0346846824224612\n",
            "Loss S11:  0.032621048320243706\n",
            "Loss S22:  0.03155931068575495\n",
            "Loss S33:  0.030676822058891797\n",
            "Loss S44:  0.03466809661043764\n",
            "Loss S11:  0.03262142757821946\n",
            "Loss S22:  0.03156526151589409\n",
            "Loss S33:  0.030726728733308714\n",
            "Loss S44:  0.03465248585354149\n",
            "Loss S11:  0.03260717868353381\n",
            "Loss S22:  0.03158636075883975\n",
            "Loss S33:  0.03072870599339793\n",
            "Loss S44:  0.03462015519733037\n",
            "Loss S11:  0.03264883759574524\n",
            "Loss S22:  0.03163275107184881\n",
            "Loss S33:  0.030737780892440888\n",
            "Loss S44:  0.03463520306738333\n",
            "Loss S11:  0.03263086251231779\n",
            "Loss S22:  0.03164278546652471\n",
            "Loss S33:  0.030746357738081202\n",
            "Loss S44:  0.03461970190009273\n",
            "Loss S11:  0.032632510573870836\n",
            "Loss S22:  0.03164018840454091\n",
            "Loss S33:  0.030770492971914025\n",
            "Loss S44:  0.03463555728281595\n",
            "Loss S11:  0.03264257434851789\n",
            "Loss S22:  0.03162704962151077\n",
            "Loss S33:  0.030783447131129647\n",
            "Loss S44:  0.03463292055904205\n",
            "Loss S11:  0.03265723800187221\n",
            "Loss S22:  0.03163464079044256\n",
            "Loss S33:  0.030790450011655104\n",
            "Loss S44:  0.034650165097145\n",
            "Loss S11:  0.032648836617770884\n",
            "Loss S22:  0.03161113994048838\n",
            "Loss S33:  0.030795291565435447\n",
            "Loss S44:  0.03467881159786506\n",
            "Loss S11:  0.03261778158974212\n",
            "Loss S22:  0.03159609465454504\n",
            "Loss S33:  0.030779437604437635\n",
            "Loss S44:  0.03470674816803679\n",
            "Loss S11:  0.03259657306590648\n",
            "Loss S22:  0.03157176513979481\n",
            "Loss S33:  0.030760106418006288\n",
            "Loss S44:  0.034717562677296795\n",
            "Loss S11:  0.03256763425014472\n",
            "Loss S22:  0.03155866232221929\n",
            "Loss S33:  0.03075490871624226\n",
            "Loss S44:  0.03471518635192764\n",
            "Loss S11:  0.03255579129334123\n",
            "Loss S22:  0.031538713171943074\n",
            "Loss S33:  0.0307432888947945\n",
            "Loss S44:  0.03471363163088924\n",
            "Loss S11:  0.03256230388393849\n",
            "Loss S22:  0.031543215206903445\n",
            "Loss S33:  0.030739521056064062\n",
            "Loss S44:  0.0347105821108451\n",
            "Loss S11:  0.03257321968059085\n",
            "Loss S22:  0.03153026520547873\n",
            "Loss S33:  0.030745872760727534\n",
            "Loss S44:  0.034740542430441265\n",
            "Loss S11:  0.03257655569060688\n",
            "Loss S22:  0.031543423702883586\n",
            "Loss S33:  0.030751227780433574\n",
            "Loss S44:  0.034756317409658365\n",
            "Loss S11:  0.032560636402381396\n",
            "Loss S22:  0.03151127287861954\n",
            "Loss S33:  0.030752400578956077\n",
            "Loss S44:  0.034715984026698087\n",
            "Loss S11:  0.032523383638207996\n",
            "Loss S22:  0.03151477801948395\n",
            "Loss S33:  0.030729469170994332\n",
            "Loss S44:  0.034688511061582356\n",
            "Loss S11:  0.032480131763288435\n",
            "Loss S22:  0.03146444575008377\n",
            "Loss S33:  0.030690919631696723\n",
            "Loss S44:  0.03468029607382729\n",
            "Loss S11:  0.03247414786192099\n",
            "Loss S22:  0.03144752834819053\n",
            "Loss S33:  0.03068031196452287\n",
            "Loss S44:  0.03468955897649476\n",
            "Loss S11:  0.032472335305200876\n",
            "Loss S22:  0.0314443601553675\n",
            "Loss S33:  0.030681464132238767\n",
            "Loss S44:  0.034690917286015775\n",
            "Loss S11:  0.03247470731212096\n",
            "Loss S22:  0.031444576124255844\n",
            "Loss S33:  0.030691852965646572\n",
            "Loss S44:  0.03469047410913573\n",
            "Loss S11:  0.03247157340380957\n",
            "Loss S22:  0.031452394650250744\n",
            "Loss S33:  0.03067555856348605\n",
            "Loss S44:  0.03467734617433941\n",
            "Loss S11:  0.03248113703802059\n",
            "Loss S22:  0.031444535656175374\n",
            "Loss S33:  0.030673310359150104\n",
            "Loss S44:  0.034674032111895056\n",
            "Loss S11:  0.03246950832693249\n",
            "Loss S22:  0.031434759700014164\n",
            "Loss S33:  0.03067078401733132\n",
            "Loss S44:  0.034673067800981505\n",
            "Loss S11:  0.03246578368521952\n",
            "Loss S22:  0.0314202139966206\n",
            "Loss S33:  0.030674980997780658\n",
            "Loss S44:  0.03467596413208063\n",
            "Loss S11:  0.03248667719748243\n",
            "Loss S22:  0.03142289597454091\n",
            "Loss S33:  0.030683101931954646\n",
            "Loss S44:  0.034669482669221624\n",
            "Loss S11:  0.03249250713681605\n",
            "Loss S22:  0.031412451146577106\n",
            "Loss S33:  0.030700401552098952\n",
            "Loss S44:  0.03468837759025751\n",
            "Loss S11:  0.03249015233454782\n",
            "Loss S22:  0.03138944220609675\n",
            "Loss S33:  0.03069084088361676\n",
            "Loss S44:  0.034671329428458894\n",
            "Validation: \n",
            " Loss S11:  0.03052223101258278\n",
            " Loss S22:  0.04547302797436714\n",
            " Loss S33:  0.03926049545407295\n",
            " Loss S44:  0.04404245316982269\n",
            " Loss S11:  0.03204172166685263\n",
            " Loss S22:  0.0433874460203307\n",
            " Loss S33:  0.043027824766579126\n",
            " Loss S44:  0.04636673576065472\n",
            " Loss S11:  0.03176162837118637\n",
            " Loss S22:  0.04303953505870772\n",
            " Loss S33:  0.043798613203008\n",
            " Loss S44:  0.04603751557992726\n",
            " Loss S11:  0.03163881129661544\n",
            " Loss S22:  0.04286958002408997\n",
            " Loss S33:  0.043833797339533194\n",
            " Loss S44:  0.045724540399234806\n",
            " Loss S11:  0.03161716884301032\n",
            " Loss S22:  0.04285184362972224\n",
            " Loss S33:  0.04381124606287038\n",
            " Loss S44:  0.0456431324099317\n",
            "\n",
            "Epoch: 53\n",
            "Loss S11:  0.03388402983546257\n",
            "Loss S22:  0.03393413871526718\n",
            "Loss S33:  0.03389594331383705\n",
            "Loss S44:  0.036195095628499985\n",
            "Loss S11:  0.032405327328226784\n",
            "Loss S22:  0.0312290701337836\n",
            "Loss S33:  0.03143375925719738\n",
            "Loss S44:  0.03451995280655948\n",
            "Loss S11:  0.032694823862541286\n",
            "Loss S22:  0.031636006864053864\n",
            "Loss S33:  0.031042219716168586\n",
            "Loss S44:  0.034883570458207815\n",
            "Loss S11:  0.03255585581064224\n",
            "Loss S22:  0.031424026155183395\n",
            "Loss S33:  0.031041960922941085\n",
            "Loss S44:  0.03473828108079972\n",
            "Loss S11:  0.03243052550568813\n",
            "Loss S22:  0.03135062540631469\n",
            "Loss S33:  0.030906622229916293\n",
            "Loss S44:  0.03483767026081318\n",
            "Loss S11:  0.032477648052222585\n",
            "Loss S22:  0.03139638696231094\n",
            "Loss S33:  0.03095093924625247\n",
            "Loss S44:  0.03494899486209832\n",
            "Loss S11:  0.03253494444318482\n",
            "Loss S22:  0.031361037926351434\n",
            "Loss S33:  0.03080585519554185\n",
            "Loss S44:  0.03487170628104053\n",
            "Loss S11:  0.032598089190645954\n",
            "Loss S22:  0.031523857737930726\n",
            "Loss S33:  0.03092092556328001\n",
            "Loss S44:  0.03484636789161555\n",
            "Loss S11:  0.03249088764466621\n",
            "Loss S22:  0.03141217715578315\n",
            "Loss S33:  0.030762732419886706\n",
            "Loss S44:  0.03469047867865474\n",
            "Loss S11:  0.03247549898594945\n",
            "Loss S22:  0.03149424104408904\n",
            "Loss S33:  0.03072862578862971\n",
            "Loss S44:  0.03467789986222\n",
            "Loss S11:  0.032390603703437465\n",
            "Loss S22:  0.03143171789032398\n",
            "Loss S33:  0.030596067483472354\n",
            "Loss S44:  0.034550365964227385\n",
            "Loss S11:  0.032349335331771825\n",
            "Loss S22:  0.03140533660110589\n",
            "Loss S33:  0.0305665826072564\n",
            "Loss S44:  0.03444942637338295\n",
            "Loss S11:  0.03232087455074156\n",
            "Loss S22:  0.031430876144200315\n",
            "Loss S33:  0.03061497836443018\n",
            "Loss S44:  0.03445811714392063\n",
            "Loss S11:  0.032365153166856474\n",
            "Loss S22:  0.03146976555292388\n",
            "Loss S33:  0.030619387877465206\n",
            "Loss S44:  0.03453157409911847\n",
            "Loss S11:  0.0323877436313646\n",
            "Loss S22:  0.03144712529495253\n",
            "Loss S33:  0.030612703399560976\n",
            "Loss S44:  0.034569676225701125\n",
            "Loss S11:  0.03238109626270683\n",
            "Loss S22:  0.03146513130836534\n",
            "Loss S33:  0.030602457692587612\n",
            "Loss S44:  0.03456760770240367\n",
            "Loss S11:  0.032289068891394955\n",
            "Loss S22:  0.031429721135067645\n",
            "Loss S33:  0.030592494418624765\n",
            "Loss S44:  0.03454383680168886\n",
            "Loss S11:  0.0323451799585631\n",
            "Loss S22:  0.03146143794626172\n",
            "Loss S33:  0.03056736224009628\n",
            "Loss S44:  0.03455012461595368\n",
            "Loss S11:  0.03235132152533663\n",
            "Loss S22:  0.031502707619528746\n",
            "Loss S33:  0.030606218330685605\n",
            "Loss S44:  0.034591918533348906\n",
            "Loss S11:  0.03232475511573684\n",
            "Loss S22:  0.0315017448396895\n",
            "Loss S33:  0.030603984195290437\n",
            "Loss S44:  0.03459596470076376\n",
            "Loss S11:  0.03227881506196598\n",
            "Loss S22:  0.03149509560594808\n",
            "Loss S33:  0.030604714055114716\n",
            "Loss S44:  0.03460321716259961\n",
            "Loss S11:  0.03230661250015288\n",
            "Loss S22:  0.03154322919899254\n",
            "Loss S33:  0.03062241041582625\n",
            "Loss S44:  0.03461344610740788\n",
            "Loss S11:  0.03229808387645769\n",
            "Loss S22:  0.031531522775330154\n",
            "Loss S33:  0.030659701658320104\n",
            "Loss S44:  0.03460740913295638\n",
            "Loss S11:  0.03231246870220739\n",
            "Loss S22:  0.031537898481537256\n",
            "Loss S33:  0.030664155662898377\n",
            "Loss S44:  0.034597875897115445\n",
            "Loss S11:  0.0323523850145429\n",
            "Loss S22:  0.03155337012067622\n",
            "Loss S33:  0.030703925230072742\n",
            "Loss S44:  0.0346242391284076\n",
            "Loss S11:  0.03233867989058988\n",
            "Loss S22:  0.03152739767178121\n",
            "Loss S33:  0.030688736283506057\n",
            "Loss S44:  0.034579775247915806\n",
            "Loss S11:  0.03234688468791287\n",
            "Loss S22:  0.031525393198618946\n",
            "Loss S33:  0.030693793134100134\n",
            "Loss S44:  0.03457571294915174\n",
            "Loss S11:  0.032367594478260105\n",
            "Loss S22:  0.031538532063969824\n",
            "Loss S33:  0.0307000403213435\n",
            "Loss S44:  0.03457922322118854\n",
            "Loss S11:  0.03239569187933228\n",
            "Loss S22:  0.03154825185859755\n",
            "Loss S33:  0.03070707778940209\n",
            "Loss S44:  0.03458634606194666\n",
            "Loss S11:  0.03239496849810135\n",
            "Loss S22:  0.03154557425515963\n",
            "Loss S33:  0.03070585183876077\n",
            "Loss S44:  0.03459808126706438\n",
            "Loss S11:  0.03239608047981793\n",
            "Loss S22:  0.03152500386303445\n",
            "Loss S33:  0.03071131426036556\n",
            "Loss S44:  0.03461000152501553\n",
            "Loss S11:  0.032375945269438614\n",
            "Loss S22:  0.03147700254676618\n",
            "Loss S33:  0.030682912018521424\n",
            "Loss S44:  0.034604689088377535\n",
            "Loss S11:  0.03236751032070579\n",
            "Loss S22:  0.03146793489631648\n",
            "Loss S33:  0.030655875958711186\n",
            "Loss S44:  0.034597905555803825\n",
            "Loss S11:  0.032366890623445235\n",
            "Loss S22:  0.031456940708923915\n",
            "Loss S33:  0.03064502243323989\n",
            "Loss S44:  0.03456207971986147\n",
            "Loss S11:  0.032361225555500674\n",
            "Loss S22:  0.03144541801748213\n",
            "Loss S33:  0.030639029798969146\n",
            "Loss S44:  0.03455840556437144\n",
            "Loss S11:  0.03237665308761461\n",
            "Loss S22:  0.03144126552080157\n",
            "Loss S33:  0.030658271779277046\n",
            "Loss S44:  0.034582469299265804\n",
            "Loss S11:  0.03238648725699355\n",
            "Loss S22:  0.03143836111036694\n",
            "Loss S33:  0.03066266142347843\n",
            "Loss S44:  0.034574491180830384\n",
            "Loss S11:  0.032351625074112836\n",
            "Loss S22:  0.031415298164412984\n",
            "Loss S33:  0.03064858831585739\n",
            "Loss S44:  0.03452086836762345\n",
            "Loss S11:  0.03232553501532772\n",
            "Loss S22:  0.031415587721338734\n",
            "Loss S33:  0.030631031132158957\n",
            "Loss S44:  0.034492688532185366\n",
            "Loss S11:  0.032295809296505225\n",
            "Loss S22:  0.0313743643719903\n",
            "Loss S33:  0.030607368354030582\n",
            "Loss S44:  0.03447271349465908\n",
            "Loss S11:  0.032284996186631575\n",
            "Loss S22:  0.0313480157553778\n",
            "Loss S33:  0.030590007810892904\n",
            "Loss S44:  0.0344700713007899\n",
            "Loss S11:  0.032277825132121137\n",
            "Loss S22:  0.031348513942347826\n",
            "Loss S33:  0.030596437005194724\n",
            "Loss S44:  0.034482991350066924\n",
            "Loss S11:  0.03229183506335612\n",
            "Loss S22:  0.03136839504373612\n",
            "Loss S33:  0.03060881721382328\n",
            "Loss S44:  0.034500883777145816\n",
            "Loss S11:  0.03227773179535661\n",
            "Loss S22:  0.031366941313801674\n",
            "Loss S33:  0.030582448584297695\n",
            "Loss S44:  0.034472941708364235\n",
            "Loss S11:  0.03229230511384486\n",
            "Loss S22:  0.03136631532608111\n",
            "Loss S33:  0.030586925341563972\n",
            "Loss S44:  0.03448740752358961\n",
            "Loss S11:  0.032298550191448956\n",
            "Loss S22:  0.031364595009522\n",
            "Loss S33:  0.03060362775068722\n",
            "Loss S44:  0.034495722366178906\n",
            "Loss S11:  0.03229686799671283\n",
            "Loss S22:  0.03136281516006996\n",
            "Loss S33:  0.03061451909178767\n",
            "Loss S44:  0.034516873775522774\n",
            "Loss S11:  0.03231322336269665\n",
            "Loss S22:  0.03136114568910275\n",
            "Loss S33:  0.03060554094501868\n",
            "Loss S44:  0.03452435149682682\n",
            "Loss S11:  0.03230043499112873\n",
            "Loss S22:  0.03134352371112101\n",
            "Loss S33:  0.030612080847832863\n",
            "Loss S44:  0.03452803904341685\n",
            "Loss S11:  0.032286615264573555\n",
            "Loss S22:  0.03133457294777187\n",
            "Loss S33:  0.030600549008264803\n",
            "Loss S44:  0.03451164430595342\n",
            "Validation: \n",
            " Loss S11:  0.03072347491979599\n",
            " Loss S22:  0.04413911700248718\n",
            " Loss S33:  0.04069137945771217\n",
            " Loss S44:  0.04373517259955406\n",
            " Loss S11:  0.0315045921930245\n",
            " Loss S22:  0.04355917125940323\n",
            " Loss S33:  0.043928331385056175\n",
            " Loss S44:  0.04595105598370234\n",
            " Loss S11:  0.03144390690254002\n",
            " Loss S22:  0.04330566734438989\n",
            " Loss S33:  0.044716672472110607\n",
            " Loss S44:  0.045600480786183985\n",
            " Loss S11:  0.03129114271675954\n",
            " Loss S22:  0.0431178798441027\n",
            " Loss S33:  0.04478389537725292\n",
            " Loss S44:  0.04530961616117446\n",
            " Loss S11:  0.03125877643900889\n",
            " Loss S22:  0.043064904709657036\n",
            " Loss S33:  0.044728460917134345\n",
            " Loss S44:  0.04522592491573758\n",
            "\n",
            "Epoch: 54\n",
            "Loss S11:  0.03214874118566513\n",
            "Loss S22:  0.030918704345822334\n",
            "Loss S33:  0.03269866108894348\n",
            "Loss S44:  0.03625832125544548\n",
            "Loss S11:  0.03157939880409024\n",
            "Loss S22:  0.03131325271996585\n",
            "Loss S33:  0.03034005720507015\n",
            "Loss S44:  0.03401440889997916\n",
            "Loss S11:  0.03209594876638481\n",
            "Loss S22:  0.03134529328062421\n",
            "Loss S33:  0.03055125049182347\n",
            "Loss S44:  0.03434205835773831\n",
            "Loss S11:  0.03221596675294061\n",
            "Loss S22:  0.031446441887847836\n",
            "Loss S33:  0.03069907757303407\n",
            "Loss S44:  0.034384604543447495\n",
            "Loss S11:  0.03214954489433184\n",
            "Loss S22:  0.03133242913499111\n",
            "Loss S33:  0.03065716161778787\n",
            "Loss S44:  0.034573219443966706\n",
            "Loss S11:  0.03219604762453659\n",
            "Loss S22:  0.031371912352886853\n",
            "Loss S33:  0.030878911744437965\n",
            "Loss S44:  0.03465190800089462\n",
            "Loss S11:  0.0323264445865252\n",
            "Loss S22:  0.03131337940204339\n",
            "Loss S33:  0.030723354458564618\n",
            "Loss S44:  0.03454701516960488\n",
            "Loss S11:  0.032318818317332736\n",
            "Loss S22:  0.03142444243733312\n",
            "Loss S33:  0.030769766400187786\n",
            "Loss S44:  0.03459694902871696\n",
            "Loss S11:  0.03230081073203941\n",
            "Loss S22:  0.03150409827997655\n",
            "Loss S33:  0.030700482933977504\n",
            "Loss S44:  0.03451962971392973\n",
            "Loss S11:  0.03225638479976864\n",
            "Loss S22:  0.03150908587561859\n",
            "Loss S33:  0.030613542380896242\n",
            "Loss S44:  0.034424182518825425\n",
            "Loss S11:  0.032138680024902416\n",
            "Loss S22:  0.03141393379704787\n",
            "Loss S33:  0.030505955459015206\n",
            "Loss S44:  0.034300526825360736\n",
            "Loss S11:  0.03206390149153031\n",
            "Loss S22:  0.03136588002110387\n",
            "Loss S33:  0.030458992431024172\n",
            "Loss S44:  0.034226397975339544\n",
            "Loss S11:  0.032074456772774707\n",
            "Loss S22:  0.03132740460520934\n",
            "Loss S33:  0.030533416596941712\n",
            "Loss S44:  0.034253538024326985\n",
            "Loss S11:  0.03208808979119053\n",
            "Loss S22:  0.031372674335618966\n",
            "Loss S33:  0.030615152350363842\n",
            "Loss S44:  0.03433543392492615\n",
            "Loss S11:  0.03205964880737853\n",
            "Loss S22:  0.031331680530140585\n",
            "Loss S33:  0.030563498734899445\n",
            "Loss S44:  0.03433735636954612\n",
            "Loss S11:  0.032085082327194564\n",
            "Loss S22:  0.031379297499033\n",
            "Loss S33:  0.030542939543625375\n",
            "Loss S44:  0.034372496141108454\n",
            "Loss S11:  0.032041711721175946\n",
            "Loss S22:  0.03133331910238503\n",
            "Loss S33:  0.03045979319198161\n",
            "Loss S44:  0.03431699686446545\n",
            "Loss S11:  0.03209692686360482\n",
            "Loss S22:  0.03135317022645334\n",
            "Loss S33:  0.030469886588248594\n",
            "Loss S44:  0.034376895872124454\n",
            "Loss S11:  0.03210355141791849\n",
            "Loss S22:  0.031413112638405016\n",
            "Loss S33:  0.030489971511354103\n",
            "Loss S44:  0.03439879534735205\n",
            "Loss S11:  0.03207888894992349\n",
            "Loss S22:  0.031374879317914\n",
            "Loss S33:  0.030489297066839578\n",
            "Loss S44:  0.03438460118873581\n",
            "Loss S11:  0.032024692419675455\n",
            "Loss S22:  0.031325926957883644\n",
            "Loss S33:  0.030471870499612086\n",
            "Loss S44:  0.03435157734289098\n",
            "Loss S11:  0.0320487381687379\n",
            "Loss S22:  0.031346490209419015\n",
            "Loss S33:  0.030467065474927706\n",
            "Loss S44:  0.03435017401573217\n",
            "Loss S11:  0.03203974938129678\n",
            "Loss S22:  0.03134547747941308\n",
            "Loss S33:  0.030517351168852586\n",
            "Loss S44:  0.0343597754330387\n",
            "Loss S11:  0.03202245319947536\n",
            "Loss S22:  0.03130425278952111\n",
            "Loss S33:  0.030483475237181694\n",
            "Loss S44:  0.0343168962714605\n",
            "Loss S11:  0.032088274878152176\n",
            "Loss S22:  0.03132824105098535\n",
            "Loss S33:  0.030537935653971934\n",
            "Loss S44:  0.03437778903210064\n",
            "Loss S11:  0.03207203227121279\n",
            "Loss S22:  0.0313070464686331\n",
            "Loss S33:  0.030528571248885646\n",
            "Loss S44:  0.034345789979475426\n",
            "Loss S11:  0.03206575112381657\n",
            "Loss S22:  0.031289432176399505\n",
            "Loss S33:  0.03053777140361824\n",
            "Loss S44:  0.034340901500135086\n",
            "Loss S11:  0.03208338760004492\n",
            "Loss S22:  0.031287890594328904\n",
            "Loss S33:  0.030554703398989136\n",
            "Loss S44:  0.03432747520286439\n",
            "Loss S11:  0.032082910540106035\n",
            "Loss S22:  0.031278685550619696\n",
            "Loss S33:  0.030558421558226554\n",
            "Loss S44:  0.03434483682425209\n",
            "Loss S11:  0.032068932403352665\n",
            "Loss S22:  0.03126159727035724\n",
            "Loss S33:  0.030569940587806536\n",
            "Loss S44:  0.03435990332763424\n",
            "Loss S11:  0.032067734983276294\n",
            "Loss S22:  0.03125798313002848\n",
            "Loss S33:  0.03056903487919177\n",
            "Loss S44:  0.03437341708207249\n",
            "Loss S11:  0.032065858764999164\n",
            "Loss S22:  0.031249738516962798\n",
            "Loss S33:  0.030557850315879397\n",
            "Loss S44:  0.03437946666738803\n",
            "Loss S11:  0.032070649735559925\n",
            "Loss S22:  0.03125237508146005\n",
            "Loss S33:  0.030541241807265446\n",
            "Loss S44:  0.03435787602075349\n",
            "Loss S11:  0.0320815118001577\n",
            "Loss S22:  0.03125669230394673\n",
            "Loss S33:  0.030509043687509986\n",
            "Loss S44:  0.034361832175865276\n",
            "Loss S11:  0.032083691324481516\n",
            "Loss S22:  0.031238714287663834\n",
            "Loss S33:  0.030489886343435458\n",
            "Loss S44:  0.03437272053419495\n",
            "Loss S11:  0.032102735845791305\n",
            "Loss S22:  0.03125349596206449\n",
            "Loss S33:  0.030516175404615552\n",
            "Loss S44:  0.034401754368404376\n",
            "Loss S11:  0.03212010556537854\n",
            "Loss S22:  0.031261574688504275\n",
            "Loss S33:  0.03052791931416189\n",
            "Loss S44:  0.03442350621244914\n",
            "Loss S11:  0.03210645939682854\n",
            "Loss S22:  0.031237422787355928\n",
            "Loss S33:  0.030520814061767328\n",
            "Loss S44:  0.034394549913804806\n",
            "Loss S11:  0.03207886841373024\n",
            "Loss S22:  0.03122782391884665\n",
            "Loss S33:  0.030500552389682748\n",
            "Loss S44:  0.03436998761408129\n",
            "Loss S11:  0.03205806369919454\n",
            "Loss S22:  0.03119431965796234\n",
            "Loss S33:  0.030482212210174105\n",
            "Loss S44:  0.034339007199801444\n",
            "Loss S11:  0.03205867746654443\n",
            "Loss S22:  0.031168697973216263\n",
            "Loss S33:  0.030484394395626394\n",
            "Loss S44:  0.03434656522331987\n",
            "Loss S11:  0.03204605346341638\n",
            "Loss S22:  0.031170675177535002\n",
            "Loss S33:  0.03048403797684795\n",
            "Loss S44:  0.03433626642736205\n",
            "Loss S11:  0.03204955461472627\n",
            "Loss S22:  0.031168977882615745\n",
            "Loss S33:  0.03048873904234157\n",
            "Loss S44:  0.03433703217929729\n",
            "Loss S11:  0.0320625741622495\n",
            "Loss S22:  0.03117348960754213\n",
            "Loss S33:  0.030468292149721608\n",
            "Loss S44:  0.03432298316393402\n",
            "Loss S11:  0.03208126670041052\n",
            "Loss S22:  0.031179616552969767\n",
            "Loss S33:  0.030467740747900235\n",
            "Loss S44:  0.03432458176651374\n",
            "Loss S11:  0.032106435446774884\n",
            "Loss S22:  0.031165621100931105\n",
            "Loss S33:  0.030481457412903694\n",
            "Loss S44:  0.03434233303840568\n",
            "Loss S11:  0.03209630674288614\n",
            "Loss S22:  0.031154198693252956\n",
            "Loss S33:  0.030485758776099717\n",
            "Loss S44:  0.03434855296981568\n",
            "Loss S11:  0.032101823558789655\n",
            "Loss S22:  0.0311412766987098\n",
            "Loss S33:  0.03048197204947092\n",
            "Loss S44:  0.03435848807548262\n",
            "Loss S11:  0.03209171931718219\n",
            "Loss S22:  0.03113045574799025\n",
            "Loss S33:  0.030483138741704628\n",
            "Loss S44:  0.03437102668126755\n",
            "Loss S11:  0.03208909005821965\n",
            "Loss S22:  0.031106004954264507\n",
            "Loss S33:  0.030467231740867774\n",
            "Loss S44:  0.034353665806459315\n",
            "Validation: \n",
            " Loss S11:  0.031228814274072647\n",
            " Loss S22:  0.0454542450606823\n",
            " Loss S33:  0.039410028606653214\n",
            " Loss S44:  0.04261519014835358\n",
            " Loss S11:  0.03163736748198668\n",
            " Loss S22:  0.043721500251974375\n",
            " Loss S33:  0.04318467509888467\n",
            " Loss S44:  0.04649541438335464\n",
            " Loss S11:  0.031532720730799\n",
            " Loss S22:  0.04340590763746238\n",
            " Loss S33:  0.043988003509073725\n",
            " Loss S44:  0.04631437161346761\n",
            " Loss S11:  0.031367197357973116\n",
            " Loss S22:  0.043249773747119746\n",
            " Loss S33:  0.04403171358538455\n",
            " Loss S44:  0.045897235941202916\n",
            " Loss S11:  0.03133351091341472\n",
            " Loss S22:  0.04322053478271873\n",
            " Loss S33:  0.04400398085514704\n",
            " Loss S44:  0.04575934619815261\n",
            "\n",
            "Epoch: 55\n",
            "Loss S11:  0.033881328999996185\n",
            "Loss S22:  0.03211337327957153\n",
            "Loss S33:  0.0335218720138073\n",
            "Loss S44:  0.03503642976284027\n",
            "Loss S11:  0.032489691776308144\n",
            "Loss S22:  0.030994623391465706\n",
            "Loss S33:  0.030491916632110424\n",
            "Loss S44:  0.0343079651621255\n",
            "Loss S11:  0.032283617006171314\n",
            "Loss S22:  0.031020379137425197\n",
            "Loss S33:  0.03042782922940595\n",
            "Loss S44:  0.034144434545721324\n",
            "Loss S11:  0.03226635214542189\n",
            "Loss S22:  0.031120155847841693\n",
            "Loss S33:  0.030619082792151357\n",
            "Loss S44:  0.03432129203311859\n",
            "Loss S11:  0.03221319865707944\n",
            "Loss S22:  0.030988242204596356\n",
            "Loss S33:  0.030562541697446894\n",
            "Loss S44:  0.03428895762417375\n",
            "Loss S11:  0.0322624717419054\n",
            "Loss S22:  0.031011135262601516\n",
            "Loss S33:  0.030663097825120476\n",
            "Loss S44:  0.034373856350487356\n",
            "Loss S11:  0.032374869696185235\n",
            "Loss S22:  0.03116205933152652\n",
            "Loss S33:  0.030609707393851437\n",
            "Loss S44:  0.03433660961321143\n",
            "Loss S11:  0.0323621293818447\n",
            "Loss S22:  0.031238825965515325\n",
            "Loss S33:  0.03065907270450827\n",
            "Loss S44:  0.03434481912515533\n",
            "Loss S11:  0.032295570237401094\n",
            "Loss S22:  0.031182411000316525\n",
            "Loss S33:  0.03050452730629915\n",
            "Loss S44:  0.03426833050670447\n",
            "Loss S11:  0.03225312351279861\n",
            "Loss S22:  0.03122438950934908\n",
            "Loss S33:  0.030449403347549857\n",
            "Loss S44:  0.03430542439877332\n",
            "Loss S11:  0.032190489960779056\n",
            "Loss S22:  0.031176401394428593\n",
            "Loss S33:  0.030322637335203663\n",
            "Loss S44:  0.03429601027971447\n",
            "Loss S11:  0.03212266259298131\n",
            "Loss S22:  0.031142062633424193\n",
            "Loss S33:  0.030236137275760237\n",
            "Loss S44:  0.034178143417513046\n",
            "Loss S11:  0.03209518579658398\n",
            "Loss S22:  0.03114910328314324\n",
            "Loss S33:  0.030322286492783176\n",
            "Loss S44:  0.034247389694383325\n",
            "Loss S11:  0.03206999095171007\n",
            "Loss S22:  0.03117227478962363\n",
            "Loss S33:  0.030338169167969973\n",
            "Loss S44:  0.03438496882565149\n",
            "Loss S11:  0.03204814998551886\n",
            "Loss S22:  0.031173357740044594\n",
            "Loss S33:  0.030312188445253574\n",
            "Loss S44:  0.03440971646114444\n",
            "Loss S11:  0.0320204169495611\n",
            "Loss S22:  0.031171123774734553\n",
            "Loss S33:  0.030301605345101547\n",
            "Loss S44:  0.03440404643878242\n",
            "Loss S11:  0.03196077440975245\n",
            "Loss S22:  0.031133027643150426\n",
            "Loss S33:  0.030256012257952127\n",
            "Loss S44:  0.03434012117593185\n",
            "Loss S11:  0.03200539137231328\n",
            "Loss S22:  0.031149807533151226\n",
            "Loss S33:  0.030279825751980145\n",
            "Loss S44:  0.0343504830410606\n",
            "Loss S11:  0.03201920739812416\n",
            "Loss S22:  0.03113320367976776\n",
            "Loss S33:  0.0302709238037714\n",
            "Loss S44:  0.03435133580756451\n",
            "Loss S11:  0.032033596402375485\n",
            "Loss S22:  0.031123286444713308\n",
            "Loss S33:  0.03027143218208358\n",
            "Loss S44:  0.03436054609208831\n",
            "Loss S11:  0.03199294044529621\n",
            "Loss S22:  0.031089500232196567\n",
            "Loss S33:  0.03029721372051915\n",
            "Loss S44:  0.034330048936590626\n",
            "Loss S11:  0.03202830789107564\n",
            "Loss S22:  0.03110454958020511\n",
            "Loss S33:  0.03029213888102798\n",
            "Loss S44:  0.03431696420032266\n",
            "Loss S11:  0.03204342193598122\n",
            "Loss S22:  0.031109310450597047\n",
            "Loss S33:  0.030326694325961138\n",
            "Loss S44:  0.034334160252663885\n",
            "Loss S11:  0.032065805329969436\n",
            "Loss S22:  0.031116498811613946\n",
            "Loss S33:  0.030319628011419145\n",
            "Loss S44:  0.03431825780862079\n",
            "Loss S11:  0.0321139240196879\n",
            "Loss S22:  0.031160509499771467\n",
            "Loss S33:  0.030351822608424915\n",
            "Loss S44:  0.034339282584388224\n",
            "Loss S11:  0.0321017489638699\n",
            "Loss S22:  0.031152236316189824\n",
            "Loss S33:  0.030312621557855038\n",
            "Loss S44:  0.03429544274787979\n",
            "Loss S11:  0.03207591859002908\n",
            "Loss S22:  0.031127770028123454\n",
            "Loss S33:  0.030305120979803275\n",
            "Loss S44:  0.034294379611010756\n",
            "Loss S11:  0.03207454599467591\n",
            "Loss S22:  0.031128850514653424\n",
            "Loss S33:  0.030288802899722682\n",
            "Loss S44:  0.03430615262406778\n",
            "Loss S11:  0.032081058832197\n",
            "Loss S22:  0.031148766740838402\n",
            "Loss S33:  0.030275878705269925\n",
            "Loss S44:  0.03429962306207185\n",
            "Loss S11:  0.032094721877595406\n",
            "Loss S22:  0.031134965269477507\n",
            "Loss S33:  0.030299239705518347\n",
            "Loss S44:  0.03431894671159102\n",
            "Loss S11:  0.03208205121201138\n",
            "Loss S22:  0.03114151543136253\n",
            "Loss S33:  0.030286934338138744\n",
            "Loss S44:  0.03432784907742592\n",
            "Loss S11:  0.03206526969598421\n",
            "Loss S22:  0.03112122527895633\n",
            "Loss S33:  0.030285056560656647\n",
            "Loss S44:  0.034344320044736004\n",
            "Loss S11:  0.03205107323059412\n",
            "Loss S22:  0.03111004427864544\n",
            "Loss S33:  0.030266873207744037\n",
            "Loss S44:  0.03432979641302352\n",
            "Loss S11:  0.03203306788876878\n",
            "Loss S22:  0.03111391616957065\n",
            "Loss S33:  0.030254892537826136\n",
            "Loss S44:  0.03431142157568312\n",
            "Loss S11:  0.03202430154483689\n",
            "Loss S22:  0.031122976974133524\n",
            "Loss S33:  0.0302605102182833\n",
            "Loss S44:  0.03432561115216999\n",
            "Loss S11:  0.032031868496893816\n",
            "Loss S22:  0.031142127596669726\n",
            "Loss S33:  0.030294814303602247\n",
            "Loss S44:  0.03436435670976625\n",
            "Loss S11:  0.03204981420302655\n",
            "Loss S22:  0.03116558218204579\n",
            "Loss S33:  0.030287014903536795\n",
            "Loss S44:  0.034368828712985786\n",
            "Loss S11:  0.032019401877877524\n",
            "Loss S22:  0.031152316959037613\n",
            "Loss S33:  0.030272198414224176\n",
            "Loss S44:  0.034336034615205305\n",
            "Loss S11:  0.031989412690241505\n",
            "Loss S22:  0.031147448962011676\n",
            "Loss S33:  0.03025145931389388\n",
            "Loss S44:  0.03428915455898275\n",
            "Loss S11:  0.03195604784390353\n",
            "Loss S22:  0.03111455673375703\n",
            "Loss S33:  0.030222353894654136\n",
            "Loss S44:  0.03424091483263866\n",
            "Loss S11:  0.03197681061236044\n",
            "Loss S22:  0.031105756234758513\n",
            "Loss S33:  0.030214932326812695\n",
            "Loss S44:  0.03423735455254515\n",
            "Loss S11:  0.03197519797514535\n",
            "Loss S22:  0.03109512487177141\n",
            "Loss S33:  0.030219904385017652\n",
            "Loss S44:  0.03423018513786677\n",
            "Loss S11:  0.03197592200441485\n",
            "Loss S22:  0.03108963287145678\n",
            "Loss S33:  0.030230157519745997\n",
            "Loss S44:  0.03423805721989012\n",
            "Loss S11:  0.031965353238783134\n",
            "Loss S22:  0.031082596616892968\n",
            "Loss S33:  0.03022271573647269\n",
            "Loss S44:  0.03423194181424698\n",
            "Loss S11:  0.03198221770628375\n",
            "Loss S22:  0.031091884623309113\n",
            "Loss S33:  0.030242758298339216\n",
            "Loss S44:  0.034238221419472546\n",
            "Loss S11:  0.031982384130623015\n",
            "Loss S22:  0.03107694129226213\n",
            "Loss S33:  0.03025926001568195\n",
            "Loss S44:  0.03424707959156211\n",
            "Loss S11:  0.03197580194929029\n",
            "Loss S22:  0.031067884294821227\n",
            "Loss S33:  0.03026499524470136\n",
            "Loss S44:  0.03427066773160031\n",
            "Loss S11:  0.03196461780309171\n",
            "Loss S22:  0.031057426176031938\n",
            "Loss S33:  0.030270404801278863\n",
            "Loss S44:  0.034255252585542684\n",
            "Loss S11:  0.0319521091413721\n",
            "Loss S22:  0.031044100606794665\n",
            "Loss S33:  0.03027503478598322\n",
            "Loss S44:  0.03423959745593725\n",
            "Loss S11:  0.0319395014321974\n",
            "Loss S22:  0.031033861767614447\n",
            "Loss S33:  0.030260487575303998\n",
            "Loss S44:  0.03422330508801456\n",
            "Validation: \n",
            " Loss S11:  0.0313464030623436\n",
            " Loss S22:  0.04454485699534416\n",
            " Loss S33:  0.039417948573827744\n",
            " Loss S44:  0.041973333805799484\n",
            " Loss S11:  0.03141002800493013\n",
            " Loss S22:  0.0436018185601348\n",
            " Loss S33:  0.043598382778110956\n",
            " Loss S44:  0.04533158810365768\n",
            " Loss S11:  0.03130691734755912\n",
            " Loss S22:  0.043413826332586565\n",
            " Loss S33:  0.04438599226314847\n",
            " Loss S44:  0.045172486817691385\n",
            " Loss S11:  0.031195348800450075\n",
            " Loss S22:  0.04331694005942736\n",
            " Loss S33:  0.04447610999961368\n",
            " Loss S44:  0.04483036959513289\n",
            " Loss S11:  0.031131075465200858\n",
            " Loss S22:  0.043297318948639765\n",
            " Loss S33:  0.04446340508667039\n",
            " Loss S44:  0.0447398166137713\n",
            "\n",
            "Epoch: 56\n",
            "Loss S11:  0.03592108562588692\n",
            "Loss S22:  0.0325416699051857\n",
            "Loss S33:  0.034987155348062515\n",
            "Loss S44:  0.037514325231313705\n",
            "Loss S11:  0.031973003833131355\n",
            "Loss S22:  0.031232489611614834\n",
            "Loss S33:  0.030426374890587547\n",
            "Loss S44:  0.03361405787820166\n",
            "Loss S11:  0.03205667799782185\n",
            "Loss S22:  0.03133965035279592\n",
            "Loss S33:  0.030323186534501258\n",
            "Loss S44:  0.033919060247994605\n",
            "Loss S11:  0.032010806424002496\n",
            "Loss S22:  0.031139844668007666\n",
            "Loss S33:  0.03018445149064064\n",
            "Loss S44:  0.033992859564961925\n",
            "Loss S11:  0.031892664321675535\n",
            "Loss S22:  0.030964777873056692\n",
            "Loss S33:  0.0301279938984208\n",
            "Loss S44:  0.033916971260091154\n",
            "Loss S11:  0.03178491235217627\n",
            "Loss S22:  0.030955630591979214\n",
            "Loss S33:  0.030213739978624324\n",
            "Loss S44:  0.03413581983277611\n",
            "Loss S11:  0.03186893279923767\n",
            "Loss S22:  0.030839883493351154\n",
            "Loss S33:  0.030111085745643397\n",
            "Loss S44:  0.03397633779610767\n",
            "Loss S11:  0.03184226994783106\n",
            "Loss S22:  0.030875694583839094\n",
            "Loss S33:  0.030190651554247022\n",
            "Loss S44:  0.03396194593482454\n",
            "Loss S11:  0.0317972597470622\n",
            "Loss S22:  0.030866370616871634\n",
            "Loss S33:  0.030073407094603703\n",
            "Loss S44:  0.03386702204560056\n",
            "Loss S11:  0.03180682257964061\n",
            "Loss S22:  0.030874670370594486\n",
            "Loss S33:  0.030081797268364456\n",
            "Loss S44:  0.033795416048103634\n",
            "Loss S11:  0.03175882569117711\n",
            "Loss S22:  0.030836265626372678\n",
            "Loss S33:  0.02997233222543013\n",
            "Loss S44:  0.03369125308893105\n",
            "Loss S11:  0.03175399823298863\n",
            "Loss S22:  0.030821643132078753\n",
            "Loss S33:  0.029942369709412258\n",
            "Loss S44:  0.03362435852555004\n",
            "Loss S11:  0.03175995123287863\n",
            "Loss S22:  0.030817274327489954\n",
            "Loss S33:  0.03001722571832582\n",
            "Loss S44:  0.03364390164059548\n",
            "Loss S11:  0.031730013863958476\n",
            "Loss S22:  0.03083800523051324\n",
            "Loss S33:  0.03002603774364213\n",
            "Loss S44:  0.03368542168488484\n",
            "Loss S11:  0.03174037617096241\n",
            "Loss S22:  0.030779277681247563\n",
            "Loss S33:  0.02999934749611726\n",
            "Loss S44:  0.033714101524323435\n",
            "Loss S11:  0.03178311404556233\n",
            "Loss S22:  0.030845513789365623\n",
            "Loss S33:  0.030010001215812387\n",
            "Loss S44:  0.03376199779970362\n",
            "Loss S11:  0.03173518350914768\n",
            "Loss S22:  0.03086102552573133\n",
            "Loss S33:  0.02999051841936126\n",
            "Loss S44:  0.033788525077126784\n",
            "Loss S11:  0.03178330606710144\n",
            "Loss S22:  0.030883150894129484\n",
            "Loss S33:  0.030002879425447587\n",
            "Loss S44:  0.03385086207274805\n",
            "Loss S11:  0.03179589854501887\n",
            "Loss S22:  0.030891870163916223\n",
            "Loss S33:  0.03003212157122338\n",
            "Loss S44:  0.03388863983075263\n",
            "Loss S11:  0.03179171307899877\n",
            "Loss S22:  0.03086502957328452\n",
            "Loss S33:  0.030056146553751686\n",
            "Loss S44:  0.033891137962409966\n",
            "Loss S11:  0.03176751935771152\n",
            "Loss S22:  0.030823881028970674\n",
            "Loss S33:  0.03006325533326289\n",
            "Loss S44:  0.033897581885554896\n",
            "Loss S11:  0.031801482247656554\n",
            "Loss S22:  0.030816121335843164\n",
            "Loss S33:  0.030033448260824828\n",
            "Loss S44:  0.0338873208910933\n",
            "Loss S11:  0.03182504917768871\n",
            "Loss S22:  0.030825297613691422\n",
            "Loss S33:  0.0300608887526784\n",
            "Loss S44:  0.033903819218075656\n",
            "Loss S11:  0.03180680261900673\n",
            "Loss S22:  0.030811263707938134\n",
            "Loss S33:  0.030062695054006782\n",
            "Loss S44:  0.03387242423378544\n",
            "Loss S11:  0.03187525602389915\n",
            "Loss S22:  0.030882969235854523\n",
            "Loss S33:  0.030138970418404246\n",
            "Loss S44:  0.03392430407254033\n",
            "Loss S11:  0.03186384799233471\n",
            "Loss S22:  0.030908112106214008\n",
            "Loss S33:  0.030134149474570476\n",
            "Loss S44:  0.03389735625262754\n",
            "Loss S11:  0.0318648822220235\n",
            "Loss S22:  0.03090207317979628\n",
            "Loss S33:  0.030127133938601648\n",
            "Loss S44:  0.03388285805324942\n",
            "Loss S11:  0.031881775253373316\n",
            "Loss S22:  0.03092162073263383\n",
            "Loss S33:  0.030125955277813317\n",
            "Loss S44:  0.03388152868792799\n",
            "Loss S11:  0.03188838176694417\n",
            "Loss S22:  0.030922362401922403\n",
            "Loss S33:  0.030109280508236955\n",
            "Loss S44:  0.03390884429242899\n",
            "Loss S11:  0.03189771803563198\n",
            "Loss S22:  0.030918438646852765\n",
            "Loss S33:  0.030122169584701562\n",
            "Loss S44:  0.03393319096173003\n",
            "Loss S11:  0.031879055799538904\n",
            "Loss S22:  0.03091016870944999\n",
            "Loss S33:  0.030091368243642818\n",
            "Loss S44:  0.033924762846475026\n",
            "Loss S11:  0.03186832419282179\n",
            "Loss S22:  0.030891023474443\n",
            "Loss S33:  0.03007951983732809\n",
            "Loss S44:  0.03393105389340706\n",
            "Loss S11:  0.0318505632229776\n",
            "Loss S22:  0.030857937915302884\n",
            "Loss S33:  0.030058862954795918\n",
            "Loss S44:  0.03393693828253174\n",
            "Loss S11:  0.031840646622584304\n",
            "Loss S22:  0.030843274988616703\n",
            "Loss S33:  0.030058868499500874\n",
            "Loss S44:  0.033925419476367194\n",
            "Loss S11:  0.031815843776829784\n",
            "Loss S22:  0.030847343886571545\n",
            "Loss S33:  0.030065492205212544\n",
            "Loss S44:  0.033934293090860164\n",
            "Loss S11:  0.03183026227070565\n",
            "Loss S22:  0.030862622976600276\n",
            "Loss S33:  0.030092853002059154\n",
            "Loss S44:  0.033965447158725175\n",
            "Loss S11:  0.0318285027606583\n",
            "Loss S22:  0.03087391251300841\n",
            "Loss S33:  0.030093325157533723\n",
            "Loss S44:  0.033959915402275706\n",
            "Loss S11:  0.03182789266129549\n",
            "Loss S22:  0.030864375851787324\n",
            "Loss S33:  0.030088979329140682\n",
            "Loss S44:  0.033951780585587184\n",
            "Loss S11:  0.031815080265477885\n",
            "Loss S22:  0.030864082027490684\n",
            "Loss S33:  0.030072519472577754\n",
            "Loss S44:  0.033931758010324845\n",
            "Loss S11:  0.03177631448220719\n",
            "Loss S22:  0.03083190992188728\n",
            "Loss S33:  0.030045684531826496\n",
            "Loss S44:  0.03389599322891601\n",
            "Loss S11:  0.031781522139089365\n",
            "Loss S22:  0.0308122418941628\n",
            "Loss S33:  0.03003036890857089\n",
            "Loss S44:  0.03390453065608505\n",
            "Loss S11:  0.03177484230494122\n",
            "Loss S22:  0.030808322852219107\n",
            "Loss S33:  0.0300440695544664\n",
            "Loss S44:  0.03391232070968534\n",
            "Loss S11:  0.03178134649514161\n",
            "Loss S22:  0.03082219959455686\n",
            "Loss S33:  0.03006232666590576\n",
            "Loss S44:  0.033919126228949235\n",
            "Loss S11:  0.03177269653194603\n",
            "Loss S22:  0.030826664626978667\n",
            "Loss S33:  0.030054497487168853\n",
            "Loss S44:  0.033910912299515754\n",
            "Loss S11:  0.03178446213218496\n",
            "Loss S22:  0.03084090510024235\n",
            "Loss S33:  0.030063331114495693\n",
            "Loss S44:  0.03392748066695099\n",
            "Loss S11:  0.03178276848089272\n",
            "Loss S22:  0.030830799917919673\n",
            "Loss S33:  0.030086063827385923\n",
            "Loss S44:  0.033938390659189015\n",
            "Loss S11:  0.03177555703402471\n",
            "Loss S22:  0.03082937466025094\n",
            "Loss S33:  0.03010365290167275\n",
            "Loss S44:  0.03394259467171485\n",
            "Loss S11:  0.03178038423971892\n",
            "Loss S22:  0.030835416966815916\n",
            "Loss S33:  0.030109871503147603\n",
            "Loss S44:  0.03395370767311432\n",
            "Loss S11:  0.03178400836358323\n",
            "Loss S22:  0.030818975985576855\n",
            "Loss S33:  0.03012135448302152\n",
            "Loss S44:  0.03396386629950232\n",
            "Loss S11:  0.03178140496011907\n",
            "Loss S22:  0.030812716115334367\n",
            "Loss S33:  0.030109399512349467\n",
            "Loss S44:  0.03395334251088426\n",
            "Validation: \n",
            " Loss S11:  0.031478989869356155\n",
            " Loss S22:  0.044372376054525375\n",
            " Loss S33:  0.039888232946395874\n",
            " Loss S44:  0.04287169501185417\n",
            " Loss S11:  0.03197048462572552\n",
            " Loss S22:  0.043505928168694176\n",
            " Loss S33:  0.04327854088374546\n",
            " Loss S44:  0.04532419340241523\n",
            " Loss S11:  0.03177566676423317\n",
            " Loss S22:  0.04316362184358806\n",
            " Loss S33:  0.04394526361692243\n",
            " Loss S44:  0.04519844464049107\n",
            " Loss S11:  0.031599140924508454\n",
            " Loss S22:  0.043013735201026575\n",
            " Loss S33:  0.04400320436622276\n",
            " Loss S44:  0.04488138378155036\n",
            " Loss S11:  0.031573879107097046\n",
            " Loss S22:  0.0429504633317759\n",
            " Loss S33:  0.04392280765337708\n",
            " Loss S44:  0.04479629583196876\n",
            "\n",
            "Epoch: 57\n",
            "Loss S11:  0.03339902311563492\n",
            "Loss S22:  0.0316276028752327\n",
            "Loss S33:  0.032338257879018784\n",
            "Loss S44:  0.036310527473688126\n",
            "Loss S11:  0.03134147568859837\n",
            "Loss S22:  0.03013818473978476\n",
            "Loss S33:  0.029599788852713325\n",
            "Loss S44:  0.034011141820387406\n",
            "Loss S11:  0.031808338881958095\n",
            "Loss S22:  0.030954421187440555\n",
            "Loss S33:  0.029891374033121837\n",
            "Loss S44:  0.03423932975246793\n",
            "Loss S11:  0.03175689856852255\n",
            "Loss S22:  0.030857043701314157\n",
            "Loss S33:  0.02995787116308366\n",
            "Loss S44:  0.03420916121573218\n",
            "Loss S11:  0.03173128265614917\n",
            "Loss S22:  0.030814684382299097\n",
            "Loss S33:  0.02998578503001027\n",
            "Loss S44:  0.034229814960825736\n",
            "Loss S11:  0.031900530388834426\n",
            "Loss S22:  0.030767798788991628\n",
            "Loss S33:  0.03001718044134916\n",
            "Loss S44:  0.03431814189488981\n",
            "Loss S11:  0.03191137145899358\n",
            "Loss S22:  0.03081959953195736\n",
            "Loss S33:  0.029884130037466032\n",
            "Loss S44:  0.034186901402522306\n",
            "Loss S11:  0.03196507631997827\n",
            "Loss S22:  0.030789382808225255\n",
            "Loss S33:  0.029976826371021673\n",
            "Loss S44:  0.0342289458855357\n",
            "Loss S11:  0.03193779131052671\n",
            "Loss S22:  0.030732264880228927\n",
            "Loss S33:  0.02989814559250702\n",
            "Loss S44:  0.0340244844785811\n",
            "Loss S11:  0.031963752930635934\n",
            "Loss S22:  0.030810217096746622\n",
            "Loss S33:  0.02992941316339996\n",
            "Loss S44:  0.03398006124892733\n",
            "Loss S11:  0.03190412800205816\n",
            "Loss S22:  0.03078118818673757\n",
            "Loss S33:  0.02989171548645095\n",
            "Loss S44:  0.03391182442924174\n",
            "Loss S11:  0.031890884377397934\n",
            "Loss S22:  0.030789876524526794\n",
            "Loss S33:  0.02986744547950792\n",
            "Loss S44:  0.03384902658830355\n",
            "Loss S11:  0.031894213419931\n",
            "Loss S22:  0.030791639034782558\n",
            "Loss S33:  0.02990902384573763\n",
            "Loss S44:  0.03388235821150058\n",
            "Loss S11:  0.03186934792529081\n",
            "Loss S22:  0.030822838933868262\n",
            "Loss S33:  0.02992745023931711\n",
            "Loss S44:  0.03394419587579847\n",
            "Loss S11:  0.031924402116672364\n",
            "Loss S22:  0.03073883478047577\n",
            "Loss S33:  0.029920000014892707\n",
            "Loss S44:  0.03398235983723867\n",
            "Loss S11:  0.03189361539502807\n",
            "Loss S22:  0.030723366874041935\n",
            "Loss S33:  0.02990457175040482\n",
            "Loss S44:  0.033957899196554495\n",
            "Loss S11:  0.031804089926136946\n",
            "Loss S22:  0.030679509182524236\n",
            "Loss S33:  0.029879800084492435\n",
            "Loss S44:  0.033919104912914104\n",
            "Loss S11:  0.03183321580726501\n",
            "Loss S22:  0.030702135640016774\n",
            "Loss S33:  0.029897617742593526\n",
            "Loss S44:  0.03394407052437813\n",
            "Loss S11:  0.031861579251470484\n",
            "Loss S22:  0.03075923228337949\n",
            "Loss S33:  0.029947959864254813\n",
            "Loss S44:  0.03397987781926084\n",
            "Loss S11:  0.03186749924383862\n",
            "Loss S22:  0.030759206802355058\n",
            "Loss S33:  0.02995966233469117\n",
            "Loss S44:  0.033980447535661505\n",
            "Loss S11:  0.03184503507777233\n",
            "Loss S22:  0.030761271585427706\n",
            "Loss S33:  0.029986149626807193\n",
            "Loss S44:  0.03394998272704841\n",
            "Loss S11:  0.031870001079558764\n",
            "Loss S22:  0.030795978726510187\n",
            "Loss S33:  0.02996822161449923\n",
            "Loss S44:  0.033965170030345285\n",
            "Loss S11:  0.03186098658964375\n",
            "Loss S22:  0.030822028890589243\n",
            "Loss S33:  0.02997537056717398\n",
            "Loss S44:  0.03396681161824934\n",
            "Loss S11:  0.031846795408498677\n",
            "Loss S22:  0.030818231774744017\n",
            "Loss S33:  0.02996446370465673\n",
            "Loss S44:  0.033940427069798176\n",
            "Loss S11:  0.03187762798270002\n",
            "Loss S22:  0.03087870277371644\n",
            "Loss S33:  0.02998312092791949\n",
            "Loss S44:  0.0339926007470402\n",
            "Loss S11:  0.03188071683405405\n",
            "Loss S22:  0.03085988808972427\n",
            "Loss S33:  0.029976521414589598\n",
            "Loss S44:  0.033991097016222924\n",
            "Loss S11:  0.031879994140445504\n",
            "Loss S22:  0.030839317888592396\n",
            "Loss S33:  0.029984634041329453\n",
            "Loss S44:  0.033994029714972125\n",
            "Loss S11:  0.031912968896081965\n",
            "Loss S22:  0.03083793832697112\n",
            "Loss S33:  0.02998570628409236\n",
            "Loss S44:  0.03398569066393639\n",
            "Loss S11:  0.03190918512695413\n",
            "Loss S22:  0.030845297672835535\n",
            "Loss S33:  0.02994961748264226\n",
            "Loss S44:  0.03399018967172854\n",
            "Loss S11:  0.031893818604014176\n",
            "Loss S22:  0.030840684823000553\n",
            "Loss S33:  0.029959837339588048\n",
            "Loss S44:  0.03399783343435153\n",
            "Loss S11:  0.031875097567209375\n",
            "Loss S22:  0.030854754774920966\n",
            "Loss S33:  0.029946193699276327\n",
            "Loss S44:  0.03399821482846309\n",
            "Loss S11:  0.03185957295885998\n",
            "Loss S22:  0.030824179812020045\n",
            "Loss S33:  0.029934465423992977\n",
            "Loss S44:  0.034002520245802366\n",
            "Loss S11:  0.03185204829660903\n",
            "Loss S22:  0.030808193255157858\n",
            "Loss S33:  0.02992153772939217\n",
            "Loss S44:  0.03398981135094834\n",
            "Loss S11:  0.03185137332025972\n",
            "Loss S22:  0.030815161674584507\n",
            "Loss S33:  0.02991515683330978\n",
            "Loss S44:  0.033997080651789996\n",
            "Loss S11:  0.03185154185725971\n",
            "Loss S22:  0.030805787710675048\n",
            "Loss S33:  0.02991767421015197\n",
            "Loss S44:  0.03400822640929229\n",
            "Loss S11:  0.031839524515164204\n",
            "Loss S22:  0.030801241941474104\n",
            "Loss S33:  0.029919956229690813\n",
            "Loss S44:  0.034019906259542515\n",
            "Loss S11:  0.03183746445727976\n",
            "Loss S22:  0.03080228885888558\n",
            "Loss S33:  0.029914756493844154\n",
            "Loss S44:  0.034016004869317086\n",
            "Loss S11:  0.03183774203262882\n",
            "Loss S22:  0.03079591949553181\n",
            "Loss S33:  0.02992263490539998\n",
            "Loss S44:  0.033974089677322586\n",
            "Loss S11:  0.03180912389784191\n",
            "Loss S22:  0.030781432629374696\n",
            "Loss S33:  0.029897393008542498\n",
            "Loss S44:  0.03393997358075121\n",
            "Loss S11:  0.031782627796463646\n",
            "Loss S22:  0.030749141098097767\n",
            "Loss S33:  0.029881093441449163\n",
            "Loss S44:  0.03390358548963924\n",
            "Loss S11:  0.031784675156051975\n",
            "Loss S22:  0.030741260273200913\n",
            "Loss S33:  0.029879035135531366\n",
            "Loss S44:  0.03388975127286596\n",
            "Loss S11:  0.03178416885018639\n",
            "Loss S22:  0.03073471454663288\n",
            "Loss S33:  0.02987535910141584\n",
            "Loss S44:  0.03386738924015032\n",
            "Loss S11:  0.03179009899390848\n",
            "Loss S22:  0.03074689400592496\n",
            "Loss S33:  0.029883918363485938\n",
            "Loss S44:  0.03385382924326242\n",
            "Loss S11:  0.03178480201402977\n",
            "Loss S22:  0.030739916618178726\n",
            "Loss S33:  0.029875081127591588\n",
            "Loss S44:  0.033831493916063465\n",
            "Loss S11:  0.031806232407689095\n",
            "Loss S22:  0.030747765738542387\n",
            "Loss S33:  0.029880384346491356\n",
            "Loss S44:  0.033830860811296236\n",
            "Loss S11:  0.0318081985762603\n",
            "Loss S22:  0.0307355731263129\n",
            "Loss S33:  0.029893812192748496\n",
            "Loss S44:  0.033832791399731076\n",
            "Loss S11:  0.031796463597525484\n",
            "Loss S22:  0.03073826302125759\n",
            "Loss S33:  0.029884645278987813\n",
            "Loss S44:  0.033838087987809275\n",
            "Loss S11:  0.03179025010555793\n",
            "Loss S22:  0.03074274485784478\n",
            "Loss S33:  0.029872011108573075\n",
            "Loss S44:  0.033838998008274224\n",
            "Loss S11:  0.031784238529986006\n",
            "Loss S22:  0.03073637255635926\n",
            "Loss S33:  0.029887593117021723\n",
            "Loss S44:  0.033832350102971594\n",
            "Loss S11:  0.031768733313090936\n",
            "Loss S22:  0.030722473218824615\n",
            "Loss S33:  0.02987067424170597\n",
            "Loss S44:  0.03383414652539975\n",
            "Validation: \n",
            " Loss S11:  0.0318334586918354\n",
            " Loss S22:  0.04484289884567261\n",
            " Loss S33:  0.03978721797466278\n",
            " Loss S44:  0.04298926144838333\n",
            " Loss S11:  0.03238264914779436\n",
            " Loss S22:  0.04320177684227625\n",
            " Loss S33:  0.043625615359771816\n",
            " Loss S44:  0.045105199373903726\n",
            " Loss S11:  0.0321926029353607\n",
            " Loss S22:  0.04283107544590787\n",
            " Loss S33:  0.04447709269276479\n",
            " Loss S44:  0.04495471088988025\n",
            " Loss S11:  0.032103172335468354\n",
            " Loss S22:  0.042659420581137544\n",
            " Loss S33:  0.044538350195669735\n",
            " Loss S44:  0.044581828058743084\n",
            " Loss S11:  0.03211158465732027\n",
            " Loss S22:  0.04258137113518185\n",
            " Loss S33:  0.04440687753168153\n",
            " Loss S44:  0.04455234400100178\n",
            "\n",
            "Epoch: 58\n",
            "Loss S11:  0.033355217427015305\n",
            "Loss S22:  0.03225288167595863\n",
            "Loss S33:  0.03488066419959068\n",
            "Loss S44:  0.038104794919490814\n",
            "Loss S11:  0.031605424867434936\n",
            "Loss S22:  0.03076233393089338\n",
            "Loss S33:  0.03030823560600931\n",
            "Loss S44:  0.03393840468065305\n",
            "Loss S11:  0.031408633948082014\n",
            "Loss S22:  0.030865079412857693\n",
            "Loss S33:  0.03000202358123802\n",
            "Loss S44:  0.033950657450727055\n",
            "Loss S11:  0.0315505009024374\n",
            "Loss S22:  0.03088897442625415\n",
            "Loss S33:  0.02992366194244354\n",
            "Loss S44:  0.03387413612536846\n",
            "Loss S11:  0.03142576946354494\n",
            "Loss S22:  0.030757023521312852\n",
            "Loss S33:  0.029933102338052377\n",
            "Loss S44:  0.03385989436107438\n",
            "Loss S11:  0.03154032626280598\n",
            "Loss S22:  0.030712825702685936\n",
            "Loss S33:  0.03002965139846007\n",
            "Loss S44:  0.033946160117492956\n",
            "Loss S11:  0.0315372997314715\n",
            "Loss S22:  0.030684540842155942\n",
            "Loss S33:  0.02992341368169081\n",
            "Loss S44:  0.033798768200346684\n",
            "Loss S11:  0.03160444817597598\n",
            "Loss S22:  0.030819889499058187\n",
            "Loss S33:  0.03003499835309848\n",
            "Loss S44:  0.03382992062350394\n",
            "Loss S11:  0.03164957659204065\n",
            "Loss S22:  0.030806796188339775\n",
            "Loss S33:  0.029912217462688316\n",
            "Loss S44:  0.03373522759863624\n",
            "Loss S11:  0.031644001828281434\n",
            "Loss S22:  0.030743726580352574\n",
            "Loss S33:  0.029860042547295382\n",
            "Loss S44:  0.03371498330526954\n",
            "Loss S11:  0.03153981736833506\n",
            "Loss S22:  0.03067013837102026\n",
            "Loss S33:  0.02973843926545417\n",
            "Loss S44:  0.03357965288923519\n",
            "Loss S11:  0.03150261008927414\n",
            "Loss S22:  0.03066424416327799\n",
            "Loss S33:  0.029761516900212916\n",
            "Loss S44:  0.033590112351350954\n",
            "Loss S11:  0.03153578345933236\n",
            "Loss S22:  0.030674135435588102\n",
            "Loss S33:  0.029891965818429782\n",
            "Loss S44:  0.0336318133044834\n",
            "Loss S11:  0.03154415942956018\n",
            "Loss S22:  0.03068043972642822\n",
            "Loss S33:  0.029917925709986505\n",
            "Loss S44:  0.033727436632145454\n",
            "Loss S11:  0.03156405098155035\n",
            "Loss S22:  0.030636492905253214\n",
            "Loss S33:  0.029880703776969133\n",
            "Loss S44:  0.03375585440625536\n",
            "Loss S11:  0.03155442026277252\n",
            "Loss S22:  0.030643756667904507\n",
            "Loss S33:  0.029852025186186595\n",
            "Loss S44:  0.03374037558570603\n",
            "Loss S11:  0.03154992957755646\n",
            "Loss S22:  0.03060934891611893\n",
            "Loss S33:  0.0297877049848715\n",
            "Loss S44:  0.03370756561063832\n",
            "Loss S11:  0.03159639811175957\n",
            "Loss S22:  0.03067993265930672\n",
            "Loss S33:  0.02977559589154539\n",
            "Loss S44:  0.03371741507224172\n",
            "Loss S11:  0.03158306357198657\n",
            "Loss S22:  0.030687717134287345\n",
            "Loss S33:  0.029814611760209938\n",
            "Loss S44:  0.03373338858396309\n",
            "Loss S11:  0.03156405119761747\n",
            "Loss S22:  0.03070321901462465\n",
            "Loss S33:  0.029825634688763095\n",
            "Loss S44:  0.033716120274435164\n",
            "Loss S11:  0.03153555235125829\n",
            "Loss S22:  0.030656442250037073\n",
            "Loss S33:  0.02981279067583938\n",
            "Loss S44:  0.03370101547878773\n",
            "Loss S11:  0.03157728861011035\n",
            "Loss S22:  0.03067464659528992\n",
            "Loss S33:  0.02984114872265201\n",
            "Loss S44:  0.033723816222630404\n",
            "Loss S11:  0.03160775599503949\n",
            "Loss S22:  0.03066444337974846\n",
            "Loss S33:  0.029883456386929182\n",
            "Loss S44:  0.03373470931099011\n",
            "Loss S11:  0.03159902106922168\n",
            "Loss S22:  0.03066694024482589\n",
            "Loss S33:  0.029874995899148833\n",
            "Loss S44:  0.033690145795608493\n",
            "Loss S11:  0.031680238684368825\n",
            "Loss S22:  0.030707763940776036\n",
            "Loss S33:  0.029925046836439505\n",
            "Loss S44:  0.0337234605490172\n",
            "Loss S11:  0.031637863452691005\n",
            "Loss S22:  0.03068975975135883\n",
            "Loss S33:  0.029901539788125046\n",
            "Loss S44:  0.03371235748300277\n",
            "Loss S11:  0.031615815749855794\n",
            "Loss S22:  0.0306900927861189\n",
            "Loss S33:  0.02990198372310149\n",
            "Loss S44:  0.03373706050331337\n",
            "Loss S11:  0.03162221994072309\n",
            "Loss S22:  0.03068569382288359\n",
            "Loss S33:  0.029904279356128174\n",
            "Loss S44:  0.03372796133463013\n",
            "Loss S11:  0.03161221633257603\n",
            "Loss S22:  0.030675814609669706\n",
            "Loss S33:  0.029900578536703068\n",
            "Loss S44:  0.03373370283147406\n",
            "Loss S11:  0.03162528611591591\n",
            "Loss S22:  0.030669653007306185\n",
            "Loss S33:  0.02990680138654111\n",
            "Loss S44:  0.03377047766806539\n",
            "Loss S11:  0.03160320935876069\n",
            "Loss S22:  0.030664315775185327\n",
            "Loss S33:  0.02988436587303382\n",
            "Loss S44:  0.03377158565452923\n",
            "Loss S11:  0.03158358209002823\n",
            "Loss S22:  0.030633027290991266\n",
            "Loss S33:  0.029890680122653387\n",
            "Loss S44:  0.033784540549783075\n",
            "Loss S11:  0.03159834977204554\n",
            "Loss S22:  0.030635985316610038\n",
            "Loss S33:  0.02988001141940879\n",
            "Loss S44:  0.03376426225719608\n",
            "Loss S11:  0.03157786429603655\n",
            "Loss S22:  0.030625113688431838\n",
            "Loss S33:  0.029864587165482816\n",
            "Loss S44:  0.03374187983122297\n",
            "Loss S11:  0.0315680475659734\n",
            "Loss S22:  0.030610999119771193\n",
            "Loss S33:  0.029863654261678894\n",
            "Loss S44:  0.033749332192662525\n",
            "Loss S11:  0.031558423226120805\n",
            "Loss S22:  0.030610145576446823\n",
            "Loss S33:  0.029889916709260722\n",
            "Loss S44:  0.03378601506501012\n",
            "Loss S11:  0.03155902584297505\n",
            "Loss S22:  0.030627711797313675\n",
            "Loss S33:  0.029912457860737957\n",
            "Loss S44:  0.03379591076100797\n",
            "Loss S11:  0.03153741398369366\n",
            "Loss S22:  0.03062550935422314\n",
            "Loss S33:  0.02989925492323955\n",
            "Loss S44:  0.03377064955081901\n",
            "Loss S11:  0.03152487793658662\n",
            "Loss S22:  0.030627818714602413\n",
            "Loss S33:  0.029894715351030582\n",
            "Loss S44:  0.033751332966165906\n",
            "Loss S11:  0.031492017438192195\n",
            "Loss S22:  0.0305989187382295\n",
            "Loss S33:  0.029866318940125463\n",
            "Loss S44:  0.03374602451749013\n",
            "Loss S11:  0.03149130390469273\n",
            "Loss S22:  0.03058566851657525\n",
            "Loss S33:  0.029876574135369195\n",
            "Loss S44:  0.033754469455217484\n",
            "Loss S11:  0.0314836494280184\n",
            "Loss S22:  0.030583589625547112\n",
            "Loss S33:  0.029875086366658952\n",
            "Loss S44:  0.03375663040001897\n",
            "Loss S11:  0.03149832949714819\n",
            "Loss S22:  0.03059582500729595\n",
            "Loss S33:  0.02990633513858086\n",
            "Loss S44:  0.033769178401121334\n",
            "Loss S11:  0.031491017431504766\n",
            "Loss S22:  0.030591873828053753\n",
            "Loss S33:  0.02988899601169639\n",
            "Loss S44:  0.03375313816950797\n",
            "Loss S11:  0.03150489411048608\n",
            "Loss S22:  0.03059125483846989\n",
            "Loss S33:  0.029892299518563578\n",
            "Loss S44:  0.03374405892443359\n",
            "Loss S11:  0.03150668123658507\n",
            "Loss S22:  0.03058756195306117\n",
            "Loss S33:  0.029893450025972927\n",
            "Loss S44:  0.033756764985604454\n",
            "Loss S11:  0.03149960552485672\n",
            "Loss S22:  0.03058073584014578\n",
            "Loss S33:  0.02990004365734449\n",
            "Loss S44:  0.03375334075274281\n",
            "Loss S11:  0.03150415537590181\n",
            "Loss S22:  0.03058409217088592\n",
            "Loss S33:  0.029894101439387935\n",
            "Loss S44:  0.03376797183304076\n",
            "Loss S11:  0.03150973615784531\n",
            "Loss S22:  0.03057680510975839\n",
            "Loss S33:  0.029900675825387427\n",
            "Loss S44:  0.0337633118110858\n",
            "Loss S11:  0.03150348213184511\n",
            "Loss S22:  0.03057447304596362\n",
            "Loss S33:  0.029887184003736\n",
            "Loss S44:  0.03373795235245024\n",
            "Validation: \n",
            " Loss S11:  0.030351106077432632\n",
            " Loss S22:  0.044764067977666855\n",
            " Loss S33:  0.03969085216522217\n",
            " Loss S44:  0.04373053461313248\n",
            " Loss S11:  0.03149878579591002\n",
            " Loss S22:  0.04400788318543207\n",
            " Loss S33:  0.04331054538488388\n",
            " Loss S44:  0.04550396047887348\n",
            " Loss S11:  0.03130737705746802\n",
            " Loss S22:  0.04389184904171199\n",
            " Loss S33:  0.04408375110204627\n",
            " Loss S44:  0.04532463912193368\n",
            " Loss S11:  0.03119390286871644\n",
            " Loss S22:  0.04372894623484768\n",
            " Loss S33:  0.044167843265611614\n",
            " Loss S44:  0.04505495433924628\n",
            " Loss S11:  0.03116226210086434\n",
            " Loss S22:  0.043666764081996164\n",
            " Loss S33:  0.04410172962112191\n",
            " Loss S44:  0.04505941211993312\n",
            "\n",
            "Epoch: 59\n",
            "Loss S11:  0.03351813182234764\n",
            "Loss S22:  0.03289884328842163\n",
            "Loss S33:  0.03189785033464432\n",
            "Loss S44:  0.03569575399160385\n",
            "Loss S11:  0.03051312691108747\n",
            "Loss S22:  0.03041771494529464\n",
            "Loss S33:  0.029797427525574512\n",
            "Loss S44:  0.03373936614529653\n",
            "Loss S11:  0.031149018378484817\n",
            "Loss S22:  0.030389218724199703\n",
            "Loss S33:  0.02960160481078284\n",
            "Loss S44:  0.03364779081727777\n",
            "Loss S11:  0.03129576717413241\n",
            "Loss S22:  0.030539529337998358\n",
            "Loss S33:  0.029656440380119508\n",
            "Loss S44:  0.03383815438757019\n",
            "Loss S11:  0.03115752080409992\n",
            "Loss S22:  0.030445533054994374\n",
            "Loss S33:  0.029612335308295924\n",
            "Loss S44:  0.03370706037413783\n",
            "Loss S11:  0.0312893819794351\n",
            "Loss S22:  0.030514391686986473\n",
            "Loss S33:  0.029828570783138275\n",
            "Loss S44:  0.03375136983745238\n",
            "Loss S11:  0.03137856062318458\n",
            "Loss S22:  0.03062120880015561\n",
            "Loss S33:  0.029778596319136073\n",
            "Loss S44:  0.03361648544058448\n",
            "Loss S11:  0.031412920350549926\n",
            "Loss S22:  0.030722367249324287\n",
            "Loss S33:  0.029800520999960497\n",
            "Loss S44:  0.03366824362794278\n",
            "Loss S11:  0.03139591157252406\n",
            "Loss S22:  0.030584756698873308\n",
            "Loss S33:  0.02971965401076976\n",
            "Loss S44:  0.03358103785617852\n",
            "Loss S11:  0.03141465191575852\n",
            "Loss S22:  0.03054795700770158\n",
            "Loss S33:  0.029688547089040934\n",
            "Loss S44:  0.033553035459020636\n",
            "Loss S11:  0.03133046569874381\n",
            "Loss S22:  0.03048347104525212\n",
            "Loss S33:  0.029617616648573687\n",
            "Loss S44:  0.03345403768638573\n",
            "Loss S11:  0.031353840117787454\n",
            "Loss S22:  0.03051904028466156\n",
            "Loss S33:  0.029636315833609383\n",
            "Loss S44:  0.033360125077468855\n",
            "Loss S11:  0.03142867631409779\n",
            "Loss S22:  0.0305264077249391\n",
            "Loss S33:  0.029726653690796252\n",
            "Loss S44:  0.03345621768231234\n",
            "Loss S11:  0.031446250738533396\n",
            "Loss S22:  0.030501113138126052\n",
            "Loss S33:  0.02974268122938753\n",
            "Loss S44:  0.03354934312915074\n",
            "Loss S11:  0.031405880386736376\n",
            "Loss S22:  0.030421040833313415\n",
            "Loss S33:  0.029739976814346956\n",
            "Loss S44:  0.03352677265961542\n",
            "Loss S11:  0.03141873051097851\n",
            "Loss S22:  0.030422233661871082\n",
            "Loss S33:  0.029790312532853608\n",
            "Loss S44:  0.03355722737480078\n",
            "Loss S11:  0.03137479558216859\n",
            "Loss S22:  0.03039606472026117\n",
            "Loss S33:  0.029755336434944817\n",
            "Loss S44:  0.033550227003067916\n",
            "Loss S11:  0.03140587753981178\n",
            "Loss S22:  0.030441185405031282\n",
            "Loss S33:  0.029761069699337606\n",
            "Loss S44:  0.03357128572394276\n",
            "Loss S11:  0.03142552654058235\n",
            "Loss S22:  0.030469205765599045\n",
            "Loss S33:  0.029806939493967684\n",
            "Loss S44:  0.033678688088339334\n",
            "Loss S11:  0.031421732411022585\n",
            "Loss S22:  0.030485319451511842\n",
            "Loss S33:  0.029814341315190205\n",
            "Loss S44:  0.033703729754109034\n",
            "Loss S11:  0.03139203428221283\n",
            "Loss S22:  0.03046358244224864\n",
            "Loss S33:  0.029791733898704324\n",
            "Loss S44:  0.03371697146241641\n",
            "Loss S11:  0.03140631534371048\n",
            "Loss S22:  0.030485264639153863\n",
            "Loss S33:  0.029768047863132016\n",
            "Loss S44:  0.03371084876076023\n",
            "Loss S11:  0.031409222143807564\n",
            "Loss S22:  0.030472031520570025\n",
            "Loss S33:  0.02980123790680553\n",
            "Loss S44:  0.03370581472890949\n",
            "Loss S11:  0.03138809479386002\n",
            "Loss S22:  0.030476316745147045\n",
            "Loss S33:  0.029783158067178416\n",
            "Loss S44:  0.03368846011787524\n",
            "Loss S11:  0.03143975469457658\n",
            "Loss S22:  0.0305113547184037\n",
            "Loss S33:  0.029793838479583196\n",
            "Loss S44:  0.03371210519178033\n",
            "Loss S11:  0.03143541388718255\n",
            "Loss S22:  0.030503345727623695\n",
            "Loss S33:  0.029770989074887507\n",
            "Loss S44:  0.03367572447722889\n",
            "Loss S11:  0.0314525804514515\n",
            "Loss S22:  0.030495054698709785\n",
            "Loss S33:  0.0297736885147419\n",
            "Loss S44:  0.0336703695071383\n",
            "Loss S11:  0.031463791990852\n",
            "Loss S22:  0.030501674475815022\n",
            "Loss S33:  0.029789681728530634\n",
            "Loss S44:  0.033675143282131954\n",
            "Loss S11:  0.0314884563888095\n",
            "Loss S22:  0.03052698157303486\n",
            "Loss S33:  0.029784514863720146\n",
            "Loss S44:  0.03369132219896622\n",
            "Loss S11:  0.03149818119519355\n",
            "Loss S22:  0.030519014371629433\n",
            "Loss S33:  0.029806781598587625\n",
            "Loss S44:  0.033737859954334204\n",
            "Loss S11:  0.031494815651315\n",
            "Loss S22:  0.030522395327588252\n",
            "Loss S33:  0.029802215028789353\n",
            "Loss S44:  0.033722223757311355\n",
            "Loss S11:  0.03149018387558759\n",
            "Loss S22:  0.030535182301733656\n",
            "Loss S33:  0.029803486570859645\n",
            "Loss S44:  0.0337411194105432\n",
            "Loss S11:  0.03148311852967813\n",
            "Loss S22:  0.030518532060434884\n",
            "Loss S33:  0.02978067091924379\n",
            "Loss S44:  0.03370980732723189\n",
            "Loss S11:  0.03147190456979224\n",
            "Loss S22:  0.03049572311296744\n",
            "Loss S33:  0.02976586946547932\n",
            "Loss S44:  0.033699490456332616\n",
            "Loss S11:  0.03149699213261828\n",
            "Loss S22:  0.030472615406004565\n",
            "Loss S33:  0.02975298481231386\n",
            "Loss S44:  0.03369615763928359\n",
            "Loss S11:  0.03149842791938544\n",
            "Loss S22:  0.030462415363544073\n",
            "Loss S33:  0.02976214779578043\n",
            "Loss S44:  0.033719042454052855\n",
            "Loss S11:  0.03149818528928585\n",
            "Loss S22:  0.030473880051864813\n",
            "Loss S33:  0.029765916018233404\n",
            "Loss S44:  0.033718810068479536\n",
            "Loss S11:  0.03146132076224387\n",
            "Loss S22:  0.030450781375531238\n",
            "Loss S33:  0.029758431449171025\n",
            "Loss S44:  0.0336739726947003\n",
            "Loss S11:  0.031442576437562784\n",
            "Loss S22:  0.03045552249831634\n",
            "Loss S33:  0.029759009952933145\n",
            "Loss S44:  0.03365900132345559\n",
            "Loss S11:  0.031420831342258725\n",
            "Loss S22:  0.03042836208134661\n",
            "Loss S33:  0.029740455515130097\n",
            "Loss S44:  0.03363897212688118\n",
            "Loss S11:  0.031417714914961946\n",
            "Loss S22:  0.030404352743243636\n",
            "Loss S33:  0.029738552445671206\n",
            "Loss S44:  0.03362910959061393\n",
            "Loss S11:  0.03141817932273205\n",
            "Loss S22:  0.030400730361323576\n",
            "Loss S33:  0.029754261107811673\n",
            "Loss S44:  0.03362050294024086\n",
            "Loss S11:  0.031420007281695576\n",
            "Loss S22:  0.030412215252811722\n",
            "Loss S33:  0.029758256441977698\n",
            "Loss S44:  0.03363142258569492\n",
            "Loss S11:  0.03141830879287349\n",
            "Loss S22:  0.03040865216222689\n",
            "Loss S33:  0.02973758683403794\n",
            "Loss S44:  0.033625671156010055\n",
            "Loss S11:  0.03142920558820236\n",
            "Loss S22:  0.030407399281251187\n",
            "Loss S33:  0.029737898501273997\n",
            "Loss S44:  0.03361210878727252\n",
            "Loss S11:  0.03143390171444601\n",
            "Loss S22:  0.030405120593712493\n",
            "Loss S33:  0.029739934492633242\n",
            "Loss S44:  0.033611377498791645\n",
            "Loss S11:  0.031434405797086416\n",
            "Loss S22:  0.030407710675626927\n",
            "Loss S33:  0.029740449222948183\n",
            "Loss S44:  0.033621411560479056\n",
            "Loss S11:  0.03144420958232854\n",
            "Loss S22:  0.030413312054458696\n",
            "Loss S33:  0.02973536585087862\n",
            "Loss S44:  0.03362990341558578\n",
            "Loss S11:  0.0314498126328673\n",
            "Loss S22:  0.030408824486135196\n",
            "Loss S33:  0.029746614264909047\n",
            "Loss S44:  0.03363038638215551\n",
            "Loss S11:  0.0314292907525165\n",
            "Loss S22:  0.030394991491816924\n",
            "Loss S33:  0.029727181776326203\n",
            "Loss S44:  0.033629520680535346\n",
            "Validation: \n",
            " Loss S11:  0.030651070177555084\n",
            " Loss S22:  0.04415358603000641\n",
            " Loss S33:  0.03989724814891815\n",
            " Loss S44:  0.043103475123643875\n",
            " Loss S11:  0.03172899197254862\n",
            " Loss S22:  0.04305372901615642\n",
            " Loss S33:  0.04314820503904706\n",
            " Loss S44:  0.04530247034771102\n",
            " Loss S11:  0.03158037596177764\n",
            " Loss S22:  0.04278923989069171\n",
            " Loss S33:  0.044089974517502434\n",
            " Loss S44:  0.045132704533454845\n",
            " Loss S11:  0.03141651867476643\n",
            " Loss S22:  0.04257098664758635\n",
            " Loss S33:  0.044147604496264065\n",
            " Loss S44:  0.04486106323902724\n",
            " Loss S11:  0.03142098070662699\n",
            " Loss S22:  0.04250426107534656\n",
            " Loss S33:  0.04403752769217079\n",
            " Loss S44:  0.044784805794924866\n",
            "\n",
            "Epoch: 60\n",
            "Loss S11:  0.03328883275389671\n",
            "Loss S22:  0.030810827389359474\n",
            "Loss S33:  0.03152240812778473\n",
            "Loss S44:  0.034723520278930664\n",
            "Loss S11:  0.031009421260519462\n",
            "Loss S22:  0.02973506904461167\n",
            "Loss S33:  0.029500334946946663\n",
            "Loss S44:  0.03335199352692474\n",
            "Loss S11:  0.031236799877314342\n",
            "Loss S22:  0.030306357890367508\n",
            "Loss S33:  0.029692101070568674\n",
            "Loss S44:  0.03368328006139824\n",
            "Loss S11:  0.031319155147479426\n",
            "Loss S22:  0.03062336921932236\n",
            "Loss S33:  0.029661448371987188\n",
            "Loss S44:  0.033619809835668535\n",
            "Loss S11:  0.03130764364287621\n",
            "Loss S22:  0.030590864625282405\n",
            "Loss S33:  0.029493703211589558\n",
            "Loss S44:  0.0336154647989244\n",
            "Loss S11:  0.03137865220653076\n",
            "Loss S22:  0.030563287217827403\n",
            "Loss S33:  0.02952929849133772\n",
            "Loss S44:  0.03363275502388384\n",
            "Loss S11:  0.03148509806296865\n",
            "Loss S22:  0.030617470043848773\n",
            "Loss S33:  0.029545648359372966\n",
            "Loss S44:  0.03355009353063146\n",
            "Loss S11:  0.031459853472843975\n",
            "Loss S22:  0.03067475192668572\n",
            "Loss S33:  0.029539960370936865\n",
            "Loss S44:  0.033555104541526715\n",
            "Loss S11:  0.031387722165680226\n",
            "Loss S22:  0.030639406385613077\n",
            "Loss S33:  0.02946324221053977\n",
            "Loss S44:  0.033447134848914026\n",
            "Loss S11:  0.031353257494149626\n",
            "Loss S22:  0.030629166637311925\n",
            "Loss S33:  0.029456542497807808\n",
            "Loss S44:  0.03340100259571285\n",
            "Loss S11:  0.03127806264870238\n",
            "Loss S22:  0.03060221884141464\n",
            "Loss S33:  0.02939762057054161\n",
            "Loss S44:  0.033292147801211563\n",
            "Loss S11:  0.031228263417745497\n",
            "Loss S22:  0.03054146057514994\n",
            "Loss S33:  0.029423567307022242\n",
            "Loss S44:  0.03322427746680406\n",
            "Loss S11:  0.03128791529654471\n",
            "Loss S22:  0.03057173511767683\n",
            "Loss S33:  0.02952783060652658\n",
            "Loss S44:  0.03323011323501748\n",
            "Loss S11:  0.03131660905787508\n",
            "Loss S22:  0.03064078813461402\n",
            "Loss S33:  0.029609140008687973\n",
            "Loss S44:  0.03337435673154492\n",
            "Loss S11:  0.031322311002311976\n",
            "Loss S22:  0.030594648212088762\n",
            "Loss S33:  0.029594490648055752\n",
            "Loss S44:  0.03335408851874213\n",
            "Loss S11:  0.03130090647432583\n",
            "Loss S22:  0.03061782814167588\n",
            "Loss S33:  0.029572818761333726\n",
            "Loss S44:  0.03332944659651905\n",
            "Loss S11:  0.03127565225644141\n",
            "Loss S22:  0.030555833848366826\n",
            "Loss S33:  0.029538642314280043\n",
            "Loss S44:  0.03329812564094615\n",
            "Loss S11:  0.031309440470578376\n",
            "Loss S22:  0.03056730645877576\n",
            "Loss S33:  0.02954876042728187\n",
            "Loss S44:  0.03332683725053804\n",
            "Loss S11:  0.031307867959718017\n",
            "Loss S22:  0.030572464051579245\n",
            "Loss S33:  0.029577096008299462\n",
            "Loss S44:  0.033386423195923234\n",
            "Loss S11:  0.031305530425453684\n",
            "Loss S22:  0.03056789411924272\n",
            "Loss S33:  0.029601371188828458\n",
            "Loss S44:  0.033383404699529655\n",
            "Loss S11:  0.031278410463116656\n",
            "Loss S22:  0.03051843677661312\n",
            "Loss S33:  0.029627372450496427\n",
            "Loss S44:  0.03338159875601382\n",
            "Loss S11:  0.031302492721315245\n",
            "Loss S22:  0.0305286926963318\n",
            "Loss S33:  0.029624083701750677\n",
            "Loss S44:  0.033385562723705554\n",
            "Loss S11:  0.031306818279340794\n",
            "Loss S22:  0.030553636179048552\n",
            "Loss S33:  0.02964069187742283\n",
            "Loss S44:  0.03341491812022563\n",
            "Loss S11:  0.03130800972059691\n",
            "Loss S22:  0.03054566594300332\n",
            "Loss S33:  0.029615927302153594\n",
            "Loss S44:  0.033385889975604044\n",
            "Loss S11:  0.031362022478056154\n",
            "Loss S22:  0.030577291274651947\n",
            "Loss S33:  0.029649421248074882\n",
            "Loss S44:  0.03342877846999535\n",
            "Loss S11:  0.031350459087179955\n",
            "Loss S22:  0.030544218410593106\n",
            "Loss S33:  0.029647487531085887\n",
            "Loss S44:  0.03342132986452712\n",
            "Loss S11:  0.031366355217622155\n",
            "Loss S22:  0.030545514498005882\n",
            "Loss S33:  0.029666925006157138\n",
            "Loss S44:  0.033442012016515164\n",
            "Loss S11:  0.03139326112461706\n",
            "Loss S22:  0.030567321135933988\n",
            "Loss S33:  0.029669558416323467\n",
            "Loss S44:  0.03344232226787238\n",
            "Loss S11:  0.03141198998641925\n",
            "Loss S22:  0.030591170008549485\n",
            "Loss S33:  0.029687416325472428\n",
            "Loss S44:  0.03346502186326158\n",
            "Loss S11:  0.03142543368143929\n",
            "Loss S22:  0.030587543358968704\n",
            "Loss S33:  0.029696358905471478\n",
            "Loss S44:  0.0334844050858541\n",
            "Loss S11:  0.03141761019801183\n",
            "Loss S22:  0.030576736681732625\n",
            "Loss S33:  0.02968526454611474\n",
            "Loss S44:  0.03346939297683809\n",
            "Loss S11:  0.031406613617800054\n",
            "Loss S22:  0.030547171118414672\n",
            "Loss S33:  0.0296663741230773\n",
            "Loss S44:  0.0334917526045317\n",
            "Loss S11:  0.03138945436807251\n",
            "Loss S22:  0.03051486502276774\n",
            "Loss S33:  0.029637609744833267\n",
            "Loss S44:  0.03349536096218776\n",
            "Loss S11:  0.0313808801682424\n",
            "Loss S22:  0.030511111561265236\n",
            "Loss S33:  0.029616260102013087\n",
            "Loss S44:  0.03348002860778408\n",
            "Loss S11:  0.031395231928031814\n",
            "Loss S22:  0.030501855078461933\n",
            "Loss S33:  0.029617115074107723\n",
            "Loss S44:  0.03349928636529928\n",
            "Loss S11:  0.031400686129927635\n",
            "Loss S22:  0.030487644764730055\n",
            "Loss S33:  0.029649704063252846\n",
            "Loss S44:  0.03352439297930977\n",
            "Loss S11:  0.03142088848240983\n",
            "Loss S22:  0.030484172275645913\n",
            "Loss S33:  0.029656346902829128\n",
            "Loss S44:  0.03351838792315646\n",
            "Loss S11:  0.03139922209586095\n",
            "Loss S22:  0.03047044962303176\n",
            "Loss S33:  0.029645200286592756\n",
            "Loss S44:  0.03349262336896597\n",
            "Loss S11:  0.03138760226507356\n",
            "Loss S22:  0.03047499850666116\n",
            "Loss S33:  0.029625616376129347\n",
            "Loss S44:  0.03347177822582834\n",
            "Loss S11:  0.03136534593961275\n",
            "Loss S22:  0.030436482432934328\n",
            "Loss S33:  0.02959487499559627\n",
            "Loss S44:  0.033438952210957135\n",
            "Loss S11:  0.03136437839626374\n",
            "Loss S22:  0.03040869889822684\n",
            "Loss S33:  0.029595050045714118\n",
            "Loss S44:  0.033439626266942954\n",
            "Loss S11:  0.031361834071304676\n",
            "Loss S22:  0.03041262010116937\n",
            "Loss S33:  0.02958547788470476\n",
            "Loss S44:  0.03342873760365367\n",
            "Loss S11:  0.03138778545847273\n",
            "Loss S22:  0.030422236593418053\n",
            "Loss S33:  0.02961059198703732\n",
            "Loss S44:  0.0334519126945819\n",
            "Loss S11:  0.03137747842706135\n",
            "Loss S22:  0.030433307905311926\n",
            "Loss S33:  0.02959589834763666\n",
            "Loss S44:  0.03343690796528366\n",
            "Loss S11:  0.03139009538303022\n",
            "Loss S22:  0.030449016788412653\n",
            "Loss S33:  0.029609628228677645\n",
            "Loss S44:  0.033460127650137124\n",
            "Loss S11:  0.03140101303860644\n",
            "Loss S22:  0.030457328616779817\n",
            "Loss S33:  0.0296323213957564\n",
            "Loss S44:  0.03345692433218602\n",
            "Loss S11:  0.03138775083513167\n",
            "Loss S22:  0.030462136722438506\n",
            "Loss S33:  0.029644708425905594\n",
            "Loss S44:  0.03346722422333317\n",
            "Loss S11:  0.03139791362019228\n",
            "Loss S22:  0.030459245077617103\n",
            "Loss S33:  0.02964880041809725\n",
            "Loss S44:  0.03346682073758033\n",
            "Loss S11:  0.031392799581937386\n",
            "Loss S22:  0.030450617178128317\n",
            "Loss S33:  0.029658315780721683\n",
            "Loss S44:  0.033447052388918624\n",
            "Loss S11:  0.031381025584130084\n",
            "Loss S22:  0.030442316301333927\n",
            "Loss S33:  0.029648217382080202\n",
            "Loss S44:  0.03343181170963221\n",
            "Validation: \n",
            " Loss S11:  0.030152596533298492\n",
            " Loss S22:  0.045147597789764404\n",
            " Loss S33:  0.0398804135620594\n",
            " Loss S44:  0.0426865890622139\n",
            " Loss S11:  0.03113166464581376\n",
            " Loss S22:  0.043589825608900616\n",
            " Loss S33:  0.04302133424651055\n",
            " Loss S44:  0.04491814687138512\n",
            " Loss S11:  0.03103450772057219\n",
            " Loss S22:  0.04331641171763583\n",
            " Loss S33:  0.0437390705252566\n",
            " Loss S44:  0.04485999438457373\n",
            " Loss S11:  0.030912519050914734\n",
            " Loss S22:  0.04319080606591506\n",
            " Loss S33:  0.04376402875927628\n",
            " Loss S44:  0.0446049227211319\n",
            " Loss S11:  0.030895537708276584\n",
            " Loss S22:  0.04321604134675897\n",
            " Loss S33:  0.04371367726061079\n",
            " Loss S44:  0.04454512894153595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net2(nn.Module):\n",
        "    def __init__(self, s11,s22,s33,s44):\n",
        "        super(Net2, self).__init__()\n",
        "        self.s11 = s11\n",
        "        self.s22 = s22\n",
        "        self.s33 = s33\n",
        "        self.s44 = s44\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.s11(x)\n",
        "        out2 = self.s22(x)\n",
        "        out3 = self.s33(x)\n",
        "        out4 = self.s44(x)\n",
        "\n",
        "        out = torch.cat((out1,out2,out3,out4),1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "net4students = Net2(s11,s22,s33,s44)\n",
        "net4students.to(device)\n",
        "summary(net4students, (3, 32, 32))"
      ],
      "metadata": {
        "id": "HuuyhtB_NQoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f66ba6c-2ea3-4de6-fed8-ab4112959b82"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "              VGG-32                  [-1, 128]               0\n",
            "           Conv2d-33           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-34           [-1, 32, 32, 32]              64\n",
            "             ReLU-35           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-36           [-1, 32, 16, 16]               0\n",
            "           Conv2d-37           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-38           [-1, 32, 16, 16]              64\n",
            "             ReLU-39           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-40             [-1, 32, 8, 8]               0\n",
            "           Conv2d-41             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-42             [-1, 64, 8, 8]             128\n",
            "             ReLU-43             [-1, 64, 8, 8]               0\n",
            "           Conv2d-44             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-45             [-1, 64, 8, 8]             128\n",
            "             ReLU-46             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-47             [-1, 64, 4, 4]               0\n",
            "           Conv2d-48            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-49            [-1, 128, 4, 4]             256\n",
            "             ReLU-50            [-1, 128, 4, 4]               0\n",
            "           Conv2d-51            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-52            [-1, 128, 4, 4]             256\n",
            "             ReLU-53            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-54            [-1, 128, 2, 2]               0\n",
            "           Conv2d-55            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-56            [-1, 128, 2, 2]             256\n",
            "             ReLU-57            [-1, 128, 2, 2]               0\n",
            "           Conv2d-58            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-59            [-1, 128, 2, 2]             256\n",
            "             ReLU-60            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-61            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-62            [-1, 128, 1, 1]               0\n",
            "           Linear-63                  [-1, 128]          16,512\n",
            "              VGG-64                  [-1, 128]               0\n",
            "           Conv2d-65           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-66           [-1, 32, 32, 32]              64\n",
            "             ReLU-67           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-68           [-1, 32, 16, 16]               0\n",
            "           Conv2d-69           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-70           [-1, 32, 16, 16]              64\n",
            "             ReLU-71           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-72             [-1, 32, 8, 8]               0\n",
            "           Conv2d-73             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-74             [-1, 64, 8, 8]             128\n",
            "             ReLU-75             [-1, 64, 8, 8]               0\n",
            "           Conv2d-76             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-77             [-1, 64, 8, 8]             128\n",
            "             ReLU-78             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-79             [-1, 64, 4, 4]               0\n",
            "           Conv2d-80            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-81            [-1, 128, 4, 4]             256\n",
            "             ReLU-82            [-1, 128, 4, 4]               0\n",
            "           Conv2d-83            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-84            [-1, 128, 4, 4]             256\n",
            "             ReLU-85            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-86            [-1, 128, 2, 2]               0\n",
            "           Conv2d-87            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-88            [-1, 128, 2, 2]             256\n",
            "             ReLU-89            [-1, 128, 2, 2]               0\n",
            "           Conv2d-90            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-91            [-1, 128, 2, 2]             256\n",
            "             ReLU-92            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-93            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-94            [-1, 128, 1, 1]               0\n",
            "           Linear-95                  [-1, 128]          16,512\n",
            "              VGG-96                  [-1, 128]               0\n",
            "           Conv2d-97           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-98           [-1, 32, 32, 32]              64\n",
            "             ReLU-99           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-100           [-1, 32, 16, 16]               0\n",
            "          Conv2d-101           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-102           [-1, 32, 16, 16]              64\n",
            "            ReLU-103           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-104             [-1, 32, 8, 8]               0\n",
            "          Conv2d-105             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-106             [-1, 64, 8, 8]             128\n",
            "            ReLU-107             [-1, 64, 8, 8]               0\n",
            "          Conv2d-108             [-1, 64, 8, 8]          36,928\n",
            "     BatchNorm2d-109             [-1, 64, 8, 8]             128\n",
            "            ReLU-110             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-111             [-1, 64, 4, 4]               0\n",
            "          Conv2d-112            [-1, 128, 4, 4]          73,856\n",
            "     BatchNorm2d-113            [-1, 128, 4, 4]             256\n",
            "            ReLU-114            [-1, 128, 4, 4]               0\n",
            "          Conv2d-115            [-1, 128, 4, 4]         147,584\n",
            "     BatchNorm2d-116            [-1, 128, 4, 4]             256\n",
            "            ReLU-117            [-1, 128, 4, 4]               0\n",
            "       MaxPool2d-118            [-1, 128, 2, 2]               0\n",
            "          Conv2d-119            [-1, 128, 2, 2]         147,584\n",
            "     BatchNorm2d-120            [-1, 128, 2, 2]             256\n",
            "            ReLU-121            [-1, 128, 2, 2]               0\n",
            "          Conv2d-122            [-1, 128, 2, 2]         147,584\n",
            "     BatchNorm2d-123            [-1, 128, 2, 2]             256\n",
            "            ReLU-124            [-1, 128, 2, 2]               0\n",
            "       MaxPool2d-125            [-1, 128, 1, 1]               0\n",
            "       AvgPool2d-126            [-1, 128, 1, 1]               0\n",
            "          Linear-127                  [-1, 128]          16,512\n",
            "             VGG-128                  [-1, 128]               0\n",
            "          Linear-129                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 2,405,514\n",
            "Trainable params: 2,405,514\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 5.34\n",
            "Params size (MB): 9.18\n",
            "Estimated Total Size (MB): 14.53\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for m in net4students.modules():\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        m.weight.requires_grad = False\n",
        "        if m.bias is not None:\n",
        "            m.bias.requires_grad = False\n",
        "net4students = net4students.to(device)\n",
        "summary(net4students, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMUeIq5X02oI",
        "outputId": "65c4f3db-7277-437e-fef3-a39356e1cbd6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "              VGG-32                  [-1, 128]               0\n",
            "           Conv2d-33           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-34           [-1, 32, 32, 32]              64\n",
            "             ReLU-35           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-36           [-1, 32, 16, 16]               0\n",
            "           Conv2d-37           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-38           [-1, 32, 16, 16]              64\n",
            "             ReLU-39           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-40             [-1, 32, 8, 8]               0\n",
            "           Conv2d-41             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-42             [-1, 64, 8, 8]             128\n",
            "             ReLU-43             [-1, 64, 8, 8]               0\n",
            "           Conv2d-44             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-45             [-1, 64, 8, 8]             128\n",
            "             ReLU-46             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-47             [-1, 64, 4, 4]               0\n",
            "           Conv2d-48            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-49            [-1, 128, 4, 4]             256\n",
            "             ReLU-50            [-1, 128, 4, 4]               0\n",
            "           Conv2d-51            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-52            [-1, 128, 4, 4]             256\n",
            "             ReLU-53            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-54            [-1, 128, 2, 2]               0\n",
            "           Conv2d-55            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-56            [-1, 128, 2, 2]             256\n",
            "             ReLU-57            [-1, 128, 2, 2]               0\n",
            "           Conv2d-58            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-59            [-1, 128, 2, 2]             256\n",
            "             ReLU-60            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-61            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-62            [-1, 128, 1, 1]               0\n",
            "           Linear-63                  [-1, 128]          16,512\n",
            "              VGG-64                  [-1, 128]               0\n",
            "           Conv2d-65           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-66           [-1, 32, 32, 32]              64\n",
            "             ReLU-67           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-68           [-1, 32, 16, 16]               0\n",
            "           Conv2d-69           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-70           [-1, 32, 16, 16]              64\n",
            "             ReLU-71           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-72             [-1, 32, 8, 8]               0\n",
            "           Conv2d-73             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-74             [-1, 64, 8, 8]             128\n",
            "             ReLU-75             [-1, 64, 8, 8]               0\n",
            "           Conv2d-76             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-77             [-1, 64, 8, 8]             128\n",
            "             ReLU-78             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-79             [-1, 64, 4, 4]               0\n",
            "           Conv2d-80            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-81            [-1, 128, 4, 4]             256\n",
            "             ReLU-82            [-1, 128, 4, 4]               0\n",
            "           Conv2d-83            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-84            [-1, 128, 4, 4]             256\n",
            "             ReLU-85            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-86            [-1, 128, 2, 2]               0\n",
            "           Conv2d-87            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-88            [-1, 128, 2, 2]             256\n",
            "             ReLU-89            [-1, 128, 2, 2]               0\n",
            "           Conv2d-90            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-91            [-1, 128, 2, 2]             256\n",
            "             ReLU-92            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-93            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-94            [-1, 128, 1, 1]               0\n",
            "           Linear-95                  [-1, 128]          16,512\n",
            "              VGG-96                  [-1, 128]               0\n",
            "           Conv2d-97           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-98           [-1, 32, 32, 32]              64\n",
            "             ReLU-99           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-100           [-1, 32, 16, 16]               0\n",
            "          Conv2d-101           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-102           [-1, 32, 16, 16]              64\n",
            "            ReLU-103           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-104             [-1, 32, 8, 8]               0\n",
            "          Conv2d-105             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-106             [-1, 64, 8, 8]             128\n",
            "            ReLU-107             [-1, 64, 8, 8]               0\n",
            "          Conv2d-108             [-1, 64, 8, 8]          36,928\n",
            "     BatchNorm2d-109             [-1, 64, 8, 8]             128\n",
            "            ReLU-110             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-111             [-1, 64, 4, 4]               0\n",
            "          Conv2d-112            [-1, 128, 4, 4]          73,856\n",
            "     BatchNorm2d-113            [-1, 128, 4, 4]             256\n",
            "            ReLU-114            [-1, 128, 4, 4]               0\n",
            "          Conv2d-115            [-1, 128, 4, 4]         147,584\n",
            "     BatchNorm2d-116            [-1, 128, 4, 4]             256\n",
            "            ReLU-117            [-1, 128, 4, 4]               0\n",
            "       MaxPool2d-118            [-1, 128, 2, 2]               0\n",
            "          Conv2d-119            [-1, 128, 2, 2]         147,584\n",
            "     BatchNorm2d-120            [-1, 128, 2, 2]             256\n",
            "            ReLU-121            [-1, 128, 2, 2]               0\n",
            "          Conv2d-122            [-1, 128, 2, 2]         147,584\n",
            "     BatchNorm2d-123            [-1, 128, 2, 2]             256\n",
            "            ReLU-124            [-1, 128, 2, 2]               0\n",
            "       MaxPool2d-125            [-1, 128, 1, 1]               0\n",
            "       AvgPool2d-126            [-1, 128, 1, 1]               0\n",
            "          Linear-127                  [-1, 128]          16,512\n",
            "             VGG-128                  [-1, 128]               0\n",
            "          Linear-129                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 2,405,514\n",
            "Trainable params: 76,810\n",
            "Non-trainable params: 2,328,704\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 5.34\n",
            "Params size (MB): 9.18\n",
            "Estimated Total Size (MB): 14.53\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net4students.parameters(), lr=0.0001)\n",
        "\n",
        "def train41(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    net4students.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        net4students.zero_grad()\n",
        "        outputs = net4students(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if(batch_idx % 200 == 0):\n",
        "          print(\"Accuracy : \",100.*correct/total,\" Loss : \", train_loss/(batch_idx+1))\n",
        "def test42(epoch):\n",
        "    net4students.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net4students(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\"Accuracy : \",100.*correct/total,\" Loss : \", test_loss/(batch_idx+1))"
      ],
      "metadata": {
        "id": "Nl6WfyLk04H7"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "    train41(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test42(epoch)"
      ],
      "metadata": {
        "id": "wi8hV35xQ6Fg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "064ff896-348d-4bb2-e459-45c37b4caa58"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Accuracy :  15.0  Loss :  2.5618889331817627\n",
            "Accuracy :  71.58208955223881  Loss :  1.2484832990228834\n",
            "Accuracy :  77.70822942643392  Loss :  0.9189815053024197\n",
            "Validation: \n",
            "Accuracy :  85.0  Loss :  0.48621246218681335\n",
            "Accuracy :  83.47619047619048  Loss :  0.5095390464578357\n",
            "Accuracy :  83.14634146341463  Loss :  0.5205037019601683\n",
            "Accuracy :  83.26229508196721  Loss :  0.5166281774395802\n",
            "Accuracy :  83.19753086419753  Loss :  0.5159508173848376\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy :  82.0  Loss :  0.5183069705963135\n",
            "Accuracy :  83.98507462686567  Loss :  0.48493431945938376\n",
            "Accuracy :  84.12967581047381  Loss :  0.4755582383594608\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4412035346031189\n",
            "Accuracy :  83.85714285714286  Loss :  0.47924554347991943\n",
            "Accuracy :  83.53658536585365  Loss :  0.4910191101271932\n",
            "Accuracy :  83.50819672131148  Loss :  0.48640326251749133\n",
            "Accuracy :  83.41975308641975  Loss :  0.48535665464989936\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy :  85.0  Loss :  0.4311978220939636\n",
            "Accuracy :  84.11940298507463  Loss :  0.460424061734878\n",
            "Accuracy :  84.24438902743142  Loss :  0.4564585564959971\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4306144416332245\n",
            "Accuracy :  84.04761904761905  Loss :  0.4738634086790539\n",
            "Accuracy :  83.8048780487805  Loss :  0.48548323642916796\n",
            "Accuracy :  83.68852459016394  Loss :  0.48083750976890816\n",
            "Accuracy :  83.58024691358025  Loss :  0.4798150625493791\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy :  86.0  Loss :  0.41730979084968567\n",
            "Accuracy :  84.59701492537313  Loss :  0.45245159228346243\n",
            "Accuracy :  84.56109725685785  Loss :  0.450830415300003\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.42202454805374146\n",
            "Accuracy :  83.9047619047619  Loss :  0.4724308309100923\n",
            "Accuracy :  83.82926829268293  Loss :  0.48390839303412087\n",
            "Accuracy :  83.72131147540983  Loss :  0.47890611603611805\n",
            "Accuracy :  83.62962962962963  Loss :  0.47788851974922936\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy :  85.0  Loss :  0.4324033260345459\n",
            "Accuracy :  84.46268656716418  Loss :  0.44820378646625214\n",
            "Accuracy :  84.52369077306733  Loss :  0.4460480500783706\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.4179092347621918\n",
            "Accuracy :  84.14285714285714  Loss :  0.4709566150392805\n",
            "Accuracy :  83.92682926829268  Loss :  0.48263911939248805\n",
            "Accuracy :  83.93442622950819  Loss :  0.47708901078974614\n",
            "Accuracy :  83.80246913580247  Loss :  0.4758650866555579\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy :  85.0  Loss :  0.4005453586578369\n",
            "Accuracy :  84.71641791044776  Loss :  0.44540653766980814\n",
            "Accuracy :  84.6359102244389  Loss :  0.44473874238215183\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.4147256910800934\n",
            "Accuracy :  84.19047619047619  Loss :  0.4688943681262788\n",
            "Accuracy :  83.8780487804878  Loss :  0.48012080061726453\n",
            "Accuracy :  83.90163934426229  Loss :  0.4747976792640373\n",
            "Accuracy :  83.77777777777777  Loss :  0.47343350413404867\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy :  84.0  Loss :  0.3885325491428375\n",
            "Accuracy :  84.69651741293532  Loss :  0.44282440767062836\n",
            "Accuracy :  84.63840399002494  Loss :  0.44274444967285354\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.40877124667167664\n",
            "Accuracy :  84.38095238095238  Loss :  0.46882724903878714\n",
            "Accuracy :  84.0  Loss :  0.47955915404529104\n",
            "Accuracy :  84.08196721311475  Loss :  0.4741915557228151\n",
            "Accuracy :  84.0  Loss :  0.4728552625503069\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy :  86.0  Loss :  0.40862366557121277\n",
            "Accuracy :  84.74626865671642  Loss :  0.4422786008214476\n",
            "Accuracy :  84.70573566084788  Loss :  0.44132447257600815\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.407806396484375\n",
            "Accuracy :  84.52380952380952  Loss :  0.46669393210184007\n",
            "Accuracy :  84.1951219512195  Loss :  0.4777401162356865\n",
            "Accuracy :  84.1639344262295  Loss :  0.47210788042818913\n",
            "Accuracy :  84.04938271604938  Loss :  0.4707952781959816\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy :  85.0  Loss :  0.4169406592845917\n",
            "Accuracy :  84.73134328358209  Loss :  0.44179899212139756\n",
            "Accuracy :  84.73316708229426  Loss :  0.4396547918159171\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.4060768783092499\n",
            "Accuracy :  84.23809523809524  Loss :  0.4668726325035095\n",
            "Accuracy :  84.07317073170732  Loss :  0.4774271780397834\n",
            "Accuracy :  84.09836065573771  Loss :  0.4716277528004568\n",
            "Accuracy :  83.96296296296296  Loss :  0.47042127634272163\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy :  88.0  Loss :  0.3855093717575073\n",
            "Accuracy :  84.86567164179104  Loss :  0.4378258337577184\n",
            "Accuracy :  84.87281795511223  Loss :  0.43751515336613406\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.40337955951690674\n",
            "Accuracy :  84.19047619047619  Loss :  0.4654857275031862\n",
            "Accuracy :  84.07317073170732  Loss :  0.47591722302320527\n",
            "Accuracy :  84.21311475409836  Loss :  0.4701882073136627\n",
            "Accuracy :  84.09876543209876  Loss :  0.46897174031646166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiple Students PART 2-- 8 students now**\n"
      ],
      "metadata": {
        "id": "J3x1rV4fS6Mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "    'VGGS1': [32, 32, 'M', 64, 64, 'M', 128, 128, 'M', 256, 256, 'M','M'],\n",
        "    'VGGS33': [32,32,'M', 32,32,'M',32,64,'M', 64,64,'M',64,64,64,'M'],\n",
        "    'VGGS2': [32,'M', 32, 'M', 64, 64, 'M', 128,128,'M', 256,'M'],\n",
        "\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(64, 64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        #out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "ss11 = VGG('VGGS33')\n",
        "ss11 = ss11.to(device)\n",
        "summary(ss11, (3, 32, 32))\n",
        "ss22 = VGG('VGGS33')\n",
        "ss22 = ss22.to(device)\n",
        "summary(ss22, (3,32,32))\n",
        "ss33 = VGG('VGGS33')\n",
        "ss33 = ss33.to(device)\n",
        "summary(ss33, (3, 32, 32))\n",
        "ss44 = VGG('VGGS33')\n",
        "ss44 = ss44.to(device)\n",
        "summary(ss44, (3, 32, 32))\n",
        "ss55 = VGG('VGGS33')\n",
        "ss55 = ss55.to(device)\n",
        "summary(ss55, (3, 32, 32))\n",
        "ss66 = VGG('VGGS33')\n",
        "ss66 = ss66.to(device)\n",
        "summary(ss66, (3,32,32))\n",
        "ss77 = VGG('VGGS33')\n",
        "ss77 = ss77.to(device)\n",
        "summary(ss77, (3, 32, 32))\n",
        "ss88 = VGG('VGGS33')\n",
        "ss88 = ss88.to(device)\n",
        "summary(ss88, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf6ZMLwaDqm0",
        "outputId": "9b2819cf-fc24-48d3-9131-a79bbe15c5c3"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TAA1_WOH = nn.Sequential(*list(s11.children())[:-1],nn.Flatten())\n",
        "summary(s11, (3, 32, 32))\n",
        "summary(TAA1_WOH, (3, 32, 32))\n",
        "\n",
        "TAA2_WOH = nn.Sequential(*list(s22.children())[:-1],nn.Flatten())\n",
        "summary(s22, (3, 32, 32))\n",
        "summary(TAA2_WOH, (3, 32, 32))\n",
        "\n",
        "TAA3_WOH = nn.Sequential(*list(s33.children())[:-1],nn.Flatten())\n",
        "summary(s33, (3, 32, 32))\n",
        "summary(TAA3_WOH, (3, 32, 32))\n",
        "\n",
        "TAA4_WOH = nn.Sequential(*list(s44.children())[:-1],nn.Flatten())\n",
        "summary(s44, (3, 32, 32))\n",
        "summary(TAA4_WOH, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oDdCLJkFvsN",
        "outputId": "4aeabba0-8e31-47b9-b4d9-28692a05c2a2"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "================================================================\n",
            "Total params: 600,096\n",
            "Trainable params: 17,920\n",
            "Non-trainable params: 582,176\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.33\n",
            "Params size (MB): 2.29\n",
            "Estimated Total Size (MB): 3.64\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "          Flatten-31                  [-1, 128]               0\n",
            "================================================================\n",
            "Total params: 583,584\n",
            "Trainable params: 1,408\n",
            "Non-trainable params: 582,176\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.33\n",
            "Params size (MB): 2.23\n",
            "Estimated Total Size (MB): 3.57\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "================================================================\n",
            "Total params: 600,096\n",
            "Trainable params: 17,920\n",
            "Non-trainable params: 582,176\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.33\n",
            "Params size (MB): 2.29\n",
            "Estimated Total Size (MB): 3.64\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "          Flatten-31                  [-1, 128]               0\n",
            "================================================================\n",
            "Total params: 583,584\n",
            "Trainable params: 1,408\n",
            "Non-trainable params: 582,176\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.33\n",
            "Params size (MB): 2.23\n",
            "Estimated Total Size (MB): 3.57\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "================================================================\n",
            "Total params: 600,096\n",
            "Trainable params: 17,920\n",
            "Non-trainable params: 582,176\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.33\n",
            "Params size (MB): 2.29\n",
            "Estimated Total Size (MB): 3.64\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "          Flatten-31                  [-1, 128]               0\n",
            "================================================================\n",
            "Total params: 583,584\n",
            "Trainable params: 1,408\n",
            "Non-trainable params: 582,176\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.33\n",
            "Params size (MB): 2.23\n",
            "Estimated Total Size (MB): 3.57\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "================================================================\n",
            "Total params: 600,096\n",
            "Trainable params: 17,920\n",
            "Non-trainable params: 582,176\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.33\n",
            "Params size (MB): 2.29\n",
            "Estimated Total Size (MB): 3.64\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "          Flatten-31                  [-1, 128]               0\n",
            "================================================================\n",
            "Total params: 583,584\n",
            "Trainable params: 1,408\n",
            "Non-trainable params: 582,176\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.33\n",
            "Params size (MB): 2.23\n",
            "Estimated Total Size (MB): 3.57\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TAA1_WOH.eval()\n",
        "TAA2_WOH.eval()\n",
        "TAA3_WOH.eval()\n",
        "TAA4_WOH.eval()\n",
        "TA2DenseTrain = None\n",
        "TA2DenseTest = None\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs1 = TAA1_WOH(inputs)\n",
        "        outputs2 = TAA2_WOH(inputs)\n",
        "        outputs3 = TAA3_WOH(inputs)\n",
        "        outputs4 = TAA4_WOH(inputs)\n",
        "        if(TA2DenseTrain == None):\n",
        "            TA2DenseTrain = torch.cat((outputs1,outputs2,outputs3,outputs4),1) \n",
        "        else:\n",
        "            totalOUTPUT = torch.cat((outputs1,outputs2,outputs3,outputs4),1)         \n",
        "            TA2DenseTrain = torch.cat((TA2DenseTrain,totalOUTPUT))\n",
        "           \n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs1 = TAA1_WOH(inputs)\n",
        "        outputs2 = TAA2_WOH(inputs)\n",
        "        outputs3 = TAA3_WOH(inputs)\n",
        "        outputs4 = TAA4_WOH(inputs)\n",
        "        if(TA2DenseTest == None):\n",
        "            TA2DenseTest = torch.cat((outputs1,outputs2,outputs3,outputs4),1) \n",
        "        else:\n",
        "            totalOUTPUT = torch.cat((outputs1,outputs2,outputs3,outputs4),1)      \n",
        "            TA2DenseTest = torch.cat((TA2DenseTest,totalOUTPUT))\n",
        "           "
      ],
      "metadata": {
        "id": "-X97wNRHGSkA"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(TA2DenseTrain.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgpdmBzrHEZl",
        "outputId": "0db65f2b-c52c-493f-947b-361b72457b5f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50000, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer1 = optim.Adam(ss11.parameters(), lr=0.0001)\n",
        "optimizer2 = optim.Adam(ss22.parameters(), lr=0.0001)\n",
        "optimizer3 = optim.Adam(ss33.parameters(), lr=0.0001)\n",
        "optimizer4 = optim.Adam(ss44.parameters(), lr=0.0001)\n",
        "optimizer5 = optim.Adam(ss55.parameters(), lr=0.0001)\n",
        "optimizer6 = optim.Adam(ss66.parameters(), lr=0.0001)\n",
        "optimizer7 = optim.Adam(ss77.parameters(), lr=0.0001)\n",
        "optimizer8 = optim.Adam(ss88.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def train8(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    s11.train()\n",
        "    train_loss1 = 0\n",
        "    train_loss2 = 0\n",
        "    train_loss3 = 0\n",
        "    train_loss4= 0\n",
        "    train_loss5 = 0\n",
        "    train_loss6 = 0\n",
        "    train_loss7 = 0\n",
        "    train_loss8= 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        #code here\n",
        "        targets = TA2DenseTrain[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "        targets.to(device)\n",
        "        ss11.zero_grad()\n",
        "        ss22.zero_grad()\n",
        "        ss33.zero_grad()\n",
        "        ss44.zero_grad()\n",
        "        ss55.zero_grad()\n",
        "        ss66.zero_grad()\n",
        "        ss77.zero_grad()\n",
        "        ss88.zero_grad()\n",
        "        output1 = ss11(inputs)\n",
        "        output2 = ss22(inputs)\n",
        "        output3 = ss33(inputs)\n",
        "        output4 = ss44(inputs)\n",
        "        output5 = ss55(inputs)\n",
        "        output6 = ss66(inputs)\n",
        "        output7 = ss77(inputs)\n",
        "        output8 = ss88(inputs)\n",
        "        \n",
        "        loss1 = criterion(output1, targets[:,:64])\n",
        "        loss2 = criterion(output2, targets[:,64:128])\n",
        "        loss3 = criterion(output3, targets[:,128:192])\n",
        "        loss4 = criterion(output4, targets[:,192:256])\n",
        "        loss5 = criterion(output5, targets[:,256:320])\n",
        "        loss6 = criterion(output6, targets[:,320:384])\n",
        "        loss7 = criterion(output7, targets[:,384:448])\n",
        "        loss8 = criterion(output8, targets[:,448:512])\n",
        "\n",
        "        loss1.backward()\n",
        "        loss2.backward()\n",
        "        loss3.backward()\n",
        "        loss4.backward()\n",
        "        loss5.backward()\n",
        "        loss6.backward()\n",
        "        loss7.backward()\n",
        "        loss8.backward()\n",
        "        optimizer1.step()\n",
        "        optimizer2.step()\n",
        "        optimizer3.step()\n",
        "        optimizer4.step()\n",
        "        optimizer5.step()\n",
        "        optimizer6.step()\n",
        "        optimizer7.step()\n",
        "        optimizer8.step()\n",
        "      \n",
        "\n",
        "        train_loss1 += loss1.item()\n",
        "        train_loss2 += loss2.item()\n",
        "        train_loss3 += loss3.item()\n",
        "        train_loss4 += loss4.item()\n",
        "        train_loss5 += loss5.item()\n",
        "        train_loss6 += loss6.item()\n",
        "        train_loss7 += loss7.item()\n",
        "        train_loss8 += loss8.item()\n",
        "        if(batch_idx % 10 == 0):\n",
        "          print(\"Loss SS11: \", train_loss1/(batch_idx+1))\n",
        "          print(\"Loss SS22: \", train_loss2/(batch_idx+1))\n",
        "          print(\"Loss SS33: \", train_loss3/(batch_idx+1))\n",
        "          print(\"Loss SS44: \", train_loss4/(batch_idx+1))\n",
        "          print(\"Loss SS55: \", train_loss5/(batch_idx+1))\n",
        "          print(\"Loss SS66: \", train_loss6/(batch_idx+1))\n",
        "          print(\"Loss SS77: \", train_loss7/(batch_idx+1))\n",
        "          print(\"Loss SS88: \", train_loss8/(batch_idx+1))\n",
        "def test8(epoch):\n",
        "    ss11.eval()\n",
        "    \n",
        "    test_loss1 = 0\n",
        "    test_loss2 = 0\n",
        "    test_loss3 = 0\n",
        "    test_loss4= 0\n",
        "    test_loss5 = 0\n",
        "    test_loss6 = 0\n",
        "    test_loss7 = 0\n",
        "    test_loss8= 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            #code here\n",
        "            targets = TA2DenseTest[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "            targets.to(device)\n",
        "            output1 = ss11(inputs)\n",
        "            output2 = ss22(inputs)\n",
        "            output3 = ss33(inputs)\n",
        "            output4 = ss44(inputs)\n",
        "            output5 = ss55(inputs)\n",
        "            output6 = ss66(inputs)\n",
        "            output7 = ss77(inputs)\n",
        "            output8 = ss88(inputs)\n",
        "            loss1 = criterion(output1, targets[:,:64])\n",
        "            loss2 = criterion(output2, targets[:,64:128])\n",
        "            loss3 = criterion(output3, targets[:,128:192])\n",
        "            loss4 = criterion(output4, targets[:,192:256])\n",
        "            loss5 = criterion(output5, targets[:,256:320])\n",
        "            loss6 = criterion(output6, targets[:,320:384])\n",
        "            loss7 = criterion(output7, targets[:,384:448])\n",
        "            loss8 = criterion(output8, targets[:,448:512])\n",
        "            test_loss1 += loss1.item()\n",
        "            test_loss2 += loss2.item()\n",
        "            test_loss3 += loss3.item()\n",
        "            test_loss4 += loss4.item()\n",
        "\n",
        "            test_loss5 += loss5.item()\n",
        "            test_loss6 += loss6.item()\n",
        "            test_loss7 += loss7.item()\n",
        "            test_loss8 += loss8.item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\" Loss SS11: \", test_loss1/(batch_idx+1))\n",
        "              print(\" Loss SS22: \", test_loss2/(batch_idx+1))\n",
        "              print(\" Loss SS33: \", test_loss3/(batch_idx+1))\n",
        "              print(\" Loss SS55: \", test_loss4/(batch_idx+1))\n",
        "              print(\" Loss SS66: \", test_loss5/(batch_idx+1))\n",
        "              print(\" Loss SS77: \", test_loss6/(batch_idx+1))\n",
        "              print(\" Loss SS88: \", test_loss7/(batch_idx+1))\n",
        "              print(\" Loss SS99: \", test_loss8/(batch_idx+1))"
      ],
      "metadata": {
        "id": "EsiP7r2XHExk"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+60):\n",
        "    train8(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test8(epoch)"
      ],
      "metadata": {
        "id": "lpbuGedJI_2y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a74084c8-eb32-4ea7-85dd-73d446b85e34"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Loss SS33:  0.04357753345347427\n",
            "Loss SS44:  0.04126878527690992\n",
            "Loss SS55:  0.04251433404929498\n",
            "Loss SS66:  0.04151284788994838\n",
            "Loss SS77:  0.047744848596317994\n",
            "Loss SS88:  0.04602122112460758\n",
            "Loss SS11:  0.04775536947528323\n",
            "Loss SS22:  0.04364519829204552\n",
            "Loss SS33:  0.04359250132022356\n",
            "Loss SS44:  0.041268776777081954\n",
            "Loss SS55:  0.04251592246783047\n",
            "Loss SS66:  0.04154304260773552\n",
            "Loss SS77:  0.04774601086788344\n",
            "Loss SS88:  0.046021094503917004\n",
            "Loss SS11:  0.04774456022538408\n",
            "Loss SS22:  0.043665816567820065\n",
            "Loss SS33:  0.04358592664782148\n",
            "Loss SS44:  0.04129708148629706\n",
            "Loss SS55:  0.042505977498571365\n",
            "Loss SS66:  0.041534137869040755\n",
            "Loss SS77:  0.047705940276819424\n",
            "Loss SS88:  0.0460050810500073\n",
            "Loss SS11:  0.04773539933873752\n",
            "Loss SS22:  0.043653303177721994\n",
            "Loss SS33:  0.04361706020890109\n",
            "Loss SS44:  0.04132139213790237\n",
            "Loss SS55:  0.04250544872391535\n",
            "Loss SS66:  0.04157397451848146\n",
            "Loss SS77:  0.04773292332144361\n",
            "Loss SS88:  0.04603007627814796\n",
            "Loss SS11:  0.04769825800527276\n",
            "Loss SS22:  0.04363048752540779\n",
            "Loss SS33:  0.04359564479016926\n",
            "Loss SS44:  0.041311212558204226\n",
            "Loss SS55:  0.04246480795803313\n",
            "Loss SS66:  0.04155579542961585\n",
            "Loss SS77:  0.04767220600436846\n",
            "Loss SS88:  0.04599706387782595\n",
            "Loss SS11:  0.04768570217719965\n",
            "Loss SS22:  0.04362578549825686\n",
            "Loss SS33:  0.0435986915294951\n",
            "Loss SS44:  0.04131368433638495\n",
            "Loss SS55:  0.0424531327640794\n",
            "Loss SS66:  0.04156145806553142\n",
            "Loss SS77:  0.04767458675568607\n",
            "Loss SS88:  0.04601895179844497\n",
            "Loss SS11:  0.04766993390648169\n",
            "Loss SS22:  0.04363608423893044\n",
            "Loss SS33:  0.04360157205101656\n",
            "Loss SS44:  0.04133324744696099\n",
            "Loss SS55:  0.042515259558239954\n",
            "Loss SS66:  0.041613856309863785\n",
            "Loss SS77:  0.04772406642195126\n",
            "Loss SS88:  0.04605253245640224\n",
            "Loss SS11:  0.047659450184368\n",
            "Loss SS22:  0.043625387631825406\n",
            "Loss SS33:  0.043624771787197106\n",
            "Loss SS44:  0.0413403816505006\n",
            "Loss SS55:  0.04256308923218876\n",
            "Loss SS66:  0.04165178383023775\n",
            "Loss SS77:  0.04771514586021481\n",
            "Loss SS88:  0.046089478386586245\n",
            "Loss SS11:  0.047645038598851795\n",
            "Loss SS22:  0.04364121462477494\n",
            "Loss SS33:  0.04362974750894397\n",
            "Loss SS44:  0.04132771461158042\n",
            "Loss SS55:  0.04259452661163235\n",
            "Loss SS66:  0.04168007094136216\n",
            "Loss SS77:  0.0477274611449925\n",
            "Loss SS88:  0.04609887098652289\n",
            "Loss SS11:  0.04764075656256904\n",
            "Loss SS22:  0.04364445051972187\n",
            "Loss SS33:  0.043609995867383206\n",
            "Loss SS44:  0.04130669660757832\n",
            "Loss SS55:  0.042610333237231644\n",
            "Loss SS66:  0.041673584138343336\n",
            "Loss SS77:  0.04771547179800805\n",
            "Loss SS88:  0.04608398543481024\n",
            "Loss SS11:  0.04759442750098992\n",
            "Loss SS22:  0.04358105557210091\n",
            "Loss SS33:  0.04357428695483023\n",
            "Loss SS44:  0.04128110810143156\n",
            "Loss SS55:  0.042560299657391436\n",
            "Loss SS66:  0.04163827942096289\n",
            "Loss SS77:  0.047659525901928936\n",
            "Loss SS88:  0.04602311002261284\n",
            "Validation: \n",
            " Loss SS11:  0.04191470146179199\n",
            " Loss SS22:  0.05699170008301735\n",
            " Loss SS33:  0.06351641565561295\n",
            " Loss SS55:  0.05530698969960213\n",
            " Loss SS66:  0.06351496279239655\n",
            " Loss SS77:  0.06193854287266731\n",
            " Loss SS88:  0.060075487941503525\n",
            " Loss SS99:  0.05849732831120491\n",
            " Loss SS11:  0.047682034295229686\n",
            " Loss SS22:  0.0653926371818497\n",
            " Loss SS33:  0.06773106381297112\n",
            " Loss SS55:  0.05819382500790414\n",
            " Loss SS66:  0.06904126739218122\n",
            " Loss SS77:  0.0648075436197576\n",
            " Loss SS88:  0.07170920517472994\n",
            " Loss SS99:  0.06515059513705117\n",
            " Loss SS11:  0.04830614795408598\n",
            " Loss SS22:  0.06419484626229216\n",
            " Loss SS33:  0.06911125034093857\n",
            " Loss SS55:  0.05861225133625472\n",
            " Loss SS66:  0.06779033654346699\n",
            " Loss SS77:  0.06457342016624242\n",
            " Loss SS88:  0.07087556172798319\n",
            " Loss SS99:  0.0644712114661205\n",
            " Loss SS11:  0.04827153627745441\n",
            " Loss SS22:  0.06411272098050742\n",
            " Loss SS33:  0.06837631230715846\n",
            " Loss SS55:  0.058267966279240906\n",
            " Loss SS66:  0.06749794314630696\n",
            " Loss SS77:  0.06421818797949885\n",
            " Loss SS88:  0.07120921493309443\n",
            " Loss SS99:  0.06431215482412792\n",
            " Loss SS11:  0.048093120119086015\n",
            " Loss SS22:  0.06370157697870407\n",
            " Loss SS33:  0.06821249747349892\n",
            " Loss SS55:  0.0581270391466441\n",
            " Loss SS66:  0.0671843217662823\n",
            " Loss SS77:  0.0641825869679451\n",
            " Loss SS88:  0.07097988180172296\n",
            " Loss SS99:  0.06445468410297676\n",
            "\n",
            "Epoch: 50\n",
            "Loss SS11:  0.04981374740600586\n",
            "Loss SS22:  0.04514915496110916\n",
            "Loss SS33:  0.0476381778717041\n",
            "Loss SS44:  0.04193635284900665\n",
            "Loss SS55:  0.04585110396146774\n",
            "Loss SS66:  0.047360267490148544\n",
            "Loss SS77:  0.05623289942741394\n",
            "Loss SS88:  0.05520370975136757\n",
            "Loss SS11:  0.04821015691215342\n",
            "Loss SS22:  0.04299077594822103\n",
            "Loss SS33:  0.04459244588559324\n",
            "Loss SS44:  0.04180043326182799\n",
            "Loss SS55:  0.04218491099097512\n",
            "Loss SS66:  0.042273543775081635\n",
            "Loss SS77:  0.04710061848163605\n",
            "Loss SS88:  0.04700164903293957\n",
            "Loss SS11:  0.04736934478084246\n",
            "Loss SS22:  0.04308266423287846\n",
            "Loss SS33:  0.04448504249254862\n",
            "Loss SS44:  0.04158547627074378\n",
            "Loss SS55:  0.04224924601259686\n",
            "Loss SS66:  0.04199941314402081\n",
            "Loss SS77:  0.046492217552094234\n",
            "Loss SS88:  0.046182279608079364\n",
            "Loss SS11:  0.047577987755498576\n",
            "Loss SS22:  0.043268461261064775\n",
            "Loss SS33:  0.04407539627244396\n",
            "Loss SS44:  0.04197253755504085\n",
            "Loss SS55:  0.04244214392477466\n",
            "Loss SS66:  0.04190241725694749\n",
            "Loss SS77:  0.04694483297005776\n",
            "Loss SS88:  0.04597342086415137\n",
            "Loss SS11:  0.047141904874545774\n",
            "Loss SS22:  0.04332337828307617\n",
            "Loss SS33:  0.043981263459455675\n",
            "Loss SS44:  0.041786163863612384\n",
            "Loss SS55:  0.0426649928274678\n",
            "Loss SS66:  0.04200596889344657\n",
            "Loss SS77:  0.047213998327894906\n",
            "Loss SS88:  0.04604955998862662\n",
            "Loss SS11:  0.04692194961449679\n",
            "Loss SS22:  0.04344119759751301\n",
            "Loss SS33:  0.04387923841382943\n",
            "Loss SS44:  0.04199222226937612\n",
            "Loss SS55:  0.04268701732450841\n",
            "Loss SS66:  0.04198472910359794\n",
            "Loss SS77:  0.04744293233927559\n",
            "Loss SS88:  0.04619286004819122\n",
            "Loss SS11:  0.04724233182238751\n",
            "Loss SS22:  0.04335028708713953\n",
            "Loss SS33:  0.04380714355922136\n",
            "Loss SS44:  0.041904492212123556\n",
            "Loss SS55:  0.04240436143562442\n",
            "Loss SS66:  0.04174371711054786\n",
            "Loss SS77:  0.047127743541705805\n",
            "Loss SS88:  0.0460270529643434\n",
            "Loss SS11:  0.047378507141076345\n",
            "Loss SS22:  0.043706186692899385\n",
            "Loss SS33:  0.0439408180684271\n",
            "Loss SS44:  0.04194667625805022\n",
            "Loss SS55:  0.04258811919831894\n",
            "Loss SS66:  0.041910675472356905\n",
            "Loss SS77:  0.047161140912015675\n",
            "Loss SS88:  0.04603940348180247\n",
            "Loss SS11:  0.04751183534110034\n",
            "Loss SS22:  0.043615149862972304\n",
            "Loss SS33:  0.043660724374615115\n",
            "Loss SS44:  0.041634156150214464\n",
            "Loss SS55:  0.04233552423524268\n",
            "Loss SS66:  0.04170855410673\n",
            "Loss SS77:  0.04698779438564807\n",
            "Loss SS88:  0.04584318082089777\n",
            "Loss SS11:  0.04763694263094074\n",
            "Loss SS22:  0.04364229333433476\n",
            "Loss SS33:  0.043646856651201354\n",
            "Loss SS44:  0.04155755644807449\n",
            "Loss SS55:  0.042381525612794436\n",
            "Loss SS66:  0.04172966080707508\n",
            "Loss SS77:  0.047011994071059174\n",
            "Loss SS88:  0.045764567637509045\n",
            "Loss SS11:  0.047635181189173516\n",
            "Loss SS22:  0.04356407720853787\n",
            "Loss SS33:  0.04349918910624957\n",
            "Loss SS44:  0.04150183745982623\n",
            "Loss SS55:  0.04221720871801424\n",
            "Loss SS66:  0.04159698738615111\n",
            "Loss SS77:  0.047026332504678484\n",
            "Loss SS88:  0.0456937594652766\n",
            "Loss SS11:  0.047636808273760045\n",
            "Loss SS22:  0.043537941702582815\n",
            "Loss SS33:  0.043386892658901645\n",
            "Loss SS44:  0.04142548224410495\n",
            "Loss SS55:  0.042284139473964505\n",
            "Loss SS66:  0.041572027176887065\n",
            "Loss SS77:  0.04712156048631883\n",
            "Loss SS88:  0.045657100575464266\n",
            "Loss SS11:  0.04764850049718353\n",
            "Loss SS22:  0.04363490434840691\n",
            "Loss SS33:  0.04351033744486896\n",
            "Loss SS44:  0.04156176431858835\n",
            "Loss SS55:  0.04237710469025226\n",
            "Loss SS66:  0.04163710736046153\n",
            "Loss SS77:  0.0472470359676633\n",
            "Loss SS88:  0.0458463748439777\n",
            "Loss SS11:  0.04765982371363931\n",
            "Loss SS22:  0.04356024712892889\n",
            "Loss SS33:  0.04355969609877535\n",
            "Loss SS44:  0.04159853573291356\n",
            "Loss SS55:  0.04238858702414818\n",
            "Loss SS66:  0.041575070214635544\n",
            "Loss SS77:  0.04734903517115207\n",
            "Loss SS88:  0.0459199329653314\n",
            "Loss SS11:  0.047565224692753864\n",
            "Loss SS22:  0.04354208390763466\n",
            "Loss SS33:  0.04352840400756674\n",
            "Loss SS44:  0.04155429648169389\n",
            "Loss SS55:  0.04241228639973816\n",
            "Loss SS66:  0.04163995564829373\n",
            "Loss SS77:  0.04748818679904261\n",
            "Loss SS88:  0.04598544564441586\n",
            "Loss SS11:  0.04745192416258995\n",
            "Loss SS22:  0.04346249602883067\n",
            "Loss SS33:  0.04345829530760942\n",
            "Loss SS44:  0.04144732397518411\n",
            "Loss SS55:  0.04234062885211793\n",
            "Loss SS66:  0.04156266658610066\n",
            "Loss SS77:  0.047412655778870674\n",
            "Loss SS88:  0.04592028331776329\n",
            "Loss SS11:  0.04746953256844734\n",
            "Loss SS22:  0.043425469572499674\n",
            "Loss SS33:  0.043457543419569915\n",
            "Loss SS44:  0.04135871120954152\n",
            "Loss SS55:  0.04225099908176416\n",
            "Loss SS66:  0.041453046596938777\n",
            "Loss SS77:  0.04735595258589117\n",
            "Loss SS88:  0.04581035761544423\n",
            "Loss SS11:  0.047532584932115346\n",
            "Loss SS22:  0.04342821286174289\n",
            "Loss SS33:  0.043457120613396516\n",
            "Loss SS44:  0.041427948081876796\n",
            "Loss SS55:  0.042321288206598216\n",
            "Loss SS66:  0.04158907891278379\n",
            "Loss SS77:  0.04748288369928187\n",
            "Loss SS88:  0.04588871501516877\n",
            "Loss SS11:  0.04760845898758641\n",
            "Loss SS22:  0.0434796294784019\n",
            "Loss SS33:  0.043519516737915535\n",
            "Loss SS44:  0.04146219549175784\n",
            "Loss SS55:  0.04236756240465364\n",
            "Loss SS66:  0.04170395271307197\n",
            "Loss SS77:  0.04759152114226673\n",
            "Loss SS88:  0.04592998786855139\n",
            "Loss SS11:  0.047742058137792566\n",
            "Loss SS22:  0.04347115673163798\n",
            "Loss SS33:  0.04345949801630999\n",
            "Loss SS44:  0.0414158705797495\n",
            "Loss SS55:  0.04230915452719359\n",
            "Loss SS66:  0.041666258224017955\n",
            "Loss SS77:  0.047579491306193836\n",
            "Loss SS88:  0.04601034009097758\n",
            "Loss SS11:  0.04768189113234999\n",
            "Loss SS22:  0.04344976941744486\n",
            "Loss SS33:  0.043446141874315725\n",
            "Loss SS44:  0.041409042288563146\n",
            "Loss SS55:  0.04227711310359969\n",
            "Loss SS66:  0.04167859995765472\n",
            "Loss SS77:  0.047509069447938486\n",
            "Loss SS88:  0.04600234133242374\n",
            "Loss SS11:  0.047657813876867294\n",
            "Loss SS22:  0.043401666303381534\n",
            "Loss SS33:  0.043446976197133134\n",
            "Loss SS44:  0.041405241936445236\n",
            "Loss SS55:  0.04224158053714517\n",
            "Loss SS66:  0.04162271440876604\n",
            "Loss SS77:  0.04743791535779198\n",
            "Loss SS88:  0.045949113700909636\n",
            "Loss SS11:  0.04767233925572348\n",
            "Loss SS22:  0.043427388609399625\n",
            "Loss SS33:  0.043432633649574684\n",
            "Loss SS44:  0.041417873907008323\n",
            "Loss SS55:  0.0421768892135016\n",
            "Loss SS66:  0.041590923957560394\n",
            "Loss SS77:  0.04739274556550505\n",
            "Loss SS88:  0.045934769379742\n",
            "Loss SS11:  0.04771119849635409\n",
            "Loss SS22:  0.043483770609676065\n",
            "Loss SS33:  0.04343250016500423\n",
            "Loss SS44:  0.04138224713859104\n",
            "Loss SS55:  0.042165248263707926\n",
            "Loss SS66:  0.0415321799132215\n",
            "Loss SS77:  0.047360465462718694\n",
            "Loss SS88:  0.045911889458760555\n",
            "Loss SS11:  0.04774244356884996\n",
            "Loss SS22:  0.04353860274142744\n",
            "Loss SS33:  0.0434461376046244\n",
            "Loss SS44:  0.04140577656970479\n",
            "Loss SS55:  0.04221625409680283\n",
            "Loss SS66:  0.04154932998213531\n",
            "Loss SS77:  0.04734610446199342\n",
            "Loss SS88:  0.04593099157530737\n",
            "Loss SS11:  0.04768375194523914\n",
            "Loss SS22:  0.043554362770451965\n",
            "Loss SS33:  0.043453317432408316\n",
            "Loss SS44:  0.04140073682860549\n",
            "Loss SS55:  0.04222372695209971\n",
            "Loss SS66:  0.041550097222703385\n",
            "Loss SS77:  0.047408592000425576\n",
            "Loss SS88:  0.045920179347949196\n",
            "Loss SS11:  0.047669155915692395\n",
            "Loss SS22:  0.04354600621194675\n",
            "Loss SS33:  0.04346647389033288\n",
            "Loss SS44:  0.041406282077461366\n",
            "Loss SS55:  0.042283392158047904\n",
            "Loss SS66:  0.04160093180692516\n",
            "Loss SS77:  0.04740116943630222\n",
            "Loss SS88:  0.04590884370593732\n",
            "Loss SS11:  0.0476150921927387\n",
            "Loss SS22:  0.04352623147278254\n",
            "Loss SS33:  0.043439386584332065\n",
            "Loss SS44:  0.04138755718610383\n",
            "Loss SS55:  0.04227397497399707\n",
            "Loss SS66:  0.04156749714784517\n",
            "Loss SS77:  0.04745267915824682\n",
            "Loss SS88:  0.04593467766013533\n",
            "Loss SS11:  0.047600966927950066\n",
            "Loss SS22:  0.043528373356497584\n",
            "Loss SS33:  0.04342955447653859\n",
            "Loss SS44:  0.04142090158530402\n",
            "Loss SS55:  0.04229734680247477\n",
            "Loss SS66:  0.04157485615369264\n",
            "Loss SS77:  0.04747748412059295\n",
            "Loss SS88:  0.04595160273523517\n",
            "Loss SS11:  0.04756430404651206\n",
            "Loss SS22:  0.04354359835539896\n",
            "Loss SS33:  0.043436427296641766\n",
            "Loss SS44:  0.041413295399589635\n",
            "Loss SS55:  0.04233138836741038\n",
            "Loss SS66:  0.04159904261122864\n",
            "Loss SS77:  0.04758482326551811\n",
            "Loss SS88:  0.046006815104754926\n",
            "Loss SS11:  0.04752940076876716\n",
            "Loss SS22:  0.04354100945433509\n",
            "Loss SS33:  0.04344787015471348\n",
            "Loss SS44:  0.04141548267284105\n",
            "Loss SS55:  0.0423556939510016\n",
            "Loss SS66:  0.04159086536727474\n",
            "Loss SS77:  0.047625201930635394\n",
            "Loss SS88:  0.045998432249622884\n",
            "Loss SS11:  0.047493239032206425\n",
            "Loss SS22:  0.04347585613608743\n",
            "Loss SS33:  0.04341783448814196\n",
            "Loss SS44:  0.04140172353751023\n",
            "Loss SS55:  0.04232997641302765\n",
            "Loss SS66:  0.04158350212014373\n",
            "Loss SS77:  0.04758800877635502\n",
            "Loss SS88:  0.04595999205275364\n",
            "Loss SS11:  0.04749945740322829\n",
            "Loss SS22:  0.04344520131793349\n",
            "Loss SS33:  0.04342917439954303\n",
            "Loss SS44:  0.04140023877873228\n",
            "Loss SS55:  0.042315078754198515\n",
            "Loss SS66:  0.041554165267127324\n",
            "Loss SS77:  0.047580883858545545\n",
            "Loss SS88:  0.0459304747180404\n",
            "Loss SS11:  0.04750714182088382\n",
            "Loss SS22:  0.04344433138647828\n",
            "Loss SS33:  0.04342797227496467\n",
            "Loss SS44:  0.0414093326315779\n",
            "Loss SS55:  0.0423033990281046\n",
            "Loss SS66:  0.041544176255468156\n",
            "Loss SS77:  0.047546377005984054\n",
            "Loss SS88:  0.04588938940012563\n",
            "Loss SS11:  0.04749753079281525\n",
            "Loss SS22:  0.04344902348177524\n",
            "Loss SS33:  0.043452585372757\n",
            "Loss SS44:  0.04139967235433391\n",
            "Loss SS55:  0.04229838945124506\n",
            "Loss SS66:  0.041533841087846114\n",
            "Loss SS77:  0.04754632184844562\n",
            "Loss SS88:  0.045852836254230346\n",
            "Loss SS11:  0.04744601217854736\n",
            "Loss SS22:  0.043442538979216516\n",
            "Loss SS33:  0.04343830435364335\n",
            "Loss SS44:  0.04138004935832105\n",
            "Loss SS55:  0.042295943988099736\n",
            "Loss SS66:  0.041534791984449426\n",
            "Loss SS77:  0.047537038808534625\n",
            "Loss SS88:  0.04583615996474214\n",
            "Loss SS11:  0.047452322096458104\n",
            "Loss SS22:  0.04344467341982427\n",
            "Loss SS33:  0.04344314899182056\n",
            "Loss SS44:  0.04138023274179311\n",
            "Loss SS55:  0.04230050531079234\n",
            "Loss SS66:  0.04153391056361291\n",
            "Loss SS77:  0.04753673369699568\n",
            "Loss SS88:  0.045826230549003276\n",
            "Loss SS11:  0.047465143961244516\n",
            "Loss SS22:  0.043416948491993616\n",
            "Loss SS33:  0.0434198709370473\n",
            "Loss SS44:  0.0413569030816343\n",
            "Loss SS55:  0.04231340372859629\n",
            "Loss SS66:  0.041528766959744966\n",
            "Loss SS77:  0.0475224223680734\n",
            "Loss SS88:  0.04581310391185097\n",
            "Loss SS11:  0.04741008421332817\n",
            "Loss SS22:  0.0433793940197608\n",
            "Loss SS33:  0.04336807318520671\n",
            "Loss SS44:  0.04131091434968112\n",
            "Loss SS55:  0.04228859474965593\n",
            "Loss SS66:  0.04151167113363274\n",
            "Loss SS77:  0.04748694523386755\n",
            "Loss SS88:  0.04577109244043433\n",
            "Loss SS11:  0.047410900330604494\n",
            "Loss SS22:  0.04333637633820629\n",
            "Loss SS33:  0.04330628098505537\n",
            "Loss SS44:  0.041276615970503645\n",
            "Loss SS55:  0.04226185691059398\n",
            "Loss SS66:  0.04148534517687605\n",
            "Loss SS77:  0.047438685660776886\n",
            "Loss SS88:  0.04572297532654479\n",
            "Loss SS11:  0.04741998416304291\n",
            "Loss SS22:  0.04332530361941628\n",
            "Loss SS33:  0.04328493569557209\n",
            "Loss SS44:  0.041231956899612024\n",
            "Loss SS55:  0.0422467615483259\n",
            "Loss SS66:  0.04147608407269095\n",
            "Loss SS77:  0.047407392644674105\n",
            "Loss SS88:  0.045716371916773316\n",
            "Loss SS11:  0.047416768696186315\n",
            "Loss SS22:  0.043336854943973886\n",
            "Loss SS33:  0.043278486116669186\n",
            "Loss SS44:  0.041202218645680556\n",
            "Loss SS55:  0.042248637390542784\n",
            "Loss SS66:  0.04146778420375211\n",
            "Loss SS77:  0.047382165807007\n",
            "Loss SS88:  0.04569525510942849\n",
            "Loss SS11:  0.047430994772217336\n",
            "Loss SS22:  0.04333533467879771\n",
            "Loss SS33:  0.04328824231078109\n",
            "Loss SS44:  0.041210861013861566\n",
            "Loss SS55:  0.042260437902956846\n",
            "Loss SS66:  0.041477362494782996\n",
            "Loss SS77:  0.04737721489755671\n",
            "Loss SS88:  0.04571430783302937\n",
            "Loss SS11:  0.04743387550115585\n",
            "Loss SS22:  0.043328822238193586\n",
            "Loss SS33:  0.043274282311411304\n",
            "Loss SS44:  0.0411848817668907\n",
            "Loss SS55:  0.04223992742096742\n",
            "Loss SS66:  0.04145016508250391\n",
            "Loss SS77:  0.04731416545825602\n",
            "Loss SS88:  0.04566897956650108\n",
            "Loss SS11:  0.04740705243111197\n",
            "Loss SS22:  0.043330490259694404\n",
            "Loss SS33:  0.04327662332224197\n",
            "Loss SS44:  0.04118065234451067\n",
            "Loss SS55:  0.04226176674707947\n",
            "Loss SS66:  0.041450328879953785\n",
            "Loss SS77:  0.047319587793142076\n",
            "Loss SS88:  0.04568436234141964\n",
            "Loss SS11:  0.04742137832513405\n",
            "Loss SS22:  0.04331466055035327\n",
            "Loss SS33:  0.043273157974965296\n",
            "Loss SS44:  0.04117686504667455\n",
            "Loss SS55:  0.04229869679475571\n",
            "Loss SS66:  0.041460479343281086\n",
            "Loss SS77:  0.04733323956738555\n",
            "Loss SS88:  0.04571193966295925\n",
            "Loss SS11:  0.04739265078416098\n",
            "Loss SS22:  0.0433105595345223\n",
            "Loss SS33:  0.04327685427154998\n",
            "Loss SS44:  0.04119063189774429\n",
            "Loss SS55:  0.0423251257930557\n",
            "Loss SS66:  0.04146369456145872\n",
            "Loss SS77:  0.04735941092388749\n",
            "Loss SS88:  0.04575745045427645\n",
            "Loss SS11:  0.04738377888079914\n",
            "Loss SS22:  0.043327121621674035\n",
            "Loss SS33:  0.04331271177169624\n",
            "Loss SS44:  0.04118365220203521\n",
            "Loss SS55:  0.0423638031377028\n",
            "Loss SS66:  0.04149570877638086\n",
            "Loss SS77:  0.04739074703234776\n",
            "Loss SS88:  0.04578421564650131\n",
            "Loss SS11:  0.04736806569996594\n",
            "Loss SS22:  0.04331518677131054\n",
            "Loss SS33:  0.043316250887705236\n",
            "Loss SS44:  0.04116581508157902\n",
            "Loss SS55:  0.0423871939246719\n",
            "Loss SS66:  0.04149939290203324\n",
            "Loss SS77:  0.04737930362289016\n",
            "Loss SS88:  0.04578274026079386\n",
            "Loss SS11:  0.04733955303241182\n",
            "Loss SS22:  0.043267405832797104\n",
            "Loss SS33:  0.04327738778229401\n",
            "Loss SS44:  0.04114678096929053\n",
            "Loss SS55:  0.042368114161515674\n",
            "Loss SS66:  0.04148267054752031\n",
            "Loss SS77:  0.047351786589853145\n",
            "Loss SS88:  0.04572818970000428\n",
            "Validation: \n",
            " Loss SS11:  0.04187873750925064\n",
            " Loss SS22:  0.0571318082511425\n",
            " Loss SS33:  0.063240185379982\n",
            " Loss SS55:  0.052821215242147446\n",
            " Loss SS66:  0.06550876796245575\n",
            " Loss SS77:  0.06292618066072464\n",
            " Loss SS88:  0.05936264991760254\n",
            " Loss SS99:  0.058414019644260406\n",
            " Loss SS11:  0.04675931997952007\n",
            " Loss SS22:  0.06468212072338377\n",
            " Loss SS33:  0.0667073769228799\n",
            " Loss SS55:  0.05692110707362493\n",
            " Loss SS66:  0.07002891777526765\n",
            " Loss SS77:  0.06507449437464986\n",
            " Loss SS88:  0.06982381216117314\n",
            " Loss SS99:  0.065884472713584\n",
            " Loss SS11:  0.047243138729799086\n",
            " Loss SS22:  0.06351991352148173\n",
            " Loss SS33:  0.06798305698647732\n",
            " Loss SS55:  0.0573711353467732\n",
            " Loss SS66:  0.0689207862426595\n",
            " Loss SS77:  0.06476698525068236\n",
            " Loss SS88:  0.06918470089028521\n",
            " Loss SS99:  0.06513535113232892\n",
            " Loss SS11:  0.04721591065897316\n",
            " Loss SS22:  0.06344540117949736\n",
            " Loss SS33:  0.06727196831927924\n",
            " Loss SS55:  0.05703139476111678\n",
            " Loss SS66:  0.06870638176065977\n",
            " Loss SS77:  0.0644636622584257\n",
            " Loss SS88:  0.06944978420363097\n",
            " Loss SS99:  0.06498821690434316\n",
            " Loss SS11:  0.04714135457704097\n",
            " Loss SS22:  0.06316762656709295\n",
            " Loss SS33:  0.06705939208651766\n",
            " Loss SS55:  0.0568678691799258\n",
            " Loss SS66:  0.06842638496999387\n",
            " Loss SS77:  0.06449619211532452\n",
            " Loss SS88:  0.06943741965073126\n",
            " Loss SS99:  0.06498720897016702\n",
            "\n",
            "Epoch: 51\n",
            "Loss SS11:  0.046994924545288086\n",
            "Loss SS22:  0.042903222143650055\n",
            "Loss SS33:  0.04771209508180618\n",
            "Loss SS44:  0.043983809649944305\n",
            "Loss SS55:  0.04581073671579361\n",
            "Loss SS66:  0.04727654531598091\n",
            "Loss SS77:  0.05809475854039192\n",
            "Loss SS88:  0.053298354148864746\n",
            "Loss SS11:  0.0475994504310868\n",
            "Loss SS22:  0.04250768030231649\n",
            "Loss SS33:  0.04472566226666624\n",
            "Loss SS44:  0.04078950089487163\n",
            "Loss SS55:  0.04183506999503483\n",
            "Loss SS66:  0.041982218623161316\n",
            "Loss SS77:  0.047719769179821014\n",
            "Loss SS88:  0.04551226340911605\n",
            "Loss SS11:  0.04653879804980187\n",
            "Loss SS22:  0.04264823081237929\n",
            "Loss SS33:  0.044411007492315205\n",
            "Loss SS44:  0.04082376705039115\n",
            "Loss SS55:  0.041663148396071936\n",
            "Loss SS66:  0.041879417995611824\n",
            "Loss SS77:  0.047340902544203256\n",
            "Loss SS88:  0.045785700636250634\n",
            "Loss SS11:  0.04617475850447532\n",
            "Loss SS22:  0.04288197789461382\n",
            "Loss SS33:  0.04408284204621469\n",
            "Loss SS44:  0.04129927557322287\n",
            "Loss SS55:  0.04200545782523771\n",
            "Loss SS66:  0.04174876729807546\n",
            "Loss SS77:  0.04757252191343615\n",
            "Loss SS88:  0.04569637583148095\n",
            "Loss SS11:  0.046258145625271446\n",
            "Loss SS22:  0.04298261480360496\n",
            "Loss SS33:  0.04381258331420945\n",
            "Loss SS44:  0.04130883151438178\n",
            "Loss SS55:  0.04232052368361775\n",
            "Loss SS66:  0.04181202991706569\n",
            "Loss SS77:  0.04762944206595421\n",
            "Loss SS88:  0.04585870782413134\n",
            "Loss SS11:  0.046359316975462674\n",
            "Loss SS22:  0.043140140234255324\n",
            "Loss SS33:  0.04377884417772293\n",
            "Loss SS44:  0.04146450538845623\n",
            "Loss SS55:  0.0423671221908401\n",
            "Loss SS66:  0.04169730308885668\n",
            "Loss SS77:  0.04794960666228743\n",
            "Loss SS88:  0.0459318327553132\n",
            "Loss SS11:  0.0463299318293079\n",
            "Loss SS22:  0.0431598929963151\n",
            "Loss SS33:  0.04362286437974602\n",
            "Loss SS44:  0.041242060114125734\n",
            "Loss SS55:  0.04230594543404267\n",
            "Loss SS66:  0.04145193967174311\n",
            "Loss SS77:  0.04775481842091826\n",
            "Loss SS88:  0.04586646449370462\n",
            "Loss SS11:  0.04639300546595748\n",
            "Loss SS22:  0.043283124519905576\n",
            "Loss SS33:  0.043687708203641464\n",
            "Loss SS44:  0.04122601187145206\n",
            "Loss SS55:  0.042474807410592764\n",
            "Loss SS66:  0.04147591226747338\n",
            "Loss SS77:  0.047724254954029134\n",
            "Loss SS88:  0.045744972644557416\n",
            "Loss SS11:  0.04647216623947944\n",
            "Loss SS22:  0.043276984014628844\n",
            "Loss SS33:  0.04337831283057177\n",
            "Loss SS44:  0.04103146925752545\n",
            "Loss SS55:  0.04232834491096897\n",
            "Loss SS66:  0.04128543512872708\n",
            "Loss SS77:  0.04751040073640553\n",
            "Loss SS88:  0.045512744673976195\n",
            "Loss SS11:  0.04651604506831903\n",
            "Loss SS22:  0.04328425187658477\n",
            "Loss SS33:  0.043295204189125\n",
            "Loss SS44:  0.04100072719566115\n",
            "Loss SS55:  0.042214013513300445\n",
            "Loss SS66:  0.041383047229968585\n",
            "Loss SS77:  0.04735584164058769\n",
            "Loss SS88:  0.04541118889228328\n",
            "Loss SS11:  0.0464919926506458\n",
            "Loss SS22:  0.04316692445242759\n",
            "Loss SS33:  0.04321221863426784\n",
            "Loss SS44:  0.040792554469391853\n",
            "Loss SS55:  0.04206475729841997\n",
            "Loss SS66:  0.041196979471657534\n",
            "Loss SS77:  0.047266472831811054\n",
            "Loss SS88:  0.04529249778773525\n",
            "Loss SS11:  0.04648005979152413\n",
            "Loss SS22:  0.04314447925971435\n",
            "Loss SS33:  0.043056235287909035\n",
            "Loss SS44:  0.040720258478645806\n",
            "Loss SS55:  0.04211510117123793\n",
            "Loss SS66:  0.04121948406100273\n",
            "Loss SS77:  0.04733087313739029\n",
            "Loss SS88:  0.045388735159560364\n",
            "Loss SS11:  0.046519356972176185\n",
            "Loss SS22:  0.04323135522648323\n",
            "Loss SS33:  0.04318216966449722\n",
            "Loss SS44:  0.04083913570839511\n",
            "Loss SS55:  0.0422301589826907\n",
            "Loss SS66:  0.041283607575272725\n",
            "Loss SS77:  0.0475344734866757\n",
            "Loss SS88:  0.04553407698500255\n",
            "Loss SS11:  0.046667161788649235\n",
            "Loss SS22:  0.04324746498738537\n",
            "Loss SS33:  0.04317785008945538\n",
            "Loss SS44:  0.040864639325451306\n",
            "Loss SS55:  0.042226568546913965\n",
            "Loss SS66:  0.04119349771556054\n",
            "Loss SS77:  0.04766744938288026\n",
            "Loss SS88:  0.045611596716041786\n",
            "Loss SS11:  0.04668959462684943\n",
            "Loss SS22:  0.043229252099990845\n",
            "Loss SS33:  0.04320858207577509\n",
            "Loss SS44:  0.04088964622388495\n",
            "Loss SS55:  0.0422607680373158\n",
            "Loss SS66:  0.041222616950882245\n",
            "Loss SS77:  0.04775065236480523\n",
            "Loss SS88:  0.04567686196231673\n",
            "Loss SS11:  0.04660017406012838\n",
            "Loss SS22:  0.043139383616234296\n",
            "Loss SS33:  0.043188603335855814\n",
            "Loss SS44:  0.0408984079512934\n",
            "Loss SS55:  0.04222716070366222\n",
            "Loss SS66:  0.041135307105369126\n",
            "Loss SS77:  0.04761248059718814\n",
            "Loss SS88:  0.045672674842228166\n",
            "Loss SS11:  0.04665227826707852\n",
            "Loss SS22:  0.04307200012277372\n",
            "Loss SS33:  0.04318646725660526\n",
            "Loss SS44:  0.04086088537244323\n",
            "Loss SS55:  0.04218424465622961\n",
            "Loss SS66:  0.041109122382187696\n",
            "Loss SS77:  0.04751639368926516\n",
            "Loss SS88:  0.04561591335797902\n",
            "Loss SS11:  0.04668188352159589\n",
            "Loss SS22:  0.04317842665732952\n",
            "Loss SS33:  0.04322062221448324\n",
            "Loss SS44:  0.040881383379823284\n",
            "Loss SS55:  0.042297699908066914\n",
            "Loss SS66:  0.04126427788832034\n",
            "Loss SS77:  0.047590631229138515\n",
            "Loss SS88:  0.04571530763168781\n",
            "Loss SS11:  0.04677759197370782\n",
            "Loss SS22:  0.043195157893290176\n",
            "Loss SS33:  0.043280117913504335\n",
            "Loss SS44:  0.04091264387767618\n",
            "Loss SS55:  0.04232507332830139\n",
            "Loss SS66:  0.0413266903623033\n",
            "Loss SS77:  0.04765633180819822\n",
            "Loss SS88:  0.04580393491199662\n",
            "Loss SS11:  0.046855914772180984\n",
            "Loss SS22:  0.043177732954006544\n",
            "Loss SS33:  0.04325276385037063\n",
            "Loss SS44:  0.04090711238418574\n",
            "Loss SS55:  0.04226713196535385\n",
            "Loss SS66:  0.041304160650644\n",
            "Loss SS77:  0.04762046138575564\n",
            "Loss SS88:  0.04579306783944524\n",
            "Loss SS11:  0.04686782787095255\n",
            "Loss SS22:  0.043118708391687764\n",
            "Loss SS33:  0.043267777791960324\n",
            "Loss SS44:  0.040914546550061576\n",
            "Loss SS55:  0.04222330684537318\n",
            "Loss SS66:  0.04128754576343802\n",
            "Loss SS77:  0.0476352189533153\n",
            "Loss SS88:  0.045776737955820504\n",
            "Loss SS11:  0.04692318673594303\n",
            "Loss SS22:  0.043107822519766775\n",
            "Loss SS33:  0.04331327468086193\n",
            "Loss SS44:  0.04090784751408473\n",
            "Loss SS55:  0.04217432672378576\n",
            "Loss SS66:  0.041308796236300356\n",
            "Loss SS77:  0.04753040758919377\n",
            "Loss SS88:  0.045696292531575075\n",
            "Loss SS11:  0.046906451833733605\n",
            "Loss SS22:  0.04309209828935058\n",
            "Loss SS33:  0.0432851510923103\n",
            "Loss SS44:  0.04091800858869272\n",
            "Loss SS55:  0.04214040055129323\n",
            "Loss SS66:  0.04128443874519874\n",
            "Loss SS77:  0.047431566579714084\n",
            "Loss SS88:  0.04565269948400524\n",
            "Loss SS11:  0.04696261284477783\n",
            "Loss SS22:  0.04316952797177034\n",
            "Loss SS33:  0.04327155315837303\n",
            "Loss SS44:  0.04089891746059641\n",
            "Loss SS55:  0.042112765406375326\n",
            "Loss SS66:  0.041244935018546654\n",
            "Loss SS77:  0.04740374649822454\n",
            "Loss SS88:  0.04565594156647657\n",
            "Loss SS11:  0.04691999830707475\n",
            "Loss SS22:  0.043212239070181034\n",
            "Loss SS33:  0.043222059120899416\n",
            "Loss SS44:  0.040896925317795936\n",
            "Loss SS55:  0.04211892767256721\n",
            "Loss SS66:  0.04124088630203884\n",
            "Loss SS77:  0.04739271334654563\n",
            "Loss SS88:  0.04565385481752301\n",
            "Loss SS11:  0.04683845963670438\n",
            "Loss SS22:  0.04325528947480646\n",
            "Loss SS33:  0.04320063078367853\n",
            "Loss SS44:  0.0409102931440589\n",
            "Loss SS55:  0.042173590807919485\n",
            "Loss SS66:  0.04126147053156241\n",
            "Loss SS77:  0.04742096373403215\n",
            "Loss SS88:  0.045671423518681434\n",
            "Loss SS11:  0.04683925723835426\n",
            "Loss SS22:  0.04322826047338745\n",
            "Loss SS33:  0.043209319670195784\n",
            "Loss SS44:  0.04091253909514325\n",
            "Loss SS55:  0.04222376728822902\n",
            "Loss SS66:  0.04132508746280524\n",
            "Loss SS77:  0.04739502246466633\n",
            "Loss SS88:  0.045691529367390266\n",
            "Loss SS11:  0.046751120447342684\n",
            "Loss SS22:  0.04324203544052325\n",
            "Loss SS33:  0.043181914258817025\n",
            "Loss SS44:  0.04092179568711242\n",
            "Loss SS55:  0.0422687758708792\n",
            "Loss SS66:  0.04132516689021209\n",
            "Loss SS77:  0.04735159691134502\n",
            "Loss SS88:  0.04574238478697534\n",
            "Loss SS11:  0.04679301581295784\n",
            "Loss SS22:  0.04327536882083611\n",
            "Loss SS33:  0.043198381109806144\n",
            "Loss SS44:  0.04094966117862705\n",
            "Loss SS55:  0.04227784276008606\n",
            "Loss SS66:  0.04134039327608309\n",
            "Loss SS77:  0.047332894030520926\n",
            "Loss SS88:  0.04574550606071737\n",
            "Loss SS11:  0.046771668870629314\n",
            "Loss SS22:  0.04332495268267864\n",
            "Loss SS33:  0.04318629852992153\n",
            "Loss SS44:  0.040954910257120725\n",
            "Loss SS55:  0.042285015480112785\n",
            "Loss SS66:  0.041323814541101456\n",
            "Loss SS77:  0.04744673018500567\n",
            "Loss SS88:  0.045770464228190914\n",
            "Loss SS11:  0.04670909174603481\n",
            "Loss SS22:  0.04329849252372089\n",
            "Loss SS33:  0.043159534504940346\n",
            "Loss SS44:  0.040947816461522714\n",
            "Loss SS55:  0.04225052817665857\n",
            "Loss SS66:  0.04131705940165789\n",
            "Loss SS77:  0.047446587899594606\n",
            "Loss SS88:  0.04579454329966311\n",
            "Loss SS11:  0.046712315015470865\n",
            "Loss SS22:  0.043235786606548685\n",
            "Loss SS33:  0.04313858713463572\n",
            "Loss SS44:  0.040951740907032005\n",
            "Loss SS55:  0.04220380079468347\n",
            "Loss SS66:  0.04129382021653307\n",
            "Loss SS77:  0.04739898251567239\n",
            "Loss SS88:  0.04576121874369226\n",
            "Loss SS11:  0.046791265651908616\n",
            "Loss SS22:  0.043230724053869365\n",
            "Loss SS33:  0.04315601091982791\n",
            "Loss SS44:  0.04097129343036922\n",
            "Loss SS55:  0.04218350379869945\n",
            "Loss SS66:  0.04129185067994572\n",
            "Loss SS77:  0.04738560983928564\n",
            "Loss SS88:  0.04574482363425311\n",
            "Loss SS11:  0.04678735393591878\n",
            "Loss SS22:  0.043197406709104144\n",
            "Loss SS33:  0.04314705031696043\n",
            "Loss SS44:  0.04098411396235858\n",
            "Loss SS55:  0.04214154703829584\n",
            "Loss SS66:  0.04128218838365417\n",
            "Loss SS77:  0.04736647733523766\n",
            "Loss SS88:  0.04570463485453064\n",
            "Loss SS11:  0.04678452234507656\n",
            "Loss SS22:  0.0431892429956418\n",
            "Loss SS33:  0.04317733902167365\n",
            "Loss SS44:  0.04096672526639927\n",
            "Loss SS55:  0.042155722937276284\n",
            "Loss SS66:  0.04126896034063132\n",
            "Loss SS77:  0.047368177417611096\n",
            "Loss SS88:  0.04567736687679445\n",
            "Loss SS11:  0.046770682096736044\n",
            "Loss SS22:  0.043186903806493496\n",
            "Loss SS33:  0.043184519981556806\n",
            "Loss SS44:  0.040954464733430805\n",
            "Loss SS55:  0.04214131422447003\n",
            "Loss SS66:  0.041269903954787135\n",
            "Loss SS77:  0.04738090580005591\n",
            "Loss SS88:  0.04568669194636861\n",
            "Loss SS11:  0.04680881210682795\n",
            "Loss SS22:  0.0431721900948362\n",
            "Loss SS33:  0.04318877554129696\n",
            "Loss SS44:  0.04094620738333282\n",
            "Loss SS55:  0.042141492274742044\n",
            "Loss SS66:  0.041272878110243674\n",
            "Loss SS77:  0.047376830316564054\n",
            "Loss SS88:  0.04565927942497578\n",
            "Loss SS11:  0.04680206602554758\n",
            "Loss SS22:  0.043147958353201975\n",
            "Loss SS33:  0.04317448475972983\n",
            "Loss SS44:  0.040932906736341125\n",
            "Loss SS55:  0.0421137912776271\n",
            "Loss SS66:  0.04126402738722508\n",
            "Loss SS77:  0.047337993477393356\n",
            "Loss SS88:  0.04564797594099032\n",
            "Loss SS11:  0.046741748673552914\n",
            "Loss SS22:  0.04312495582216368\n",
            "Loss SS33:  0.04314954203414166\n",
            "Loss SS44:  0.04088759798742342\n",
            "Loss SS55:  0.04208527884771192\n",
            "Loss SS66:  0.041247311021321086\n",
            "Loss SS77:  0.047285082860021144\n",
            "Loss SS88:  0.04558572303161534\n",
            "Loss SS11:  0.046702218920830874\n",
            "Loss SS22:  0.04310203255023188\n",
            "Loss SS33:  0.0431075219608024\n",
            "Loss SS44:  0.04086222231883527\n",
            "Loss SS55:  0.04206824351264083\n",
            "Loss SS66:  0.0412390448553178\n",
            "Loss SS77:  0.04722290181214242\n",
            "Loss SS88:  0.04555840447278279\n",
            "Loss SS11:  0.046692996371788274\n",
            "Loss SS22:  0.04307665072028476\n",
            "Loss SS33:  0.043108951300382614\n",
            "Loss SS44:  0.04084574555666964\n",
            "Loss SS55:  0.042061200574449176\n",
            "Loss SS66:  0.04123589902781786\n",
            "Loss SS77:  0.047213707964616526\n",
            "Loss SS88:  0.04554264453655466\n",
            "Loss SS11:  0.04667580032544415\n",
            "Loss SS22:  0.04308178843227906\n",
            "Loss SS33:  0.04310767881010281\n",
            "Loss SS44:  0.04083545544975575\n",
            "Loss SS55:  0.042054049190306024\n",
            "Loss SS66:  0.04122579502907112\n",
            "Loss SS77:  0.047189514621765946\n",
            "Loss SS88:  0.04552863251843882\n",
            "Loss SS11:  0.04668616984844774\n",
            "Loss SS22:  0.04309249118926123\n",
            "Loss SS33:  0.043113252382521956\n",
            "Loss SS44:  0.040845430201702615\n",
            "Loss SS55:  0.04204774282501033\n",
            "Loss SS66:  0.04126474430293482\n",
            "Loss SS77:  0.047215942452681604\n",
            "Loss SS88:  0.04557544840698854\n",
            "Loss SS11:  0.046672518656798695\n",
            "Loss SS22:  0.04307647937964951\n",
            "Loss SS33:  0.043066568499789716\n",
            "Loss SS44:  0.04081788463755716\n",
            "Loss SS55:  0.042030884985857386\n",
            "Loss SS66:  0.04124311874609812\n",
            "Loss SS77:  0.047199283253289155\n",
            "Loss SS88:  0.045522762821196404\n",
            "Loss SS11:  0.046660692192394444\n",
            "Loss SS22:  0.04309332533792303\n",
            "Loss SS33:  0.043047930310372594\n",
            "Loss SS44:  0.040828340912824855\n",
            "Loss SS55:  0.04201982685632987\n",
            "Loss SS66:  0.04124465578841784\n",
            "Loss SS77:  0.047211728525472604\n",
            "Loss SS88:  0.04552837671572659\n",
            "Loss SS11:  0.046654940650957384\n",
            "Loss SS22:  0.04309124759751519\n",
            "Loss SS33:  0.04304802743201774\n",
            "Loss SS44:  0.04084881773412624\n",
            "Loss SS55:  0.042090398410207154\n",
            "Loss SS66:  0.041263365758760544\n",
            "Loss SS77:  0.04724511928740732\n",
            "Loss SS88:  0.0455302353741853\n",
            "Loss SS11:  0.04661487945539315\n",
            "Loss SS22:  0.043093108093273096\n",
            "Loss SS33:  0.043045107291409354\n",
            "Loss SS44:  0.0408631092393993\n",
            "Loss SS55:  0.04209867278174568\n",
            "Loss SS66:  0.04128159918779924\n",
            "Loss SS77:  0.04725593237161895\n",
            "Loss SS88:  0.045554609342989334\n",
            "Loss SS11:  0.04661083098050136\n",
            "Loss SS22:  0.043129395068704196\n",
            "Loss SS33:  0.04302321417917618\n",
            "Loss SS44:  0.04085321907120146\n",
            "Loss SS55:  0.04211264330648566\n",
            "Loss SS66:  0.04128983361357843\n",
            "Loss SS77:  0.04726903944131928\n",
            "Loss SS88:  0.04556666793115832\n",
            "Loss SS11:  0.046627969486859634\n",
            "Loss SS22:  0.043155807885583375\n",
            "Loss SS33:  0.04300475460544941\n",
            "Loss SS44:  0.04085338973825538\n",
            "Loss SS55:  0.042124509168575804\n",
            "Loss SS66:  0.04128008977105365\n",
            "Loss SS77:  0.04726963485600795\n",
            "Loss SS88:  0.0455531208561017\n",
            "Loss SS11:  0.04659240139750745\n",
            "Loss SS22:  0.04310052199270235\n",
            "Loss SS33:  0.042960882763512995\n",
            "Loss SS44:  0.040833405619240586\n",
            "Loss SS55:  0.042103296923479576\n",
            "Loss SS66:  0.04124652895537753\n",
            "Loss SS77:  0.047210612958354034\n",
            "Loss SS88:  0.04550055695162771\n",
            "Validation: \n",
            " Loss SS11:  0.039965175092220306\n",
            " Loss SS22:  0.056663159281015396\n",
            " Loss SS33:  0.06091434881091118\n",
            " Loss SS55:  0.05286061018705368\n",
            " Loss SS66:  0.06309792399406433\n",
            " Loss SS77:  0.06225154176354408\n",
            " Loss SS88:  0.05812769755721092\n",
            " Loss SS99:  0.05647434666752815\n",
            " Loss SS11:  0.0455527717158908\n",
            " Loss SS22:  0.06556434042397\n",
            " Loss SS33:  0.06700572265045983\n",
            " Loss SS55:  0.05699489638209343\n",
            " Loss SS66:  0.06791938202721733\n",
            " Loss SS77:  0.064566969871521\n",
            " Loss SS88:  0.07010376400181226\n",
            " Loss SS99:  0.06398997881582805\n",
            " Loss SS11:  0.046078123879141925\n",
            " Loss SS22:  0.06438420667517476\n",
            " Loss SS33:  0.06846588486578406\n",
            " Loss SS55:  0.05757665852221047\n",
            " Loss SS66:  0.06688579826093302\n",
            " Loss SS77:  0.06423156045195533\n",
            " Loss SS88:  0.06933221675273848\n",
            " Loss SS99:  0.06333611651164729\n",
            " Loss SS11:  0.046183048701677165\n",
            " Loss SS22:  0.06424546315044653\n",
            " Loss SS33:  0.06774107904219237\n",
            " Loss SS55:  0.057257482079697435\n",
            " Loss SS66:  0.0666164215348783\n",
            " Loss SS77:  0.06398025997838036\n",
            " Loss SS88:  0.06958462035313981\n",
            " Loss SS99:  0.06316395348212758\n",
            " Loss SS11:  0.04610728366691389\n",
            " Loss SS22:  0.0639161002120854\n",
            " Loss SS33:  0.06755056347192069\n",
            " Loss SS55:  0.057194688345915005\n",
            " Loss SS66:  0.0664215408909468\n",
            " Loss SS77:  0.06396297599982333\n",
            " Loss SS88:  0.06947449479758004\n",
            " Loss SS99:  0.06326916399929258\n",
            "\n",
            "Epoch: 52\n",
            "Loss SS11:  0.046299856156110764\n",
            "Loss SS22:  0.045305460691452026\n",
            "Loss SS33:  0.050142936408519745\n",
            "Loss SS44:  0.044965408742427826\n",
            "Loss SS55:  0.04593036696314812\n",
            "Loss SS66:  0.043382227420806885\n",
            "Loss SS77:  0.055825017392635345\n",
            "Loss SS88:  0.05112944915890694\n",
            "Loss SS11:  0.04584933038462292\n",
            "Loss SS22:  0.04119095273993232\n",
            "Loss SS33:  0.0437500730834224\n",
            "Loss SS44:  0.04024663465944203\n",
            "Loss SS55:  0.04147389057007703\n",
            "Loss SS66:  0.040863526138392364\n",
            "Loss SS77:  0.04694304649125446\n",
            "Loss SS88:  0.04574753852053122\n",
            "Loss SS11:  0.04543449410370418\n",
            "Loss SS22:  0.04168947431303206\n",
            "Loss SS33:  0.043293193160068424\n",
            "Loss SS44:  0.04034137371040526\n",
            "Loss SS55:  0.041718408642780216\n",
            "Loss SS66:  0.0406524705744925\n",
            "Loss SS77:  0.04671608567947433\n",
            "Loss SS88:  0.045812436512538364\n",
            "Loss SS11:  0.04530509645419736\n",
            "Loss SS22:  0.042467287591388146\n",
            "Loss SS33:  0.043271825318374944\n",
            "Loss SS44:  0.04092731098494222\n",
            "Loss SS55:  0.04208902606079655\n",
            "Loss SS66:  0.04080201565257965\n",
            "Loss SS77:  0.04715890076852614\n",
            "Loss SS88:  0.0457036828081454\n",
            "Loss SS11:  0.044977018291630394\n",
            "Loss SS22:  0.04250708513143586\n",
            "Loss SS33:  0.04326272465106917\n",
            "Loss SS44:  0.04088183937639725\n",
            "Loss SS55:  0.042476370476368\n",
            "Loss SS66:  0.040881344367091246\n",
            "Loss SS77:  0.04710046701678416\n",
            "Loss SS88:  0.045714309393632704\n",
            "Loss SS11:  0.04534826215867903\n",
            "Loss SS22:  0.04255868940084588\n",
            "Loss SS33:  0.043176564661895525\n",
            "Loss SS44:  0.041080604362137174\n",
            "Loss SS55:  0.04241763705424234\n",
            "Loss SS66:  0.040889820266588064\n",
            "Loss SS77:  0.04736936618300045\n",
            "Loss SS88:  0.0455711331291526\n",
            "Loss SS11:  0.045476415362514434\n",
            "Loss SS22:  0.04249435321229403\n",
            "Loss SS33:  0.04299998246744031\n",
            "Loss SS44:  0.040936057868062474\n",
            "Loss SS55:  0.042221261218923035\n",
            "Loss SS66:  0.04063841218098265\n",
            "Loss SS77:  0.04705803065759237\n",
            "Loss SS88:  0.0454741321870538\n",
            "Loss SS11:  0.04550463606564092\n",
            "Loss SS22:  0.04279450342898637\n",
            "Loss SS33:  0.04306531193810449\n",
            "Loss SS44:  0.041175407947788775\n",
            "Loss SS55:  0.042352271594212086\n",
            "Loss SS66:  0.0408676001702396\n",
            "Loss SS77:  0.04718041157638523\n",
            "Loss SS88:  0.04557727644561042\n",
            "Loss SS11:  0.045638599099568376\n",
            "Loss SS22:  0.042779325473087805\n",
            "Loss SS33:  0.04294459213629181\n",
            "Loss SS44:  0.04098677317853327\n",
            "Loss SS55:  0.04211034795936243\n",
            "Loss SS66:  0.04067499917230488\n",
            "Loss SS77:  0.0469380776271408\n",
            "Loss SS88:  0.04533588044621326\n",
            "Loss SS11:  0.045931826458199994\n",
            "Loss SS22:  0.04283795735010734\n",
            "Loss SS33:  0.042905620411857144\n",
            "Loss SS44:  0.041066623773876126\n",
            "Loss SS55:  0.04196386731096676\n",
            "Loss SS66:  0.04074997929753838\n",
            "Loss SS77:  0.04688596971087403\n",
            "Loss SS88:  0.04528510816149659\n",
            "Loss SS11:  0.04597122200054697\n",
            "Loss SS22:  0.04277316410795297\n",
            "Loss SS33:  0.04283906545231838\n",
            "Loss SS44:  0.04099197051312664\n",
            "Loss SS55:  0.04175846356123981\n",
            "Loss SS66:  0.040612682368200606\n",
            "Loss SS77:  0.0467862105089249\n",
            "Loss SS88:  0.04514016642576397\n",
            "Loss SS11:  0.04596348821714118\n",
            "Loss SS22:  0.04277459491749067\n",
            "Loss SS33:  0.042692240394719014\n",
            "Loss SS44:  0.0409691523726996\n",
            "Loss SS55:  0.041736813446691445\n",
            "Loss SS66:  0.040684046911763715\n",
            "Loss SS77:  0.04678267341207813\n",
            "Loss SS88:  0.045212092264010025\n",
            "Loss SS11:  0.04596812548962506\n",
            "Loss SS22:  0.042795848778702995\n",
            "Loss SS33:  0.04282313628383905\n",
            "Loss SS44:  0.041080339819439184\n",
            "Loss SS55:  0.041883903747994056\n",
            "Loss SS66:  0.04076829096995109\n",
            "Loss SS77:  0.046916624638906196\n",
            "Loss SS88:  0.045397947426916156\n",
            "Loss SS11:  0.0460058267562444\n",
            "Loss SS22:  0.04280512987543608\n",
            "Loss SS33:  0.042899152107593666\n",
            "Loss SS44:  0.04107657655049826\n",
            "Loss SS55:  0.04185761727449548\n",
            "Loss SS66:  0.04076799847014988\n",
            "Loss SS77:  0.04695171441968161\n",
            "Loss SS88:  0.04548381220634657\n",
            "Loss SS11:  0.045919004457850826\n",
            "Loss SS22:  0.04276182166967832\n",
            "Loss SS33:  0.0429052563899375\n",
            "Loss SS44:  0.04103788659504965\n",
            "Loss SS55:  0.04186812962623353\n",
            "Loss SS66:  0.040774027585771914\n",
            "Loss SS77:  0.046969604803949384\n",
            "Loss SS88:  0.04552898143834256\n",
            "Loss SS11:  0.046001591133755564\n",
            "Loss SS22:  0.042707675290818245\n",
            "Loss SS33:  0.04291320120952777\n",
            "Loss SS44:  0.04100787355903758\n",
            "Loss SS55:  0.04189167946379706\n",
            "Loss SS66:  0.04073826196473955\n",
            "Loss SS77:  0.046893615596341774\n",
            "Loss SS88:  0.04557619152578297\n",
            "Loss SS11:  0.046026866142609105\n",
            "Loss SS22:  0.042623030449848\n",
            "Loss SS33:  0.04288729384356404\n",
            "Loss SS44:  0.04092031298494487\n",
            "Loss SS55:  0.041819373203545625\n",
            "Loss SS66:  0.04067353617330516\n",
            "Loss SS77:  0.04677506233131663\n",
            "Loss SS88:  0.04547537342639443\n",
            "Loss SS11:  0.046041787890662926\n",
            "Loss SS22:  0.04270954042324546\n",
            "Loss SS33:  0.04290818745455547\n",
            "Loss SS44:  0.04092522714918817\n",
            "Loss SS55:  0.04193246425592411\n",
            "Loss SS66:  0.04078931940926446\n",
            "Loss SS77:  0.04686158969562653\n",
            "Loss SS88:  0.04552555622326003\n",
            "Loss SS11:  0.0461678559897025\n",
            "Loss SS22:  0.042770047296476627\n",
            "Loss SS33:  0.04293793351363741\n",
            "Loss SS44:  0.040977300221913426\n",
            "Loss SS55:  0.04198052643577038\n",
            "Loss SS66:  0.04091476231237143\n",
            "Loss SS77:  0.04693339917376555\n",
            "Loss SS88:  0.045571538027972806\n",
            "Loss SS11:  0.0463452258815316\n",
            "Loss SS22:  0.04278747294663759\n",
            "Loss SS33:  0.04286919319660876\n",
            "Loss SS44:  0.04091873675272727\n",
            "Loss SS55:  0.04196465173394892\n",
            "Loss SS66:  0.04093620175232438\n",
            "Loss SS77:  0.04698889608938656\n",
            "Loss SS88:  0.045587001592701015\n",
            "Loss SS11:  0.04632944211512063\n",
            "Loss SS22:  0.04279185876027861\n",
            "Loss SS33:  0.04283818763805859\n",
            "Loss SS44:  0.040900564516213406\n",
            "Loss SS55:  0.04190599518035775\n",
            "Loss SS66:  0.04089725082415847\n",
            "Loss SS77:  0.04697397171487262\n",
            "Loss SS88:  0.0455620441937921\n",
            "Loss SS11:  0.046347009291722314\n",
            "Loss SS22:  0.04275311649693132\n",
            "Loss SS33:  0.042871584065293816\n",
            "Loss SS44:  0.04089990367680364\n",
            "Loss SS55:  0.041862940424567716\n",
            "Loss SS66:  0.040873456640407375\n",
            "Loss SS77:  0.04692986886506962\n",
            "Loss SS88:  0.045506864927391304\n",
            "Loss SS11:  0.046330236522185854\n",
            "Loss SS22:  0.042731938757238346\n",
            "Loss SS33:  0.042839543464092106\n",
            "Loss SS44:  0.04092580941400377\n",
            "Loss SS55:  0.04181824631281029\n",
            "Loss SS66:  0.04085534013706635\n",
            "Loss SS77:  0.0469252854012526\n",
            "Loss SS88:  0.04548849200838292\n",
            "Loss SS11:  0.04635195453445633\n",
            "Loss SS22:  0.04278339343992146\n",
            "Loss SS33:  0.04284283260762434\n",
            "Loss SS44:  0.040900890267901606\n",
            "Loss SS55:  0.04180667083307262\n",
            "Loss SS66:  0.04082772984004124\n",
            "Loss SS77:  0.046899621427317205\n",
            "Loss SS88:  0.04549841737592375\n",
            "Loss SS11:  0.04638446913838881\n",
            "Loss SS22:  0.042845829594803056\n",
            "Loss SS33:  0.04282818983513784\n",
            "Loss SS44:  0.04086868570058672\n",
            "Loss SS55:  0.041849297374734244\n",
            "Loss SS66:  0.040843637142314954\n",
            "Loss SS77:  0.04689586754594601\n",
            "Loss SS88:  0.04545347117413129\n",
            "Loss SS11:  0.046397452097964954\n",
            "Loss SS22:  0.042877130833754974\n",
            "Loss SS33:  0.04285729275756623\n",
            "Loss SS44:  0.0408854974485726\n",
            "Loss SS55:  0.04191512935071352\n",
            "Loss SS66:  0.04091506607207169\n",
            "Loss SS77:  0.04692699927910866\n",
            "Loss SS88:  0.04549889495054564\n",
            "Loss SS11:  0.04636831975531304\n",
            "Loss SS22:  0.042861185168626206\n",
            "Loss SS33:  0.04288871230921526\n",
            "Loss SS44:  0.040851589934579255\n",
            "Loss SS55:  0.04194333074592996\n",
            "Loss SS66:  0.04094178353746732\n",
            "Loss SS77:  0.04692310448807318\n",
            "Loss SS88:  0.04548465423428692\n",
            "Loss SS11:  0.04634820448571465\n",
            "Loss SS22:  0.04285206240206627\n",
            "Loss SS33:  0.04286220338456745\n",
            "Loss SS44:  0.040859118200206226\n",
            "Loss SS55:  0.0419003913570814\n",
            "Loss SS66:  0.04091873162896871\n",
            "Loss SS77:  0.04694509988726285\n",
            "Loss SS88:  0.045552987522854575\n",
            "Loss SS11:  0.04633946016802058\n",
            "Loss SS22:  0.04289692943895839\n",
            "Loss SS33:  0.04286582897884566\n",
            "Loss SS44:  0.040888354115957044\n",
            "Loss SS55:  0.041921717429606514\n",
            "Loss SS66:  0.040939790259688774\n",
            "Loss SS77:  0.04691604843025106\n",
            "Loss SS88:  0.045577370054569105\n",
            "Loss SS11:  0.04633091957247544\n",
            "Loss SS22:  0.04296706453016943\n",
            "Loss SS33:  0.042880486658553485\n",
            "Loss SS44:  0.04088856927615261\n",
            "Loss SS55:  0.04193677058078579\n",
            "Loss SS66:  0.04094965647125162\n",
            "Loss SS77:  0.04699161998236302\n",
            "Loss SS88:  0.045620743428513764\n",
            "Loss SS11:  0.04631150172299325\n",
            "Loss SS22:  0.042986986211863074\n",
            "Loss SS33:  0.042866625040274525\n",
            "Loss SS44:  0.040870518989836256\n",
            "Loss SS55:  0.041955223523600156\n",
            "Loss SS66:  0.04097017758460932\n",
            "Loss SS77:  0.04699881226319411\n",
            "Loss SS88:  0.04563423205303195\n",
            "Loss SS11:  0.04627972277078981\n",
            "Loss SS22:  0.04296791106702047\n",
            "Loss SS33:  0.04284664259031654\n",
            "Loss SS44:  0.04084556494686765\n",
            "Loss SS55:  0.04191072582240273\n",
            "Loss SS66:  0.04098383231849149\n",
            "Loss SS77:  0.04696695808429043\n",
            "Loss SS88:  0.0455999006292636\n",
            "Loss SS11:  0.046268108454634464\n",
            "Loss SS22:  0.0429546977988099\n",
            "Loss SS33:  0.04286291113709364\n",
            "Loss SS44:  0.04082954923311869\n",
            "Loss SS55:  0.04192223625019703\n",
            "Loss SS66:  0.04100510789040836\n",
            "Loss SS77:  0.04694983826201653\n",
            "Loss SS88:  0.04556532468751212\n",
            "Loss SS11:  0.04627135178788911\n",
            "Loss SS22:  0.04293239940509335\n",
            "Loss SS33:  0.042856655846099476\n",
            "Loss SS44:  0.0408061182525943\n",
            "Loss SS55:  0.041911784210594036\n",
            "Loss SS66:  0.040999665518721785\n",
            "Loss SS77:  0.04694034721916176\n",
            "Loss SS88:  0.04554054031874478\n",
            "Loss SS11:  0.04624720032482553\n",
            "Loss SS22:  0.04294469404168143\n",
            "Loss SS33:  0.04285803784635409\n",
            "Loss SS44:  0.04078880004868829\n",
            "Loss SS55:  0.04193895784291354\n",
            "Loss SS66:  0.041011106594304535\n",
            "Loss SS77:  0.04691243947338848\n",
            "Loss SS88:  0.04550248193295121\n",
            "Loss SS11:  0.04624153863628026\n",
            "Loss SS22:  0.04293447793421582\n",
            "Loss SS33:  0.04289564891503407\n",
            "Loss SS44:  0.04080362729409821\n",
            "Loss SS55:  0.04197018949097378\n",
            "Loss SS66:  0.0410211741223804\n",
            "Loss SS77:  0.0469569703643648\n",
            "Loss SS88:  0.045514224654334225\n",
            "Loss SS11:  0.04626162563788594\n",
            "Loss SS22:  0.04290771725848111\n",
            "Loss SS33:  0.042893495513874405\n",
            "Loss SS44:  0.04079121619545522\n",
            "Loss SS55:  0.04194829847219908\n",
            "Loss SS66:  0.041005379966669135\n",
            "Loss SS77:  0.046955614621619436\n",
            "Loss SS88:  0.04551257879326218\n",
            "Loss SS11:  0.0463081465034954\n",
            "Loss SS22:  0.0428929608767084\n",
            "Loss SS33:  0.04285451201496099\n",
            "Loss SS44:  0.040805858964065335\n",
            "Loss SS55:  0.041968228210095125\n",
            "Loss SS66:  0.04100867643992535\n",
            "Loss SS77:  0.046955235824591386\n",
            "Loss SS88:  0.0455022587408274\n",
            "Loss SS11:  0.04631857131057837\n",
            "Loss SS22:  0.042858792239130326\n",
            "Loss SS33:  0.042804424247638445\n",
            "Loss SS44:  0.04078847582493554\n",
            "Loss SS55:  0.041955114985153745\n",
            "Loss SS66:  0.040991188376516184\n",
            "Loss SS77:  0.04691901139113221\n",
            "Loss SS88:  0.045459617124845975\n",
            "Loss SS11:  0.04633125934340155\n",
            "Loss SS22:  0.042824179115121624\n",
            "Loss SS33:  0.04277455671440305\n",
            "Loss SS44:  0.04075773261354097\n",
            "Loss SS55:  0.041938708394842074\n",
            "Loss SS66:  0.04098008318668436\n",
            "Loss SS77:  0.04686448380084294\n",
            "Loss SS88:  0.045433038031048784\n",
            "Loss SS11:  0.04637297333921875\n",
            "Loss SS22:  0.042808759325831905\n",
            "Loss SS33:  0.042759493597204844\n",
            "Loss SS44:  0.040752661031231915\n",
            "Loss SS55:  0.041947237469088704\n",
            "Loss SS66:  0.04097359020848226\n",
            "Loss SS77:  0.046861042461341755\n",
            "Loss SS88:  0.045424537068962156\n",
            "Loss SS11:  0.0463805823358016\n",
            "Loss SS22:  0.04281508492944885\n",
            "Loss SS33:  0.04273063666810374\n",
            "Loss SS44:  0.04072296752655593\n",
            "Loss SS55:  0.04192843070648013\n",
            "Loss SS66:  0.040989688604417505\n",
            "Loss SS77:  0.04684880168297285\n",
            "Loss SS88:  0.04541390919446075\n",
            "Loss SS11:  0.04639487581730172\n",
            "Loss SS22:  0.04280197984072205\n",
            "Loss SS33:  0.042755631345043274\n",
            "Loss SS44:  0.040755972903301484\n",
            "Loss SS55:  0.04192885961781771\n",
            "Loss SS66:  0.041012133588912654\n",
            "Loss SS77:  0.04686088946751914\n",
            "Loss SS88:  0.0454440569934256\n",
            "Loss SS11:  0.04636916108515866\n",
            "Loss SS22:  0.04277794559220427\n",
            "Loss SS33:  0.0427199579481459\n",
            "Loss SS44:  0.040723906047728665\n",
            "Loss SS55:  0.04190613741699062\n",
            "Loss SS66:  0.04098834340263962\n",
            "Loss SS77:  0.046823358202132716\n",
            "Loss SS88:  0.04541476484033735\n",
            "Loss SS11:  0.046362630102166784\n",
            "Loss SS22:  0.04278957398604103\n",
            "Loss SS33:  0.04274162445681976\n",
            "Loss SS44:  0.04073201137639228\n",
            "Loss SS55:  0.04191598600270797\n",
            "Loss SS66:  0.04100871859054987\n",
            "Loss SS77:  0.046827706290818664\n",
            "Loss SS88:  0.04540248073282696\n",
            "Loss SS11:  0.046361640252652034\n",
            "Loss SS22:  0.04279387176466358\n",
            "Loss SS33:  0.0427330677250933\n",
            "Loss SS44:  0.04075216756361287\n",
            "Loss SS55:  0.04197997959979093\n",
            "Loss SS66:  0.04102857877601541\n",
            "Loss SS77:  0.0468608125994713\n",
            "Loss SS88:  0.04540635568570932\n",
            "Loss SS11:  0.04638636883938131\n",
            "Loss SS22:  0.0428026430147201\n",
            "Loss SS33:  0.042743715613175366\n",
            "Loss SS44:  0.040768954531166406\n",
            "Loss SS55:  0.04200904392142358\n",
            "Loss SS66:  0.04104858928576209\n",
            "Loss SS77:  0.046911067105102434\n",
            "Loss SS88:  0.045455146589015415\n",
            "Loss SS11:  0.04637921188667322\n",
            "Loss SS22:  0.04284071138488512\n",
            "Loss SS33:  0.04277081281389654\n",
            "Loss SS44:  0.04077623236426123\n",
            "Loss SS55:  0.04204698391021437\n",
            "Loss SS66:  0.04109035070882734\n",
            "Loss SS77:  0.04694710941609557\n",
            "Loss SS88:  0.04546716479454071\n",
            "Loss SS11:  0.046381479411883565\n",
            "Loss SS22:  0.04282717120963422\n",
            "Loss SS33:  0.04275429022659135\n",
            "Loss SS44:  0.04074604874341255\n",
            "Loss SS55:  0.042044677676753045\n",
            "Loss SS66:  0.041117529301534325\n",
            "Loss SS77:  0.04694171813694206\n",
            "Loss SS88:  0.045455726773600075\n",
            "Loss SS11:  0.04631803161986248\n",
            "Loss SS22:  0.04277297896152844\n",
            "Loss SS33:  0.042708561780375034\n",
            "Loss SS44:  0.040724969846471504\n",
            "Loss SS55:  0.042038174061571267\n",
            "Loss SS66:  0.04109857096327298\n",
            "Loss SS77:  0.04688191504925186\n",
            "Loss SS88:  0.04539944369596279\n",
            "Validation: \n",
            " Loss SS11:  0.041401613503694534\n",
            " Loss SS22:  0.058712709695100784\n",
            " Loss SS33:  0.06137622520327568\n",
            " Loss SS55:  0.052989233285188675\n",
            " Loss SS66:  0.06492877751588821\n",
            " Loss SS77:  0.06340692937374115\n",
            " Loss SS88:  0.0597117617726326\n",
            " Loss SS99:  0.05982217565178871\n",
            " Loss SS11:  0.04605220967815036\n",
            " Loss SS22:  0.06660594560560726\n",
            " Loss SS33:  0.06665468517513502\n",
            " Loss SS55:  0.05684555144537063\n",
            " Loss SS66:  0.06953374331905729\n",
            " Loss SS77:  0.06648286343330428\n",
            " Loss SS88:  0.07065212620156151\n",
            " Loss SS99:  0.06675740065319198\n",
            " Loss SS11:  0.0465980205775761\n",
            " Loss SS22:  0.06523120303342982\n",
            " Loss SS33:  0.06791496531265538\n",
            " Loss SS55:  0.05741004882062354\n",
            " Loss SS66:  0.06845497694320796\n",
            " Loss SS77:  0.06594649411556197\n",
            " Loss SS88:  0.06963589851085733\n",
            " Loss SS99:  0.06585798330786752\n",
            " Loss SS11:  0.046720180843697216\n",
            " Loss SS22:  0.06516062010262834\n",
            " Loss SS33:  0.06724528919477932\n",
            " Loss SS55:  0.05693245160042262\n",
            " Loss SS66:  0.06821349391438922\n",
            " Loss SS77:  0.06556782346279895\n",
            " Loss SS88:  0.07000448463148758\n",
            " Loss SS99:  0.06560442920346729\n",
            " Loss SS11:  0.046660537429061934\n",
            " Loss SS22:  0.06482592248070387\n",
            " Loss SS33:  0.06707953072992372\n",
            " Loss SS55:  0.05670989405961684\n",
            " Loss SS66:  0.06800168163982438\n",
            " Loss SS77:  0.06543747004535463\n",
            " Loss SS88:  0.06983354282967838\n",
            " Loss SS99:  0.06567407100472922\n",
            "\n",
            "Epoch: 53\n",
            "Loss SS11:  0.04646049439907074\n",
            "Loss SS22:  0.04712846502661705\n",
            "Loss SS33:  0.046945832669734955\n",
            "Loss SS44:  0.042378176003694534\n",
            "Loss SS55:  0.047693364322185516\n",
            "Loss SS66:  0.04564787819981575\n",
            "Loss SS77:  0.053901150822639465\n",
            "Loss SS88:  0.05043119937181473\n",
            "Loss SS11:  0.04680702056397091\n",
            "Loss SS22:  0.04205784811214967\n",
            "Loss SS33:  0.04224748096682809\n",
            "Loss SS44:  0.04020124538378282\n",
            "Loss SS55:  0.041130373762412506\n",
            "Loss SS66:  0.04057602618228306\n",
            "Loss SS77:  0.046303872696378014\n",
            "Loss SS88:  0.04515764049508355\n",
            "Loss SS11:  0.046092369549331214\n",
            "Loss SS22:  0.04221447982958385\n",
            "Loss SS33:  0.04317111646135648\n",
            "Loss SS44:  0.0401804635212535\n",
            "Loss SS55:  0.041846527939751035\n",
            "Loss SS66:  0.04076099431230908\n",
            "Loss SS77:  0.045920814077059426\n",
            "Loss SS88:  0.04505944766459011\n",
            "Loss SS11:  0.0470234966806827\n",
            "Loss SS22:  0.0425710879987286\n",
            "Loss SS33:  0.04340796893642795\n",
            "Loss SS44:  0.040802323049114596\n",
            "Loss SS55:  0.041926433242136435\n",
            "Loss SS66:  0.04088545386349001\n",
            "Loss SS77:  0.046613158237549565\n",
            "Loss SS88:  0.045534149052635316\n",
            "Loss SS11:  0.046417750145603974\n",
            "Loss SS22:  0.04265942424535751\n",
            "Loss SS33:  0.043453800605564585\n",
            "Loss SS44:  0.04085241576156965\n",
            "Loss SS55:  0.0422920055869149\n",
            "Loss SS66:  0.0410820467443001\n",
            "Loss SS77:  0.04651447494582432\n",
            "Loss SS88:  0.04552624002099037\n",
            "Loss SS11:  0.0465662537106112\n",
            "Loss SS22:  0.042647293969696645\n",
            "Loss SS33:  0.04328426054003192\n",
            "Loss SS44:  0.041020125661994894\n",
            "Loss SS55:  0.0423275199444855\n",
            "Loss SS66:  0.041077740490436554\n",
            "Loss SS77:  0.04672366059293934\n",
            "Loss SS88:  0.04561581820541737\n",
            "Loss SS11:  0.046750977391102275\n",
            "Loss SS22:  0.04260889904909446\n",
            "Loss SS33:  0.04315919880984259\n",
            "Loss SS44:  0.04090954106850702\n",
            "Loss SS55:  0.04213597442283005\n",
            "Loss SS66:  0.040832066511521575\n",
            "Loss SS77:  0.046593885624506434\n",
            "Loss SS88:  0.04552702306479704\n",
            "Loss SS11:  0.04683372514768386\n",
            "Loss SS22:  0.04284025608024127\n",
            "Loss SS33:  0.043367628218002725\n",
            "Loss SS44:  0.04106135068225189\n",
            "Loss SS55:  0.042248189659185816\n",
            "Loss SS66:  0.04095966930330639\n",
            "Loss SS77:  0.046716678520323525\n",
            "Loss SS88:  0.045528411707827746\n",
            "Loss SS11:  0.04674329576484951\n",
            "Loss SS22:  0.04286993248963061\n",
            "Loss SS33:  0.04318788376303367\n",
            "Loss SS44:  0.0408172885208954\n",
            "Loss SS55:  0.041997736756816326\n",
            "Loss SS66:  0.04073783750703305\n",
            "Loss SS77:  0.04654632067238843\n",
            "Loss SS88:  0.04527707020809621\n",
            "Loss SS11:  0.04685174722920407\n",
            "Loss SS22:  0.042905804506697495\n",
            "Loss SS33:  0.0431605471441379\n",
            "Loss SS44:  0.04070106938808829\n",
            "Loss SS55:  0.04187612112734344\n",
            "Loss SS66:  0.04076990431972912\n",
            "Loss SS77:  0.046576735410061514\n",
            "Loss SS88:  0.04524862602516845\n",
            "Loss SS11:  0.04683917355124313\n",
            "Loss SS22:  0.042849892768824456\n",
            "Loss SS33:  0.04313825753213155\n",
            "Loss SS44:  0.04066401424974498\n",
            "Loss SS55:  0.04167967525744202\n",
            "Loss SS66:  0.04059702965735209\n",
            "Loss SS77:  0.04658177871220183\n",
            "Loss SS88:  0.04517635917014415\n",
            "Loss SS11:  0.04673179309513118\n",
            "Loss SS22:  0.042915255200487\n",
            "Loss SS33:  0.043035309124100314\n",
            "Loss SS44:  0.040655468632509045\n",
            "Loss SS55:  0.041730484077790835\n",
            "Loss SS66:  0.04068439235692626\n",
            "Loss SS77:  0.04658093555150805\n",
            "Loss SS88:  0.04512226326508565\n",
            "Loss SS11:  0.04652990326901113\n",
            "Loss SS22:  0.04299226695717859\n",
            "Loss SS33:  0.04314081420090573\n",
            "Loss SS44:  0.040754173382008374\n",
            "Loss SS55:  0.04186157974576162\n",
            "Loss SS66:  0.04085096966259736\n",
            "Loss SS77:  0.04669137252879537\n",
            "Loss SS88:  0.045289127571769984\n",
            "Loss SS11:  0.04659054084247305\n",
            "Loss SS22:  0.04299167461181415\n",
            "Loss SS33:  0.04315870303575319\n",
            "Loss SS44:  0.040775873307280865\n",
            "Loss SS55:  0.04192577316674567\n",
            "Loss SS66:  0.0408516034256411\n",
            "Loss SS77:  0.046779424634598596\n",
            "Loss SS88:  0.04537418635405657\n",
            "Loss SS11:  0.046584513831011795\n",
            "Loss SS22:  0.04298636235349567\n",
            "Loss SS33:  0.043148026847881625\n",
            "Loss SS44:  0.04076623964182874\n",
            "Loss SS55:  0.041974496783305566\n",
            "Loss SS66:  0.04090579622920523\n",
            "Loss SS77:  0.04685481960046376\n",
            "Loss SS88:  0.04541216597806477\n",
            "Loss SS11:  0.04645342017147715\n",
            "Loss SS22:  0.04291767930451608\n",
            "Loss SS33:  0.043098075673082804\n",
            "Loss SS44:  0.04078319908076564\n",
            "Loss SS55:  0.04196462725961445\n",
            "Loss SS66:  0.04086956571762925\n",
            "Loss SS77:  0.0467408781760181\n",
            "Loss SS88:  0.045436437802993696\n",
            "Loss SS11:  0.046472993048821916\n",
            "Loss SS22:  0.04283300046780095\n",
            "Loss SS33:  0.04304734332868772\n",
            "Loss SS44:  0.040660024568531086\n",
            "Loss SS55:  0.04182074350877578\n",
            "Loss SS66:  0.04080510410185186\n",
            "Loss SS77:  0.04666590667474344\n",
            "Loss SS88:  0.04538167848812867\n",
            "Loss SS11:  0.046443886684569696\n",
            "Loss SS22:  0.042907302687216924\n",
            "Loss SS33:  0.04306570887129906\n",
            "Loss SS44:  0.04073349538950892\n",
            "Loss SS55:  0.04190508125905405\n",
            "Loss SS66:  0.04087458561822685\n",
            "Loss SS77:  0.04673963669709295\n",
            "Loss SS88:  0.045463335988988655\n",
            "Loss SS11:  0.04654455948518126\n",
            "Loss SS22:  0.04295977882176473\n",
            "Loss SS33:  0.04316246182302744\n",
            "Loss SS44:  0.04078986586292804\n",
            "Loss SS55:  0.04198481850531879\n",
            "Loss SS66:  0.04097816179305809\n",
            "Loss SS77:  0.046835942821608065\n",
            "Loss SS88:  0.04557857296084831\n",
            "Loss SS11:  0.04665372114999132\n",
            "Loss SS22:  0.04293447615900589\n",
            "Loss SS33:  0.04310844670883648\n",
            "Loss SS44:  0.04074186851216861\n",
            "Loss SS55:  0.041960345356876316\n",
            "Loss SS66:  0.04097636599893345\n",
            "Loss SS77:  0.046819879590528794\n",
            "Loss SS88:  0.045572394261341444\n",
            "Loss SS11:  0.046631354912744824\n",
            "Loss SS22:  0.042919571960891655\n",
            "Loss SS33:  0.04307786619930125\n",
            "Loss SS44:  0.04072716113050186\n",
            "Loss SS55:  0.04195320609938446\n",
            "Loss SS66:  0.040977224281326455\n",
            "Loss SS77:  0.04685735468988988\n",
            "Loss SS88:  0.04549741463281622\n",
            "Loss SS11:  0.04664353445454796\n",
            "Loss SS22:  0.042869568771519366\n",
            "Loss SS33:  0.04306484098541793\n",
            "Loss SS44:  0.040707116743540876\n",
            "Loss SS55:  0.04191548435538301\n",
            "Loss SS66:  0.04091910811326515\n",
            "Loss SS77:  0.04679140529785111\n",
            "Loss SS88:  0.045390608045162185\n",
            "Loss SS11:  0.04659727615266364\n",
            "Loss SS22:  0.04285399551709852\n",
            "Loss SS33:  0.04295488945903821\n",
            "Loss SS44:  0.04067741963186415\n",
            "Loss SS55:  0.041815421514516504\n",
            "Loss SS66:  0.040854382973450884\n",
            "Loss SS77:  0.04674805614810724\n",
            "Loss SS88:  0.04534604949918807\n",
            "Loss SS11:  0.04661351384280564\n",
            "Loss SS22:  0.04291666584549012\n",
            "Loss SS33:  0.04297445101655407\n",
            "Loss SS44:  0.04064385244598637\n",
            "Loss SS55:  0.04177840670078864\n",
            "Loss SS66:  0.040813002299952814\n",
            "Loss SS77:  0.046726703289009276\n",
            "Loss SS88:  0.045297808590389434\n",
            "Loss SS11:  0.046569986616674795\n",
            "Loss SS22:  0.042959332203345676\n",
            "Loss SS33:  0.042980249012777916\n",
            "Loss SS44:  0.040647264369048516\n",
            "Loss SS55:  0.041822372027576216\n",
            "Loss SS66:  0.040822633105442235\n",
            "Loss SS77:  0.04671847494125861\n",
            "Loss SS88:  0.04528080554547646\n",
            "Loss SS11:  0.0464980883219565\n",
            "Loss SS22:  0.04302097794662909\n",
            "Loss SS33:  0.04299618854406345\n",
            "Loss SS44:  0.04065350897283668\n",
            "Loss SS55:  0.041819574214903954\n",
            "Loss SS66:  0.04087363265128249\n",
            "Loss SS77:  0.04674303798799021\n",
            "Loss SS88:  0.04528623206385103\n",
            "Loss SS11:  0.04647032282818323\n",
            "Loss SS22:  0.04301048338527424\n",
            "Loss SS33:  0.04300765506209542\n",
            "Loss SS44:  0.040654226609459325\n",
            "Loss SS55:  0.04187687188101454\n",
            "Loss SS66:  0.04093019904076368\n",
            "Loss SS77:  0.046745715156140455\n",
            "Loss SS88:  0.045264737587778964\n",
            "Loss SS11:  0.046405523135653284\n",
            "Loss SS22:  0.04297526753971497\n",
            "Loss SS33:  0.042960725587232525\n",
            "Loss SS44:  0.040654694187245245\n",
            "Loss SS55:  0.04184196564984058\n",
            "Loss SS66:  0.04089372944622902\n",
            "Loss SS77:  0.04676279394004178\n",
            "Loss SS88:  0.045259386314787105\n",
            "Loss SS11:  0.04635893161080486\n",
            "Loss SS22:  0.04302146328195558\n",
            "Loss SS33:  0.04295489654687376\n",
            "Loss SS44:  0.04067873942841414\n",
            "Loss SS55:  0.04184979728002141\n",
            "Loss SS66:  0.04089069910002773\n",
            "Loss SS77:  0.04679090792174017\n",
            "Loss SS88:  0.04526768095817854\n",
            "Loss SS11:  0.046347220009032804\n",
            "Loss SS22:  0.043073966548717306\n",
            "Loss SS33:  0.042986030226310916\n",
            "Loss SS44:  0.04069019742694098\n",
            "Loss SS55:  0.04187451387496339\n",
            "Loss SS66:  0.04091130472284412\n",
            "Loss SS77:  0.046881980349108114\n",
            "Loss SS88:  0.04528169488835171\n",
            "Loss SS11:  0.04632915242070772\n",
            "Loss SS22:  0.04304166031735284\n",
            "Loss SS33:  0.04297526490153664\n",
            "Loss SS44:  0.04066389214952919\n",
            "Loss SS55:  0.041870593504850256\n",
            "Loss SS66:  0.040898455498721516\n",
            "Loss SS77:  0.04688919830658903\n",
            "Loss SS88:  0.045308311330618654\n",
            "Loss SS11:  0.0463247753895364\n",
            "Loss SS22:  0.04296388253090466\n",
            "Loss SS33:  0.042945320858736896\n",
            "Loss SS44:  0.04066777876144054\n",
            "Loss SS55:  0.04183245090905493\n",
            "Loss SS66:  0.040858054635417425\n",
            "Loss SS77:  0.04687972411130019\n",
            "Loss SS88:  0.04526771000104318\n",
            "Loss SS11:  0.0463434609691859\n",
            "Loss SS22:  0.0429125976780679\n",
            "Loss SS33:  0.04295086535709298\n",
            "Loss SS44:  0.04065809093671053\n",
            "Loss SS55:  0.04181850901181081\n",
            "Loss SS66:  0.04086011686475478\n",
            "Loss SS77:  0.04683095752914375\n",
            "Loss SS88:  0.045251444887335054\n",
            "Loss SS11:  0.046373449806180245\n",
            "Loss SS22:  0.04292794373280332\n",
            "Loss SS33:  0.04296226998695434\n",
            "Loss SS44:  0.04067065113215648\n",
            "Loss SS55:  0.04180850227028582\n",
            "Loss SS66:  0.04086891839736538\n",
            "Loss SS77:  0.04679993214789114\n",
            "Loss SS88:  0.04524140289542898\n",
            "Loss SS11:  0.04635896400447465\n",
            "Loss SS22:  0.04291410374938568\n",
            "Loss SS33:  0.04299952823919984\n",
            "Loss SS44:  0.040685702956491904\n",
            "Loss SS55:  0.041826296586794584\n",
            "Loss SS66:  0.040875662587954505\n",
            "Loss SS77:  0.04681844881509056\n",
            "Loss SS88:  0.045216488370622705\n",
            "Loss SS11:  0.04637082584435784\n",
            "Loss SS22:  0.04290009787746644\n",
            "Loss SS33:  0.043007835045329526\n",
            "Loss SS44:  0.04068563919928339\n",
            "Loss SS55:  0.04183112830984966\n",
            "Loss SS66:  0.04086126969792904\n",
            "Loss SS77:  0.046831106740524965\n",
            "Loss SS88:  0.04520682163770043\n",
            "Loss SS11:  0.046399599307544344\n",
            "Loss SS22:  0.04288610856247411\n",
            "Loss SS33:  0.04298375518038002\n",
            "Loss SS44:  0.04068464194514745\n",
            "Loss SS55:  0.04183194250735219\n",
            "Loss SS66:  0.04086441831492981\n",
            "Loss SS77:  0.04680447586892054\n",
            "Loss SS88:  0.045175036338062496\n",
            "Loss SS11:  0.04640149958732957\n",
            "Loss SS22:  0.04287706973216926\n",
            "Loss SS33:  0.04293973199681773\n",
            "Loss SS44:  0.04069135787914384\n",
            "Loss SS55:  0.04183052431018847\n",
            "Loss SS66:  0.04086083884669764\n",
            "Loss SS77:  0.046787456672066303\n",
            "Loss SS88:  0.04514864861282377\n",
            "Loss SS11:  0.046374515882079685\n",
            "Loss SS22:  0.04284695122302987\n",
            "Loss SS33:  0.042880083231397186\n",
            "Loss SS44:  0.04065311693261302\n",
            "Loss SS55:  0.04183026406122005\n",
            "Loss SS66:  0.04085218551473355\n",
            "Loss SS77:  0.0467425351791338\n",
            "Loss SS88:  0.045110618192066985\n",
            "Loss SS11:  0.046399848854831414\n",
            "Loss SS22:  0.0428135878194476\n",
            "Loss SS33:  0.042857053191841715\n",
            "Loss SS44:  0.04061501801890485\n",
            "Loss SS55:  0.04181721738880248\n",
            "Loss SS66:  0.04083078465117213\n",
            "Loss SS77:  0.046712150859177265\n",
            "Loss SS88:  0.045074810285854826\n",
            "Loss SS11:  0.046436067670583725\n",
            "Loss SS22:  0.04280022659020828\n",
            "Loss SS33:  0.04284061681005426\n",
            "Loss SS44:  0.0406093592767109\n",
            "Loss SS55:  0.0418022882815757\n",
            "Loss SS66:  0.04085391186679391\n",
            "Loss SS77:  0.04669439799136057\n",
            "Loss SS88:  0.0450652164878542\n",
            "Loss SS11:  0.046451005404883054\n",
            "Loss SS22:  0.042802979358428875\n",
            "Loss SS33:  0.04282015013687512\n",
            "Loss SS44:  0.040601400073862426\n",
            "Loss SS55:  0.041798486326732776\n",
            "Loss SS66:  0.04084556396606485\n",
            "Loss SS77:  0.04668297545209418\n",
            "Loss SS88:  0.045044247840515306\n",
            "Loss SS11:  0.04646085588495975\n",
            "Loss SS22:  0.04278218262163024\n",
            "Loss SS33:  0.042823253531283266\n",
            "Loss SS44:  0.040603702343699484\n",
            "Loss SS55:  0.04178953394647732\n",
            "Loss SS66:  0.0408478579848934\n",
            "Loss SS77:  0.0466824680425209\n",
            "Loss SS88:  0.04504905568519851\n",
            "Loss SS11:  0.04647133194273143\n",
            "Loss SS22:  0.04275295929226964\n",
            "Loss SS33:  0.04278938761823692\n",
            "Loss SS44:  0.04058865825980556\n",
            "Loss SS55:  0.041786649050471955\n",
            "Loss SS66:  0.04083571383370877\n",
            "Loss SS77:  0.04666561174385785\n",
            "Loss SS88:  0.04502476890962528\n",
            "Loss SS11:  0.04650495514746696\n",
            "Loss SS22:  0.042757663951844584\n",
            "Loss SS33:  0.042794969693130375\n",
            "Loss SS44:  0.04061046116003374\n",
            "Loss SS55:  0.04178552521295558\n",
            "Loss SS66:  0.04085118478151406\n",
            "Loss SS77:  0.04667521988690035\n",
            "Loss SS88:  0.0450185346512162\n",
            "Loss SS11:  0.04648974702224763\n",
            "Loss SS22:  0.04274240407076749\n",
            "Loss SS33:  0.04277429883337338\n",
            "Loss SS44:  0.0405948149853958\n",
            "Loss SS55:  0.04182510457090422\n",
            "Loss SS66:  0.040854872173388096\n",
            "Loss SS77:  0.046705580380499497\n",
            "Loss SS88:  0.04503838817884282\n",
            "Loss SS11:  0.04649268940026486\n",
            "Loss SS22:  0.04275682011776787\n",
            "Loss SS33:  0.04279906198447025\n",
            "Loss SS44:  0.04061966853260735\n",
            "Loss SS55:  0.041859118385066695\n",
            "Loss SS66:  0.04088270685225402\n",
            "Loss SS77:  0.04673954234367596\n",
            "Loss SS88:  0.045090177716634795\n",
            "Loss SS11:  0.04648083086163122\n",
            "Loss SS22:  0.04277520928919442\n",
            "Loss SS33:  0.04281714973049812\n",
            "Loss SS44:  0.04059980789915265\n",
            "Loss SS55:  0.04187724002164387\n",
            "Loss SS66:  0.04090939626698788\n",
            "Loss SS77:  0.04675555024335592\n",
            "Loss SS88:  0.045081693346902825\n",
            "Loss SS11:  0.04647508469601926\n",
            "Loss SS22:  0.04276473375653031\n",
            "Loss SS33:  0.04279482758094764\n",
            "Loss SS44:  0.04057765468614265\n",
            "Loss SS55:  0.04187848854213643\n",
            "Loss SS66:  0.040905915618934155\n",
            "Loss SS77:  0.04670045535678427\n",
            "Loss SS88:  0.04507069071278503\n",
            "Loss SS11:  0.04643862086012504\n",
            "Loss SS22:  0.0427300992161946\n",
            "Loss SS33:  0.04277042061272561\n",
            "Loss SS44:  0.040551786790857976\n",
            "Loss SS55:  0.04185393334284819\n",
            "Loss SS66:  0.04087310192571638\n",
            "Loss SS77:  0.046642114220532285\n",
            "Loss SS88:  0.04502166586106761\n",
            "Validation: \n",
            " Loss SS11:  0.041114550083875656\n",
            " Loss SS22:  0.05659256875514984\n",
            " Loss SS33:  0.0614856518805027\n",
            " Loss SS55:  0.053438227623701096\n",
            " Loss SS66:  0.06475792080163956\n",
            " Loss SS77:  0.061577387154102325\n",
            " Loss SS88:  0.06096713989973068\n",
            " Loss SS99:  0.05962419509887695\n",
            " Loss SS11:  0.045432522892951965\n",
            " Loss SS22:  0.06478213589815866\n",
            " Loss SS33:  0.06695747943151564\n",
            " Loss SS55:  0.05727094908555349\n",
            " Loss SS66:  0.06833459348196075\n",
            " Loss SS77:  0.06434077503425735\n",
            " Loss SS88:  0.07021647975558326\n",
            " Loss SS99:  0.06489250188072522\n",
            " Loss SS11:  0.045789759878705184\n",
            " Loss SS22:  0.06342233390342898\n",
            " Loss SS33:  0.06829608368074022\n",
            " Loss SS55:  0.05771577458192662\n",
            " Loss SS66:  0.06746615478541793\n",
            " Loss SS77:  0.06376618646630426\n",
            " Loss SS88:  0.06920244880929226\n",
            " Loss SS99:  0.06406386223871534\n",
            " Loss SS11:  0.045818479517932796\n",
            " Loss SS22:  0.06319482648958925\n",
            " Loss SS33:  0.06765342986241715\n",
            " Loss SS55:  0.057393997785497884\n",
            " Loss SS66:  0.06724816444711607\n",
            " Loss SS77:  0.06351216448867908\n",
            " Loss SS88:  0.06953773684188967\n",
            " Loss SS99:  0.0639405358033102\n",
            " Loss SS11:  0.045829173269463175\n",
            " Loss SS22:  0.06289779105120236\n",
            " Loss SS33:  0.06751536210010081\n",
            " Loss SS55:  0.05730383824787022\n",
            " Loss SS66:  0.06702758040693071\n",
            " Loss SS77:  0.06349680007055954\n",
            " Loss SS88:  0.06941536901357734\n",
            " Loss SS99:  0.0640564824419993\n",
            "\n",
            "Epoch: 54\n",
            "Loss SS11:  0.0497884564101696\n",
            "Loss SS22:  0.04732516035437584\n",
            "Loss SS33:  0.04918069392442703\n",
            "Loss SS44:  0.04325786605477333\n",
            "Loss SS55:  0.04574485123157501\n",
            "Loss SS66:  0.047243036329746246\n",
            "Loss SS77:  0.05567542836070061\n",
            "Loss SS88:  0.052292898297309875\n",
            "Loss SS11:  0.045034306293184105\n",
            "Loss SS22:  0.04173894768411463\n",
            "Loss SS33:  0.04272272674874826\n",
            "Loss SS44:  0.040347781709649345\n",
            "Loss SS55:  0.040864684703675186\n",
            "Loss SS66:  0.04076321321454915\n",
            "Loss SS77:  0.04623034088449045\n",
            "Loss SS88:  0.044852581891146576\n",
            "Loss SS11:  0.04481861651653335\n",
            "Loss SS22:  0.041876080845083506\n",
            "Loss SS33:  0.043065941404728664\n",
            "Loss SS44:  0.040375530010177976\n",
            "Loss SS55:  0.04075748633061137\n",
            "Loss SS66:  0.04079642998320716\n",
            "Loss SS77:  0.04563506107245173\n",
            "Loss SS88:  0.04495555783311526\n",
            "Loss SS11:  0.0449284907550581\n",
            "Loss SS22:  0.0424952746158646\n",
            "Loss SS33:  0.043070519523274516\n",
            "Loss SS44:  0.040564390800652966\n",
            "Loss SS55:  0.04106537865534905\n",
            "Loss SS66:  0.041156855922552846\n",
            "Loss SS77:  0.046003610015876835\n",
            "Loss SS88:  0.0454041629789337\n",
            "Loss SS11:  0.044919461193608075\n",
            "Loss SS22:  0.042597773961904575\n",
            "Loss SS33:  0.042959121850932515\n",
            "Loss SS44:  0.04051998257637024\n",
            "Loss SS55:  0.041269805282354355\n",
            "Loss SS66:  0.04120751506671673\n",
            "Loss SS77:  0.04613805725807097\n",
            "Loss SS88:  0.04533571250191549\n",
            "Loss SS11:  0.045102046985252234\n",
            "Loss SS22:  0.04258178919553757\n",
            "Loss SS33:  0.043069708069749905\n",
            "Loss SS44:  0.04064645479414977\n",
            "Loss SS55:  0.041386759821690766\n",
            "Loss SS66:  0.041148278131788854\n",
            "Loss SS77:  0.04612018745027337\n",
            "Loss SS88:  0.04510011684660818\n",
            "Loss SS11:  0.045152269731291\n",
            "Loss SS22:  0.04245286941772602\n",
            "Loss SS33:  0.04290253888876712\n",
            "Loss SS44:  0.040466871479007065\n",
            "Loss SS55:  0.0413348059795919\n",
            "Loss SS66:  0.0409040992132953\n",
            "Loss SS77:  0.046088182718538845\n",
            "Loss SS88:  0.04503957322630726\n",
            "Loss SS11:  0.04521406127113692\n",
            "Loss SS22:  0.04262897141382728\n",
            "Loss SS33:  0.0429680177443464\n",
            "Loss SS44:  0.04050143126031043\n",
            "Loss SS55:  0.04159962370152205\n",
            "Loss SS66:  0.04101060215436237\n",
            "Loss SS77:  0.04605968254552761\n",
            "Loss SS88:  0.04505530786766133\n",
            "Loss SS11:  0.04537710220909413\n",
            "Loss SS22:  0.04260523235540331\n",
            "Loss SS33:  0.04279144798164015\n",
            "Loss SS44:  0.040328720653498615\n",
            "Loss SS55:  0.04133534790189178\n",
            "Loss SS66:  0.04071262386845954\n",
            "Loss SS77:  0.04581417005371164\n",
            "Loss SS88:  0.044917695592215034\n",
            "Loss SS11:  0.045509788033726455\n",
            "Loss SS22:  0.042702175226512845\n",
            "Loss SS33:  0.04275598675831334\n",
            "Loss SS44:  0.0404552022752526\n",
            "Loss SS55:  0.041258306948693244\n",
            "Loss SS66:  0.04071082199340338\n",
            "Loss SS77:  0.04588496631809643\n",
            "Loss SS88:  0.04499126307584427\n",
            "Loss SS11:  0.04549482007427971\n",
            "Loss SS22:  0.04261926945188258\n",
            "Loss SS33:  0.04265345629341531\n",
            "Loss SS44:  0.04030504623557081\n",
            "Loss SS55:  0.04108775971402036\n",
            "Loss SS66:  0.04056680327889943\n",
            "Loss SS77:  0.04581628824667175\n",
            "Loss SS88:  0.044897996036723106\n",
            "Loss SS11:  0.04546558685802125\n",
            "Loss SS22:  0.042663697656747455\n",
            "Loss SS33:  0.04250980802887195\n",
            "Loss SS44:  0.040282003850013286\n",
            "Loss SS55:  0.041023316079968807\n",
            "Loss SS66:  0.040571693968665494\n",
            "Loss SS77:  0.04590174203386178\n",
            "Loss SS88:  0.0448921149311302\n",
            "Loss SS11:  0.0454025169418863\n",
            "Loss SS22:  0.04271678192433247\n",
            "Loss SS33:  0.042671154661119474\n",
            "Loss SS44:  0.04036153588166907\n",
            "Loss SS55:  0.04117575712686728\n",
            "Loss SS66:  0.04057003868636021\n",
            "Loss SS77:  0.04601814231473552\n",
            "Loss SS88:  0.04498188311526598\n",
            "Loss SS11:  0.04541620541051144\n",
            "Loss SS22:  0.042739346794044696\n",
            "Loss SS33:  0.04273996888908721\n",
            "Loss SS44:  0.04045775251192901\n",
            "Loss SS55:  0.04124759714908272\n",
            "Loss SS66:  0.040511449187539\n",
            "Loss SS77:  0.04620526253267099\n",
            "Loss SS88:  0.04511894211741804\n",
            "Loss SS11:  0.045454504959126736\n",
            "Loss SS22:  0.042756804760466234\n",
            "Loss SS33:  0.04265032325547638\n",
            "Loss SS44:  0.04046001805798382\n",
            "Loss SS55:  0.04126181659546304\n",
            "Loss SS66:  0.040524362460941286\n",
            "Loss SS77:  0.04627278444509134\n",
            "Loss SS88:  0.045117925535491175\n",
            "Loss SS11:  0.04547612166759984\n",
            "Loss SS22:  0.04269349552839007\n",
            "Loss SS33:  0.04261190032228729\n",
            "Loss SS44:  0.04044773920581041\n",
            "Loss SS55:  0.04127642184199876\n",
            "Loss SS66:  0.04047356965328684\n",
            "Loss SS77:  0.04621543680989979\n",
            "Loss SS88:  0.045114998835206824\n",
            "Loss SS11:  0.04548127710911798\n",
            "Loss SS22:  0.04256498485063174\n",
            "Loss SS33:  0.04257605055937116\n",
            "Loss SS44:  0.040382918895956896\n",
            "Loss SS55:  0.0412199850315633\n",
            "Loss SS66:  0.04039289647285242\n",
            "Loss SS77:  0.04618170154020653\n",
            "Loss SS88:  0.045045882810531936\n",
            "Loss SS11:  0.045466339030461\n",
            "Loss SS22:  0.04259498334593243\n",
            "Loss SS33:  0.04260561952901165\n",
            "Loss SS44:  0.04042833156840146\n",
            "Loss SS55:  0.04131000946488297\n",
            "Loss SS66:  0.04052058600804262\n",
            "Loss SS77:  0.04625374159356307\n",
            "Loss SS88:  0.04510618491392387\n",
            "Loss SS11:  0.04556004910920206\n",
            "Loss SS22:  0.042572018664680134\n",
            "Loss SS33:  0.042665379440916175\n",
            "Loss SS44:  0.040458835455594144\n",
            "Loss SS55:  0.04141610642658413\n",
            "Loss SS66:  0.04057841548356562\n",
            "Loss SS77:  0.046316805177301336\n",
            "Loss SS88:  0.04518440127043434\n",
            "Loss SS11:  0.045737706924452205\n",
            "Loss SS22:  0.042570060050768375\n",
            "Loss SS33:  0.04266224148151762\n",
            "Loss SS44:  0.040432084566323544\n",
            "Loss SS55:  0.04138274997940863\n",
            "Loss SS66:  0.04060688800134584\n",
            "Loss SS77:  0.046333700906075734\n",
            "Loss SS88:  0.0451648090990426\n",
            "Loss SS11:  0.045708715637673196\n",
            "Loss SS22:  0.04256587684969997\n",
            "Loss SS33:  0.04268106019393129\n",
            "Loss SS44:  0.0404376597583887\n",
            "Loss SS55:  0.041371358419532205\n",
            "Loss SS66:  0.040587595594463066\n",
            "Loss SS77:  0.0463027999323992\n",
            "Loss SS88:  0.04508710307861442\n",
            "Loss SS11:  0.04575606723320428\n",
            "Loss SS22:  0.04256846886393018\n",
            "Loss SS33:  0.04270199613901676\n",
            "Loss SS44:  0.040453380300394165\n",
            "Loss SS55:  0.04137593082271481\n",
            "Loss SS66:  0.040573797660981306\n",
            "Loss SS77:  0.04624832919423614\n",
            "Loss SS88:  0.04504757501643981\n",
            "Loss SS11:  0.04576292934056321\n",
            "Loss SS22:  0.04256178627456475\n",
            "Loss SS33:  0.04263575005922382\n",
            "Loss SS44:  0.04043668904657817\n",
            "Loss SS55:  0.041340050418182735\n",
            "Loss SS66:  0.04055967434768763\n",
            "Loss SS77:  0.046209487946055054\n",
            "Loss SS88:  0.045037520720678216\n",
            "Loss SS11:  0.04578296314457278\n",
            "Loss SS22:  0.04260621563374222\n",
            "Loss SS33:  0.0425940736386425\n",
            "Loss SS44:  0.04039598775632454\n",
            "Loss SS55:  0.04129137522923998\n",
            "Loss SS66:  0.040507928665840263\n",
            "Loss SS77:  0.04622274666250526\n",
            "Loss SS88:  0.04503055990903408\n",
            "Loss SS11:  0.045767221754393635\n",
            "Loss SS22:  0.04262672513105068\n",
            "Loss SS33:  0.04256211032634949\n",
            "Loss SS44:  0.040381024951014775\n",
            "Loss SS55:  0.04132757230789335\n",
            "Loss SS66:  0.040490821282151326\n",
            "Loss SS77:  0.046184628738159955\n",
            "Loss SS88:  0.044955089069995645\n",
            "Loss SS11:  0.04574242824933444\n",
            "Loss SS22:  0.04266105017873396\n",
            "Loss SS33:  0.04255671815508865\n",
            "Loss SS44:  0.040386405183499556\n",
            "Loss SS55:  0.041356037515568066\n",
            "Loss SS66:  0.04052311955279563\n",
            "Loss SS77:  0.04623803334585224\n",
            "Loss SS88:  0.04497519907248448\n",
            "Loss SS11:  0.04572711842156005\n",
            "Loss SS22:  0.04262931977480764\n",
            "Loss SS33:  0.04254964065391899\n",
            "Loss SS44:  0.040362225096116124\n",
            "Loss SS55:  0.04138761830170036\n",
            "Loss SS66:  0.04057190314620391\n",
            "Loss SS77:  0.04626885384777953\n",
            "Loss SS88:  0.0449323783346748\n",
            "Loss SS11:  0.045735290610812246\n",
            "Loss SS22:  0.04264084920562061\n",
            "Loss SS33:  0.04249627561036951\n",
            "Loss SS44:  0.040343655637599445\n",
            "Loss SS55:  0.041370813518322704\n",
            "Loss SS66:  0.04057557072586679\n",
            "Loss SS77:  0.04631536242266863\n",
            "Loss SS88:  0.04501386933935964\n",
            "Loss SS11:  0.04573264252725869\n",
            "Loss SS22:  0.04266263679783539\n",
            "Loss SS33:  0.042524633644102305\n",
            "Loss SS44:  0.04034946207951396\n",
            "Loss SS55:  0.041374946205323275\n",
            "Loss SS66:  0.0405767022953551\n",
            "Loss SS77:  0.046317174911392964\n",
            "Loss SS88:  0.04499727610856613\n",
            "Loss SS11:  0.04573436325921636\n",
            "Loss SS22:  0.0427028229428116\n",
            "Loss SS33:  0.04253713536969165\n",
            "Loss SS44:  0.04035580174875833\n",
            "Loss SS55:  0.04139820365971306\n",
            "Loss SS66:  0.04058151937944373\n",
            "Loss SS77:  0.046432257761148246\n",
            "Loss SS88:  0.04501346254215617\n",
            "Loss SS11:  0.045732822751880085\n",
            "Loss SS22:  0.04269858884108423\n",
            "Loss SS33:  0.04253569609204004\n",
            "Loss SS44:  0.04033477773945593\n",
            "Loss SS55:  0.04138871406102893\n",
            "Loss SS66:  0.04058078135614379\n",
            "Loss SS77:  0.04644687161889187\n",
            "Loss SS88:  0.04503790461195268\n",
            "Loss SS11:  0.04573561840502012\n",
            "Loss SS22:  0.04265379103146182\n",
            "Loss SS33:  0.04251637015120393\n",
            "Loss SS44:  0.04035663298088638\n",
            "Loss SS55:  0.041360529025842904\n",
            "Loss SS66:  0.04060366239482567\n",
            "Loss SS77:  0.04647720128393633\n",
            "Loss SS88:  0.04502875960381085\n",
            "Loss SS11:  0.04575237697947805\n",
            "Loss SS22:  0.04261650927248774\n",
            "Loss SS33:  0.042497000230528485\n",
            "Loss SS44:  0.04033355721246416\n",
            "Loss SS55:  0.041324682684609454\n",
            "Loss SS66:  0.040584915732483254\n",
            "Loss SS77:  0.046454993978931894\n",
            "Loss SS88:  0.04498050215645371\n",
            "Loss SS11:  0.04575468455736731\n",
            "Loss SS22:  0.0426012060500884\n",
            "Loss SS33:  0.04250035782730111\n",
            "Loss SS44:  0.04031910651625103\n",
            "Loss SS55:  0.04131698915650477\n",
            "Loss SS66:  0.040594729321960954\n",
            "Loss SS77:  0.04640120436687844\n",
            "Loss SS88:  0.044960793152766645\n",
            "Loss SS11:  0.04574100391343892\n",
            "Loss SS22:  0.04260605582393859\n",
            "Loss SS33:  0.04254543579175325\n",
            "Loss SS44:  0.04031370107160985\n",
            "Loss SS55:  0.04135011124514764\n",
            "Loss SS66:  0.04061010755227108\n",
            "Loss SS77:  0.04639579119745238\n",
            "Loss SS88:  0.04494588398251715\n",
            "Loss SS11:  0.04575385508841259\n",
            "Loss SS22:  0.042609490029662425\n",
            "Loss SS33:  0.04254054791822053\n",
            "Loss SS44:  0.04029405461969199\n",
            "Loss SS55:  0.0413675908200931\n",
            "Loss SS66:  0.040619607929915105\n",
            "Loss SS77:  0.046434595544114075\n",
            "Loss SS88:  0.044943090931557524\n",
            "Loss SS11:  0.045771372553549315\n",
            "Loss SS22:  0.04261928183797984\n",
            "Loss SS33:  0.04251930216673008\n",
            "Loss SS44:  0.04027377246489485\n",
            "Loss SS55:  0.04136528612380213\n",
            "Loss SS66:  0.04061957774812825\n",
            "Loss SS77:  0.04644502411464905\n",
            "Loss SS88:  0.04489451152507288\n",
            "Loss SS11:  0.04575228156908503\n",
            "Loss SS22:  0.04258288944145097\n",
            "Loss SS33:  0.042487677839406414\n",
            "Loss SS44:  0.04026915448293532\n",
            "Loss SS55:  0.041363551068016785\n",
            "Loss SS66:  0.04061427267614722\n",
            "Loss SS77:  0.04641870233529019\n",
            "Loss SS88:  0.04485980161237267\n",
            "Loss SS11:  0.04570843157140914\n",
            "Loss SS22:  0.04252852024290505\n",
            "Loss SS33:  0.04245405297071289\n",
            "Loss SS44:  0.04024312560131231\n",
            "Loss SS55:  0.041365454076077994\n",
            "Loss SS66:  0.040593001179964214\n",
            "Loss SS77:  0.046369452430350885\n",
            "Loss SS88:  0.04483124648961495\n",
            "Loss SS11:  0.04569538483572433\n",
            "Loss SS22:  0.042480663532186345\n",
            "Loss SS33:  0.04241464247979471\n",
            "Loss SS44:  0.040223890443897\n",
            "Loss SS55:  0.04134967958416475\n",
            "Loss SS66:  0.04056393576171392\n",
            "Loss SS77:  0.046303848209588425\n",
            "Loss SS88:  0.044791264721499684\n",
            "Loss SS11:  0.045718075925869836\n",
            "Loss SS22:  0.042480113695461556\n",
            "Loss SS33:  0.04239495604895891\n",
            "Loss SS44:  0.04021363937646671\n",
            "Loss SS55:  0.041355238284479054\n",
            "Loss SS66:  0.04056233998174382\n",
            "Loss SS77:  0.04632449967605515\n",
            "Loss SS88:  0.04480227007718752\n",
            "Loss SS11:  0.04571010930114709\n",
            "Loss SS22:  0.04249458806249347\n",
            "Loss SS33:  0.042381909442499026\n",
            "Loss SS44:  0.04021006513504796\n",
            "Loss SS55:  0.04136555641889572\n",
            "Loss SS66:  0.04054712729841253\n",
            "Loss SS77:  0.04631219222380297\n",
            "Loss SS88:  0.04478564843934238\n",
            "Loss SS11:  0.04572236867988761\n",
            "Loss SS22:  0.04247980797552723\n",
            "Loss SS33:  0.04239514524077293\n",
            "Loss SS44:  0.0402330454890207\n",
            "Loss SS55:  0.041373256461212586\n",
            "Loss SS66:  0.04054584701030101\n",
            "Loss SS77:  0.04631884412888677\n",
            "Loss SS88:  0.044808589877680195\n",
            "Loss SS11:  0.04572253920604624\n",
            "Loss SS22:  0.0424401998485324\n",
            "Loss SS33:  0.04235982448035493\n",
            "Loss SS44:  0.04020827311132181\n",
            "Loss SS55:  0.041372292279643966\n",
            "Loss SS66:  0.040513704874468516\n",
            "Loss SS77:  0.04627059637774171\n",
            "Loss SS88:  0.04477387967491371\n",
            "Loss SS11:  0.045739114918068154\n",
            "Loss SS22:  0.04245859397207798\n",
            "Loss SS33:  0.042369074857626914\n",
            "Loss SS44:  0.04021282722147144\n",
            "Loss SS55:  0.0413646819386758\n",
            "Loss SS66:  0.040519002461784824\n",
            "Loss SS77:  0.04628851495328404\n",
            "Loss SS88:  0.04476882626308876\n",
            "Loss SS11:  0.04573970482975839\n",
            "Loss SS22:  0.04245258865262611\n",
            "Loss SS33:  0.042350731847886236\n",
            "Loss SS44:  0.04022746731397053\n",
            "Loss SS55:  0.04140819599253111\n",
            "Loss SS66:  0.04054056205466687\n",
            "Loss SS77:  0.046330459366352224\n",
            "Loss SS88:  0.04480359060669157\n",
            "Loss SS11:  0.04574715712970353\n",
            "Loss SS22:  0.04246272687313355\n",
            "Loss SS33:  0.04236238149202827\n",
            "Loss SS44:  0.040213238407500136\n",
            "Loss SS55:  0.04142882867257662\n",
            "Loss SS66:  0.04055168663117219\n",
            "Loss SS77:  0.0463406161249589\n",
            "Loss SS88:  0.044852474716506135\n",
            "Loss SS11:  0.045744539683411835\n",
            "Loss SS22:  0.04246955142690624\n",
            "Loss SS33:  0.04239868405106974\n",
            "Loss SS44:  0.04022166742735608\n",
            "Loss SS55:  0.04144594349330904\n",
            "Loss SS66:  0.04058833349092751\n",
            "Loss SS77:  0.046341309881514046\n",
            "Loss SS88:  0.04487953102512724\n",
            "Loss SS11:  0.04575955231137682\n",
            "Loss SS22:  0.04248581099442038\n",
            "Loss SS33:  0.04236347954758983\n",
            "Loss SS44:  0.04020441038506938\n",
            "Loss SS55:  0.04145833987765897\n",
            "Loss SS66:  0.040597771958538996\n",
            "Loss SS77:  0.04634013725541485\n",
            "Loss SS88:  0.0448803656933957\n",
            "Loss SS11:  0.0457102650067107\n",
            "Loss SS22:  0.0424446261159757\n",
            "Loss SS33:  0.0423226892917802\n",
            "Loss SS44:  0.0401718915798635\n",
            "Loss SS55:  0.04142686669276591\n",
            "Loss SS66:  0.04057213227035072\n",
            "Loss SS77:  0.04629154862217651\n",
            "Loss SS88:  0.044820699749738284\n",
            "Validation: \n",
            " Loss SS11:  0.04019246995449066\n",
            " Loss SS22:  0.056695107370615005\n",
            " Loss SS33:  0.0608343668282032\n",
            " Loss SS55:  0.05213775485754013\n",
            " Loss SS66:  0.06321680545806885\n",
            " Loss SS77:  0.06067771837115288\n",
            " Loss SS88:  0.059042274951934814\n",
            " Loss SS99:  0.058128662407398224\n",
            " Loss SS11:  0.04518472935472216\n",
            " Loss SS22:  0.06427803600118273\n",
            " Loss SS33:  0.06587190979293414\n",
            " Loss SS55:  0.05688244317259107\n",
            " Loss SS66:  0.068002196117526\n",
            " Loss SS77:  0.06384133707199778\n",
            " Loss SS88:  0.07011924932400386\n",
            " Loss SS99:  0.06384134718350001\n",
            " Loss SS11:  0.04587094667481213\n",
            " Loss SS22:  0.0629195144808874\n",
            " Loss SS33:  0.06733948028669125\n",
            " Loss SS55:  0.05758131804262719\n",
            " Loss SS66:  0.06697116710427331\n",
            " Loss SS77:  0.06364748445225925\n",
            " Loss SS88:  0.0695335473410967\n",
            " Loss SS99:  0.06325398276491863\n",
            " Loss SS11:  0.04598570547875811\n",
            " Loss SS22:  0.06274231694272307\n",
            " Loss SS33:  0.06668394034514662\n",
            " Loss SS55:  0.0570192318715033\n",
            " Loss SS66:  0.06687368452548981\n",
            " Loss SS77:  0.06339772088361569\n",
            " Loss SS88:  0.0698403885252163\n",
            " Loss SS99:  0.06300331639950393\n",
            " Loss SS11:  0.04597709889029279\n",
            " Loss SS22:  0.062438933891646654\n",
            " Loss SS33:  0.066560868817715\n",
            " Loss SS55:  0.056789216895898185\n",
            " Loss SS66:  0.06670772868358059\n",
            " Loss SS77:  0.06335736466226755\n",
            " Loss SS88:  0.06972948843498289\n",
            " Loss SS99:  0.06302379094708113\n",
            "\n",
            "Epoch: 55\n",
            "Loss SS11:  0.04937029629945755\n",
            "Loss SS22:  0.04463106021285057\n",
            "Loss SS33:  0.048392605036497116\n",
            "Loss SS44:  0.04234332963824272\n",
            "Loss SS55:  0.0443437322974205\n",
            "Loss SS66:  0.04463537409901619\n",
            "Loss SS77:  0.0551561564207077\n",
            "Loss SS88:  0.05047282204031944\n",
            "Loss SS11:  0.047067437659610405\n",
            "Loss SS22:  0.04104213518175212\n",
            "Loss SS33:  0.04222717474807392\n",
            "Loss SS44:  0.039808154783465645\n",
            "Loss SS55:  0.041072774678468704\n",
            "Loss SS66:  0.04014660316434773\n",
            "Loss SS77:  0.04672590541568669\n",
            "Loss SS88:  0.045192531564018944\n",
            "Loss SS11:  0.045850190555765516\n",
            "Loss SS22:  0.04159598371812275\n",
            "Loss SS33:  0.04236430071649097\n",
            "Loss SS44:  0.03998667179119019\n",
            "Loss SS55:  0.04088787681290081\n",
            "Loss SS66:  0.04051853929247175\n",
            "Loss SS77:  0.04650141369728815\n",
            "Loss SS88:  0.045281960318485893\n",
            "Loss SS11:  0.04588753958382914\n",
            "Loss SS22:  0.042003111493202946\n",
            "Loss SS33:  0.042252186084947276\n",
            "Loss SS44:  0.04031125768538444\n",
            "Loss SS55:  0.0415555855199214\n",
            "Loss SS66:  0.04046650435174665\n",
            "Loss SS77:  0.04686937192755361\n",
            "Loss SS88:  0.04541136973327206\n",
            "Loss SS11:  0.0456243353645976\n",
            "Loss SS22:  0.04213591565082713\n",
            "Loss SS33:  0.042529087059381535\n",
            "Loss SS44:  0.04047921627033048\n",
            "Loss SS55:  0.04180363365789739\n",
            "Loss SS66:  0.04074101586167405\n",
            "Loss SS77:  0.046739267866785936\n",
            "Loss SS88:  0.04538714585871231\n",
            "Loss SS11:  0.04557445784117661\n",
            "Loss SS22:  0.04219131032918014\n",
            "Loss SS33:  0.042568158796604944\n",
            "Loss SS44:  0.04070525645625358\n",
            "Loss SS55:  0.041805644391798506\n",
            "Loss SS66:  0.040744817505280174\n",
            "Loss SS77:  0.046724992785968034\n",
            "Loss SS88:  0.04520704684888616\n",
            "Loss SS11:  0.04554948424462412\n",
            "Loss SS22:  0.04200682007387036\n",
            "Loss SS33:  0.042435331728126184\n",
            "Loss SS44:  0.04047247749127326\n",
            "Loss SS55:  0.041596907694808775\n",
            "Loss SS66:  0.04048903646772025\n",
            "Loss SS77:  0.04643155420657064\n",
            "Loss SS88:  0.04491810659404661\n",
            "Loss SS11:  0.04546892726925057\n",
            "Loss SS22:  0.042160541553732375\n",
            "Loss SS33:  0.042552496136074335\n",
            "Loss SS44:  0.040715690702199936\n",
            "Loss SS55:  0.04177514311503357\n",
            "Loss SS66:  0.04056358442340099\n",
            "Loss SS77:  0.04631185793960598\n",
            "Loss SS88:  0.04490593201677564\n",
            "Loss SS11:  0.04558060263042097\n",
            "Loss SS22:  0.04212735528931206\n",
            "Loss SS33:  0.04252429669837893\n",
            "Loss SS44:  0.0404993666358936\n",
            "Loss SS55:  0.04152326391618929\n",
            "Loss SS66:  0.04033473920491007\n",
            "Loss SS77:  0.04619116814416132\n",
            "Loss SS88:  0.044742990055201964\n",
            "Loss SS11:  0.045744171468438684\n",
            "Loss SS22:  0.042226140888837665\n",
            "Loss SS33:  0.04252244679482429\n",
            "Loss SS44:  0.040509863991986264\n",
            "Loss SS55:  0.04148291800539572\n",
            "Loss SS66:  0.04047398351050995\n",
            "Loss SS77:  0.04610618310315268\n",
            "Loss SS88:  0.044698454439640045\n",
            "Loss SS11:  0.045813986681180426\n",
            "Loss SS22:  0.042191675873381074\n",
            "Loss SS33:  0.04242406556806942\n",
            "Loss SS44:  0.04036761883019221\n",
            "Loss SS55:  0.04125523589330145\n",
            "Loss SS66:  0.040334846971943825\n",
            "Loss SS77:  0.04596328753793594\n",
            "Loss SS88:  0.044528994451064875\n",
            "Loss SS11:  0.04571834384455337\n",
            "Loss SS22:  0.04219638814663028\n",
            "Loss SS33:  0.042281072721019525\n",
            "Loss SS44:  0.040224760535869514\n",
            "Loss SS55:  0.041222802724118705\n",
            "Loss SS66:  0.04032812452128342\n",
            "Loss SS77:  0.04599457757698523\n",
            "Loss SS88:  0.04451981965485994\n",
            "Loss SS11:  0.04566028052248246\n",
            "Loss SS22:  0.04227588002469914\n",
            "Loss SS33:  0.042307886733742785\n",
            "Loss SS44:  0.04030628834874177\n",
            "Loss SS55:  0.04126316810811847\n",
            "Loss SS66:  0.04040888866240328\n",
            "Loss SS77:  0.04616919972679832\n",
            "Loss SS88:  0.04470557802595383\n",
            "Loss SS11:  0.04565629556433845\n",
            "Loss SS22:  0.042336958396525785\n",
            "Loss SS33:  0.04237955680671539\n",
            "Loss SS44:  0.04042785344574288\n",
            "Loss SS55:  0.04137118510503805\n",
            "Loss SS66:  0.040385880059640826\n",
            "Loss SS77:  0.04620226543709522\n",
            "Loss SS88:  0.04482400374671885\n",
            "Loss SS11:  0.045601041868646094\n",
            "Loss SS22:  0.04230939118680379\n",
            "Loss SS33:  0.042408108737663174\n",
            "Loss SS44:  0.040403259138688974\n",
            "Loss SS55:  0.04144309727963826\n",
            "Loss SS66:  0.04038646352206562\n",
            "Loss SS77:  0.04629687346676563\n",
            "Loss SS88:  0.04481949356008083\n",
            "Loss SS11:  0.04560866586814653\n",
            "Loss SS22:  0.0422257592050445\n",
            "Loss SS33:  0.04235402080199576\n",
            "Loss SS44:  0.04037292989970043\n",
            "Loss SS55:  0.04142363690185231\n",
            "Loss SS66:  0.0403286957780257\n",
            "Loss SS77:  0.046226505560196\n",
            "Loss SS88:  0.04476192406076469\n",
            "Loss SS11:  0.04554331906556343\n",
            "Loss SS22:  0.042174813239285666\n",
            "Loss SS33:  0.0423301264689946\n",
            "Loss SS44:  0.04031412696801357\n",
            "Loss SS55:  0.04137386740272089\n",
            "Loss SS66:  0.04028141269495028\n",
            "Loss SS77:  0.046170120977837105\n",
            "Loss SS88:  0.04473125191856615\n",
            "Loss SS11:  0.04559447058634451\n",
            "Loss SS22:  0.04221876710653305\n",
            "Loss SS33:  0.04234305771383626\n",
            "Loss SS44:  0.04029729846886724\n",
            "Loss SS55:  0.04143606993364312\n",
            "Loss SS66:  0.04041027872447382\n",
            "Loss SS77:  0.046247302178750956\n",
            "Loss SS88:  0.044793627350128186\n",
            "Loss SS11:  0.04566144694198561\n",
            "Loss SS22:  0.04225866329851072\n",
            "Loss SS33:  0.042357091725037245\n",
            "Loss SS44:  0.040305662612065425\n",
            "Loss SS55:  0.04142230858213335\n",
            "Loss SS66:  0.04051725179450947\n",
            "Loss SS77:  0.04633862626075086\n",
            "Loss SS88:  0.04484340991022179\n",
            "Loss SS11:  0.04573046064969757\n",
            "Loss SS22:  0.04228195259396318\n",
            "Loss SS33:  0.042383277018344837\n",
            "Loss SS44:  0.04026161766177073\n",
            "Loss SS55:  0.0414304944311137\n",
            "Loss SS66:  0.04052895245127653\n",
            "Loss SS77:  0.04638521999354762\n",
            "Loss SS88:  0.04487992881636345\n",
            "Loss SS11:  0.0456799976342353\n",
            "Loss SS22:  0.04226292680893371\n",
            "Loss SS33:  0.04236529569201802\n",
            "Loss SS44:  0.040259138917300236\n",
            "Loss SS55:  0.04134025702725595\n",
            "Loss SS66:  0.04049563652543879\n",
            "Loss SS77:  0.046390841571401005\n",
            "Loss SS88:  0.04483651542174282\n",
            "Loss SS11:  0.04572650320580785\n",
            "Loss SS22:  0.0422655271798796\n",
            "Loss SS33:  0.04243073594796149\n",
            "Loss SS44:  0.04024204196876259\n",
            "Loss SS55:  0.04129986177194175\n",
            "Loss SS66:  0.04042396931004185\n",
            "Loss SS77:  0.04625119491352289\n",
            "Loss SS88:  0.044779793275476064\n",
            "Loss SS11:  0.045757384338664796\n",
            "Loss SS22:  0.042266837494001126\n",
            "Loss SS33:  0.04239417599067429\n",
            "Loss SS44:  0.04025127533064708\n",
            "Loss SS55:  0.04125029018898895\n",
            "Loss SS66:  0.04039128207310832\n",
            "Loss SS77:  0.04624927736717652\n",
            "Loss SS88:  0.044759053382938264\n",
            "Loss SS11:  0.04581412813299662\n",
            "Loss SS22:  0.042271147671587024\n",
            "Loss SS33:  0.04237160530118715\n",
            "Loss SS44:  0.040234733276165925\n",
            "Loss SS55:  0.04123982741977229\n",
            "Loss SS66:  0.040344773384528756\n",
            "Loss SS77:  0.046202154805907954\n",
            "Loss SS88:  0.04474713777502378\n",
            "Loss SS11:  0.0458212474446079\n",
            "Loss SS22:  0.04229551079176768\n",
            "Loss SS33:  0.042382381361797145\n",
            "Loss SS44:  0.04022591301390739\n",
            "Loss SS55:  0.041264205289950506\n",
            "Loss SS66:  0.04035966064301764\n",
            "Loss SS77:  0.04620714157943409\n",
            "Loss SS88:  0.04472826379525216\n",
            "Loss SS11:  0.045773927434032184\n",
            "Loss SS22:  0.04231732132365979\n",
            "Loss SS33:  0.04240734441821794\n",
            "Loss SS44:  0.04020449554777715\n",
            "Loss SS55:  0.041295497006629094\n",
            "Loss SS66:  0.040409119376742034\n",
            "Loss SS77:  0.046221940566700294\n",
            "Loss SS88:  0.04473901859139541\n",
            "Loss SS11:  0.04570418166646099\n",
            "Loss SS22:  0.04230503461027511\n",
            "Loss SS33:  0.04239717733928527\n",
            "Loss SS44:  0.040210905802432605\n",
            "Loss SS55:  0.04132914065001568\n",
            "Loss SS66:  0.04043024151775115\n",
            "Loss SS77:  0.04620271631413036\n",
            "Loss SS88:  0.04471949750550405\n",
            "Loss SS11:  0.04571975429449574\n",
            "Loss SS22:  0.04231528792104158\n",
            "Loss SS33:  0.04237861110660423\n",
            "Loss SS44:  0.04017524962825529\n",
            "Loss SS55:  0.04133865406317465\n",
            "Loss SS66:  0.040434899530287595\n",
            "Loss SS77:  0.04623494386617988\n",
            "Loss SS88:  0.04475131596531375\n",
            "Loss SS11:  0.045742882939314077\n",
            "Loss SS22:  0.042351574116648305\n",
            "Loss SS33:  0.04239376138537804\n",
            "Loss SS44:  0.040220370602353187\n",
            "Loss SS55:  0.04138837600942184\n",
            "Loss SS66:  0.0404637540786716\n",
            "Loss SS77:  0.046266540009992405\n",
            "Loss SS88:  0.04476625359620488\n",
            "Loss SS11:  0.04573035278578395\n",
            "Loss SS22:  0.042404214733142626\n",
            "Loss SS33:  0.04241768064623846\n",
            "Loss SS44:  0.04025749995331584\n",
            "Loss SS55:  0.041419898483044504\n",
            "Loss SS66:  0.04049757716991648\n",
            "Loss SS77:  0.04636197283710401\n",
            "Loss SS88:  0.044815434930250815\n",
            "Loss SS11:  0.04572252858665298\n",
            "Loss SS22:  0.04239693718445658\n",
            "Loss SS33:  0.042403737844224784\n",
            "Loss SS44:  0.04021978562703957\n",
            "Loss SS55:  0.04139341608283924\n",
            "Loss SS66:  0.04049541303683753\n",
            "Loss SS77:  0.04641202762970496\n",
            "Loss SS88:  0.04485599150837854\n",
            "Loss SS11:  0.045692716871906326\n",
            "Loss SS22:  0.04237084263199012\n",
            "Loss SS33:  0.042384324110205915\n",
            "Loss SS44:  0.04021282571927911\n",
            "Loss SS55:  0.041377805149440214\n",
            "Loss SS66:  0.0405041689226865\n",
            "Loss SS77:  0.04638754365624339\n",
            "Loss SS88:  0.04483464070359227\n",
            "Loss SS11:  0.04570860357670769\n",
            "Loss SS22:  0.0423567013994927\n",
            "Loss SS33:  0.04240424754649308\n",
            "Loss SS44:  0.04021081409628889\n",
            "Loss SS55:  0.041374916019283725\n",
            "Loss SS66:  0.04050417208458033\n",
            "Loss SS77:  0.046383050859345826\n",
            "Loss SS88:  0.044819758305575615\n",
            "Loss SS11:  0.04573613973587661\n",
            "Loss SS22:  0.04235052877862648\n",
            "Loss SS33:  0.04241719042876696\n",
            "Loss SS44:  0.04021233247080958\n",
            "Loss SS55:  0.041364999054691945\n",
            "Loss SS66:  0.04051713747875568\n",
            "Loss SS77:  0.04636752052834747\n",
            "Loss SS88:  0.044826394459092006\n",
            "Loss SS11:  0.04570537099827769\n",
            "Loss SS22:  0.042335027703179644\n",
            "Loss SS33:  0.04246409323689176\n",
            "Loss SS44:  0.04020620206278091\n",
            "Loss SS55:  0.04137776892555774\n",
            "Loss SS66:  0.04049514754077207\n",
            "Loss SS77:  0.046348066514363385\n",
            "Loss SS88:  0.04476912532689984\n",
            "Loss SS11:  0.04568832515184356\n",
            "Loss SS22:  0.04232368767898307\n",
            "Loss SS33:  0.042453697879939335\n",
            "Loss SS44:  0.04022598190185351\n",
            "Loss SS55:  0.041373532817319585\n",
            "Loss SS66:  0.04052075940022441\n",
            "Loss SS77:  0.04637586650175926\n",
            "Loss SS88:  0.044778489478548025\n",
            "Loss SS11:  0.04572816035217526\n",
            "Loss SS22:  0.042306538093370746\n",
            "Loss SS33:  0.0424760241484543\n",
            "Loss SS44:  0.040263503088185\n",
            "Loss SS55:  0.041359982270117945\n",
            "Loss SS66:  0.04054227996607236\n",
            "Loss SS77:  0.046367615017616846\n",
            "Loss SS88:  0.04476103907930884\n",
            "Loss SS11:  0.0457387764378378\n",
            "Loss SS22:  0.04229942945176058\n",
            "Loss SS33:  0.04246629892052666\n",
            "Loss SS44:  0.04025467192990118\n",
            "Loss SS55:  0.0413593361380126\n",
            "Loss SS66:  0.040552561896830555\n",
            "Loss SS77:  0.046364053190156136\n",
            "Loss SS88:  0.04474058805530605\n",
            "Loss SS11:  0.0456941557407692\n",
            "Loss SS22:  0.042259209493561364\n",
            "Loss SS33:  0.0424138518866748\n",
            "Loss SS44:  0.040224074525391965\n",
            "Loss SS55:  0.04133457472435446\n",
            "Loss SS66:  0.04053646421886179\n",
            "Loss SS77:  0.04631945871188259\n",
            "Loss SS88:  0.04468768738268867\n",
            "Loss SS11:  0.045682679328238565\n",
            "Loss SS22:  0.042216929271245554\n",
            "Loss SS33:  0.04238657535189558\n",
            "Loss SS44:  0.040201337517374924\n",
            "Loss SS55:  0.0413221136745437\n",
            "Loss SS66:  0.040515816501339376\n",
            "Loss SS77:  0.0462824394330954\n",
            "Loss SS88:  0.04464417958960814\n",
            "Loss SS11:  0.04569270499886717\n",
            "Loss SS22:  0.04222094533300756\n",
            "Loss SS33:  0.04235259919169538\n",
            "Loss SS44:  0.04019917071125751\n",
            "Loss SS55:  0.04130493568957892\n",
            "Loss SS66:  0.04050324258958907\n",
            "Loss SS77:  0.04626778440731125\n",
            "Loss SS88:  0.044630868392915204\n",
            "Loss SS11:  0.04567475741108259\n",
            "Loss SS22:  0.042216806878718725\n",
            "Loss SS33:  0.04234296546619014\n",
            "Loss SS44:  0.04019140909876846\n",
            "Loss SS55:  0.04130627852105456\n",
            "Loss SS66:  0.040502228613244935\n",
            "Loss SS77:  0.046240164107504846\n",
            "Loss SS88:  0.044633231857687305\n",
            "Loss SS11:  0.04569350887364276\n",
            "Loss SS22:  0.04221424607299003\n",
            "Loss SS33:  0.04235299901412955\n",
            "Loss SS44:  0.04021000861236715\n",
            "Loss SS55:  0.04130948772056652\n",
            "Loss SS66:  0.04050663990497306\n",
            "Loss SS77:  0.046228835443278105\n",
            "Loss SS88:  0.04463847903556892\n",
            "Loss SS11:  0.04572599950806171\n",
            "Loss SS22:  0.04218944553674789\n",
            "Loss SS33:  0.04231700067799495\n",
            "Loss SS44:  0.040205005867506124\n",
            "Loss SS55:  0.04130024254633213\n",
            "Loss SS66:  0.04048629082618346\n",
            "Loss SS77:  0.046201335331057726\n",
            "Loss SS88:  0.04460209773166274\n",
            "Loss SS11:  0.04571192969236514\n",
            "Loss SS22:  0.04219728242817109\n",
            "Loss SS33:  0.042327847737236086\n",
            "Loss SS44:  0.04022488750669421\n",
            "Loss SS55:  0.04130335204522896\n",
            "Loss SS66:  0.04048813200321327\n",
            "Loss SS77:  0.04621965663672305\n",
            "Loss SS88:  0.044606230325169034\n",
            "Loss SS11:  0.04572615749357545\n",
            "Loss SS22:  0.04219586191546098\n",
            "Loss SS33:  0.04231488527915959\n",
            "Loss SS44:  0.04020983355735199\n",
            "Loss SS55:  0.041360601229374266\n",
            "Loss SS66:  0.04050333517089387\n",
            "Loss SS77:  0.04623503975586722\n",
            "Loss SS88:  0.04461095767909302\n",
            "Loss SS11:  0.045733628289495266\n",
            "Loss SS22:  0.04219850917991227\n",
            "Loss SS33:  0.04233431435491154\n",
            "Loss SS44:  0.04020609380980376\n",
            "Loss SS55:  0.04138305157810902\n",
            "Loss SS66:  0.040530226890345196\n",
            "Loss SS77:  0.04624531650814731\n",
            "Loss SS88:  0.04465064386192733\n",
            "Loss SS11:  0.04575685561177837\n",
            "Loss SS22:  0.04221005735408728\n",
            "Loss SS33:  0.0423501352927867\n",
            "Loss SS44:  0.04021446400513821\n",
            "Loss SS55:  0.04140164142359847\n",
            "Loss SS66:  0.04054955838370728\n",
            "Loss SS77:  0.04625686303951684\n",
            "Loss SS88:  0.044664804074288425\n",
            "Loss SS11:  0.04577206903894329\n",
            "Loss SS22:  0.0422085469236245\n",
            "Loss SS33:  0.042338938736927983\n",
            "Loss SS44:  0.04020568825258039\n",
            "Loss SS55:  0.04141407275645995\n",
            "Loss SS66:  0.040548553973002645\n",
            "Loss SS77:  0.04623865438412232\n",
            "Loss SS88:  0.04465976224247978\n",
            "Loss SS11:  0.045743823491507296\n",
            "Loss SS22:  0.04214933997896441\n",
            "Loss SS33:  0.042291088875709384\n",
            "Loss SS44:  0.04018134482644969\n",
            "Loss SS55:  0.041386423575598935\n",
            "Loss SS66:  0.040534565213862846\n",
            "Loss SS77:  0.04620303067164363\n",
            "Loss SS88:  0.044613680754865985\n",
            "Validation: \n",
            " Loss SS11:  0.04182129725813866\n",
            " Loss SS22:  0.05855629965662956\n",
            " Loss SS33:  0.06243199110031128\n",
            " Loss SS55:  0.05265006795525551\n",
            " Loss SS66:  0.06385090947151184\n",
            " Loss SS77:  0.061575621366500854\n",
            " Loss SS88:  0.05954144522547722\n",
            " Loss SS99:  0.05760982260107994\n",
            " Loss SS11:  0.04665752819606236\n",
            " Loss SS22:  0.06607271198715482\n",
            " Loss SS33:  0.0667192433916387\n",
            " Loss SS55:  0.057331057354098276\n",
            " Loss SS66:  0.06842374783896264\n",
            " Loss SS77:  0.06527089717842284\n",
            " Loss SS88:  0.07078011653253011\n",
            " Loss SS99:  0.06498843307296436\n",
            " Loss SS11:  0.04713121510860396\n",
            " Loss SS22:  0.064934360544856\n",
            " Loss SS33:  0.06835345115240027\n",
            " Loss SS55:  0.05786535389176229\n",
            " Loss SS66:  0.0674039466170276\n",
            " Loss SS77:  0.06483789106331221\n",
            " Loss SS88:  0.07012739105195534\n",
            " Loss SS99:  0.06436414526003163\n",
            " Loss SS11:  0.04716019869827833\n",
            " Loss SS22:  0.06484507610563373\n",
            " Loss SS33:  0.06772158282702087\n",
            " Loss SS55:  0.057487194288949495\n",
            " Loss SS66:  0.06711395683347202\n",
            " Loss SS77:  0.06454375917549993\n",
            " Loss SS88:  0.07029241115831938\n",
            " Loss SS99:  0.06422031414313395\n",
            " Loss SS11:  0.04715829751925704\n",
            " Loss SS22:  0.0645549785759714\n",
            " Loss SS33:  0.06759744868786247\n",
            " Loss SS55:  0.057355766311103916\n",
            " Loss SS66:  0.0669087092909548\n",
            " Loss SS77:  0.06447508577027439\n",
            " Loss SS88:  0.07014510798969387\n",
            " Loss SS99:  0.06422739443771633\n",
            "\n",
            "Epoch: 56\n",
            "Loss SS11:  0.05093735456466675\n",
            "Loss SS22:  0.04268166422843933\n",
            "Loss SS33:  0.047196634113788605\n",
            "Loss SS44:  0.04241384565830231\n",
            "Loss SS55:  0.04317290335893631\n",
            "Loss SS66:  0.04133038967847824\n",
            "Loss SS77:  0.05516619607806206\n",
            "Loss SS88:  0.05155250057578087\n",
            "Loss SS11:  0.0457890396091071\n",
            "Loss SS22:  0.0414873483506116\n",
            "Loss SS33:  0.04249916936863552\n",
            "Loss SS44:  0.0397602997042916\n",
            "Loss SS55:  0.040661445395513016\n",
            "Loss SS66:  0.040560430085117165\n",
            "Loss SS77:  0.046700949357314545\n",
            "Loss SS88:  0.044901191172274674\n",
            "Loss SS11:  0.04492957677159991\n",
            "Loss SS22:  0.04101195026721273\n",
            "Loss SS33:  0.04282464725630624\n",
            "Loss SS44:  0.03982161446696236\n",
            "Loss SS55:  0.04068925621963683\n",
            "Loss SS66:  0.04059082092273803\n",
            "Loss SS77:  0.0457833693141029\n",
            "Loss SS88:  0.044381262291045415\n",
            "Loss SS11:  0.044585872922212846\n",
            "Loss SS22:  0.04163827374577522\n",
            "Loss SS33:  0.04285881019407703\n",
            "Loss SS44:  0.04000832657179525\n",
            "Loss SS55:  0.041162506466911684\n",
            "Loss SS66:  0.04056608100091257\n",
            "Loss SS77:  0.04616865587811316\n",
            "Loss SS88:  0.04472387269619973\n",
            "Loss SS11:  0.04440080047380633\n",
            "Loss SS22:  0.04162869547925344\n",
            "Loss SS33:  0.04279404896788481\n",
            "Loss SS44:  0.0401060773832042\n",
            "Loss SS55:  0.041527226476407636\n",
            "Loss SS66:  0.04079139368926606\n",
            "Loss SS77:  0.04617176477502032\n",
            "Loss SS88:  0.04483090232058269\n",
            "Loss SS11:  0.04470143401447464\n",
            "Loss SS22:  0.04183454928444881\n",
            "Loss SS33:  0.042754643761059814\n",
            "Loss SS44:  0.04039879026366215\n",
            "Loss SS55:  0.04166002454710942\n",
            "Loss SS66:  0.040714498390169704\n",
            "Loss SS77:  0.04638269214945681\n",
            "Loss SS88:  0.04500465796274297\n",
            "Loss SS11:  0.044902721084043626\n",
            "Loss SS22:  0.041941281835563844\n",
            "Loss SS33:  0.04261019819828331\n",
            "Loss SS44:  0.0403750263398788\n",
            "Loss SS55:  0.04169983056480767\n",
            "Loss SS66:  0.0406166774938341\n",
            "Loss SS77:  0.046360132574546534\n",
            "Loss SS88:  0.045065478528620764\n",
            "Loss SS11:  0.044875188874946514\n",
            "Loss SS22:  0.04223634056012395\n",
            "Loss SS33:  0.042706708253269464\n",
            "Loss SS44:  0.040584783738767595\n",
            "Loss SS55:  0.04169557820743238\n",
            "Loss SS66:  0.04059983050109635\n",
            "Loss SS77:  0.04647233688705404\n",
            "Loss SS88:  0.04506839051003188\n",
            "Loss SS11:  0.04484043633680285\n",
            "Loss SS22:  0.042306934103921605\n",
            "Loss SS33:  0.042602251203339776\n",
            "Loss SS44:  0.040404451344116236\n",
            "Loss SS55:  0.041328635710625\n",
            "Loss SS66:  0.0404134037685983\n",
            "Loss SS77:  0.046392655896919745\n",
            "Loss SS88:  0.04495661287212077\n",
            "Loss SS11:  0.04494104554856217\n",
            "Loss SS22:  0.04223085419981034\n",
            "Loss SS33:  0.04251617262815381\n",
            "Loss SS44:  0.04039413633418607\n",
            "Loss SS55:  0.0412462008195919\n",
            "Loss SS66:  0.04041594045830297\n",
            "Loss SS77:  0.04630604324923767\n",
            "Loss SS88:  0.04483706635105741\n",
            "Loss SS11:  0.04496766641588494\n",
            "Loss SS22:  0.04207437312101374\n",
            "Loss SS33:  0.04241555652553492\n",
            "Loss SS44:  0.040225483108275006\n",
            "Loss SS55:  0.0409899440289724\n",
            "Loss SS66:  0.0403291853790236\n",
            "Loss SS77:  0.046187672616526634\n",
            "Loss SS88:  0.044677535201063254\n",
            "Loss SS11:  0.04499457756409774\n",
            "Loss SS22:  0.042163041797844136\n",
            "Loss SS33:  0.04225389226465612\n",
            "Loss SS44:  0.040161029511206855\n",
            "Loss SS55:  0.04094967440710411\n",
            "Loss SS66:  0.040402454063967544\n",
            "Loss SS77:  0.04628044833336865\n",
            "Loss SS88:  0.04464468964048334\n",
            "Loss SS11:  0.04491225876345122\n",
            "Loss SS22:  0.042141920186517655\n",
            "Loss SS33:  0.0423154041232649\n",
            "Loss SS44:  0.040202374577768575\n",
            "Loss SS55:  0.04106038162284646\n",
            "Loss SS66:  0.04040953920276697\n",
            "Loss SS77:  0.04635584071155422\n",
            "Loss SS88:  0.04474001143835793\n",
            "Loss SS11:  0.04497798523716344\n",
            "Loss SS22:  0.042108021193105756\n",
            "Loss SS33:  0.0423218528846748\n",
            "Loss SS44:  0.040263981082057226\n",
            "Loss SS55:  0.04108532444211363\n",
            "Loss SS66:  0.04042457584433883\n",
            "Loss SS77:  0.046342061590829875\n",
            "Loss SS88:  0.04478896264242762\n",
            "Loss SS11:  0.044923568976686354\n",
            "Loss SS22:  0.042073990753356445\n",
            "Loss SS33:  0.04224931266396604\n",
            "Loss SS44:  0.04028812684911363\n",
            "Loss SS55:  0.04108311420848183\n",
            "Loss SS66:  0.04037741312743924\n",
            "Loss SS77:  0.04636335330652007\n",
            "Loss SS88:  0.04477887642933122\n",
            "Loss SS11:  0.04485554970948901\n",
            "Loss SS22:  0.04201354004985449\n",
            "Loss SS33:  0.04220854759907091\n",
            "Loss SS44:  0.040263268825234164\n",
            "Loss SS55:  0.04105521104489731\n",
            "Loss SS66:  0.0403091028727443\n",
            "Loss SS77:  0.04620054243317503\n",
            "Loss SS88:  0.04476608254558203\n",
            "Loss SS11:  0.044941271661046125\n",
            "Loss SS22:  0.041955265384283125\n",
            "Loss SS33:  0.04226395008819444\n",
            "Loss SS44:  0.04022929213524605\n",
            "Loss SS55:  0.04100752383563089\n",
            "Loss SS66:  0.040297815310103555\n",
            "Loss SS77:  0.046193260051634\n",
            "Loss SS88:  0.04477710788201841\n",
            "Loss SS11:  0.04502422606561616\n",
            "Loss SS22:  0.04205411901338059\n",
            "Loss SS33:  0.04232793665158818\n",
            "Loss SS44:  0.04030485832464625\n",
            "Loss SS55:  0.04108813668639339\n",
            "Loss SS66:  0.04036056936571473\n",
            "Loss SS77:  0.04618725882113328\n",
            "Loss SS88:  0.0448417168175965\n",
            "Loss SS11:  0.04508315998217019\n",
            "Loss SS22:  0.04211850542554539\n",
            "Loss SS33:  0.0424269652868832\n",
            "Loss SS44:  0.040374504968277\n",
            "Loss SS55:  0.04115429176892365\n",
            "Loss SS66:  0.040497707130994584\n",
            "Loss SS77:  0.04628852059169369\n",
            "Loss SS88:  0.04492633649822098\n",
            "Loss SS11:  0.04512085449633174\n",
            "Loss SS22:  0.04208325485440449\n",
            "Loss SS33:  0.04238295270354336\n",
            "Loss SS44:  0.04031459587328721\n",
            "Loss SS55:  0.04113294734224599\n",
            "Loss SS66:  0.04047698970318465\n",
            "Loss SS77:  0.046295731986692436\n",
            "Loss SS88:  0.04495335912517228\n",
            "Loss SS11:  0.045120859194306\n",
            "Loss SS22:  0.04202802132685386\n",
            "Loss SS33:  0.04238921025795723\n",
            "Loss SS44:  0.0402978637918311\n",
            "Loss SS55:  0.041080911083156195\n",
            "Loss SS66:  0.04042287728753849\n",
            "Loss SS77:  0.04626294082744205\n",
            "Loss SS88:  0.04488496413797288\n",
            "Loss SS11:  0.045201777264263956\n",
            "Loss SS22:  0.042042810010825285\n",
            "Loss SS33:  0.04241916982201038\n",
            "Loss SS44:  0.04030892641340952\n",
            "Loss SS55:  0.04107900730100288\n",
            "Loss SS66:  0.04040770908879443\n",
            "Loss SS77:  0.04618415506600768\n",
            "Loss SS88:  0.044884743054189956\n",
            "Loss SS11:  0.04522214961402556\n",
            "Loss SS22:  0.04202599562198868\n",
            "Loss SS33:  0.04236782234313801\n",
            "Loss SS44:  0.04027896738564806\n",
            "Loss SS55:  0.041037375756383485\n",
            "Loss SS66:  0.040343885103501884\n",
            "Loss SS77:  0.04614189518320615\n",
            "Loss SS88:  0.044841255520668505\n",
            "Loss SS11:  0.045267691892204864\n",
            "Loss SS22:  0.0421208521246394\n",
            "Loss SS33:  0.04236898359991771\n",
            "Loss SS44:  0.04027456630553518\n",
            "Loss SS55:  0.041039123018453645\n",
            "Loss SS66:  0.040280823712741144\n",
            "Loss SS77:  0.04611646156612929\n",
            "Loss SS88:  0.044830236761342915\n",
            "Loss SS11:  0.045304802118866276\n",
            "Loss SS22:  0.04211582741178418\n",
            "Loss SS33:  0.04236895931088578\n",
            "Loss SS44:  0.04026163568820696\n",
            "Loss SS55:  0.04105245439467094\n",
            "Loss SS66:  0.040302442796373766\n",
            "Loss SS77:  0.04608174320696795\n",
            "Loss SS88:  0.04478747961864926\n",
            "Loss SS11:  0.04527047470924389\n",
            "Loss SS22:  0.042127640049652276\n",
            "Loss SS33:  0.042389814908879685\n",
            "Loss SS44:  0.04026858484840013\n",
            "Loss SS55:  0.04110852907972032\n",
            "Loss SS66:  0.04031433386216126\n",
            "Loss SS77:  0.04607333800944674\n",
            "Loss SS88:  0.044797621741594074\n",
            "Loss SS11:  0.04526623939360239\n",
            "Loss SS22:  0.042130419920230734\n",
            "Loss SS33:  0.04238871430756945\n",
            "Loss SS44:  0.040266370193825826\n",
            "Loss SS55:  0.04115167735465642\n",
            "Loss SS66:  0.04035626283322258\n",
            "Loss SS77:  0.046046507726798115\n",
            "Loss SS88:  0.044744043714470334\n",
            "Loss SS11:  0.04526115200616337\n",
            "Loss SS22:  0.0421406207984224\n",
            "Loss SS33:  0.04233632754960623\n",
            "Loss SS44:  0.04024856505963635\n",
            "Loss SS55:  0.041169288688480195\n",
            "Loss SS66:  0.04036531837938896\n",
            "Loss SS77:  0.046095841608914064\n",
            "Loss SS88:  0.04479571422252708\n",
            "Loss SS11:  0.045239019621945785\n",
            "Loss SS22:  0.042175501611521234\n",
            "Loss SS33:  0.0423245864698259\n",
            "Loss SS44:  0.04025900199532085\n",
            "Loss SS55:  0.04117599398799214\n",
            "Loss SS66:  0.04037567190958512\n",
            "Loss SS77:  0.046115055720046746\n",
            "Loss SS88:  0.044787612143784655\n",
            "Loss SS11:  0.04522849050993772\n",
            "Loss SS22:  0.04218199145036055\n",
            "Loss SS33:  0.042263755069155876\n",
            "Loss SS44:  0.040211934839532136\n",
            "Loss SS55:  0.04117560975535219\n",
            "Loss SS66:  0.040335672097824696\n",
            "Loss SS77:  0.04616682589873416\n",
            "Loss SS88:  0.04478149164685678\n",
            "Loss SS11:  0.04519962326335748\n",
            "Loss SS22:  0.04213387749321833\n",
            "Loss SS33:  0.042250297765597154\n",
            "Loss SS44:  0.04019625442863699\n",
            "Loss SS55:  0.041178262931761946\n",
            "Loss SS66:  0.04029313565241142\n",
            "Loss SS77:  0.04616325093662224\n",
            "Loss SS88:  0.04478008071102969\n",
            "Loss SS11:  0.045167264639373\n",
            "Loss SS22:  0.04208534939208598\n",
            "Loss SS33:  0.042211746539429454\n",
            "Loss SS44:  0.0401829166571427\n",
            "Loss SS55:  0.04115031531433967\n",
            "Loss SS66:  0.04028282237877033\n",
            "Loss SS77:  0.0461408077017479\n",
            "Loss SS88:  0.04473605605854482\n",
            "Loss SS11:  0.045119421829315734\n",
            "Loss SS22:  0.04206588598565892\n",
            "Loss SS33:  0.04221223082358592\n",
            "Loss SS44:  0.04016079248808255\n",
            "Loss SS55:  0.04114742238202199\n",
            "Loss SS66:  0.04027676265512672\n",
            "Loss SS77:  0.04614836793404502\n",
            "Loss SS88:  0.04473000781094174\n",
            "Loss SS11:  0.04514603791730282\n",
            "Loss SS22:  0.04205874875863153\n",
            "Loss SS33:  0.042195627584018014\n",
            "Loss SS44:  0.040135067744107404\n",
            "Loss SS55:  0.0411245408847793\n",
            "Loss SS66:  0.0402518942434442\n",
            "Loss SS77:  0.0461076936850555\n",
            "Loss SS88:  0.044701424328852275\n",
            "Loss SS11:  0.045120835140391176\n",
            "Loss SS22:  0.04205779068150129\n",
            "Loss SS33:  0.04224816713034233\n",
            "Loss SS44:  0.04013952348501452\n",
            "Loss SS55:  0.04112357613453068\n",
            "Loss SS66:  0.040240696437460234\n",
            "Loss SS77:  0.04608451258227273\n",
            "Loss SS88:  0.04468485017107729\n",
            "Loss SS11:  0.04508420791041817\n",
            "Loss SS22:  0.04205427852663559\n",
            "Loss SS33:  0.04223555009122248\n",
            "Loss SS44:  0.04013005925337134\n",
            "Loss SS55:  0.041113065019716906\n",
            "Loss SS66:  0.04024777205878513\n",
            "Loss SS77:  0.04611678773860986\n",
            "Loss SS88:  0.04469932294172099\n",
            "Loss SS11:  0.04510368835026207\n",
            "Loss SS22:  0.04203023823427031\n",
            "Loss SS33:  0.042246277632393005\n",
            "Loss SS44:  0.040101433714588595\n",
            "Loss SS55:  0.041103491740213535\n",
            "Loss SS66:  0.040239597570120134\n",
            "Loss SS77:  0.04611192062662249\n",
            "Loss SS88:  0.04469839110970497\n",
            "Loss SS11:  0.04508964899134122\n",
            "Loss SS22:  0.04200436242145669\n",
            "Loss SS33:  0.04222969605394129\n",
            "Loss SS44:  0.040075750966277725\n",
            "Loss SS55:  0.041121873050687124\n",
            "Loss SS66:  0.0402328285044737\n",
            "Loss SS77:  0.046098443948114014\n",
            "Loss SS88:  0.044683054953090585\n",
            "Loss SS11:  0.04506281817092357\n",
            "Loss SS22:  0.04199083087833848\n",
            "Loss SS33:  0.042195670629345525\n",
            "Loss SS44:  0.04005628812500811\n",
            "Loss SS55:  0.04110779330681941\n",
            "Loss SS66:  0.04020479180681424\n",
            "Loss SS77:  0.046027439981307885\n",
            "Loss SS88:  0.04461964296074364\n",
            "Loss SS11:  0.04509109448250907\n",
            "Loss SS22:  0.04197472803618597\n",
            "Loss SS33:  0.0421713771551009\n",
            "Loss SS44:  0.04001718497527835\n",
            "Loss SS55:  0.04109380241778805\n",
            "Loss SS66:  0.04020999997015804\n",
            "Loss SS77:  0.045989840250948204\n",
            "Loss SS88:  0.044585715541068244\n",
            "Loss SS11:  0.04512440641770636\n",
            "Loss SS22:  0.04198775478543486\n",
            "Loss SS33:  0.042164515155806506\n",
            "Loss SS44:  0.04001944084788796\n",
            "Loss SS55:  0.04109984142078723\n",
            "Loss SS66:  0.040214439756480834\n",
            "Loss SS77:  0.045998110967755614\n",
            "Loss SS88:  0.04460737530021299\n",
            "Loss SS11:  0.04509429767764108\n",
            "Loss SS22:  0.04200517127874994\n",
            "Loss SS33:  0.04213295770025021\n",
            "Loss SS44:  0.04001554406254831\n",
            "Loss SS55:  0.041101691251906165\n",
            "Loss SS66:  0.04021543990412768\n",
            "Loss SS77:  0.04595357496880557\n",
            "Loss SS88:  0.04457913095317328\n",
            "Loss SS11:  0.04511530257566808\n",
            "Loss SS22:  0.04201194429836477\n",
            "Loss SS33:  0.0421310654109136\n",
            "Loss SS44:  0.0400281106951401\n",
            "Loss SS55:  0.04112089178447485\n",
            "Loss SS66:  0.040229963957201555\n",
            "Loss SS77:  0.04595432870284678\n",
            "Loss SS88:  0.04460094168322103\n",
            "Loss SS11:  0.04508832850754953\n",
            "Loss SS22:  0.04198292486386897\n",
            "Loss SS33:  0.04208405024594608\n",
            "Loss SS44:  0.04000971594224673\n",
            "Loss SS55:  0.04109021279692927\n",
            "Loss SS66:  0.04022085511940261\n",
            "Loss SS77:  0.045900226330812456\n",
            "Loss SS88:  0.04457748813550323\n",
            "Loss SS11:  0.0451078603208876\n",
            "Loss SS22:  0.04197252782530525\n",
            "Loss SS33:  0.04209874065216977\n",
            "Loss SS44:  0.04001038492807185\n",
            "Loss SS55:  0.04109114634037829\n",
            "Loss SS66:  0.04023800470154572\n",
            "Loss SS77:  0.04592215022296051\n",
            "Loss SS88:  0.04459559868455203\n",
            "Loss SS11:  0.04512563431243146\n",
            "Loss SS22:  0.041979571122948714\n",
            "Loss SS33:  0.042097961360758\n",
            "Loss SS44:  0.040024016721549954\n",
            "Loss SS55:  0.04112897670420998\n",
            "Loss SS66:  0.040258664299141274\n",
            "Loss SS77:  0.045934074006958175\n",
            "Loss SS88:  0.04460724859504636\n",
            "Loss SS11:  0.04512994908555987\n",
            "Loss SS22:  0.04198097542671733\n",
            "Loss SS33:  0.04210402757543545\n",
            "Loss SS44:  0.0400174011894524\n",
            "Loss SS55:  0.04113317587273498\n",
            "Loss SS66:  0.04026830745881137\n",
            "Loss SS77:  0.045959540867042646\n",
            "Loss SS88:  0.04461731314012647\n",
            "Loss SS11:  0.045136751041543965\n",
            "Loss SS22:  0.042001058159676835\n",
            "Loss SS33:  0.042128632244537334\n",
            "Loss SS44:  0.040011911018259204\n",
            "Loss SS55:  0.04116165383289827\n",
            "Loss SS66:  0.04030709150553762\n",
            "Loss SS77:  0.045984119509056114\n",
            "Loss SS88:  0.04463428537303981\n",
            "Loss SS11:  0.04512353813016241\n",
            "Loss SS22:  0.042007569332070754\n",
            "Loss SS33:  0.04210792056705005\n",
            "Loss SS44:  0.03998423655074004\n",
            "Loss SS55:  0.04118102537990856\n",
            "Loss SS66:  0.040324556906543005\n",
            "Loss SS77:  0.04595388748572671\n",
            "Loss SS88:  0.04461895227587149\n",
            "Loss SS11:  0.045090793349349574\n",
            "Loss SS22:  0.04195738488205096\n",
            "Loss SS33:  0.04206889760803788\n",
            "Loss SS44:  0.039962048594917636\n",
            "Loss SS55:  0.04116884247985974\n",
            "Loss SS66:  0.040299053681358055\n",
            "Loss SS77:  0.045916073911969384\n",
            "Loss SS88:  0.04457714857795331\n",
            "Validation: \n",
            " Loss SS11:  0.03834572061896324\n",
            " Loss SS22:  0.057068176567554474\n",
            " Loss SS33:  0.059089336544275284\n",
            " Loss SS55:  0.05417134240269661\n",
            " Loss SS66:  0.06289196759462357\n",
            " Loss SS77:  0.059975676238536835\n",
            " Loss SS88:  0.05915771424770355\n",
            " Loss SS99:  0.05705288425087929\n",
            " Loss SS11:  0.04421194518605868\n",
            " Loss SS22:  0.06403272563502901\n",
            " Loss SS33:  0.0632660346371787\n",
            " Loss SS55:  0.05741460071433158\n",
            " Loss SS66:  0.06777662021063623\n",
            " Loss SS77:  0.06393056274169967\n",
            " Loss SS88:  0.06942497158334368\n",
            " Loss SS99:  0.06454708267535482\n",
            " Loss SS11:  0.044984484500274424\n",
            " Loss SS22:  0.06297990352642245\n",
            " Loss SS33:  0.06476761836831163\n",
            " Loss SS55:  0.05796282438606751\n",
            " Loss SS66:  0.06663698248746919\n",
            " Loss SS77:  0.06350264907246683\n",
            " Loss SS88:  0.06868006089111654\n",
            " Loss SS99:  0.06372747970063512\n",
            " Loss SS11:  0.044951127567252176\n",
            " Loss SS22:  0.06281053323726185\n",
            " Loss SS33:  0.06425651816315338\n",
            " Loss SS55:  0.05753558257319888\n",
            " Loss SS66:  0.06639046093723813\n",
            " Loss SS77:  0.06321202078070796\n",
            " Loss SS88:  0.06881658855031748\n",
            " Loss SS99:  0.06339174539583628\n",
            " Loss SS11:  0.044956396989248415\n",
            " Loss SS22:  0.06250154429379805\n",
            " Loss SS33:  0.06414268124434683\n",
            " Loss SS55:  0.05739522283827817\n",
            " Loss SS66:  0.06620211296795327\n",
            " Loss SS77:  0.06306648907470114\n",
            " Loss SS88:  0.0687342361535555\n",
            " Loss SS99:  0.06346128733805668\n",
            "\n",
            "Epoch: 57\n",
            "Loss SS11:  0.0486597865819931\n",
            "Loss SS22:  0.04407230392098427\n",
            "Loss SS33:  0.04610315337777138\n",
            "Loss SS44:  0.043013956397771835\n",
            "Loss SS55:  0.04549865797162056\n",
            "Loss SS66:  0.042508333921432495\n",
            "Loss SS77:  0.051748692989349365\n",
            "Loss SS88:  0.05059605464339256\n",
            "Loss SS11:  0.04703880812634121\n",
            "Loss SS22:  0.040482379496097565\n",
            "Loss SS33:  0.04123434932394461\n",
            "Loss SS44:  0.03923730416731401\n",
            "Loss SS55:  0.04104016213254495\n",
            "Loss SS66:  0.039794727143916214\n",
            "Loss SS77:  0.045653855936093765\n",
            "Loss SS88:  0.04432162947275422\n",
            "Loss SS11:  0.04542409930200804\n",
            "Loss SS22:  0.040972114141498296\n",
            "Loss SS33:  0.04183955756681306\n",
            "Loss SS44:  0.03965749201320466\n",
            "Loss SS55:  0.04102070221588725\n",
            "Loss SS66:  0.039918964285226094\n",
            "Loss SS77:  0.045202845973627906\n",
            "Loss SS88:  0.0441886034040224\n",
            "Loss SS11:  0.04531362941188197\n",
            "Loss SS22:  0.04147741871495401\n",
            "Loss SS33:  0.04200624277995479\n",
            "Loss SS44:  0.040008611496417754\n",
            "Loss SS55:  0.04121719264695721\n",
            "Loss SS66:  0.03998090987724642\n",
            "Loss SS77:  0.04554689699603665\n",
            "Loss SS88:  0.04435153005103911\n",
            "Loss SS11:  0.04480589063065808\n",
            "Loss SS22:  0.04174617232709396\n",
            "Loss SS33:  0.04216394455331128\n",
            "Loss SS44:  0.04020186177477604\n",
            "Loss SS55:  0.04155647245849051\n",
            "Loss SS66:  0.04006417440931972\n",
            "Loss SS77:  0.04552739495184363\n",
            "Loss SS88:  0.04456961400261739\n",
            "Loss SS11:  0.044558154455586974\n",
            "Loss SS22:  0.04185927053000413\n",
            "Loss SS33:  0.042162903602801116\n",
            "Loss SS44:  0.04034561371686412\n",
            "Loss SS55:  0.04162046181804994\n",
            "Loss SS66:  0.040259664184322544\n",
            "Loss SS77:  0.045627115476949545\n",
            "Loss SS88:  0.044563868758725186\n",
            "Loss SS11:  0.044514525620663756\n",
            "Loss SS22:  0.04179528160173385\n",
            "Loss SS33:  0.042015541711303055\n",
            "Loss SS44:  0.040123851511810645\n",
            "Loss SS55:  0.04158392139389867\n",
            "Loss SS66:  0.04011441266439\n",
            "Loss SS77:  0.04552088537421383\n",
            "Loss SS88:  0.0444458541689349\n",
            "Loss SS11:  0.04433541839391413\n",
            "Loss SS22:  0.04194622413373329\n",
            "Loss SS33:  0.042197181672697336\n",
            "Loss SS44:  0.04019265889491833\n",
            "Loss SS55:  0.04169315173172615\n",
            "Loss SS66:  0.0403096128622411\n",
            "Loss SS77:  0.04562108213423004\n",
            "Loss SS88:  0.04441116131107572\n",
            "Loss SS11:  0.044304627695201354\n",
            "Loss SS22:  0.041955070593106894\n",
            "Loss SS33:  0.0420691221492526\n",
            "Loss SS44:  0.04001049669804396\n",
            "Loss SS55:  0.04132817124510989\n",
            "Loss SS66:  0.03999691438159825\n",
            "Loss SS77:  0.045521407261674786\n",
            "Loss SS88:  0.04434979251689381\n",
            "Loss SS11:  0.04429918118230589\n",
            "Loss SS22:  0.041955860679621225\n",
            "Loss SS33:  0.04198051239926737\n",
            "Loss SS44:  0.04002387240365311\n",
            "Loss SS55:  0.04128104358256518\n",
            "Loss SS66:  0.04005317434996039\n",
            "Loss SS77:  0.045506503187365585\n",
            "Loss SS88:  0.04430325570833552\n",
            "Loss SS11:  0.04435493583136266\n",
            "Loss SS22:  0.041820637180958645\n",
            "Loss SS33:  0.04185922683613135\n",
            "Loss SS44:  0.03990546676634562\n",
            "Loss SS55:  0.04106950000076011\n",
            "Loss SS66:  0.039898435963262424\n",
            "Loss SS77:  0.04541693472921258\n",
            "Loss SS88:  0.04414100332720445\n",
            "Loss SS11:  0.04441722636824256\n",
            "Loss SS22:  0.041860840576994525\n",
            "Loss SS33:  0.041825940144491626\n",
            "Loss SS44:  0.03987830662512564\n",
            "Loss SS55:  0.04104879122596603\n",
            "Loss SS66:  0.03995408778926274\n",
            "Loss SS77:  0.04552653176827474\n",
            "Loss SS88:  0.04413178247642947\n",
            "Loss SS11:  0.044425953579835656\n",
            "Loss SS22:  0.04195631707995391\n",
            "Loss SS33:  0.04196288895385324\n",
            "Loss SS44:  0.03996565359190476\n",
            "Loss SS55:  0.04123182890336376\n",
            "Loss SS66:  0.04001487163472767\n",
            "Loss SS77:  0.045655024168658846\n",
            "Loss SS88:  0.04435786087650898\n",
            "Loss SS11:  0.04461524726552818\n",
            "Loss SS22:  0.04194130555136513\n",
            "Loss SS33:  0.04206344880561792\n",
            "Loss SS44:  0.0399966829435061\n",
            "Loss SS55:  0.04127810983598687\n",
            "Loss SS66:  0.03993253566267836\n",
            "Loss SS77:  0.04587321407126106\n",
            "Loss SS88:  0.04446634755448531\n",
            "Loss SS11:  0.044718187328771496\n",
            "Loss SS22:  0.04192822283886848\n",
            "Loss SS33:  0.042051334911627124\n",
            "Loss SS44:  0.03997240638267909\n",
            "Loss SS55:  0.04126938091630631\n",
            "Loss SS66:  0.03996101650574529\n",
            "Loss SS77:  0.04595926596868968\n",
            "Loss SS88:  0.04446894168219668\n",
            "Loss SS11:  0.04471520455369097\n",
            "Loss SS22:  0.04182826022043923\n",
            "Loss SS33:  0.04201923818975095\n",
            "Loss SS44:  0.03991161101799927\n",
            "Loss SS55:  0.041204263561016675\n",
            "Loss SS66:  0.03987665701386155\n",
            "Loss SS77:  0.04590605344894706\n",
            "Loss SS88:  0.04441349839927345\n",
            "Loss SS11:  0.04476342056386219\n",
            "Loss SS22:  0.0416823862733678\n",
            "Loss SS33:  0.04195256658500026\n",
            "Loss SS44:  0.03985674628374739\n",
            "Loss SS55:  0.041061172620480105\n",
            "Loss SS66:  0.03981672137142708\n",
            "Loss SS77:  0.04577755171412266\n",
            "Loss SS88:  0.04430996443821777\n",
            "Loss SS11:  0.04492175023545299\n",
            "Loss SS22:  0.0417437937481013\n",
            "Loss SS33:  0.04196050185819118\n",
            "Loss SS44:  0.03994560655620363\n",
            "Loss SS55:  0.04115395622643811\n",
            "Loss SS66:  0.03993438334580053\n",
            "Loss SS77:  0.04587244967881002\n",
            "Loss SS88:  0.04441803970444969\n",
            "Loss SS11:  0.04497762232277933\n",
            "Loss SS22:  0.041761172369369486\n",
            "Loss SS33:  0.04200945607288766\n",
            "Loss SS44:  0.03996174319425999\n",
            "Loss SS55:  0.041149630586909985\n",
            "Loss SS66:  0.040016250304095655\n",
            "Loss SS77:  0.045949285016369426\n",
            "Loss SS88:  0.04453900627256757\n",
            "Loss SS11:  0.045047196359690574\n",
            "Loss SS22:  0.04174488669758692\n",
            "Loss SS33:  0.04195315578064993\n",
            "Loss SS44:  0.03996787475743843\n",
            "Loss SS55:  0.041144919430554226\n",
            "Loss SS66:  0.04006836042588294\n",
            "Loss SS77:  0.04593263638659297\n",
            "Loss SS88:  0.044584226623879675\n",
            "Loss SS11:  0.04498306865715862\n",
            "Loss SS22:  0.041717160178061145\n",
            "Loss SS33:  0.041988437800709884\n",
            "Loss SS44:  0.03996940420486441\n",
            "Loss SS55:  0.0411651144869885\n",
            "Loss SS66:  0.04007576276265567\n",
            "Loss SS77:  0.04593381398723493\n",
            "Loss SS88:  0.044577485997581955\n",
            "Loss SS11:  0.044948461620022334\n",
            "Loss SS22:  0.04169993685178847\n",
            "Loss SS33:  0.04199613083454105\n",
            "Loss SS44:  0.039975897748888385\n",
            "Loss SS55:  0.041123142527742976\n",
            "Loss SS66:  0.04006009728987635\n",
            "Loss SS77:  0.045845462388901914\n",
            "Loss SS88:  0.04450053177879885\n",
            "Loss SS11:  0.04495490340692965\n",
            "Loss SS22:  0.0417706319615582\n",
            "Loss SS33:  0.04196888156134079\n",
            "Loss SS44:  0.03997346308773459\n",
            "Loss SS55:  0.04104922848387002\n",
            "Loss SS66:  0.040053137860416824\n",
            "Loss SS77:  0.04578641785216008\n",
            "Loss SS88:  0.04446328510593505\n",
            "Loss SS11:  0.04498551560170723\n",
            "Loss SS22:  0.041828104105346646\n",
            "Loss SS33:  0.04193659484773487\n",
            "Loss SS44:  0.03996903642818525\n",
            "Loss SS55:  0.04096365360406054\n",
            "Loss SS66:  0.040012606068865045\n",
            "Loss SS77:  0.04578742799369288\n",
            "Loss SS88:  0.04443156219277031\n",
            "Loss SS11:  0.04495883185833816\n",
            "Loss SS22:  0.041840858896614604\n",
            "Loss SS33:  0.04194856226382414\n",
            "Loss SS44:  0.039907351800885935\n",
            "Loss SS55:  0.0409924988924715\n",
            "Loss SS66:  0.03999554492625953\n",
            "Loss SS77:  0.045743628872628034\n",
            "Loss SS88:  0.04437560515470525\n",
            "Loss SS11:  0.04498964108793859\n",
            "Loss SS22:  0.041890286873892485\n",
            "Loss SS33:  0.041929678642654795\n",
            "Loss SS44:  0.039906301717477964\n",
            "Loss SS55:  0.041039633308630065\n",
            "Loss SS66:  0.04002735989383492\n",
            "Loss SS77:  0.04571177426028062\n",
            "Loss SS88:  0.04440164048061428\n",
            "Loss SS11:  0.04499189243063159\n",
            "Loss SS22:  0.0419125174140108\n",
            "Loss SS33:  0.041960763962675326\n",
            "Loss SS44:  0.03988467839143285\n",
            "Loss SS55:  0.04108359239338915\n",
            "Loss SS66:  0.0400696840008785\n",
            "Loss SS77:  0.04574458443798781\n",
            "Loss SS88:  0.04441033871808728\n",
            "Loss SS11:  0.044952793012465936\n",
            "Loss SS22:  0.041930207329702554\n",
            "Loss SS33:  0.04195790533045121\n",
            "Loss SS44:  0.0398856749778744\n",
            "Loss SS55:  0.041071713465606154\n",
            "Loss SS66:  0.040074061437396544\n",
            "Loss SS77:  0.04574978939941449\n",
            "Loss SS88:  0.044436700284700995\n",
            "Loss SS11:  0.04498449421331976\n",
            "Loss SS22:  0.0420035295904319\n",
            "Loss SS33:  0.04199618958864772\n",
            "Loss SS44:  0.03990680540106475\n",
            "Loss SS55:  0.0411413084065999\n",
            "Loss SS66:  0.040099864925035804\n",
            "Loss SS77:  0.045770213856391635\n",
            "Loss SS88:  0.04444663585876231\n",
            "Loss SS11:  0.04495336779767705\n",
            "Loss SS22:  0.042047730157363045\n",
            "Loss SS33:  0.042001680179764724\n",
            "Loss SS44:  0.03989080726281064\n",
            "Loss SS55:  0.041154777547952645\n",
            "Loss SS66:  0.04011056636207292\n",
            "Loss SS77:  0.04583956330656186\n",
            "Loss SS88:  0.04448120029726389\n",
            "Loss SS11:  0.044937628015034224\n",
            "Loss SS22:  0.0420142280253461\n",
            "Loss SS33:  0.041979441685335975\n",
            "Loss SS44:  0.03988648757982095\n",
            "Loss SS55:  0.04112379009095933\n",
            "Loss SS66:  0.04008052370625477\n",
            "Loss SS77:  0.045853941964350664\n",
            "Loss SS88:  0.044486982135281614\n",
            "Loss SS11:  0.04489590634870376\n",
            "Loss SS22:  0.04195107523220145\n",
            "Loss SS33:  0.04193270483153043\n",
            "Loss SS44:  0.03989322788989429\n",
            "Loss SS55:  0.04109702894158685\n",
            "Loss SS66:  0.040047152441030916\n",
            "Loss SS77:  0.045825023371786164\n",
            "Loss SS88:  0.0444343616389385\n",
            "Loss SS11:  0.04491032485928491\n",
            "Loss SS22:  0.04189141058708277\n",
            "Loss SS33:  0.041921785107097154\n",
            "Loss SS44:  0.039895692254152626\n",
            "Loss SS55:  0.04109924468806599\n",
            "Loss SS66:  0.04003030959969369\n",
            "Loss SS77:  0.045819344120883496\n",
            "Loss SS88:  0.04443802269077004\n",
            "Loss SS11:  0.044943637335588565\n",
            "Loss SS22:  0.04187196899441794\n",
            "Loss SS33:  0.04192324459912914\n",
            "Loss SS44:  0.03988584908698981\n",
            "Loss SS55:  0.04107804347696261\n",
            "Loss SS66:  0.04003136662423071\n",
            "Loss SS77:  0.0458308007241736\n",
            "Loss SS88:  0.04440667513994292\n",
            "Loss SS11:  0.044934464558478326\n",
            "Loss SS22:  0.04186838315772521\n",
            "Loss SS33:  0.041942802855957986\n",
            "Loss SS44:  0.03987875271621338\n",
            "Loss SS55:  0.04107920691108773\n",
            "Loss SS66:  0.040044703911389076\n",
            "Loss SS77:  0.04583650839424903\n",
            "Loss SS88:  0.044348738497652966\n",
            "Loss SS11:  0.04492966255486181\n",
            "Loss SS22:  0.04188776768722765\n",
            "Loss SS33:  0.041935409002664084\n",
            "Loss SS44:  0.03987547057943466\n",
            "Loss SS55:  0.04107127306807754\n",
            "Loss SS66:  0.040037033585911125\n",
            "Loss SS77:  0.04585783632519918\n",
            "Loss SS88:  0.04435241297736467\n",
            "Loss SS11:  0.04498225327715319\n",
            "Loss SS22:  0.041872618188488184\n",
            "Loss SS33:  0.04194047663185405\n",
            "Loss SS44:  0.03987862397800522\n",
            "Loss SS55:  0.04107582110942566\n",
            "Loss SS66:  0.040046722604957646\n",
            "Loss SS77:  0.045870397260562204\n",
            "Loss SS88:  0.04432621616818568\n",
            "Loss SS11:  0.04499346819126381\n",
            "Loss SS22:  0.0418482618431518\n",
            "Loss SS33:  0.041914179439171945\n",
            "Loss SS44:  0.03988089584155546\n",
            "Loss SS55:  0.04106363744507582\n",
            "Loss SS66:  0.04004627658993086\n",
            "Loss SS77:  0.04586321592451427\n",
            "Loss SS88:  0.04429650366948942\n",
            "Loss SS11:  0.04498217145367244\n",
            "Loss SS22:  0.04183478901783625\n",
            "Loss SS33:  0.041859300929339226\n",
            "Loss SS44:  0.03984510406773547\n",
            "Loss SS55:  0.04103025560343047\n",
            "Loss SS66:  0.04001291498072504\n",
            "Loss SS77:  0.04581074681612137\n",
            "Loss SS88:  0.04425143874849234\n",
            "Loss SS11:  0.044966141481305025\n",
            "Loss SS22:  0.041785674002926675\n",
            "Loss SS33:  0.04182334757788712\n",
            "Loss SS44:  0.03980437989163277\n",
            "Loss SS55:  0.04099516605820192\n",
            "Loss SS66:  0.03998448684468599\n",
            "Loss SS77:  0.045757911270460506\n",
            "Loss SS88:  0.04422131334157551\n",
            "Loss SS11:  0.04498457238821318\n",
            "Loss SS22:  0.04178603507225353\n",
            "Loss SS33:  0.04178570384656699\n",
            "Loss SS44:  0.03978044522968016\n",
            "Loss SS55:  0.04098394577231193\n",
            "Loss SS66:  0.03998425150489866\n",
            "Loss SS77:  0.04572563251466525\n",
            "Loss SS88:  0.04421301919065806\n",
            "Loss SS11:  0.04499714610386649\n",
            "Loss SS22:  0.04180526941869671\n",
            "Loss SS33:  0.04176699562301891\n",
            "Loss SS44:  0.03977009849826785\n",
            "Loss SS55:  0.0409927759818062\n",
            "Loss SS66:  0.03998531267010672\n",
            "Loss SS77:  0.04569784658813709\n",
            "Loss SS88:  0.04417735255728963\n",
            "Loss SS11:  0.04499221417937879\n",
            "Loss SS22:  0.04180397326900953\n",
            "Loss SS33:  0.04179382214495235\n",
            "Loss SS44:  0.0397939243112777\n",
            "Loss SS55:  0.04100951270118462\n",
            "Loss SS66:  0.03998518273269762\n",
            "Loss SS77:  0.0456793758743874\n",
            "Loss SS88:  0.04418772739533008\n",
            "Loss SS11:  0.04498869566173399\n",
            "Loss SS22:  0.04178113734950322\n",
            "Loss SS33:  0.041759043292352055\n",
            "Loss SS44:  0.03975881171129696\n",
            "Loss SS55:  0.0409807964917402\n",
            "Loss SS66:  0.039962621574266326\n",
            "Loss SS77:  0.04566582635459933\n",
            "Loss SS88:  0.04415121425608748\n",
            "Loss SS11:  0.044973176961042444\n",
            "Loss SS22:  0.0417889755582458\n",
            "Loss SS33:  0.04179089084740669\n",
            "Loss SS44:  0.03978561340512332\n",
            "Loss SS55:  0.04099910608292166\n",
            "Loss SS66:  0.03999162025225946\n",
            "Loss SS77:  0.04567358955369245\n",
            "Loss SS88:  0.044171704718517876\n",
            "Loss SS11:  0.04497466995453094\n",
            "Loss SS22:  0.04176812195229425\n",
            "Loss SS33:  0.041793847510264874\n",
            "Loss SS44:  0.03981596073693288\n",
            "Loss SS55:  0.04103923850835295\n",
            "Loss SS66:  0.040004361486950364\n",
            "Loss SS77:  0.04571105961062163\n",
            "Loss SS88:  0.04420301945933746\n",
            "Loss SS11:  0.0449325823570539\n",
            "Loss SS22:  0.041745190797555475\n",
            "Loss SS33:  0.041807204024328845\n",
            "Loss SS44:  0.039801506547827006\n",
            "Loss SS55:  0.041049543267152315\n",
            "Loss SS66:  0.04001387584461566\n",
            "Loss SS77:  0.04572678877772064\n",
            "Loss SS88:  0.04422389015296793\n",
            "Loss SS11:  0.044909081198354184\n",
            "Loss SS22:  0.04175143332997705\n",
            "Loss SS33:  0.0418032078732857\n",
            "Loss SS44:  0.039808071604010405\n",
            "Loss SS55:  0.04104513538934117\n",
            "Loss SS66:  0.04001007518570894\n",
            "Loss SS77:  0.045729964038242456\n",
            "Loss SS88:  0.04424039023728634\n",
            "Loss SS11:  0.044925893682080345\n",
            "Loss SS22:  0.041759408576696674\n",
            "Loss SS33:  0.04177486373735069\n",
            "Loss SS44:  0.03979087594640974\n",
            "Loss SS55:  0.04107174235483217\n",
            "Loss SS66:  0.04001667163104376\n",
            "Loss SS77:  0.04572277405374759\n",
            "Loss SS88:  0.0442390661539506\n",
            "Loss SS11:  0.04487761094134354\n",
            "Loss SS22:  0.0417122492251838\n",
            "Loss SS33:  0.04173807444737545\n",
            "Loss SS44:  0.03974796085032329\n",
            "Loss SS55:  0.04105241582283178\n",
            "Loss SS66:  0.039983881535027514\n",
            "Loss SS77:  0.04567868778455524\n",
            "Loss SS88:  0.04419163901305733\n",
            "Validation: \n",
            " Loss SS11:  0.03830651566386223\n",
            " Loss SS22:  0.05652863532304764\n",
            " Loss SS33:  0.061514873057603836\n",
            " Loss SS55:  0.05320579558610916\n",
            " Loss SS66:  0.061985548585653305\n",
            " Loss SS77:  0.06059464439749718\n",
            " Loss SS88:  0.05973905324935913\n",
            " Loss SS99:  0.057607147842645645\n",
            " Loss SS11:  0.04365583863996324\n",
            " Loss SS22:  0.06373713750924383\n",
            " Loss SS33:  0.0668796795819487\n",
            " Loss SS55:  0.05690202454016322\n",
            " Loss SS66:  0.0677789847056071\n",
            " Loss SS77:  0.06460075098134223\n",
            " Loss SS88:  0.07044044527269545\n",
            " Loss SS99:  0.06463626452854701\n",
            " Loss SS11:  0.044255355145873095\n",
            " Loss SS22:  0.06262745889948636\n",
            " Loss SS33:  0.0683608434912635\n",
            " Loss SS55:  0.05759529878453511\n",
            " Loss SS66:  0.06680590477658481\n",
            " Loss SS77:  0.0642558845078073\n",
            " Loss SS88:  0.06953768114127763\n",
            " Loss SS99:  0.06386344162065809\n",
            " Loss SS11:  0.04426231113125066\n",
            " Loss SS22:  0.062440723180770874\n",
            " Loss SS33:  0.06766076599721049\n",
            " Loss SS55:  0.05705586271207841\n",
            " Loss SS66:  0.06652204924431003\n",
            " Loss SS77:  0.06392067078439916\n",
            " Loss SS88:  0.06965057539646743\n",
            " Loss SS99:  0.06359116769716387\n",
            " Loss SS11:  0.044206193613785284\n",
            " Loss SS22:  0.06215836489458143\n",
            " Loss SS33:  0.06753372837915833\n",
            " Loss SS55:  0.05687048934676029\n",
            " Loss SS66:  0.06628492713710409\n",
            " Loss SS77:  0.06389980059531\n",
            " Loss SS88:  0.06955254303268445\n",
            " Loss SS99:  0.06359139678102953\n",
            "\n",
            "Epoch: 58\n",
            "Loss SS11:  0.0475657656788826\n",
            "Loss SS22:  0.04372095316648483\n",
            "Loss SS33:  0.046324823051691055\n",
            "Loss SS44:  0.04207446426153183\n",
            "Loss SS55:  0.045567646622657776\n",
            "Loss SS66:  0.045750465244054794\n",
            "Loss SS77:  0.055783141404390335\n",
            "Loss SS88:  0.050964806228876114\n",
            "Loss SS11:  0.044084648178382355\n",
            "Loss SS22:  0.040723168714479965\n",
            "Loss SS33:  0.04134662144563415\n",
            "Loss SS44:  0.03954634578390555\n",
            "Loss SS55:  0.040684737942435524\n",
            "Loss SS66:  0.03991990705782717\n",
            "Loss SS77:  0.044988164508884605\n",
            "Loss SS88:  0.04468031803315336\n",
            "Loss SS11:  0.043534548864478155\n",
            "Loss SS22:  0.04109884656610943\n",
            "Loss SS33:  0.042179224569173085\n",
            "Loss SS44:  0.03947355030548005\n",
            "Loss SS55:  0.04049367475367728\n",
            "Loss SS66:  0.04005562673721995\n",
            "Loss SS77:  0.04515875148631278\n",
            "Loss SS88:  0.04436345238770757\n",
            "Loss SS11:  0.04391292146136684\n",
            "Loss SS22:  0.04192001973429034\n",
            "Loss SS33:  0.042149620310914133\n",
            "Loss SS44:  0.039674915493496006\n",
            "Loss SS55:  0.04080097088890691\n",
            "Loss SS66:  0.04014157047194819\n",
            "Loss SS77:  0.045552136076073495\n",
            "Loss SS88:  0.04429704239291529\n",
            "Loss SS11:  0.043392456704523505\n",
            "Loss SS22:  0.04198036433720007\n",
            "Loss SS33:  0.042116918124076794\n",
            "Loss SS44:  0.039595623387069234\n",
            "Loss SS55:  0.04089472888082993\n",
            "Loss SS66:  0.0403273970615573\n",
            "Loss SS77:  0.04531822698872264\n",
            "Loss SS88:  0.04464283849044544\n",
            "Loss SS11:  0.04354023064176241\n",
            "Loss SS22:  0.0420336529758631\n",
            "Loss SS33:  0.0421259919656258\n",
            "Loss SS44:  0.039721322118067275\n",
            "Loss SS55:  0.04103001937562344\n",
            "Loss SS66:  0.04049890371514302\n",
            "Loss SS77:  0.0453206341640622\n",
            "Loss SS88:  0.04432552607328284\n",
            "Loss SS11:  0.0436234427768676\n",
            "Loss SS22:  0.04186145173477345\n",
            "Loss SS33:  0.04217029460629479\n",
            "Loss SS44:  0.03966974528109441\n",
            "Loss SS55:  0.040890124366908774\n",
            "Loss SS66:  0.040251690711154316\n",
            "Loss SS77:  0.045094861847455384\n",
            "Loss SS88:  0.04413748208860882\n",
            "Loss SS11:  0.043835319683585366\n",
            "Loss SS22:  0.04189026208830551\n",
            "Loss SS33:  0.04221687471153031\n",
            "Loss SS44:  0.03990635606394687\n",
            "Loss SS55:  0.04102159852922802\n",
            "Loss SS66:  0.04035883365382611\n",
            "Loss SS77:  0.045108200641165316\n",
            "Loss SS88:  0.0441398485235765\n",
            "Loss SS11:  0.043997746880775616\n",
            "Loss SS22:  0.04179847130069026\n",
            "Loss SS33:  0.04194293897828938\n",
            "Loss SS44:  0.03959997987121711\n",
            "Loss SS55:  0.04079631895378784\n",
            "Loss SS66:  0.04004549138523914\n",
            "Loss SS77:  0.045049332864122625\n",
            "Loss SS88:  0.04399503624917549\n",
            "Loss SS11:  0.04429651588037774\n",
            "Loss SS22:  0.041858625280988083\n",
            "Loss SS33:  0.04194215394474648\n",
            "Loss SS44:  0.03958931340129821\n",
            "Loss SS55:  0.040734739626174445\n",
            "Loss SS66:  0.04011722527198739\n",
            "Loss SS77:  0.04505128155534084\n",
            "Loss SS88:  0.044102316947428735\n",
            "Loss SS11:  0.044317602447353964\n",
            "Loss SS22:  0.04173171225160655\n",
            "Loss SS33:  0.04178830807899485\n",
            "Loss SS44:  0.03953921005572423\n",
            "Loss SS55:  0.040624873436028414\n",
            "Loss SS66:  0.039990523627193846\n",
            "Loss SS77:  0.044931747756972173\n",
            "Loss SS88:  0.043978282942040134\n",
            "Loss SS11:  0.044449087769330084\n",
            "Loss SS22:  0.04180503462080483\n",
            "Loss SS33:  0.04172050278331783\n",
            "Loss SS44:  0.03957287118107349\n",
            "Loss SS55:  0.04064870031701552\n",
            "Loss SS66:  0.04004680408953546\n",
            "Loss SS77:  0.04497354467575614\n",
            "Loss SS88:  0.043934336083160865\n",
            "Loss SS11:  0.04441202241034547\n",
            "Loss SS22:  0.04176792422355699\n",
            "Loss SS33:  0.04183612409825167\n",
            "Loss SS44:  0.0396381785308034\n",
            "Loss SS55:  0.040763736694804896\n",
            "Loss SS66:  0.040008880351939474\n",
            "Loss SS77:  0.04513737152923237\n",
            "Loss SS88:  0.044053315377432456\n",
            "Loss SS11:  0.04455320622402293\n",
            "Loss SS22:  0.041732170341806554\n",
            "Loss SS33:  0.041906780669934876\n",
            "Loss SS44:  0.03968480545032115\n",
            "Loss SS55:  0.040825211421679\n",
            "Loss SS66:  0.0399413603919608\n",
            "Loss SS77:  0.04534946055016445\n",
            "Loss SS88:  0.04412361478987541\n",
            "Loss SS11:  0.0445508877119274\n",
            "Loss SS22:  0.04173637797118079\n",
            "Loss SS33:  0.04190903368359762\n",
            "Loss SS44:  0.03967106374020272\n",
            "Loss SS55:  0.04085983967421748\n",
            "Loss SS66:  0.03992167487740517\n",
            "Loss SS77:  0.04545642710323875\n",
            "Loss SS88:  0.044140190335241615\n",
            "Loss SS11:  0.04450417433353449\n",
            "Loss SS22:  0.04168771495980932\n",
            "Loss SS33:  0.041823944755342624\n",
            "Loss SS44:  0.03972166359720641\n",
            "Loss SS55:  0.040793688486743446\n",
            "Loss SS66:  0.03986134552797734\n",
            "Loss SS77:  0.045419629539875\n",
            "Loss SS88:  0.0441266219347518\n",
            "Loss SS11:  0.04447116482979763\n",
            "Loss SS22:  0.04161088555033163\n",
            "Loss SS33:  0.041799189858369945\n",
            "Loss SS44:  0.03966782931585489\n",
            "Loss SS55:  0.040698515309125\n",
            "Loss SS66:  0.03978570756612357\n",
            "Loss SS77:  0.04535159068355649\n",
            "Loss SS88:  0.04404391850466314\n",
            "Loss SS11:  0.04450031298032978\n",
            "Loss SS22:  0.04164796533901789\n",
            "Loss SS33:  0.041854791212500186\n",
            "Loss SS44:  0.03978742476095233\n",
            "Loss SS55:  0.04079582115188677\n",
            "Loss SS66:  0.039930912834859035\n",
            "Loss SS77:  0.04543094705158507\n",
            "Loss SS88:  0.0441369374462387\n",
            "Loss SS11:  0.0446093435816014\n",
            "Loss SS22:  0.04163488961797393\n",
            "Loss SS33:  0.041882072158610624\n",
            "Loss SS44:  0.0398260196516527\n",
            "Loss SS55:  0.04079438999130581\n",
            "Loss SS66:  0.040035801450850554\n",
            "Loss SS77:  0.04554540958276111\n",
            "Loss SS88:  0.044251530395192994\n",
            "Loss SS11:  0.04469709266074665\n",
            "Loss SS22:  0.041658987822644995\n",
            "Loss SS33:  0.04188674146124206\n",
            "Loss SS44:  0.039835917719527686\n",
            "Loss SS55:  0.04075071525745367\n",
            "Loss SS66:  0.040014044796141034\n",
            "Loss SS77:  0.04555168287604267\n",
            "Loss SS88:  0.044317716449811195\n",
            "Loss SS11:  0.04467973934106566\n",
            "Loss SS22:  0.0416838203311263\n",
            "Loss SS33:  0.04188475279665705\n",
            "Loss SS44:  0.0398203434784021\n",
            "Loss SS55:  0.04076470325316363\n",
            "Loss SS66:  0.04003528676074536\n",
            "Loss SS77:  0.04552960347625153\n",
            "Loss SS88:  0.044338739796805735\n",
            "Loss SS11:  0.04472024947122375\n",
            "Loss SS22:  0.04165912458399461\n",
            "Loss SS33:  0.04187922047283412\n",
            "Loss SS44:  0.03984188745725211\n",
            "Loss SS55:  0.0406907941280948\n",
            "Loss SS66:  0.03998756041458998\n",
            "Loss SS77:  0.045502166965561454\n",
            "Loss SS88:  0.04425452820850775\n",
            "Loss SS11:  0.044729589438276596\n",
            "Loss SS22:  0.04168041841004769\n",
            "Loss SS33:  0.04179762674672571\n",
            "Loss SS44:  0.03985429631032016\n",
            "Loss SS55:  0.04063339246546521\n",
            "Loss SS66:  0.039935957016718336\n",
            "Loss SS77:  0.04549613133140279\n",
            "Loss SS88:  0.04426631990533609\n",
            "Loss SS11:  0.044742556603310946\n",
            "Loss SS22:  0.0416951822157288\n",
            "Loss SS33:  0.04178576460067844\n",
            "Loss SS44:  0.03986918835928946\n",
            "Loss SS55:  0.04062548797432478\n",
            "Loss SS66:  0.039875732597721605\n",
            "Loss SS77:  0.04550036654134333\n",
            "Loss SS88:  0.04423096385740098\n",
            "Loss SS11:  0.04476323593948887\n",
            "Loss SS22:  0.041691891137989726\n",
            "Loss SS33:  0.04177436278939742\n",
            "Loss SS44:  0.039806866763291024\n",
            "Loss SS55:  0.040605633770776486\n",
            "Loss SS66:  0.03984803006176632\n",
            "Loss SS77:  0.045452856423943866\n",
            "Loss SS88:  0.04419777327491535\n",
            "Loss SS11:  0.04475678458987954\n",
            "Loss SS22:  0.04172284105621961\n",
            "Loss SS33:  0.041737207838619846\n",
            "Loss SS44:  0.03978255834890552\n",
            "Loss SS55:  0.040641312550501046\n",
            "Loss SS66:  0.0398725468620836\n",
            "Loss SS77:  0.045473330004875405\n",
            "Loss SS88:  0.044186656797430904\n",
            "Loss SS11:  0.04473163828367931\n",
            "Loss SS22:  0.04170673269193291\n",
            "Loss SS33:  0.0417564590372345\n",
            "Loss SS44:  0.03977065659003239\n",
            "Loss SS55:  0.040698788831745526\n",
            "Loss SS66:  0.03989011837148118\n",
            "Loss SS77:  0.0454665896736114\n",
            "Loss SS88:  0.044149156745480395\n",
            "Loss SS11:  0.044711600315526845\n",
            "Loss SS22:  0.041715365508055775\n",
            "Loss SS33:  0.04174346232106325\n",
            "Loss SS44:  0.03976252419372326\n",
            "Loss SS55:  0.040699655073265305\n",
            "Loss SS66:  0.03988297073328627\n",
            "Loss SS77:  0.04553302972684047\n",
            "Loss SS88:  0.04418663870254566\n",
            "Loss SS11:  0.044691790133926795\n",
            "Loss SS22:  0.04178018468925961\n",
            "Loss SS33:  0.0417785666538303\n",
            "Loss SS44:  0.03981193812610416\n",
            "Loss SS55:  0.040705693733225516\n",
            "Loss SS66:  0.03995257553733965\n",
            "Loss SS77:  0.045555552142900926\n",
            "Loss SS88:  0.04421364742718982\n",
            "Loss SS11:  0.04468554511819918\n",
            "Loss SS22:  0.04181575051851289\n",
            "Loss SS33:  0.0418040656526269\n",
            "Loss SS44:  0.039814756741536034\n",
            "Loss SS55:  0.040698032685366696\n",
            "Loss SS66:  0.03994467849379143\n",
            "Loss SS77:  0.04564057684845941\n",
            "Loss SS88:  0.04422253059204092\n",
            "Loss SS11:  0.04467679186417811\n",
            "Loss SS22:  0.041823573138230666\n",
            "Loss SS33:  0.04178600738511925\n",
            "Loss SS44:  0.039794344120841485\n",
            "Loss SS55:  0.04069268829699767\n",
            "Loss SS66:  0.0399449786103247\n",
            "Loss SS77:  0.045676412772498655\n",
            "Loss SS88:  0.04425143620054983\n",
            "Loss SS11:  0.044627181910146085\n",
            "Loss SS22:  0.041791555893957805\n",
            "Loss SS33:  0.04174290270427799\n",
            "Loss SS44:  0.03978076184294232\n",
            "Loss SS55:  0.040641166952549455\n",
            "Loss SS66:  0.0399494996935225\n",
            "Loss SS77:  0.045624112728324354\n",
            "Loss SS88:  0.044230723232510004\n",
            "Loss SS11:  0.04463462060392831\n",
            "Loss SS22:  0.041750978109806866\n",
            "Loss SS33:  0.041745623324147635\n",
            "Loss SS44:  0.03978860356215376\n",
            "Loss SS55:  0.04064133330436882\n",
            "Loss SS66:  0.039955932855884606\n",
            "Loss SS77:  0.04561985703040135\n",
            "Loss SS88:  0.0442116764545255\n",
            "Loss SS11:  0.04464518563579937\n",
            "Loss SS22:  0.041755029397068426\n",
            "Loss SS33:  0.04174326472772212\n",
            "Loss SS44:  0.03979268228899676\n",
            "Loss SS55:  0.0406160490007739\n",
            "Loss SS66:  0.0399399755629528\n",
            "Loss SS77:  0.0456231590979409\n",
            "Loss SS88:  0.04422442897523998\n",
            "Loss SS11:  0.044609533300585064\n",
            "Loss SS22:  0.041777162225452684\n",
            "Loss SS33:  0.04176698727779025\n",
            "Loss SS44:  0.039795521166055434\n",
            "Loss SS55:  0.04062622438165799\n",
            "Loss SS66:  0.03994618905735505\n",
            "Loss SS77:  0.04560855929016018\n",
            "Loss SS88:  0.04419555201514725\n",
            "Loss SS11:  0.04459414373223598\n",
            "Loss SS22:  0.04175287436301212\n",
            "Loss SS33:  0.04177391558800667\n",
            "Loss SS44:  0.039794975642295304\n",
            "Loss SS55:  0.040626305036055736\n",
            "Loss SS66:  0.039952851502898755\n",
            "Loss SS77:  0.04563212021123989\n",
            "Loss SS88:  0.044200563691874854\n",
            "Loss SS11:  0.04465872700483515\n",
            "Loss SS22:  0.04174248032127391\n",
            "Loss SS33:  0.041804271391554224\n",
            "Loss SS44:  0.03981680472554262\n",
            "Loss SS55:  0.04065324855230522\n",
            "Loss SS66:  0.039963600161250605\n",
            "Loss SS77:  0.04564443203798622\n",
            "Loss SS88:  0.04420186627728457\n",
            "Loss SS11:  0.044696934711981336\n",
            "Loss SS22:  0.041735446908965586\n",
            "Loss SS33:  0.041789717008322076\n",
            "Loss SS44:  0.03979976538498447\n",
            "Loss SS55:  0.040658772714697124\n",
            "Loss SS66:  0.039960410335475226\n",
            "Loss SS77:  0.045622557662728665\n",
            "Loss SS88:  0.04417790037722922\n",
            "Loss SS11:  0.04460593989593151\n",
            "Loss SS22:  0.04170335894768319\n",
            "Loss SS33:  0.0417614508214898\n",
            "Loss SS44:  0.03976562835295682\n",
            "Loss SS55:  0.04061339133641538\n",
            "Loss SS66:  0.03993501614906344\n",
            "Loss SS77:  0.0455509824702746\n",
            "Loss SS88:  0.04414056356888744\n",
            "Loss SS11:  0.04455160936507423\n",
            "Loss SS22:  0.04166488198901686\n",
            "Loss SS33:  0.04174504875946228\n",
            "Loss SS44:  0.03972528841527526\n",
            "Loss SS55:  0.04060441143143817\n",
            "Loss SS66:  0.039904390959559805\n",
            "Loss SS77:  0.0455380483242252\n",
            "Loss SS88:  0.04409224649562555\n",
            "Loss SS11:  0.044566184867572904\n",
            "Loss SS22:  0.04166820541284328\n",
            "Loss SS33:  0.04173550396191509\n",
            "Loss SS44:  0.039717623531967026\n",
            "Loss SS55:  0.040575534012400896\n",
            "Loss SS66:  0.03989378323876055\n",
            "Loss SS77:  0.04554435513226172\n",
            "Loss SS88:  0.044061094104426164\n",
            "Loss SS11:  0.04454446949264139\n",
            "Loss SS22:  0.04166246907119334\n",
            "Loss SS33:  0.04172910508810749\n",
            "Loss SS44:  0.039704408551002074\n",
            "Loss SS55:  0.04057859086461021\n",
            "Loss SS66:  0.039912850543892875\n",
            "Loss SS77:  0.045494825835283074\n",
            "Loss SS88:  0.04403546753445971\n",
            "Loss SS11:  0.04452660330294713\n",
            "Loss SS22:  0.04165512054379932\n",
            "Loss SS33:  0.04174721563973789\n",
            "Loss SS44:  0.03972679119990727\n",
            "Loss SS55:  0.040611638902131285\n",
            "Loss SS66:  0.03992709403331093\n",
            "Loss SS77:  0.04549331806440818\n",
            "Loss SS88:  0.04405050280541536\n",
            "Loss SS11:  0.04449654231388165\n",
            "Loss SS22:  0.04163437661673796\n",
            "Loss SS33:  0.04168852703892161\n",
            "Loss SS44:  0.03968313676920955\n",
            "Loss SS55:  0.04058612608231967\n",
            "Loss SS66:  0.0399142125159018\n",
            "Loss SS77:  0.04544710369646687\n",
            "Loss SS88:  0.0440125137895968\n",
            "Loss SS11:  0.044515105416309805\n",
            "Loss SS22:  0.041655490390297506\n",
            "Loss SS33:  0.04169530694345498\n",
            "Loss SS44:  0.039684815691306746\n",
            "Loss SS55:  0.04058840672344037\n",
            "Loss SS66:  0.03992472422602766\n",
            "Loss SS77:  0.04547200415308784\n",
            "Loss SS88:  0.0440093013806408\n",
            "Loss SS11:  0.04451172108330378\n",
            "Loss SS22:  0.04165217910631004\n",
            "Loss SS33:  0.04170064256421214\n",
            "Loss SS44:  0.03968736348454809\n",
            "Loss SS55:  0.040641635887638694\n",
            "Loss SS66:  0.03993890764080236\n",
            "Loss SS77:  0.045518133880161654\n",
            "Loss SS88:  0.0440505318294079\n",
            "Loss SS11:  0.04449664155696064\n",
            "Loss SS22:  0.041654289019275904\n",
            "Loss SS33:  0.04170060574749804\n",
            "Loss SS44:  0.039693734384345904\n",
            "Loss SS55:  0.040666226572367216\n",
            "Loss SS66:  0.03996873097474042\n",
            "Loss SS77:  0.04553367104583345\n",
            "Loss SS88:  0.044075724594052595\n",
            "Loss SS11:  0.044501314884854475\n",
            "Loss SS22:  0.04167978561259885\n",
            "Loss SS33:  0.04170185847963244\n",
            "Loss SS44:  0.03968650082842053\n",
            "Loss SS55:  0.040691484319493015\n",
            "Loss SS66:  0.03997530147528193\n",
            "Loss SS77:  0.045536366364505386\n",
            "Loss SS88:  0.04408571983820061\n",
            "Loss SS11:  0.04449852401092494\n",
            "Loss SS22:  0.041690409733389125\n",
            "Loss SS33:  0.041687539393341716\n",
            "Loss SS44:  0.039660210808324714\n",
            "Loss SS55:  0.04069751370194796\n",
            "Loss SS66:  0.03997312325852576\n",
            "Loss SS77:  0.045507293117083535\n",
            "Loss SS88:  0.044075581030203745\n",
            "Loss SS11:  0.04446827790978727\n",
            "Loss SS22:  0.04162928898289587\n",
            "Loss SS33:  0.04164454447154843\n",
            "Loss SS44:  0.03962762439111343\n",
            "Loss SS55:  0.04065909889164379\n",
            "Loss SS66:  0.03994553065014711\n",
            "Loss SS77:  0.04544633475953595\n",
            "Loss SS88:  0.04402022941565319\n",
            "Validation: \n",
            " Loss SS11:  0.03964526951313019\n",
            " Loss SS22:  0.056601427495479584\n",
            " Loss SS33:  0.05998113378882408\n",
            " Loss SS55:  0.052603572607040405\n",
            " Loss SS66:  0.06399059295654297\n",
            " Loss SS77:  0.06105196848511696\n",
            " Loss SS88:  0.06011498346924782\n",
            " Loss SS99:  0.05775385722517967\n",
            " Loss SS11:  0.0453752644714855\n",
            " Loss SS22:  0.06358893126958892\n",
            " Loss SS33:  0.06649557305943399\n",
            " Loss SS55:  0.05689143016934395\n",
            " Loss SS66:  0.06772151252343542\n",
            " Loss SS77:  0.06485085278039887\n",
            " Loss SS88:  0.07142154659543719\n",
            " Loss SS99:  0.06343054363415354\n",
            " Loss SS11:  0.04605685837748574\n",
            " Loss SS22:  0.06261481453732747\n",
            " Loss SS33:  0.068085873908386\n",
            " Loss SS55:  0.05747312607198227\n",
            " Loss SS66:  0.06675334302027051\n",
            " Loss SS77:  0.06454426840674586\n",
            " Loss SS88:  0.07018101851387722\n",
            " Loss SS99:  0.06269970554404142\n",
            " Loss SS11:  0.04601327244375573\n",
            " Loss SS22:  0.06243910145808439\n",
            " Loss SS33:  0.06747156851848618\n",
            " Loss SS55:  0.05701636480259114\n",
            " Loss SS66:  0.06641563296806617\n",
            " Loss SS77:  0.06409359961503842\n",
            " Loss SS88:  0.07042287448879148\n",
            " Loss SS99:  0.06243445150187758\n",
            " Loss SS11:  0.045940353960534676\n",
            " Loss SS22:  0.06208091459156555\n",
            " Loss SS33:  0.06731102739771207\n",
            " Loss SS55:  0.056890691458075134\n",
            " Loss SS66:  0.06624972815682859\n",
            " Loss SS77:  0.06398495971972559\n",
            " Loss SS88:  0.07018397569104477\n",
            " Loss SS99:  0.06247156554902041\n",
            "\n",
            "Epoch: 59\n",
            "Loss SS11:  0.0471331924200058\n",
            "Loss SS22:  0.043462734669446945\n",
            "Loss SS33:  0.044894225895404816\n",
            "Loss SS44:  0.04223670810461044\n",
            "Loss SS55:  0.04135676473379135\n",
            "Loss SS66:  0.04683142527937889\n",
            "Loss SS77:  0.051396407186985016\n",
            "Loss SS88:  0.051206544041633606\n",
            "Loss SS11:  0.046155078505927864\n",
            "Loss SS22:  0.0405986105853861\n",
            "Loss SS33:  0.041044126180085266\n",
            "Loss SS44:  0.03932076997377656\n",
            "Loss SS55:  0.04011364755305377\n",
            "Loss SS66:  0.03941298106854612\n",
            "Loss SS77:  0.04539369351484559\n",
            "Loss SS88:  0.04374917596578598\n",
            "Loss SS11:  0.044988229870796204\n",
            "Loss SS22:  0.040686809768279396\n",
            "Loss SS33:  0.04140416160225868\n",
            "Loss SS44:  0.0387172315801893\n",
            "Loss SS55:  0.0404362277615638\n",
            "Loss SS66:  0.039485394422497065\n",
            "Loss SS77:  0.044686032902626766\n",
            "Loss SS88:  0.043702048559983574\n",
            "Loss SS11:  0.045422171512919086\n",
            "Loss SS22:  0.04126231300254022\n",
            "Loss SS33:  0.041489546457605976\n",
            "Loss SS44:  0.03903616484134428\n",
            "Loss SS55:  0.04087163796347956\n",
            "Loss SS66:  0.039483110029851234\n",
            "Loss SS77:  0.04492680391957683\n",
            "Loss SS88:  0.04405993343360962\n",
            "Loss SS11:  0.04490607704331235\n",
            "Loss SS22:  0.041371239667258614\n",
            "Loss SS33:  0.04164974945710927\n",
            "Loss SS44:  0.03916336541495672\n",
            "Loss SS55:  0.041079692724274426\n",
            "Loss SS66:  0.0397747591501329\n",
            "Loss SS77:  0.04498326460399279\n",
            "Loss SS88:  0.04409385763290452\n",
            "Loss SS11:  0.044941634524102304\n",
            "Loss SS22:  0.04150817185348155\n",
            "Loss SS33:  0.04154914515275581\n",
            "Loss SS44:  0.039458340873905255\n",
            "Loss SS55:  0.04125680445748217\n",
            "Loss SS66:  0.039845889397695955\n",
            "Loss SS77:  0.04527353349269605\n",
            "Loss SS88:  0.04401103356013111\n",
            "Loss SS11:  0.044893019756332773\n",
            "Loss SS22:  0.04151806284169682\n",
            "Loss SS33:  0.041433795187316956\n",
            "Loss SS44:  0.03943294695899135\n",
            "Loss SS55:  0.041086400996466155\n",
            "Loss SS66:  0.039682698909376486\n",
            "Loss SS77:  0.04520744183024422\n",
            "Loss SS88:  0.04402022656114375\n",
            "Loss SS11:  0.04457315902265025\n",
            "Loss SS22:  0.04167092318685962\n",
            "Loss SS33:  0.04159253825184325\n",
            "Loss SS44:  0.039582534470188786\n",
            "Loss SS55:  0.04108378275389403\n",
            "Loss SS66:  0.03976301481606255\n",
            "Loss SS77:  0.04527944562510705\n",
            "Loss SS88:  0.04407611274173562\n",
            "Loss SS11:  0.04448912600860184\n",
            "Loss SS22:  0.04170888464575932\n",
            "Loss SS33:  0.041463835141908975\n",
            "Loss SS44:  0.03957240228299742\n",
            "Loss SS55:  0.04083430813050565\n",
            "Loss SS66:  0.03954800687454365\n",
            "Loss SS77:  0.04516314935904962\n",
            "Loss SS88:  0.04392916221677521\n",
            "Loss SS11:  0.0447023457305117\n",
            "Loss SS22:  0.04185285509287656\n",
            "Loss SS33:  0.04137827410966485\n",
            "Loss SS44:  0.03955087700224185\n",
            "Loss SS55:  0.04080184753779527\n",
            "Loss SS66:  0.039635150805934445\n",
            "Loss SS77:  0.04519682082828585\n",
            "Loss SS88:  0.04392072505184582\n",
            "Loss SS11:  0.04477279563203897\n",
            "Loss SS22:  0.0418207049369812\n",
            "Loss SS33:  0.04131079029918897\n",
            "Loss SS44:  0.03948269563146157\n",
            "Loss SS55:  0.04062905638377265\n",
            "Loss SS66:  0.0395305242647629\n",
            "Loss SS77:  0.04508501206441681\n",
            "Loss SS88:  0.04378845347183766\n",
            "Loss SS11:  0.044768705460670834\n",
            "Loss SS22:  0.04180003491205138\n",
            "Loss SS33:  0.04118467924428416\n",
            "Loss SS44:  0.03944722247553301\n",
            "Loss SS55:  0.040667938555146124\n",
            "Loss SS66:  0.039585645839169216\n",
            "Loss SS77:  0.0452123203121864\n",
            "Loss SS88:  0.04370619792927493\n",
            "Loss SS11:  0.04467364529932826\n",
            "Loss SS22:  0.041702922402827206\n",
            "Loss SS33:  0.04137298781886574\n",
            "Loss SS44:  0.03952205356610708\n",
            "Loss SS55:  0.04077686557222989\n",
            "Loss SS66:  0.03968999830405574\n",
            "Loss SS77:  0.0453542809286886\n",
            "Loss SS88:  0.043809890469982606\n",
            "Loss SS11:  0.04471474254631814\n",
            "Loss SS22:  0.04171518825051439\n",
            "Loss SS33:  0.04147468530039751\n",
            "Loss SS44:  0.03958989924374427\n",
            "Loss SS55:  0.040861915455747196\n",
            "Loss SS66:  0.03970992650467021\n",
            "Loss SS77:  0.04546500699779459\n",
            "Loss SS88:  0.043925064836294596\n",
            "Loss SS11:  0.04470515623688698\n",
            "Loss SS22:  0.041685915849310286\n",
            "Loss SS33:  0.04150278462057418\n",
            "Loss SS44:  0.03957833775074769\n",
            "Loss SS55:  0.040905606556446\n",
            "Loss SS66:  0.03970059205559974\n",
            "Loss SS77:  0.045580765139972064\n",
            "Loss SS88:  0.04405979426405954\n",
            "Loss SS11:  0.044619503498866855\n",
            "Loss SS22:  0.04162010935363391\n",
            "Loss SS33:  0.041452189278326286\n",
            "Loss SS44:  0.039526441041996936\n",
            "Loss SS55:  0.040951054962660306\n",
            "Loss SS66:  0.03964176793761601\n",
            "Loss SS77:  0.04557144335958342\n",
            "Loss SS88:  0.044039506711904576\n",
            "Loss SS11:  0.04464455832790884\n",
            "Loss SS22:  0.041582397815788756\n",
            "Loss SS33:  0.04149464279505777\n",
            "Loss SS44:  0.039414475191824186\n",
            "Loss SS55:  0.04087066423633824\n",
            "Loss SS66:  0.039585315940542994\n",
            "Loss SS77:  0.04550974367197996\n",
            "Loss SS88:  0.04394716570466201\n",
            "Loss SS11:  0.04469253050914982\n",
            "Loss SS22:  0.04166132887030206\n",
            "Loss SS33:  0.04156489474208731\n",
            "Loss SS44:  0.039452901774505424\n",
            "Loss SS55:  0.04091421967884253\n",
            "Loss SS66:  0.03969155916432191\n",
            "Loss SS77:  0.04549044322723534\n",
            "Loss SS88:  0.04404217364233837\n",
            "Loss SS11:  0.04471558161615008\n",
            "Loss SS22:  0.041685535001326664\n",
            "Loss SS33:  0.04162224999613525\n",
            "Loss SS44:  0.0394645596941861\n",
            "Loss SS55:  0.04092574776124559\n",
            "Loss SS66:  0.03976113071346151\n",
            "Loss SS77:  0.04558032215958801\n",
            "Loss SS88:  0.04410588722778947\n",
            "Loss SS11:  0.044812274248344115\n",
            "Loss SS22:  0.041727958159296925\n",
            "Loss SS33:  0.04162994483534578\n",
            "Loss SS44:  0.039399232676359995\n",
            "Loss SS55:  0.04092280322890631\n",
            "Loss SS66:  0.03979099606266197\n",
            "Loss SS77:  0.04560309917670894\n",
            "Loss SS88:  0.044190463146770186\n",
            "Loss SS11:  0.04477751219821213\n",
            "Loss SS22:  0.041694621949350065\n",
            "Loss SS33:  0.0416595883926942\n",
            "Loss SS44:  0.03941596527375392\n",
            "Loss SS55:  0.040891179286721924\n",
            "Loss SS66:  0.039790589195578846\n",
            "Loss SS77:  0.04559498291407058\n",
            "Loss SS88:  0.04416257673086812\n",
            "Loss SS11:  0.04485102278573253\n",
            "Loss SS22:  0.0416984965592199\n",
            "Loss SS33:  0.04170732574440292\n",
            "Loss SS44:  0.03946469635901293\n",
            "Loss SS55:  0.04090369328573982\n",
            "Loss SS66:  0.03982658818434765\n",
            "Loss SS77:  0.04554728648108894\n",
            "Loss SS88:  0.04414306868803445\n",
            "Loss SS11:  0.044879275058054814\n",
            "Loss SS22:  0.04171311315907612\n",
            "Loss SS33:  0.04167439589673038\n",
            "Loss SS44:  0.03950350734982555\n",
            "Loss SS55:  0.040880965987616535\n",
            "Loss SS66:  0.03982845906218792\n",
            "Loss SS77:  0.04555015397422454\n",
            "Loss SS88:  0.044188956076877686\n",
            "Loss SS11:  0.04491620147963623\n",
            "Loss SS22:  0.04176497009473962\n",
            "Loss SS33:  0.041668444404225326\n",
            "Loss SS44:  0.03949401990482301\n",
            "Loss SS55:  0.04079766894057715\n",
            "Loss SS66:  0.03978050098597229\n",
            "Loss SS77:  0.04555512903443663\n",
            "Loss SS88:  0.0441856998869609\n",
            "Loss SS11:  0.0449441874793951\n",
            "Loss SS22:  0.041791720833644824\n",
            "Loss SS33:  0.041663234381012895\n",
            "Loss SS44:  0.039503734234210365\n",
            "Loss SS55:  0.040825293256410426\n",
            "Loss SS66:  0.03979776804316093\n",
            "Loss SS77:  0.04556689766927379\n",
            "Loss SS88:  0.04418448218109697\n",
            "Loss SS11:  0.04491007408179135\n",
            "Loss SS22:  0.04177050785239474\n",
            "Loss SS33:  0.041668618402158124\n",
            "Loss SS44:  0.03951481477435367\n",
            "Loss SS55:  0.04084622482973266\n",
            "Loss SS66:  0.039820383967037694\n",
            "Loss SS77:  0.045576248763210744\n",
            "Loss SS88:  0.04418839489024474\n",
            "Loss SS11:  0.04491541952865334\n",
            "Loss SS22:  0.04171944633697185\n",
            "Loss SS33:  0.04167172629600284\n",
            "Loss SS44:  0.0395301961950187\n",
            "Loss SS55:  0.04084981272846346\n",
            "Loss SS66:  0.03983761353054266\n",
            "Loss SS77:  0.04555514066612127\n",
            "Loss SS88:  0.04418363717609439\n",
            "Loss SS11:  0.04488240752877785\n",
            "Loss SS22:  0.04170415722301086\n",
            "Loss SS33:  0.041647400638274164\n",
            "Loss SS44:  0.039477544572080636\n",
            "Loss SS55:  0.04082617694271447\n",
            "Loss SS66:  0.03982019501418645\n",
            "Loss SS77:  0.045583429289260034\n",
            "Loss SS88:  0.044194857091591364\n",
            "Loss SS11:  0.044864984050234015\n",
            "Loss SS22:  0.041731459642114166\n",
            "Loss SS33:  0.041657220996465544\n",
            "Loss SS44:  0.0394861764436939\n",
            "Loss SS55:  0.040836385786003064\n",
            "Loss SS66:  0.039843583897127374\n",
            "Loss SS77:  0.045576078001926805\n",
            "Loss SS88:  0.04417493804175659\n",
            "Loss SS11:  0.04482507950200658\n",
            "Loss SS22:  0.04177640053312393\n",
            "Loss SS33:  0.0416775667508648\n",
            "Loss SS44:  0.03948477546834864\n",
            "Loss SS55:  0.04082760040702689\n",
            "Loss SS66:  0.0398524368495466\n",
            "Loss SS77:  0.04563064183207722\n",
            "Loss SS88:  0.04421220086130899\n",
            "Loss SS11:  0.04477291882186633\n",
            "Loss SS22:  0.04176796884879321\n",
            "Loss SS33:  0.04164003463233032\n",
            "Loss SS44:  0.03945312736883908\n",
            "Loss SS55:  0.04081511162097668\n",
            "Loss SS66:  0.03984765517850255\n",
            "Loss SS77:  0.045665392002394035\n",
            "Loss SS88:  0.044237541585566593\n",
            "Loss SS11:  0.04476333374810372\n",
            "Loss SS22:  0.041703373852554244\n",
            "Loss SS33:  0.04159680333503573\n",
            "Loss SS44:  0.039409489397833966\n",
            "Loss SS55:  0.0407659436944405\n",
            "Loss SS66:  0.03982860921998883\n",
            "Loss SS77:  0.04563335984514074\n",
            "Loss SS88:  0.044203549875036316\n",
            "Loss SS11:  0.044781049661268696\n",
            "Loss SS22:  0.04171265167332141\n",
            "Loss SS33:  0.0416088765516088\n",
            "Loss SS44:  0.03940414341392918\n",
            "Loss SS55:  0.04073691077971384\n",
            "Loss SS66:  0.039813757128433276\n",
            "Loss SS77:  0.0455892683932343\n",
            "Loss SS88:  0.0441974628705102\n",
            "Loss SS11:  0.04483510659630565\n",
            "Loss SS22:  0.041728526365090716\n",
            "Loss SS33:  0.04160900989102813\n",
            "Loss SS44:  0.03942973177958831\n",
            "Loss SS55:  0.04073072906887423\n",
            "Loss SS66:  0.039823216782865926\n",
            "Loss SS77:  0.045585484967040873\n",
            "Loss SS88:  0.044185324850579404\n",
            "Loss SS11:  0.04481986671948363\n",
            "Loss SS22:  0.041712232600471494\n",
            "Loss SS33:  0.04162723697787506\n",
            "Loss SS44:  0.03942655813484248\n",
            "Loss SS55:  0.040737219231306986\n",
            "Loss SS66:  0.039812751584126456\n",
            "Loss SS77:  0.04559426290417347\n",
            "Loss SS88:  0.044151837699923695\n",
            "Loss SS11:  0.04479510225674026\n",
            "Loss SS22:  0.041673646077641055\n",
            "Loss SS33:  0.04161399711146314\n",
            "Loss SS44:  0.03941023265916398\n",
            "Loss SS55:  0.040740485204930316\n",
            "Loss SS66:  0.039805098529639746\n",
            "Loss SS77:  0.04558964599019442\n",
            "Loss SS88:  0.044159703264464\n",
            "Loss SS11:  0.044821070016999\n",
            "Loss SS22:  0.0416660290263036\n",
            "Loss SS33:  0.0416143142471188\n",
            "Loss SS44:  0.03941268848456504\n",
            "Loss SS55:  0.04072308715773421\n",
            "Loss SS66:  0.0397946081353852\n",
            "Loss SS77:  0.04557590782023203\n",
            "Loss SS88:  0.04412720349315461\n",
            "Loss SS11:  0.04481709819799164\n",
            "Loss SS22:  0.04165056805726332\n",
            "Loss SS33:  0.0415887027035023\n",
            "Loss SS44:  0.03939948160693331\n",
            "Loss SS55:  0.040711381445033855\n",
            "Loss SS66:  0.039789014297595245\n",
            "Loss SS77:  0.045560741852316254\n",
            "Loss SS88:  0.044099137337622295\n",
            "Loss SS11:  0.04477394615688662\n",
            "Loss SS22:  0.04161015257552227\n",
            "Loss SS33:  0.04155841949848053\n",
            "Loss SS44:  0.03936844102040989\n",
            "Loss SS55:  0.04068340191964715\n",
            "Loss SS66:  0.03977524142057251\n",
            "Loss SS77:  0.04548713958912634\n",
            "Loss SS88:  0.044044742580708555\n",
            "Loss SS11:  0.04476734771943458\n",
            "Loss SS22:  0.041579573927328105\n",
            "Loss SS33:  0.04154652464763283\n",
            "Loss SS44:  0.0393440850707881\n",
            "Loss SS55:  0.04066920731111866\n",
            "Loss SS66:  0.03975512042565419\n",
            "Loss SS77:  0.04543071683219937\n",
            "Loss SS88:  0.044014111532808264\n",
            "Loss SS11:  0.04475402046097187\n",
            "Loss SS22:  0.04157130057897948\n",
            "Loss SS33:  0.041544225837970315\n",
            "Loss SS44:  0.03931611032222869\n",
            "Loss SS55:  0.040653453531333636\n",
            "Loss SS66:  0.03975784875210978\n",
            "Loss SS77:  0.045417242383867726\n",
            "Loss SS88:  0.04399263059855102\n",
            "Loss SS11:  0.04473461693837115\n",
            "Loss SS22:  0.041601415732626205\n",
            "Loss SS33:  0.04155241519466514\n",
            "Loss SS44:  0.03933132564934501\n",
            "Loss SS55:  0.040646850138250055\n",
            "Loss SS66:  0.03975803730442866\n",
            "Loss SS77:  0.045408728693378524\n",
            "Loss SS88:  0.04395763011345608\n",
            "Loss SS11:  0.044750486100158894\n",
            "Loss SS22:  0.0415931137858264\n",
            "Loss SS33:  0.041563487206884346\n",
            "Loss SS44:  0.03936440044584863\n",
            "Loss SS55:  0.04068009569497686\n",
            "Loss SS66:  0.039787470462628045\n",
            "Loss SS77:  0.04543670586303691\n",
            "Loss SS88:  0.04400070106296528\n",
            "Loss SS11:  0.04471535739551564\n",
            "Loss SS22:  0.041556005973342955\n",
            "Loss SS33:  0.04154223657435718\n",
            "Loss SS44:  0.03934876290592959\n",
            "Loss SS55:  0.04065831391931687\n",
            "Loss SS66:  0.03979503031465681\n",
            "Loss SS77:  0.04539347159094988\n",
            "Loss SS88:  0.0439859803619075\n",
            "Loss SS11:  0.044682949992602104\n",
            "Loss SS22:  0.04154196732134776\n",
            "Loss SS33:  0.04154450607712036\n",
            "Loss SS44:  0.039330422743107454\n",
            "Loss SS55:  0.0406734796828018\n",
            "Loss SS66:  0.039800911449115565\n",
            "Loss SS77:  0.045396153280794486\n",
            "Loss SS88:  0.043969824251268996\n",
            "Loss SS11:  0.044675897261718424\n",
            "Loss SS22:  0.04153463552538677\n",
            "Loss SS33:  0.041533477678729794\n",
            "Loss SS44:  0.0393213074315678\n",
            "Loss SS55:  0.0407182727496402\n",
            "Loss SS66:  0.03982014388275516\n",
            "Loss SS77:  0.04543794995533125\n",
            "Loss SS88:  0.0439875702437699\n",
            "Loss SS11:  0.044658466211950444\n",
            "Loss SS22:  0.041527985500216226\n",
            "Loss SS33:  0.041545335615534584\n",
            "Loss SS44:  0.03932716446946862\n",
            "Loss SS55:  0.04072767633484139\n",
            "Loss SS66:  0.0398457735502151\n",
            "Loss SS77:  0.045472033903681534\n",
            "Loss SS88:  0.0440182250130461\n",
            "Loss SS11:  0.04465612697152069\n",
            "Loss SS22:  0.04153857802509502\n",
            "Loss SS33:  0.04156487978472831\n",
            "Loss SS44:  0.03930948492764414\n",
            "Loss SS55:  0.04076093031912093\n",
            "Loss SS66:  0.03986085903081418\n",
            "Loss SS77:  0.04548879186850192\n",
            "Loss SS88:  0.044030139661708455\n",
            "Loss SS11:  0.04467535647805664\n",
            "Loss SS22:  0.04154377528262981\n",
            "Loss SS33:  0.04155363910008617\n",
            "Loss SS44:  0.039303780190662135\n",
            "Loss SS55:  0.04076869305602726\n",
            "Loss SS66:  0.03986497945975117\n",
            "Loss SS77:  0.04547144178293351\n",
            "Loss SS88:  0.04402448846464841\n",
            "Loss SS11:  0.04462577410178854\n",
            "Loss SS22:  0.04151234834202681\n",
            "Loss SS33:  0.04151584135933709\n",
            "Loss SS44:  0.03926889291090052\n",
            "Loss SS55:  0.04073658697079496\n",
            "Loss SS66:  0.039836498914388924\n",
            "Loss SS77:  0.04543288085465519\n",
            "Loss SS88:  0.04396437700314337\n",
            "Validation: \n",
            " Loss SS11:  0.04023316130042076\n",
            " Loss SS22:  0.0573379248380661\n",
            " Loss SS33:  0.061447951942682266\n",
            " Loss SS55:  0.05233296379446983\n",
            " Loss SS66:  0.0637512281537056\n",
            " Loss SS77:  0.061192333698272705\n",
            " Loss SS88:  0.057185105979442596\n",
            " Loss SS99:  0.057195551693439484\n",
            " Loss SS11:  0.0445926318920794\n",
            " Loss SS22:  0.06442498078658468\n",
            " Loss SS33:  0.06595150150713466\n",
            " Loss SS55:  0.055715909671215785\n",
            " Loss SS66:  0.0684962249582722\n",
            " Loss SS77:  0.06490890522088323\n",
            " Loss SS88:  0.06932771631649562\n",
            " Loss SS99:  0.0646609596553303\n",
            " Loss SS11:  0.04498484176470012\n",
            " Loss SS22:  0.0633753813439753\n",
            " Loss SS33:  0.06756959537543901\n",
            " Loss SS55:  0.05630771534108534\n",
            " Loss SS66:  0.06767006245691602\n",
            " Loss SS77:  0.06477046194599896\n",
            " Loss SS88:  0.06844283585868231\n",
            " Loss SS99:  0.06388008967041969\n",
            " Loss SS11:  0.04513129201091704\n",
            " Loss SS22:  0.06312450210823388\n",
            " Loss SS33:  0.06682434659756598\n",
            " Loss SS55:  0.05576179200997118\n",
            " Loss SS66:  0.06740616042105878\n",
            " Loss SS77:  0.06440955516500552\n",
            " Loss SS88:  0.06870364532118937\n",
            " Loss SS99:  0.06365100795128306\n",
            " Loss SS11:  0.04509385463632183\n",
            " Loss SS22:  0.06287130354731171\n",
            " Loss SS33:  0.06656831272958237\n",
            " Loss SS55:  0.05562160430866995\n",
            " Loss SS66:  0.06716988589844586\n",
            " Loss SS77:  0.0643799309561282\n",
            " Loss SS88:  0.06858345387894431\n",
            " Loss SS99:  0.0637294144174199\n",
            "\n",
            "Epoch: 60\n",
            "Loss SS11:  0.05003261938691139\n",
            "Loss SS22:  0.04305718466639519\n",
            "Loss SS33:  0.04315798729658127\n",
            "Loss SS44:  0.04370038956403732\n",
            "Loss SS55:  0.04582442715764046\n",
            "Loss SS66:  0.044204626232385635\n",
            "Loss SS77:  0.0591307133436203\n",
            "Loss SS88:  0.05202244222164154\n",
            "Loss SS11:  0.04540852863680233\n",
            "Loss SS22:  0.04024291986768896\n",
            "Loss SS33:  0.04078007218512622\n",
            "Loss SS44:  0.0403032164004716\n",
            "Loss SS55:  0.04012265428900719\n",
            "Loss SS66:  0.03943470526825298\n",
            "Loss SS77:  0.04627651077779857\n",
            "Loss SS88:  0.04446032812649554\n",
            "Loss SS11:  0.04453800618648529\n",
            "Loss SS22:  0.04027669007579485\n",
            "Loss SS33:  0.040902541152068546\n",
            "Loss SS44:  0.03952188559231304\n",
            "Loss SS55:  0.04015411933263143\n",
            "Loss SS66:  0.03949649401363872\n",
            "Loss SS77:  0.04543000424192065\n",
            "Loss SS88:  0.04406447956959406\n",
            "Loss SS11:  0.04485578702822808\n",
            "Loss SS22:  0.04126887900694724\n",
            "Loss SS33:  0.041307777887390505\n",
            "Loss SS44:  0.03953083736761924\n",
            "Loss SS55:  0.04060541045281195\n",
            "Loss SS66:  0.03950260579586029\n",
            "Loss SS77:  0.04557663815156106\n",
            "Loss SS88:  0.04415543281262921\n",
            "Loss SS11:  0.04477679638600931\n",
            "Loss SS22:  0.041398316803501874\n",
            "Loss SS33:  0.04135670540172879\n",
            "Loss SS44:  0.039505999626182925\n",
            "Loss SS55:  0.040980023854389425\n",
            "Loss SS66:  0.03972471623522479\n",
            "Loss SS77:  0.0453624427318573\n",
            "Loss SS88:  0.04424511350509597\n",
            "Loss SS11:  0.04455686776953585\n",
            "Loss SS22:  0.04153062746513123\n",
            "Loss SS33:  0.041322143346655606\n",
            "Loss SS44:  0.0396140517995638\n",
            "Loss SS55:  0.04083916143167252\n",
            "Loss SS66:  0.039588350510480354\n",
            "Loss SS77:  0.0452723849345656\n",
            "Loss SS88:  0.0441471322202215\n",
            "Loss SS11:  0.04451749027996767\n",
            "Loss SS22:  0.04131846288677122\n",
            "Loss SS33:  0.04127468223698803\n",
            "Loss SS44:  0.03944541092534534\n",
            "Loss SS55:  0.040690707012278136\n",
            "Loss SS66:  0.03932906094877446\n",
            "Loss SS77:  0.0452761603061293\n",
            "Loss SS88:  0.04392356517129257\n",
            "Loss SS11:  0.04435698924140191\n",
            "Loss SS22:  0.04128789969942939\n",
            "Loss SS33:  0.04151898470353073\n",
            "Loss SS44:  0.03954167622075954\n",
            "Loss SS55:  0.040833563871786625\n",
            "Loss SS66:  0.039504492734099776\n",
            "Loss SS77:  0.04528079874498744\n",
            "Loss SS88:  0.0439105758994398\n",
            "Loss SS11:  0.04430601515887696\n",
            "Loss SS22:  0.04131711945857531\n",
            "Loss SS33:  0.04135634308611905\n",
            "Loss SS44:  0.03937677588359809\n",
            "Loss SS55:  0.04057188906971319\n",
            "Loss SS66:  0.039262531578172874\n",
            "Loss SS77:  0.045092097817011824\n",
            "Loss SS88:  0.043769453916652705\n",
            "Loss SS11:  0.04448749288752839\n",
            "Loss SS22:  0.04128918305530653\n",
            "Loss SS33:  0.04130619874858594\n",
            "Loss SS44:  0.03937914410790244\n",
            "Loss SS55:  0.040519182212077655\n",
            "Loss SS66:  0.03940569228687129\n",
            "Loss SS77:  0.04491826146841049\n",
            "Loss SS88:  0.0436852579670293\n",
            "Loss SS11:  0.04451585863486375\n",
            "Loss SS22:  0.04130024123604935\n",
            "Loss SS33:  0.041193598014588405\n",
            "Loss SS44:  0.03932431989377088\n",
            "Loss SS55:  0.04038427334905851\n",
            "Loss SS66:  0.03931529060153678\n",
            "Loss SS77:  0.04492114672418868\n",
            "Loss SS88:  0.0435529100422812\n",
            "Loss SS11:  0.04442239945402016\n",
            "Loss SS22:  0.04133939095311337\n",
            "Loss SS33:  0.041094859508243764\n",
            "Loss SS44:  0.03934017484923741\n",
            "Loss SS55:  0.04033088653876975\n",
            "Loss SS66:  0.03937770812226846\n",
            "Loss SS77:  0.044993228975448524\n",
            "Loss SS88:  0.043464618525258056\n",
            "Loss SS11:  0.044220669703050094\n",
            "Loss SS22:  0.04138820187365713\n",
            "Loss SS33:  0.04126221394120169\n",
            "Loss SS44:  0.03940669619593738\n",
            "Loss SS55:  0.040468042239177326\n",
            "Loss SS66:  0.039479931585552276\n",
            "Loss SS77:  0.04519136173050266\n",
            "Loss SS88:  0.04360569754907907\n",
            "Loss SS11:  0.044251787469132255\n",
            "Loss SS22:  0.04146080461280947\n",
            "Loss SS33:  0.04139680150924748\n",
            "Loss SS44:  0.03945683898361585\n",
            "Loss SS55:  0.04055490518230518\n",
            "Loss SS66:  0.039404407740776776\n",
            "Loss SS77:  0.04526699891754689\n",
            "Loss SS88:  0.04371795248780542\n",
            "Loss SS11:  0.04411300189845951\n",
            "Loss SS22:  0.041418683787821035\n",
            "Loss SS33:  0.041421673161552305\n",
            "Loss SS44:  0.039435177380945666\n",
            "Loss SS55:  0.040551907046044125\n",
            "Loss SS66:  0.03938621079139676\n",
            "Loss SS77:  0.045329624377455274\n",
            "Loss SS88:  0.04378036383195972\n",
            "Loss SS11:  0.044026112640338226\n",
            "Loss SS22:  0.04135525034160804\n",
            "Loss SS33:  0.04140052142620876\n",
            "Loss SS44:  0.03944916714796957\n",
            "Loss SS55:  0.04055310620476078\n",
            "Loss SS66:  0.039371636973706305\n",
            "Loss SS77:  0.04532274914695727\n",
            "Loss SS88:  0.043765661707579694\n",
            "Loss SS11:  0.04406378337222597\n",
            "Loss SS22:  0.04130656142597613\n",
            "Loss SS33:  0.041437973008022544\n",
            "Loss SS44:  0.03940380600668629\n",
            "Loss SS55:  0.04045679677069557\n",
            "Loss SS66:  0.03934166705386239\n",
            "Loss SS77:  0.04520622048503864\n",
            "Loss SS88:  0.04370060644072035\n",
            "Loss SS11:  0.04404817436609352\n",
            "Loss SS22:  0.041366073384619596\n",
            "Loss SS33:  0.0414435203835281\n",
            "Loss SS44:  0.039433229714632034\n",
            "Loss SS55:  0.040570119188891515\n",
            "Loss SS66:  0.03939525787419046\n",
            "Loss SS77:  0.045312620721190994\n",
            "Loss SS88:  0.04381076011218523\n",
            "Loss SS11:  0.04408213475626477\n",
            "Loss SS22:  0.041369520848133286\n",
            "Loss SS33:  0.041496302170977405\n",
            "Loss SS44:  0.03944164718300598\n",
            "Loss SS55:  0.040579548301123786\n",
            "Loss SS66:  0.039523055169793125\n",
            "Loss SS77:  0.045356524665711336\n",
            "Loss SS88:  0.04385135028490704\n",
            "Loss SS11:  0.04412859955227188\n",
            "Loss SS22:  0.04130857995667383\n",
            "Loss SS33:  0.0414626016549727\n",
            "Loss SS44:  0.0394280442275614\n",
            "Loss SS55:  0.04053369685227334\n",
            "Loss SS66:  0.039548186907125395\n",
            "Loss SS77:  0.04535648503696731\n",
            "Loss SS88:  0.04390082904259572\n",
            "Loss SS11:  0.04400823862101901\n",
            "Loss SS22:  0.041240875789923456\n",
            "Loss SS33:  0.04143053550847727\n",
            "Loss SS44:  0.039394541055112337\n",
            "Loss SS55:  0.04049535549769354\n",
            "Loss SS66:  0.03950813412666321\n",
            "Loss SS77:  0.04529107962526492\n",
            "Loss SS88:  0.04382207350277189\n",
            "Loss SS11:  0.044041412842782186\n",
            "Loss SS22:  0.041244060101243554\n",
            "Loss SS33:  0.04147545453072724\n",
            "Loss SS44:  0.03938083080530732\n",
            "Loss SS55:  0.04040150407931251\n",
            "Loss SS66:  0.03942938175458479\n",
            "Loss SS77:  0.045213547592620716\n",
            "Loss SS88:  0.04375217560131403\n",
            "Loss SS11:  0.04403095343466258\n",
            "Loss SS22:  0.04125047132912265\n",
            "Loss SS33:  0.041409527655370634\n",
            "Loss SS44:  0.0394105291609311\n",
            "Loss SS55:  0.04038378178264221\n",
            "Loss SS66:  0.039391025125440966\n",
            "Loss SS77:  0.045165980900574595\n",
            "Loss SS88:  0.04375141963564972\n",
            "Loss SS11:  0.04409020165086309\n",
            "Loss SS22:  0.0413169881159609\n",
            "Loss SS33:  0.0413812372623842\n",
            "Loss SS44:  0.03938734307240098\n",
            "Loss SS55:  0.040344337412676255\n",
            "Loss SS66:  0.039356902790017974\n",
            "Loss SS77:  0.04512308921777841\n",
            "Loss SS88:  0.04372554107919916\n",
            "Loss SS11:  0.04414873443451165\n",
            "Loss SS22:  0.04135163544186418\n",
            "Loss SS33:  0.0413710858471908\n",
            "Loss SS44:  0.039350497926666526\n",
            "Loss SS55:  0.040409078272793796\n",
            "Loss SS66:  0.039388477740446064\n",
            "Loss SS77:  0.04514431016697429\n",
            "Loss SS88:  0.04372005969966101\n",
            "Loss SS11:  0.04415249345133979\n",
            "Loss SS22:  0.04140470507907203\n",
            "Loss SS33:  0.0414286912882708\n",
            "Loss SS44:  0.03936095897894931\n",
            "Loss SS55:  0.040448485454358904\n",
            "Loss SS66:  0.0394567343818239\n",
            "Loss SS77:  0.04517460909794526\n",
            "Loss SS88:  0.04374492867475012\n",
            "Loss SS11:  0.04419674097749465\n",
            "Loss SS22:  0.041400894437028074\n",
            "Loss SS33:  0.0414467703451142\n",
            "Loss SS44:  0.039345629541809986\n",
            "Loss SS55:  0.040483471334437304\n",
            "Loss SS66:  0.039488689124698384\n",
            "Loss SS77:  0.045198693657400965\n",
            "Loss SS88:  0.043750843468525404\n",
            "Loss SS11:  0.04418969192847994\n",
            "Loss SS22:  0.04140273183309284\n",
            "Loss SS33:  0.041389729497727436\n",
            "Loss SS44:  0.03932304079627199\n",
            "Loss SS55:  0.0404409098256778\n",
            "Loss SS66:  0.03948896906142745\n",
            "Loss SS77:  0.04521922187061767\n",
            "Loss SS88:  0.04378560661059904\n",
            "Loss SS11:  0.044203634204813595\n",
            "Loss SS22:  0.04146076127674656\n",
            "Loss SS33:  0.041388715620673\n",
            "Loss SS44:  0.039362204629013124\n",
            "Loss SS55:  0.040444241591407734\n",
            "Loss SS66:  0.039520218305528376\n",
            "Loss SS77:  0.04521872689185193\n",
            "Loss SS88:  0.043810073750299064\n",
            "Loss SS11:  0.044159773263529815\n",
            "Loss SS22:  0.04146357176383746\n",
            "Loss SS33:  0.04139949449200401\n",
            "Loss SS44:  0.039339992688181474\n",
            "Loss SS55:  0.04044704157154994\n",
            "Loss SS66:  0.0395335263198184\n",
            "Loss SS77:  0.0452548902212959\n",
            "Loss SS88:  0.04384518752700275\n",
            "Loss SS11:  0.044168564370304246\n",
            "Loss SS22:  0.04143227652903411\n",
            "Loss SS33:  0.041431077505365954\n",
            "Loss SS44:  0.03932143143839218\n",
            "Loss SS55:  0.04044022100460886\n",
            "Loss SS66:  0.039536782784814456\n",
            "Loss SS77:  0.045277724556154586\n",
            "Loss SS88:  0.043854550764806245\n",
            "Loss SS11:  0.044168010665481115\n",
            "Loss SS22:  0.0413805512633546\n",
            "Loss SS33:  0.04135997169366604\n",
            "Loss SS44:  0.039317978575969435\n",
            "Loss SS55:  0.040397414081157976\n",
            "Loss SS66:  0.03950953446448424\n",
            "Loss SS77:  0.04523276160336384\n",
            "Loss SS88:  0.04382279361942573\n",
            "Loss SS11:  0.04416451615940002\n",
            "Loss SS22:  0.04134882110348\n",
            "Loss SS33:  0.041376579030651915\n",
            "Loss SS44:  0.039319084371176094\n",
            "Loss SS55:  0.0403746224452402\n",
            "Loss SS66:  0.03948491008667931\n",
            "Loss SS77:  0.04517422850908149\n",
            "Loss SS88:  0.043782508679639516\n",
            "Loss SS11:  0.044184403015677665\n",
            "Loss SS22:  0.04134155300043501\n",
            "Loss SS33:  0.04137300463601542\n",
            "Loss SS44:  0.039309380131455705\n",
            "Loss SS55:  0.040364987923119364\n",
            "Loss SS66:  0.03950667542228166\n",
            "Loss SS77:  0.045151481624366295\n",
            "Loss SS88:  0.043763136530570755\n",
            "Loss SS11:  0.0441685307148265\n",
            "Loss SS22:  0.041344820054745046\n",
            "Loss SS33:  0.04140215712296298\n",
            "Loss SS44:  0.03930964705444152\n",
            "Loss SS55:  0.04038250142615562\n",
            "Loss SS66:  0.03953742093879107\n",
            "Loss SS77:  0.0451648816728522\n",
            "Loss SS88:  0.0437528934886903\n",
            "Loss SS11:  0.04418118708790877\n",
            "Loss SS22:  0.041339597566599856\n",
            "Loss SS33:  0.04140486687082171\n",
            "Loss SS44:  0.039315422949118495\n",
            "Loss SS55:  0.04037313806450265\n",
            "Loss SS66:  0.039542388839599416\n",
            "Loss SS77:  0.04516864364218508\n",
            "Loss SS88:  0.04376786514225169\n",
            "Loss SS11:  0.04420156655136568\n",
            "Loss SS22:  0.041311481910811895\n",
            "Loss SS33:  0.041395304347917314\n",
            "Loss SS44:  0.039310926721119153\n",
            "Loss SS55:  0.04036763550873609\n",
            "Loss SS66:  0.039513469082909605\n",
            "Loss SS77:  0.04517404024084189\n",
            "Loss SS88:  0.043745055805035245\n",
            "Loss SS11:  0.04419900565295207\n",
            "Loss SS22:  0.04129712856964602\n",
            "Loss SS33:  0.04139537464656599\n",
            "Loss SS44:  0.03931121195784476\n",
            "Loss SS55:  0.04038028317601533\n",
            "Loss SS66:  0.039482938301450804\n",
            "Loss SS77:  0.045186115293248966\n",
            "Loss SS88:  0.04373213935974794\n",
            "Loss SS11:  0.0441771749201722\n",
            "Loss SS22:  0.04128001265599346\n",
            "Loss SS33:  0.04133709865253116\n",
            "Loss SS44:  0.03930275476588978\n",
            "Loss SS55:  0.04036721199007798\n",
            "Loss SS66:  0.03946840461939964\n",
            "Loss SS77:  0.045134506317887096\n",
            "Loss SS88:  0.04370241690417287\n",
            "Loss SS11:  0.04412940065459827\n",
            "Loss SS22:  0.04122918215401642\n",
            "Loss SS33:  0.041284393528690726\n",
            "Loss SS44:  0.03925345051090431\n",
            "Loss SS55:  0.04033780345678939\n",
            "Loss SS66:  0.039434796527904624\n",
            "Loss SS77:  0.045082719496372714\n",
            "Loss SS88:  0.04365166618734065\n",
            "Loss SS11:  0.04414722847373705\n",
            "Loss SS22:  0.04121154239238646\n",
            "Loss SS33:  0.04126985930370868\n",
            "Loss SS44:  0.03924355982916611\n",
            "Loss SS55:  0.04031704155324106\n",
            "Loss SS66:  0.03944120184516074\n",
            "Loss SS77:  0.045060738428795426\n",
            "Loss SS88:  0.043638376226448956\n",
            "Loss SS11:  0.04412182196612196\n",
            "Loss SS22:  0.04123625552639764\n",
            "Loss SS33:  0.04126361945140971\n",
            "Loss SS44:  0.03925170818312035\n",
            "Loss SS55:  0.04031604670731401\n",
            "Loss SS66:  0.039455224345200254\n",
            "Loss SS77:  0.04504508617585593\n",
            "Loss SS88:  0.04362726555978585\n",
            "Loss SS11:  0.04408634910603317\n",
            "Loss SS22:  0.04124652019164341\n",
            "Loss SS33:  0.04127584164187914\n",
            "Loss SS44:  0.039266125482646415\n",
            "Loss SS55:  0.04033609868122393\n",
            "Loss SS66:  0.03947399510623723\n",
            "Loss SS77:  0.0450609503247534\n",
            "Loss SS88:  0.04363646628136023\n",
            "Loss SS11:  0.04405803228199067\n",
            "Loss SS22:  0.041201267901401786\n",
            "Loss SS33:  0.04123902726235633\n",
            "Loss SS44:  0.03924483859483714\n",
            "Loss SS55:  0.0403070656924126\n",
            "Loss SS66:  0.03945281157775711\n",
            "Loss SS77:  0.04500672339597877\n",
            "Loss SS88:  0.04357558640104712\n",
            "Loss SS11:  0.04404871565313025\n",
            "Loss SS22:  0.041194767917162166\n",
            "Loss SS33:  0.041244222260927124\n",
            "Loss SS44:  0.039250509133414616\n",
            "Loss SS55:  0.040312652769678034\n",
            "Loss SS66:  0.039475391629466665\n",
            "Loss SS77:  0.045026052312380604\n",
            "Loss SS88:  0.043587659720052664\n",
            "Loss SS11:  0.044037217575800654\n",
            "Loss SS22:  0.04118871159372467\n",
            "Loss SS33:  0.041266263448147975\n",
            "Loss SS44:  0.039267766972933535\n",
            "Loss SS55:  0.04036023999727486\n",
            "Loss SS66:  0.039499510046582\n",
            "Loss SS77:  0.045044468439074684\n",
            "Loss SS88:  0.04358126398200735\n",
            "Loss SS11:  0.044033605613444736\n",
            "Loss SS22:  0.041198869759051245\n",
            "Loss SS33:  0.0412861447369199\n",
            "Loss SS44:  0.0392860908570362\n",
            "Loss SS55:  0.040398464229062944\n",
            "Loss SS66:  0.03953910040428737\n",
            "Loss SS77:  0.045088587163133374\n",
            "Loss SS88:  0.04362011234601253\n",
            "Loss SS11:  0.044045830877090464\n",
            "Loss SS22:  0.041228599822635104\n",
            "Loss SS33:  0.04130119344737626\n",
            "Loss SS44:  0.03928299136697107\n",
            "Loss SS55:  0.040438046920135015\n",
            "Loss SS66:  0.03956768677556084\n",
            "Loss SS77:  0.04509346085875404\n",
            "Loss SS88:  0.043632190943460304\n",
            "Loss SS11:  0.04404323161608712\n",
            "Loss SS22:  0.04124489833219384\n",
            "Loss SS33:  0.0412801394002849\n",
            "Loss SS44:  0.039264602900294904\n",
            "Loss SS55:  0.04044370429121779\n",
            "Loss SS66:  0.03958711063725537\n",
            "Loss SS77:  0.04506822603439095\n",
            "Loss SS88:  0.043629989740506525\n",
            "Loss SS11:  0.04400829197706127\n",
            "Loss SS22:  0.041199415383477316\n",
            "Loss SS33:  0.04123663781647041\n",
            "Loss SS44:  0.03924560851878401\n",
            "Loss SS55:  0.04042263936001026\n",
            "Loss SS66:  0.039566334048684405\n",
            "Loss SS77:  0.045036699546998\n",
            "Loss SS88:  0.04357044673439441\n",
            "Validation: \n",
            " Loss SS11:  0.03734857589006424\n",
            " Loss SS22:  0.05662982910871506\n",
            " Loss SS33:  0.059093695133924484\n",
            " Loss SS55:  0.05462868511676788\n",
            " Loss SS66:  0.06462238729000092\n",
            " Loss SS77:  0.06136518344283104\n",
            " Loss SS88:  0.05985307693481445\n",
            " Loss SS99:  0.058262307196855545\n",
            " Loss SS11:  0.0430351360922768\n",
            " Loss SS22:  0.0638670784731706\n",
            " Loss SS33:  0.06482264488225892\n",
            " Loss SS55:  0.057112121156283786\n",
            " Loss SS66:  0.06848833372905141\n",
            " Loss SS77:  0.06500456588608879\n",
            " Loss SS88:  0.07063767402654603\n",
            " Loss SS99:  0.06483950324001767\n",
            " Loss SS11:  0.04362296676490365\n",
            " Loss SS22:  0.06292879281610977\n",
            " Loss SS33:  0.06634911658560358\n",
            " Loss SS55:  0.05751874679472388\n",
            " Loss SS66:  0.06757147646531826\n",
            " Loss SS77:  0.06464949095757996\n",
            " Loss SS88:  0.06975863719495332\n",
            " Loss SS99:  0.06393378800371798\n",
            " Loss SS11:  0.04374409149416157\n",
            " Loss SS22:  0.062647708309967\n",
            " Loss SS33:  0.06586217379472295\n",
            " Loss SS55:  0.056978045794807496\n",
            " Loss SS66:  0.06715467608854418\n",
            " Loss SS77:  0.06431777830250927\n",
            " Loss SS88:  0.07016344252424161\n",
            " Loss SS99:  0.06369193930362092\n",
            " Loss SS11:  0.043700559179724\n",
            " Loss SS22:  0.06231250508147993\n",
            " Loss SS33:  0.06571259381778446\n",
            " Loss SS55:  0.056858826980546666\n",
            " Loss SS66:  0.06692260329002216\n",
            " Loss SS77:  0.06431821278399891\n",
            " Loss SS88:  0.07002485253744656\n",
            " Loss SS99:  0.0637003830001678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net4(nn.Module):\n",
        "    def __init__(self, ss11,ss22,ss33,ss44,ss55,ss66,ss77,ss88):\n",
        "        super(Net4, self).__init__()\n",
        "        self.ss11 = ss11\n",
        "        self.ss22 = ss22\n",
        "        self.ss33 = ss33\n",
        "        self.ss44 = ss44\n",
        "        self.ss55 = ss55\n",
        "        self.ss66 = ss66\n",
        "        self.ss77 = ss77\n",
        "        self.ss88 = ss88\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.ss11(x)\n",
        "        out2 = self.ss22(x)\n",
        "        out3 = self.ss33(x)\n",
        "        out4 = self.ss44(x)\n",
        "        out5 = self.ss55(x)\n",
        "        out6 = self.ss66(x)\n",
        "        out7 = self.ss77(x)\n",
        "        out8 = self.ss88(x)\n",
        "\n",
        "        out = torch.cat((out1,out2,out3,out4,out5,out6,out7,out8),1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "net8students = Net4( ss11,ss22,ss33,ss44,ss55,ss66,ss77,ss88)\n",
        "net8students.to(device)\n",
        "summary(net8students, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPbCl6H0KSJX",
        "outputId": "9232cfc5-7434-432e-ab18-7d9604740b15"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "              VGG-40                   [-1, 64]               0\n",
            "           Conv2d-41           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-42           [-1, 32, 32, 32]              64\n",
            "             ReLU-43           [-1, 32, 32, 32]               0\n",
            "           Conv2d-44           [-1, 32, 32, 32]           9,248\n",
            "      BatchNorm2d-45           [-1, 32, 32, 32]              64\n",
            "             ReLU-46           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-47           [-1, 32, 16, 16]               0\n",
            "           Conv2d-48           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-49           [-1, 32, 16, 16]              64\n",
            "             ReLU-50           [-1, 32, 16, 16]               0\n",
            "           Conv2d-51           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-52           [-1, 32, 16, 16]              64\n",
            "             ReLU-53           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-54             [-1, 32, 8, 8]               0\n",
            "           Conv2d-55             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-56             [-1, 32, 8, 8]              64\n",
            "             ReLU-57             [-1, 32, 8, 8]               0\n",
            "           Conv2d-58             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-59             [-1, 64, 8, 8]             128\n",
            "             ReLU-60             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-61             [-1, 64, 4, 4]               0\n",
            "           Conv2d-62             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-63             [-1, 64, 4, 4]             128\n",
            "             ReLU-64             [-1, 64, 4, 4]               0\n",
            "           Conv2d-65             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-66             [-1, 64, 4, 4]             128\n",
            "             ReLU-67             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-68             [-1, 64, 2, 2]               0\n",
            "           Conv2d-69             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-70             [-1, 64, 2, 2]             128\n",
            "             ReLU-71             [-1, 64, 2, 2]               0\n",
            "           Conv2d-72             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-73             [-1, 64, 2, 2]             128\n",
            "             ReLU-74             [-1, 64, 2, 2]               0\n",
            "           Conv2d-75             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-76             [-1, 64, 2, 2]             128\n",
            "             ReLU-77             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-78             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-79             [-1, 64, 1, 1]               0\n",
            "              VGG-80                   [-1, 64]               0\n",
            "           Conv2d-81           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-82           [-1, 32, 32, 32]              64\n",
            "             ReLU-83           [-1, 32, 32, 32]               0\n",
            "           Conv2d-84           [-1, 32, 32, 32]           9,248\n",
            "      BatchNorm2d-85           [-1, 32, 32, 32]              64\n",
            "             ReLU-86           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-87           [-1, 32, 16, 16]               0\n",
            "           Conv2d-88           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-89           [-1, 32, 16, 16]              64\n",
            "             ReLU-90           [-1, 32, 16, 16]               0\n",
            "           Conv2d-91           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-92           [-1, 32, 16, 16]              64\n",
            "             ReLU-93           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-94             [-1, 32, 8, 8]               0\n",
            "           Conv2d-95             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-96             [-1, 32, 8, 8]              64\n",
            "             ReLU-97             [-1, 32, 8, 8]               0\n",
            "           Conv2d-98             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-99             [-1, 64, 8, 8]             128\n",
            "            ReLU-100             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-101             [-1, 64, 4, 4]               0\n",
            "          Conv2d-102             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-103             [-1, 64, 4, 4]             128\n",
            "            ReLU-104             [-1, 64, 4, 4]               0\n",
            "          Conv2d-105             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-106             [-1, 64, 4, 4]             128\n",
            "            ReLU-107             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-108             [-1, 64, 2, 2]               0\n",
            "          Conv2d-109             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-110             [-1, 64, 2, 2]             128\n",
            "            ReLU-111             [-1, 64, 2, 2]               0\n",
            "          Conv2d-112             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-113             [-1, 64, 2, 2]             128\n",
            "            ReLU-114             [-1, 64, 2, 2]               0\n",
            "          Conv2d-115             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-116             [-1, 64, 2, 2]             128\n",
            "            ReLU-117             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-118             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-119             [-1, 64, 1, 1]               0\n",
            "             VGG-120                   [-1, 64]               0\n",
            "          Conv2d-121           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-122           [-1, 32, 32, 32]              64\n",
            "            ReLU-123           [-1, 32, 32, 32]               0\n",
            "          Conv2d-124           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-125           [-1, 32, 32, 32]              64\n",
            "            ReLU-126           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-127           [-1, 32, 16, 16]               0\n",
            "          Conv2d-128           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-129           [-1, 32, 16, 16]              64\n",
            "            ReLU-130           [-1, 32, 16, 16]               0\n",
            "          Conv2d-131           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-132           [-1, 32, 16, 16]              64\n",
            "            ReLU-133           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-134             [-1, 32, 8, 8]               0\n",
            "          Conv2d-135             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-136             [-1, 32, 8, 8]              64\n",
            "            ReLU-137             [-1, 32, 8, 8]               0\n",
            "          Conv2d-138             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-139             [-1, 64, 8, 8]             128\n",
            "            ReLU-140             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-141             [-1, 64, 4, 4]               0\n",
            "          Conv2d-142             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-143             [-1, 64, 4, 4]             128\n",
            "            ReLU-144             [-1, 64, 4, 4]               0\n",
            "          Conv2d-145             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-146             [-1, 64, 4, 4]             128\n",
            "            ReLU-147             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-148             [-1, 64, 2, 2]               0\n",
            "          Conv2d-149             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-150             [-1, 64, 2, 2]             128\n",
            "            ReLU-151             [-1, 64, 2, 2]               0\n",
            "          Conv2d-152             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-153             [-1, 64, 2, 2]             128\n",
            "            ReLU-154             [-1, 64, 2, 2]               0\n",
            "          Conv2d-155             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-156             [-1, 64, 2, 2]             128\n",
            "            ReLU-157             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-158             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-159             [-1, 64, 1, 1]               0\n",
            "             VGG-160                   [-1, 64]               0\n",
            "          Conv2d-161           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-162           [-1, 32, 32, 32]              64\n",
            "            ReLU-163           [-1, 32, 32, 32]               0\n",
            "          Conv2d-164           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-165           [-1, 32, 32, 32]              64\n",
            "            ReLU-166           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-167           [-1, 32, 16, 16]               0\n",
            "          Conv2d-168           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-169           [-1, 32, 16, 16]              64\n",
            "            ReLU-170           [-1, 32, 16, 16]               0\n",
            "          Conv2d-171           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-172           [-1, 32, 16, 16]              64\n",
            "            ReLU-173           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-174             [-1, 32, 8, 8]               0\n",
            "          Conv2d-175             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-176             [-1, 32, 8, 8]              64\n",
            "            ReLU-177             [-1, 32, 8, 8]               0\n",
            "          Conv2d-178             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-179             [-1, 64, 8, 8]             128\n",
            "            ReLU-180             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-181             [-1, 64, 4, 4]               0\n",
            "          Conv2d-182             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-183             [-1, 64, 4, 4]             128\n",
            "            ReLU-184             [-1, 64, 4, 4]               0\n",
            "          Conv2d-185             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-186             [-1, 64, 4, 4]             128\n",
            "            ReLU-187             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-188             [-1, 64, 2, 2]               0\n",
            "          Conv2d-189             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-190             [-1, 64, 2, 2]             128\n",
            "            ReLU-191             [-1, 64, 2, 2]               0\n",
            "          Conv2d-192             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-193             [-1, 64, 2, 2]             128\n",
            "            ReLU-194             [-1, 64, 2, 2]               0\n",
            "          Conv2d-195             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-196             [-1, 64, 2, 2]             128\n",
            "            ReLU-197             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-198             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-199             [-1, 64, 1, 1]               0\n",
            "             VGG-200                   [-1, 64]               0\n",
            "          Conv2d-201           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-202           [-1, 32, 32, 32]              64\n",
            "            ReLU-203           [-1, 32, 32, 32]               0\n",
            "          Conv2d-204           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-205           [-1, 32, 32, 32]              64\n",
            "            ReLU-206           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-207           [-1, 32, 16, 16]               0\n",
            "          Conv2d-208           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-209           [-1, 32, 16, 16]              64\n",
            "            ReLU-210           [-1, 32, 16, 16]               0\n",
            "          Conv2d-211           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-212           [-1, 32, 16, 16]              64\n",
            "            ReLU-213           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-214             [-1, 32, 8, 8]               0\n",
            "          Conv2d-215             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-216             [-1, 32, 8, 8]              64\n",
            "            ReLU-217             [-1, 32, 8, 8]               0\n",
            "          Conv2d-218             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-219             [-1, 64, 8, 8]             128\n",
            "            ReLU-220             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-221             [-1, 64, 4, 4]               0\n",
            "          Conv2d-222             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-223             [-1, 64, 4, 4]             128\n",
            "            ReLU-224             [-1, 64, 4, 4]               0\n",
            "          Conv2d-225             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-226             [-1, 64, 4, 4]             128\n",
            "            ReLU-227             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-228             [-1, 64, 2, 2]               0\n",
            "          Conv2d-229             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-230             [-1, 64, 2, 2]             128\n",
            "            ReLU-231             [-1, 64, 2, 2]               0\n",
            "          Conv2d-232             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-233             [-1, 64, 2, 2]             128\n",
            "            ReLU-234             [-1, 64, 2, 2]               0\n",
            "          Conv2d-235             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-236             [-1, 64, 2, 2]             128\n",
            "            ReLU-237             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-238             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-239             [-1, 64, 1, 1]               0\n",
            "             VGG-240                   [-1, 64]               0\n",
            "          Conv2d-241           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-242           [-1, 32, 32, 32]              64\n",
            "            ReLU-243           [-1, 32, 32, 32]               0\n",
            "          Conv2d-244           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-245           [-1, 32, 32, 32]              64\n",
            "            ReLU-246           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-247           [-1, 32, 16, 16]               0\n",
            "          Conv2d-248           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-249           [-1, 32, 16, 16]              64\n",
            "            ReLU-250           [-1, 32, 16, 16]               0\n",
            "          Conv2d-251           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-252           [-1, 32, 16, 16]              64\n",
            "            ReLU-253           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-254             [-1, 32, 8, 8]               0\n",
            "          Conv2d-255             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-256             [-1, 32, 8, 8]              64\n",
            "            ReLU-257             [-1, 32, 8, 8]               0\n",
            "          Conv2d-258             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-259             [-1, 64, 8, 8]             128\n",
            "            ReLU-260             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-261             [-1, 64, 4, 4]               0\n",
            "          Conv2d-262             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-263             [-1, 64, 4, 4]             128\n",
            "            ReLU-264             [-1, 64, 4, 4]               0\n",
            "          Conv2d-265             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-266             [-1, 64, 4, 4]             128\n",
            "            ReLU-267             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-268             [-1, 64, 2, 2]               0\n",
            "          Conv2d-269             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-270             [-1, 64, 2, 2]             128\n",
            "            ReLU-271             [-1, 64, 2, 2]               0\n",
            "          Conv2d-272             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-273             [-1, 64, 2, 2]             128\n",
            "            ReLU-274             [-1, 64, 2, 2]               0\n",
            "          Conv2d-275             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-276             [-1, 64, 2, 2]             128\n",
            "            ReLU-277             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-278             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-279             [-1, 64, 1, 1]               0\n",
            "             VGG-280                   [-1, 64]               0\n",
            "          Conv2d-281           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-282           [-1, 32, 32, 32]              64\n",
            "            ReLU-283           [-1, 32, 32, 32]               0\n",
            "          Conv2d-284           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-285           [-1, 32, 32, 32]              64\n",
            "            ReLU-286           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-287           [-1, 32, 16, 16]               0\n",
            "          Conv2d-288           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-289           [-1, 32, 16, 16]              64\n",
            "            ReLU-290           [-1, 32, 16, 16]               0\n",
            "          Conv2d-291           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-292           [-1, 32, 16, 16]              64\n",
            "            ReLU-293           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-294             [-1, 32, 8, 8]               0\n",
            "          Conv2d-295             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-296             [-1, 32, 8, 8]              64\n",
            "            ReLU-297             [-1, 32, 8, 8]               0\n",
            "          Conv2d-298             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-299             [-1, 64, 8, 8]             128\n",
            "            ReLU-300             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-301             [-1, 64, 4, 4]               0\n",
            "          Conv2d-302             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-303             [-1, 64, 4, 4]             128\n",
            "            ReLU-304             [-1, 64, 4, 4]               0\n",
            "          Conv2d-305             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-306             [-1, 64, 4, 4]             128\n",
            "            ReLU-307             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-308             [-1, 64, 2, 2]               0\n",
            "          Conv2d-309             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-310             [-1, 64, 2, 2]             128\n",
            "            ReLU-311             [-1, 64, 2, 2]               0\n",
            "          Conv2d-312             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-313             [-1, 64, 2, 2]             128\n",
            "            ReLU-314             [-1, 64, 2, 2]               0\n",
            "          Conv2d-315             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-316             [-1, 64, 2, 2]             128\n",
            "            ReLU-317             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-318             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-319             [-1, 64, 1, 1]               0\n",
            "             VGG-320                   [-1, 64]               0\n",
            "          Linear-321                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 1,942,026\n",
            "Trainable params: 1,942,026\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 17.36\n",
            "Params size (MB): 7.41\n",
            "Estimated Total Size (MB): 24.78\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for m in net8students.modules():\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        m.weight.requires_grad = False\n",
        "        if m.bias is not None:\n",
        "            m.bias.requires_grad = False\n",
        "net8students = net8students.to(device)\n",
        "summary(net8students, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZAB4QuQLbvG",
        "outputId": "3a7702ba-420b-48a4-dcbe-516f28f85230"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "              VGG-40                   [-1, 64]               0\n",
            "           Conv2d-41           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-42           [-1, 32, 32, 32]              64\n",
            "             ReLU-43           [-1, 32, 32, 32]               0\n",
            "           Conv2d-44           [-1, 32, 32, 32]           9,248\n",
            "      BatchNorm2d-45           [-1, 32, 32, 32]              64\n",
            "             ReLU-46           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-47           [-1, 32, 16, 16]               0\n",
            "           Conv2d-48           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-49           [-1, 32, 16, 16]              64\n",
            "             ReLU-50           [-1, 32, 16, 16]               0\n",
            "           Conv2d-51           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-52           [-1, 32, 16, 16]              64\n",
            "             ReLU-53           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-54             [-1, 32, 8, 8]               0\n",
            "           Conv2d-55             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-56             [-1, 32, 8, 8]              64\n",
            "             ReLU-57             [-1, 32, 8, 8]               0\n",
            "           Conv2d-58             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-59             [-1, 64, 8, 8]             128\n",
            "             ReLU-60             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-61             [-1, 64, 4, 4]               0\n",
            "           Conv2d-62             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-63             [-1, 64, 4, 4]             128\n",
            "             ReLU-64             [-1, 64, 4, 4]               0\n",
            "           Conv2d-65             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-66             [-1, 64, 4, 4]             128\n",
            "             ReLU-67             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-68             [-1, 64, 2, 2]               0\n",
            "           Conv2d-69             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-70             [-1, 64, 2, 2]             128\n",
            "             ReLU-71             [-1, 64, 2, 2]               0\n",
            "           Conv2d-72             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-73             [-1, 64, 2, 2]             128\n",
            "             ReLU-74             [-1, 64, 2, 2]               0\n",
            "           Conv2d-75             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-76             [-1, 64, 2, 2]             128\n",
            "             ReLU-77             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-78             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-79             [-1, 64, 1, 1]               0\n",
            "              VGG-80                   [-1, 64]               0\n",
            "           Conv2d-81           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-82           [-1, 32, 32, 32]              64\n",
            "             ReLU-83           [-1, 32, 32, 32]               0\n",
            "           Conv2d-84           [-1, 32, 32, 32]           9,248\n",
            "      BatchNorm2d-85           [-1, 32, 32, 32]              64\n",
            "             ReLU-86           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-87           [-1, 32, 16, 16]               0\n",
            "           Conv2d-88           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-89           [-1, 32, 16, 16]              64\n",
            "             ReLU-90           [-1, 32, 16, 16]               0\n",
            "           Conv2d-91           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-92           [-1, 32, 16, 16]              64\n",
            "             ReLU-93           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-94             [-1, 32, 8, 8]               0\n",
            "           Conv2d-95             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-96             [-1, 32, 8, 8]              64\n",
            "             ReLU-97             [-1, 32, 8, 8]               0\n",
            "           Conv2d-98             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-99             [-1, 64, 8, 8]             128\n",
            "            ReLU-100             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-101             [-1, 64, 4, 4]               0\n",
            "          Conv2d-102             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-103             [-1, 64, 4, 4]             128\n",
            "            ReLU-104             [-1, 64, 4, 4]               0\n",
            "          Conv2d-105             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-106             [-1, 64, 4, 4]             128\n",
            "            ReLU-107             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-108             [-1, 64, 2, 2]               0\n",
            "          Conv2d-109             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-110             [-1, 64, 2, 2]             128\n",
            "            ReLU-111             [-1, 64, 2, 2]               0\n",
            "          Conv2d-112             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-113             [-1, 64, 2, 2]             128\n",
            "            ReLU-114             [-1, 64, 2, 2]               0\n",
            "          Conv2d-115             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-116             [-1, 64, 2, 2]             128\n",
            "            ReLU-117             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-118             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-119             [-1, 64, 1, 1]               0\n",
            "             VGG-120                   [-1, 64]               0\n",
            "          Conv2d-121           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-122           [-1, 32, 32, 32]              64\n",
            "            ReLU-123           [-1, 32, 32, 32]               0\n",
            "          Conv2d-124           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-125           [-1, 32, 32, 32]              64\n",
            "            ReLU-126           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-127           [-1, 32, 16, 16]               0\n",
            "          Conv2d-128           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-129           [-1, 32, 16, 16]              64\n",
            "            ReLU-130           [-1, 32, 16, 16]               0\n",
            "          Conv2d-131           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-132           [-1, 32, 16, 16]              64\n",
            "            ReLU-133           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-134             [-1, 32, 8, 8]               0\n",
            "          Conv2d-135             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-136             [-1, 32, 8, 8]              64\n",
            "            ReLU-137             [-1, 32, 8, 8]               0\n",
            "          Conv2d-138             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-139             [-1, 64, 8, 8]             128\n",
            "            ReLU-140             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-141             [-1, 64, 4, 4]               0\n",
            "          Conv2d-142             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-143             [-1, 64, 4, 4]             128\n",
            "            ReLU-144             [-1, 64, 4, 4]               0\n",
            "          Conv2d-145             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-146             [-1, 64, 4, 4]             128\n",
            "            ReLU-147             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-148             [-1, 64, 2, 2]               0\n",
            "          Conv2d-149             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-150             [-1, 64, 2, 2]             128\n",
            "            ReLU-151             [-1, 64, 2, 2]               0\n",
            "          Conv2d-152             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-153             [-1, 64, 2, 2]             128\n",
            "            ReLU-154             [-1, 64, 2, 2]               0\n",
            "          Conv2d-155             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-156             [-1, 64, 2, 2]             128\n",
            "            ReLU-157             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-158             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-159             [-1, 64, 1, 1]               0\n",
            "             VGG-160                   [-1, 64]               0\n",
            "          Conv2d-161           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-162           [-1, 32, 32, 32]              64\n",
            "            ReLU-163           [-1, 32, 32, 32]               0\n",
            "          Conv2d-164           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-165           [-1, 32, 32, 32]              64\n",
            "            ReLU-166           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-167           [-1, 32, 16, 16]               0\n",
            "          Conv2d-168           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-169           [-1, 32, 16, 16]              64\n",
            "            ReLU-170           [-1, 32, 16, 16]               0\n",
            "          Conv2d-171           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-172           [-1, 32, 16, 16]              64\n",
            "            ReLU-173           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-174             [-1, 32, 8, 8]               0\n",
            "          Conv2d-175             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-176             [-1, 32, 8, 8]              64\n",
            "            ReLU-177             [-1, 32, 8, 8]               0\n",
            "          Conv2d-178             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-179             [-1, 64, 8, 8]             128\n",
            "            ReLU-180             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-181             [-1, 64, 4, 4]               0\n",
            "          Conv2d-182             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-183             [-1, 64, 4, 4]             128\n",
            "            ReLU-184             [-1, 64, 4, 4]               0\n",
            "          Conv2d-185             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-186             [-1, 64, 4, 4]             128\n",
            "            ReLU-187             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-188             [-1, 64, 2, 2]               0\n",
            "          Conv2d-189             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-190             [-1, 64, 2, 2]             128\n",
            "            ReLU-191             [-1, 64, 2, 2]               0\n",
            "          Conv2d-192             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-193             [-1, 64, 2, 2]             128\n",
            "            ReLU-194             [-1, 64, 2, 2]               0\n",
            "          Conv2d-195             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-196             [-1, 64, 2, 2]             128\n",
            "            ReLU-197             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-198             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-199             [-1, 64, 1, 1]               0\n",
            "             VGG-200                   [-1, 64]               0\n",
            "          Conv2d-201           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-202           [-1, 32, 32, 32]              64\n",
            "            ReLU-203           [-1, 32, 32, 32]               0\n",
            "          Conv2d-204           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-205           [-1, 32, 32, 32]              64\n",
            "            ReLU-206           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-207           [-1, 32, 16, 16]               0\n",
            "          Conv2d-208           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-209           [-1, 32, 16, 16]              64\n",
            "            ReLU-210           [-1, 32, 16, 16]               0\n",
            "          Conv2d-211           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-212           [-1, 32, 16, 16]              64\n",
            "            ReLU-213           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-214             [-1, 32, 8, 8]               0\n",
            "          Conv2d-215             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-216             [-1, 32, 8, 8]              64\n",
            "            ReLU-217             [-1, 32, 8, 8]               0\n",
            "          Conv2d-218             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-219             [-1, 64, 8, 8]             128\n",
            "            ReLU-220             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-221             [-1, 64, 4, 4]               0\n",
            "          Conv2d-222             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-223             [-1, 64, 4, 4]             128\n",
            "            ReLU-224             [-1, 64, 4, 4]               0\n",
            "          Conv2d-225             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-226             [-1, 64, 4, 4]             128\n",
            "            ReLU-227             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-228             [-1, 64, 2, 2]               0\n",
            "          Conv2d-229             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-230             [-1, 64, 2, 2]             128\n",
            "            ReLU-231             [-1, 64, 2, 2]               0\n",
            "          Conv2d-232             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-233             [-1, 64, 2, 2]             128\n",
            "            ReLU-234             [-1, 64, 2, 2]               0\n",
            "          Conv2d-235             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-236             [-1, 64, 2, 2]             128\n",
            "            ReLU-237             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-238             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-239             [-1, 64, 1, 1]               0\n",
            "             VGG-240                   [-1, 64]               0\n",
            "          Conv2d-241           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-242           [-1, 32, 32, 32]              64\n",
            "            ReLU-243           [-1, 32, 32, 32]               0\n",
            "          Conv2d-244           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-245           [-1, 32, 32, 32]              64\n",
            "            ReLU-246           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-247           [-1, 32, 16, 16]               0\n",
            "          Conv2d-248           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-249           [-1, 32, 16, 16]              64\n",
            "            ReLU-250           [-1, 32, 16, 16]               0\n",
            "          Conv2d-251           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-252           [-1, 32, 16, 16]              64\n",
            "            ReLU-253           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-254             [-1, 32, 8, 8]               0\n",
            "          Conv2d-255             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-256             [-1, 32, 8, 8]              64\n",
            "            ReLU-257             [-1, 32, 8, 8]               0\n",
            "          Conv2d-258             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-259             [-1, 64, 8, 8]             128\n",
            "            ReLU-260             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-261             [-1, 64, 4, 4]               0\n",
            "          Conv2d-262             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-263             [-1, 64, 4, 4]             128\n",
            "            ReLU-264             [-1, 64, 4, 4]               0\n",
            "          Conv2d-265             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-266             [-1, 64, 4, 4]             128\n",
            "            ReLU-267             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-268             [-1, 64, 2, 2]               0\n",
            "          Conv2d-269             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-270             [-1, 64, 2, 2]             128\n",
            "            ReLU-271             [-1, 64, 2, 2]               0\n",
            "          Conv2d-272             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-273             [-1, 64, 2, 2]             128\n",
            "            ReLU-274             [-1, 64, 2, 2]               0\n",
            "          Conv2d-275             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-276             [-1, 64, 2, 2]             128\n",
            "            ReLU-277             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-278             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-279             [-1, 64, 1, 1]               0\n",
            "             VGG-280                   [-1, 64]               0\n",
            "          Conv2d-281           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-282           [-1, 32, 32, 32]              64\n",
            "            ReLU-283           [-1, 32, 32, 32]               0\n",
            "          Conv2d-284           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-285           [-1, 32, 32, 32]              64\n",
            "            ReLU-286           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-287           [-1, 32, 16, 16]               0\n",
            "          Conv2d-288           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-289           [-1, 32, 16, 16]              64\n",
            "            ReLU-290           [-1, 32, 16, 16]               0\n",
            "          Conv2d-291           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-292           [-1, 32, 16, 16]              64\n",
            "            ReLU-293           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-294             [-1, 32, 8, 8]               0\n",
            "          Conv2d-295             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-296             [-1, 32, 8, 8]              64\n",
            "            ReLU-297             [-1, 32, 8, 8]               0\n",
            "          Conv2d-298             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-299             [-1, 64, 8, 8]             128\n",
            "            ReLU-300             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-301             [-1, 64, 4, 4]               0\n",
            "          Conv2d-302             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-303             [-1, 64, 4, 4]             128\n",
            "            ReLU-304             [-1, 64, 4, 4]               0\n",
            "          Conv2d-305             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-306             [-1, 64, 4, 4]             128\n",
            "            ReLU-307             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-308             [-1, 64, 2, 2]               0\n",
            "          Conv2d-309             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-310             [-1, 64, 2, 2]             128\n",
            "            ReLU-311             [-1, 64, 2, 2]               0\n",
            "          Conv2d-312             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-313             [-1, 64, 2, 2]             128\n",
            "            ReLU-314             [-1, 64, 2, 2]               0\n",
            "          Conv2d-315             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-316             [-1, 64, 2, 2]             128\n",
            "            ReLU-317             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-318             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-319             [-1, 64, 1, 1]               0\n",
            "             VGG-320                   [-1, 64]               0\n",
            "          Linear-321                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 1,942,026\n",
            "Trainable params: 13,834\n",
            "Non-trainable params: 1,928,192\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 17.36\n",
            "Params size (MB): 7.41\n",
            "Estimated Total Size (MB): 24.78\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net8students.parameters(), lr=0.0001)\n",
        "\n",
        "def train81(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    net8students.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        net8students.zero_grad()\n",
        "        outputs = net8students(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if(batch_idx % 200 == 0):\n",
        "          print(\"Accuracy : \",100.*correct/total,\" Loss : \", train_loss/(batch_idx+1))\n",
        "def test82(epoch):\n",
        "    net8students.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net8students(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\"Accuracy : \",100.*correct/total,\" Loss : \", test_loss/(batch_idx+1))"
      ],
      "metadata": {
        "id": "mrvLxB_eNVcI"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "    train81(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test82(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tGEXdFBNpfV",
        "outputId": "9bbdca5e-0903-4d1f-83ac-dd6a5a1ea5a8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Accuracy :  13.0  Loss :  2.4914193153381348\n",
            "Accuracy :  61.3134328358209  Loss :  1.5904693505657252\n",
            "Accuracy :  71.43391521197007  Loss :  1.2125079175480584\n",
            "Validation: \n",
            "Accuracy :  85.0  Loss :  0.5873266458511353\n",
            "Accuracy :  82.19047619047619  Loss :  0.6366797643048423\n",
            "Accuracy :  81.65853658536585  Loss :  0.6468475860793416\n",
            "Accuracy :  81.8688524590164  Loss :  0.6428736130722233\n",
            "Accuracy :  81.88888888888889  Loss :  0.6411071018672284\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy :  82.0  Loss :  0.6646072268486023\n",
            "Accuracy :  82.1592039800995  Loss :  0.6031519663867666\n",
            "Accuracy :  82.28927680798004  Loss :  0.5806360327246183\n",
            "Validation: \n",
            "Accuracy :  85.0  Loss :  0.4746418297290802\n",
            "Accuracy :  82.61904761904762  Loss :  0.5367023277850378\n",
            "Accuracy :  82.36585365853658  Loss :  0.5476735205185123\n",
            "Accuracy :  82.44262295081967  Loss :  0.5427290938916753\n",
            "Accuracy :  82.41975308641975  Loss :  0.5398232013355067\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy :  83.0  Loss :  0.5042957067489624\n",
            "Accuracy :  82.54228855721394  Loss :  0.5269157569206769\n",
            "Accuracy :  82.70573566084788  Loss :  0.5202517244881228\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.44675102829933167\n",
            "Accuracy :  82.71428571428571  Loss :  0.5172897094771975\n",
            "Accuracy :  82.58536585365853  Loss :  0.5283751923863481\n",
            "Accuracy :  82.62295081967213  Loss :  0.5234863933969717\n",
            "Accuracy :  82.58024691358025  Loss :  0.5202170415425006\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy :  83.0  Loss :  0.47116565704345703\n",
            "Accuracy :  82.68159203980099  Loss :  0.5080512830274022\n",
            "Accuracy :  82.86783042394015  Loss :  0.5033363217576187\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4353768825531006\n",
            "Accuracy :  83.04761904761905  Loss :  0.5113783067180997\n",
            "Accuracy :  82.82926829268293  Loss :  0.5221817217222074\n",
            "Accuracy :  82.91803278688525  Loss :  0.516994690797368\n",
            "Accuracy :  82.79012345679013  Loss :  0.5135002121513272\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy :  85.0  Loss :  0.46607789397239685\n",
            "Accuracy :  82.8955223880597  Loss :  0.501059283961111\n",
            "Accuracy :  83.02493765586036  Loss :  0.498594181867609\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4282030761241913\n",
            "Accuracy :  82.95238095238095  Loss :  0.509269057285218\n",
            "Accuracy :  82.73170731707317  Loss :  0.5194092334770575\n",
            "Accuracy :  82.78688524590164  Loss :  0.5142233498760911\n",
            "Accuracy :  82.70370370370371  Loss :  0.5107592587117795\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy :  84.0  Loss :  0.46245723962783813\n",
            "Accuracy :  82.7412935323383  Loss :  0.496827569618747\n",
            "Accuracy :  82.91271820448878  Loss :  0.4943212539775116\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4238857924938202\n",
            "Accuracy :  83.04761904761905  Loss :  0.5070927483694894\n",
            "Accuracy :  82.8048780487805  Loss :  0.5169771051988369\n",
            "Accuracy :  82.88524590163935  Loss :  0.5118777336644345\n",
            "Accuracy :  82.81481481481481  Loss :  0.5082772377832436\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy :  84.0  Loss :  0.46323126554489136\n",
            "Accuracy :  82.94527363184079  Loss :  0.49354292252170506\n",
            "Accuracy :  82.98004987531172  Loss :  0.4915108870241113\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4229329824447632\n",
            "Accuracy :  83.14285714285714  Loss :  0.5055089976106372\n",
            "Accuracy :  82.82926829268293  Loss :  0.5151707668129991\n",
            "Accuracy :  82.9672131147541  Loss :  0.5101453227097871\n",
            "Accuracy :  82.91358024691358  Loss :  0.5063946707013213\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy :  86.0  Loss :  0.44448140263557434\n",
            "Accuracy :  83.14925373134328  Loss :  0.4929555962927899\n",
            "Accuracy :  83.2069825436409  Loss :  0.49002380197184936\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4206676185131073\n",
            "Accuracy :  82.95238095238095  Loss :  0.5044538492248172\n",
            "Accuracy :  83.0  Loss :  0.5137983887660794\n",
            "Accuracy :  83.1311475409836  Loss :  0.5086340884693333\n",
            "Accuracy :  83.06172839506173  Loss :  0.5048556747259917\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy :  85.0  Loss :  0.43176257610321045\n",
            "Accuracy :  83.3731343283582  Loss :  0.48913183378342967\n",
            "Accuracy :  83.24438902743142  Loss :  0.4868155173084088\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4168529510498047\n",
            "Accuracy :  83.04761904761905  Loss :  0.5025555647554851\n",
            "Accuracy :  82.92682926829268  Loss :  0.5118035310652198\n",
            "Accuracy :  83.06557377049181  Loss :  0.506742888298191\n",
            "Accuracy :  83.01234567901234  Loss :  0.5030361914340361\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy :  84.0  Loss :  0.44746464490890503\n",
            "Accuracy :  83.14427860696517  Loss :  0.48823892892296633\n",
            "Accuracy :  83.17456359102245  Loss :  0.4856145332133086\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4168003797531128\n",
            "Accuracy :  83.0952380952381  Loss :  0.5017148497558775\n",
            "Accuracy :  82.90243902439025  Loss :  0.5107704633619727\n",
            "Accuracy :  83.06557377049181  Loss :  0.5054739606185038\n",
            "Accuracy :  83.08641975308642  Loss :  0.501789665148582\n"
          ]
        }
      ]
    }
  ]
}