{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "All_Trained_on_Student",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4c5263f1dca04283845b812a05a250d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6715bbfc8804464389ab171091ceee2c",
              "IPY_MODEL_bd39e10c7bb641b3a2444131ffedbd90",
              "IPY_MODEL_dc401bd554f74485bcb73052087c9466"
            ],
            "layout": "IPY_MODEL_46ad270953d540d0b0793ca2397898c5"
          }
        },
        "6715bbfc8804464389ab171091ceee2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9258b6de4e54fb9bc5be5f5db43f26f",
            "placeholder": "​",
            "style": "IPY_MODEL_47482a357b5c47a2acc29ed5aaf50190",
            "value": ""
          }
        },
        "bd39e10c7bb641b3a2444131ffedbd90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33b0a221c4a24a75a67dcc49c080e7d7",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6178facf87914343a45cbd771b4a497f",
            "value": 170498071
          }
        },
        "dc401bd554f74485bcb73052087c9466": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6282f16aeb854d599d5bdb4b3a1beb4b",
            "placeholder": "​",
            "style": "IPY_MODEL_fbe862fc9ba04a9587ca32fd0e9025db",
            "value": " 170499072/? [00:05&lt;00:00, 32395324.19it/s]"
          }
        },
        "46ad270953d540d0b0793ca2397898c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9258b6de4e54fb9bc5be5f5db43f26f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47482a357b5c47a2acc29ed5aaf50190": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33b0a221c4a24a75a67dcc49c080e7d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6178facf87914343a45cbd771b4a497f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6282f16aeb854d599d5bdb4b3a1beb4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbe862fc9ba04a9587ca32fd0e9025db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Knowledge Distillation \n",
        "We will impliment [TCN](https://www.bmvc2021-virtualconference.com/conference/papers/paper_0831.html) paper. It is a varient of knowledge distillation which uses dense feature vactors instead of logits to transfer knowledge from teacher to student.  "
      ],
      "metadata": {
        "id": "NM1cxLfnctDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71XqWTiofkMi",
        "outputId": "d905d5a0-4b32-4036-f07f-a77a5b90cf7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training base teacher network\n",
        "This section is not graded"
      ],
      "metadata": {
        "id": "sOqQ-lRNesRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "import tqdm\n",
        "\n",
        "batch_size = 100\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "4c5263f1dca04283845b812a05a250d9",
            "6715bbfc8804464389ab171091ceee2c",
            "bd39e10c7bb641b3a2444131ffedbd90",
            "dc401bd554f74485bcb73052087c9466",
            "46ad270953d540d0b0793ca2397898c5",
            "d9258b6de4e54fb9bc5be5f5db43f26f",
            "47482a357b5c47a2acc29ed5aaf50190",
            "33b0a221c4a24a75a67dcc49c080e7d7",
            "6178facf87914343a45cbd771b4a497f",
            "6282f16aeb854d599d5bdb4b3a1beb4b",
            "fbe862fc9ba04a9587ca32fd0e9025db"
          ]
        },
        "id": "KSnbXgTWmFsJ",
        "outputId": "b6dcf6a4-35ea-407d-8a98-60f44afaca7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c5263f1dca04283845b812a05a250d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "teacher = VGG('VGG16')\n",
        "teacher = teacher.to(device)"
      ],
      "metadata": {
        "id": "Up5ob6ujFKex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(teacher.parameters(), lr=0.0001)\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    teacher.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        teacher.zero_grad()\n",
        "        outputs = teacher(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if(batch_idx % 200 == 0):\n",
        "          print(\"Accuracy : \",100.*correct/total,\" Loss : \", train_loss/(batch_idx+1))\n",
        "def test(epoch):\n",
        "    teacher.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = teacher(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\"Accuracy : \",100.*correct/total,\" Loss : \", test_loss/(batch_idx+1))"
      ],
      "metadata": {
        "id": "2wyVCEgqFxxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "    train(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xfNOBETGFNR",
        "outputId": "f8111f75-8c9b-4c37-c9b5-bdabfe9c5b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Accuracy :  14.0  Loss :  2.3733975887298584\n",
            "Accuracy :  41.19900497512438  Loss :  1.5865661386233658\n",
            "Accuracy :  47.80548628428928  Loss :  1.41774071199341\n",
            "Validation: \n",
            "Accuracy :  67.0  Loss :  0.9107257127761841\n",
            "Accuracy :  65.33333333333333  Loss :  0.9950999220212301\n",
            "Accuracy :  64.17073170731707  Loss :  1.0056527911162958\n",
            "Accuracy :  64.47540983606558  Loss :  0.9981735944747925\n",
            "Accuracy :  64.24691358024691  Loss :  1.005907082999194\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy :  64.0  Loss :  0.9784829616546631\n",
            "Accuracy :  63.975124378109456  Loss :  1.007334947289519\n",
            "Accuracy :  65.65336658354114  Loss :  0.9614940865378725\n",
            "Validation: \n",
            "Accuracy :  75.0  Loss :  0.7087078094482422\n",
            "Accuracy :  73.0952380952381  Loss :  0.7782325758820489\n",
            "Accuracy :  72.07317073170732  Loss :  0.8009776901908037\n",
            "Accuracy :  72.45901639344262  Loss :  0.7891016011355353\n",
            "Accuracy :  72.55555555555556  Loss :  0.7947008230803926\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy :  64.0  Loss :  0.8603017330169678\n",
            "Accuracy :  72.07462686567165  Loss :  0.7997423499377806\n",
            "Accuracy :  72.94513715710723  Loss :  0.7742086663805041\n",
            "Validation: \n",
            "Accuracy :  79.0  Loss :  0.6711878180503845\n",
            "Accuracy :  74.76190476190476  Loss :  0.7303563171908969\n",
            "Accuracy :  74.34146341463415  Loss :  0.7372472046352014\n",
            "Accuracy :  74.85245901639344  Loss :  0.7317987011104333\n",
            "Accuracy :  74.92592592592592  Loss :  0.7305009972166132\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy :  79.0  Loss :  0.5780044794082642\n",
            "Accuracy :  76.72139303482587  Loss :  0.6739745083732984\n",
            "Accuracy :  77.12219451371571  Loss :  0.6625515817852686\n",
            "Validation: \n",
            "Accuracy :  77.0  Loss :  0.6510322093963623\n",
            "Accuracy :  76.38095238095238  Loss :  0.6998276596977597\n",
            "Accuracy :  76.53658536585365  Loss :  0.7156145616275508\n",
            "Accuracy :  76.21311475409836  Loss :  0.7109470562856706\n",
            "Accuracy :  76.12345679012346  Loss :  0.7078349800021561\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy :  77.0  Loss :  0.6177147030830383\n",
            "Accuracy :  79.4776119402985  Loss :  0.5986515867769422\n",
            "Accuracy :  79.75062344139651  Loss :  0.5906961390799715\n",
            "Validation: \n",
            "Accuracy :  84.0  Loss :  0.5090551376342773\n",
            "Accuracy :  78.66666666666667  Loss :  0.6368099678130377\n",
            "Accuracy :  78.0  Loss :  0.6627424199406694\n",
            "Accuracy :  77.98360655737704  Loss :  0.6582581303158744\n",
            "Accuracy :  78.0246913580247  Loss :  0.6549958369614165\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy :  75.0  Loss :  0.5102257132530212\n",
            "Accuracy :  81.50248756218906  Loss :  0.5391105981311988\n",
            "Accuracy :  81.57107231920199  Loss :  0.5350365822303325\n",
            "Validation: \n",
            "Accuracy :  83.0  Loss :  0.49757125973701477\n",
            "Accuracy :  82.61904761904762  Loss :  0.5151596452508654\n",
            "Accuracy :  82.0  Loss :  0.5324131686513017\n",
            "Accuracy :  82.01639344262296  Loss :  0.5297190204995578\n",
            "Accuracy :  82.06172839506173  Loss :  0.5265540598351278\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy :  83.0  Loss :  0.44234907627105713\n",
            "Accuracy :  83.60199004975124  Loss :  0.4862030011356173\n",
            "Accuracy :  83.44389027431421  Loss :  0.48413379181947497\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.3825371563434601\n",
            "Accuracy :  82.42857142857143  Loss :  0.5258803821745373\n",
            "Accuracy :  81.60975609756098  Loss :  0.5497385663230244\n",
            "Accuracy :  81.62295081967213  Loss :  0.5458922777019564\n",
            "Accuracy :  81.23456790123457  Loss :  0.5467918978797065\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy :  84.0  Loss :  0.36962124705314636\n",
            "Accuracy :  84.67164179104478  Loss :  0.44874195969519926\n",
            "Accuracy :  84.55860349127182  Loss :  0.44864371693936966\n",
            "Validation: \n",
            "Accuracy :  80.0  Loss :  0.5324685573577881\n",
            "Accuracy :  81.42857142857143  Loss :  0.5608684661842528\n",
            "Accuracy :  81.1219512195122  Loss :  0.5700179571058692\n",
            "Accuracy :  80.9672131147541  Loss :  0.57261781272341\n",
            "Accuracy :  81.07407407407408  Loss :  0.5733076157393279\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy :  85.0  Loss :  0.4485841691493988\n",
            "Accuracy :  85.57711442786069  Loss :  0.41721999919533137\n",
            "Accuracy :  85.56109725685785  Loss :  0.41495818572300036\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4822770953178406\n",
            "Accuracy :  82.52380952380952  Loss :  0.5443572174935114\n",
            "Accuracy :  82.2439024390244  Loss :  0.549246766218325\n",
            "Accuracy :  82.19672131147541  Loss :  0.547752359851462\n",
            "Accuracy :  82.18518518518519  Loss :  0.5443754104184516\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy :  93.0  Loss :  0.21290676295757294\n",
            "Accuracy :  86.90049751243781  Loss :  0.38545506681079295\n",
            "Accuracy :  86.8927680798005  Loss :  0.38235597234414404\n",
            "Validation: \n",
            "Accuracy :  85.0  Loss :  0.49091336131095886\n",
            "Accuracy :  83.23809523809524  Loss :  0.5201689849297205\n",
            "Accuracy :  82.97560975609755  Loss :  0.5335028255131187\n",
            "Accuracy :  83.0  Loss :  0.5326328812564005\n",
            "Accuracy :  82.88888888888889  Loss :  0.5320950560731652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_name = 'teacher.pt'\n",
        "path = F\"/content/gdrive/MyDrive/{model_save_name}\" \n",
        "#torch.save(teacher.state_dict(), path)\n",
        "teacher.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKgOR2afg0Ea",
        "outputId": "ea0a409d-8197-47b4-af48-93d79d2daf52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Dense Feature Dataset\n",
        "1.1 In this cell we remove the head of teacher network(i.e: last fullyconnected layer) and add a flatten layer at the end."
      ],
      "metadata": {
        "id": "mERdZFH7fBXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(teacher, (3, 32, 32))"
      ],
      "metadata": {
        "id": "Ubp5SeKSCRDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4601a0a-e656-47c7-a5c1-8c6f6d9229b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          36,928\n",
            "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
            "              ReLU-6           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 64, 16, 16]               0\n",
            "            Conv2d-8          [-1, 128, 16, 16]          73,856\n",
            "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
            "             ReLU-10          [-1, 128, 16, 16]               0\n",
            "           Conv2d-11          [-1, 128, 16, 16]         147,584\n",
            "      BatchNorm2d-12          [-1, 128, 16, 16]             256\n",
            "             ReLU-13          [-1, 128, 16, 16]               0\n",
            "        MaxPool2d-14            [-1, 128, 8, 8]               0\n",
            "           Conv2d-15            [-1, 256, 8, 8]         295,168\n",
            "      BatchNorm2d-16            [-1, 256, 8, 8]             512\n",
            "             ReLU-17            [-1, 256, 8, 8]               0\n",
            "           Conv2d-18            [-1, 256, 8, 8]         590,080\n",
            "      BatchNorm2d-19            [-1, 256, 8, 8]             512\n",
            "             ReLU-20            [-1, 256, 8, 8]               0\n",
            "           Conv2d-21            [-1, 256, 8, 8]         590,080\n",
            "      BatchNorm2d-22            [-1, 256, 8, 8]             512\n",
            "             ReLU-23            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 512, 4, 4]       1,180,160\n",
            "      BatchNorm2d-26            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-27            [-1, 512, 4, 4]               0\n",
            "           Conv2d-28            [-1, 512, 4, 4]       2,359,808\n",
            "      BatchNorm2d-29            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-30            [-1, 512, 4, 4]               0\n",
            "           Conv2d-31            [-1, 512, 4, 4]       2,359,808\n",
            "      BatchNorm2d-32            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-33            [-1, 512, 4, 4]               0\n",
            "        MaxPool2d-34            [-1, 512, 2, 2]               0\n",
            "           Conv2d-35            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-36            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-37            [-1, 512, 2, 2]               0\n",
            "           Conv2d-38            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-39            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-40            [-1, 512, 2, 2]               0\n",
            "           Conv2d-41            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-42            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-43            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-44            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-45            [-1, 512, 1, 1]               0\n",
            "           Linear-46                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 14,728,266\n",
            "Trainable params: 14,728,266\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 6.57\n",
            "Params size (MB): 56.18\n",
            "Estimated Total Size (MB): 62.77\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_WOH = nn.Sequential(*list(teacher.children())[:-1],nn.Flatten())"
      ],
      "metadata": {
        "id": "OIK5Vcfo8Y19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The summery of the new teacher without head :"
      ],
      "metadata": {
        "id": "JG2u-KMYbvI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "55V5zBa_vZjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from torchsummary import summary\n",
        "summary(teacher_WOH, (3, 32, 32))"
      ],
      "metadata": {
        "id": "rFtzu7DD8uev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "618c2479-143d-4eee-b30e-4016ffbb7734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          36,928\n",
            "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
            "              ReLU-6           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 64, 16, 16]               0\n",
            "            Conv2d-8          [-1, 128, 16, 16]          73,856\n",
            "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
            "             ReLU-10          [-1, 128, 16, 16]               0\n",
            "           Conv2d-11          [-1, 128, 16, 16]         147,584\n",
            "      BatchNorm2d-12          [-1, 128, 16, 16]             256\n",
            "             ReLU-13          [-1, 128, 16, 16]               0\n",
            "        MaxPool2d-14            [-1, 128, 8, 8]               0\n",
            "           Conv2d-15            [-1, 256, 8, 8]         295,168\n",
            "      BatchNorm2d-16            [-1, 256, 8, 8]             512\n",
            "             ReLU-17            [-1, 256, 8, 8]               0\n",
            "           Conv2d-18            [-1, 256, 8, 8]         590,080\n",
            "      BatchNorm2d-19            [-1, 256, 8, 8]             512\n",
            "             ReLU-20            [-1, 256, 8, 8]               0\n",
            "           Conv2d-21            [-1, 256, 8, 8]         590,080\n",
            "      BatchNorm2d-22            [-1, 256, 8, 8]             512\n",
            "             ReLU-23            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 512, 4, 4]       1,180,160\n",
            "      BatchNorm2d-26            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-27            [-1, 512, 4, 4]               0\n",
            "           Conv2d-28            [-1, 512, 4, 4]       2,359,808\n",
            "      BatchNorm2d-29            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-30            [-1, 512, 4, 4]               0\n",
            "           Conv2d-31            [-1, 512, 4, 4]       2,359,808\n",
            "      BatchNorm2d-32            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-33            [-1, 512, 4, 4]               0\n",
            "        MaxPool2d-34            [-1, 512, 2, 2]               0\n",
            "           Conv2d-35            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-36            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-37            [-1, 512, 2, 2]               0\n",
            "           Conv2d-38            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-39            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-40            [-1, 512, 2, 2]               0\n",
            "           Conv2d-41            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-42            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-43            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-44            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-45            [-1, 512, 1, 1]               0\n",
            "          Flatten-46                  [-1, 512]               0\n",
            "================================================================\n",
            "Total params: 14,723,136\n",
            "Trainable params: 14,723,136\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 6.57\n",
            "Params size (MB): 56.16\n",
            "Estimated Total Size (MB): 62.75\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 In this cell you have to create dense feature labels dataset(i.e: the outputs of teacher network without head). For that you have to do forward pass on whole dataset and append the outputs in a variable. "
      ],
      "metadata": {
        "id": "FJsum1ioc2tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# teacher_WOH.eval()\n",
        "# DenseTrain = None\n",
        "# DenseTest = None\n",
        "# with torch.no_grad():\n",
        "#     for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "#         inputs, targets = inputs.to(device), targets.to(device)\n",
        "#         outputs = teacher_WOH(inputs)\n",
        "#         if(DenseTrain == None):\n",
        "#             DenseTrain = outputs\n",
        "#         else:\n",
        "#             DenseTrain = torch.cat((DenseTrain,outputs))\n",
        "# with torch.no_grad():\n",
        "#     for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "#         inputs, targets = inputs.to(device), targets.to(device)\n",
        "#         outputs = teacher_WOH(inputs)\n",
        "#         if(DenseTest == None):\n",
        "#             DenseTest = outputs\n",
        "#         else:\n",
        "#             DenseTest = torch.cat((DenseTest,outputs))"
      ],
      "metadata": {
        "id": "jb8FytxoHM62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating ad-hoc student network\n",
        "we create an ad-hoc student network "
      ],
      "metadata": {
        "id": "xzcPi5zuhqOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "    'VGGS': [32, 32, 'M', 64, 64, 'M', 128, 128, 'M', 256, 256, 'M',512,'M'],\n",
        "\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(512, 512)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "s1 = VGG('VGGS')\n",
        "s1 = s1.to(device)\n",
        "summary(s1, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9mnHRpJhpl5",
        "outputId": "bfe2387e-af19-4bca-97df-9f60ac499f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "           Conv2d-29            [-1, 512, 2, 2]       1,180,160\n",
            "      BatchNorm2d-30            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-31            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-32            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-33            [-1, 512, 1, 1]               0\n",
            "           Linear-34                  [-1, 512]         262,656\n",
            "================================================================\n",
            "Total params: 2,618,016\n",
            "Trainable params: 2,618,016\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.99\n",
            "Params size (MB): 9.99\n",
            "Estimated Total Size (MB): 12.99\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Student\n",
        "We will train the student network using Dense Features that we created.\n",
        "Dataset datagen will provide data in batches so we need to extract the corresponding batch of targets from our Dense feature variable from 1.2, for this we use the following formula:\n",
        "\n",
        "batch_index * batch_size --> (batch_index * batch_size) + batch_size"
      ],
      "metadata": {
        "id": "8z6cgCZ7h8F5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer = optim.Adam(s1.parameters(), lr=0.0001)\n",
        "# criterion = nn.MSELoss()\n",
        "\n",
        "# def train(epoch):\n",
        "#     print('\\nEpoch: %d' % (epoch+1))\n",
        "#     s1.train()\n",
        "#     train_loss = 0\n",
        "#     for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "#         inputs, targets = inputs.to(device), targets.to(device)\n",
        "#         targets = DenseTrain[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "        \n",
        "#         inputs = Variable(inputs, requires_grad=False)\n",
        "#         targets = Variable(targets)\n",
        "        \n",
        "#         s1.zero_grad()\n",
        "#         outputs = s1(inputs)\n",
        "#         loss = criterion(outputs, targets)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         train_loss += loss.item()\n",
        "#         if(batch_idx % 10 == 0):\n",
        "#           print(\"Loss : \", train_loss/(batch_idx+1))\n",
        "# def test(epoch):\n",
        "#     s1.eval()\n",
        "#     test_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "#             inputs, targets = inputs.to(device), targets.to(device)\n",
        "#             targets = DenseTest[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "#             outputs = s1(inputs)\n",
        "#             loss = criterion(outputs, targets)\n",
        "\n",
        "#             test_loss += loss.item()\n",
        "#             if(batch_idx % 20 == 0):\n",
        "#               print(\" Loss : \", test_loss/(batch_idx+1))"
      ],
      "metadata": {
        "id": "eXXtMar7h6jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+60):\n",
        "    train(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1IThg_HiK6q",
        "outputId": "53886561-b8c4-44d3-d2b6-42c049ccad86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Loss :  3.6982531547546387\n",
            "Loss :  2.6693457798524336\n",
            "Loss :  2.040993488970257\n",
            "Loss :  1.6936085147242392\n",
            "Loss :  1.467016842307114\n",
            "Loss :  1.3230298921173693\n",
            "Loss :  1.2173858148152712\n",
            "Loss :  1.1410718870834566\n",
            "Loss :  1.0773069829116633\n",
            "Loss :  1.0279522839483324\n",
            "Loss :  0.9826088010674656\n",
            "Loss :  0.9441749147466711\n",
            "Loss :  0.913011798188706\n",
            "Loss :  0.8847250028420951\n",
            "Loss :  0.8595928154515882\n",
            "Loss :  0.8373164838513002\n",
            "Loss :  0.8162185294287545\n",
            "Loss :  0.7974991399293755\n",
            "Loss :  0.7824804616567179\n",
            "Loss :  0.7662618968499268\n",
            "Loss :  0.751274244405737\n",
            "Loss :  0.7378353629067046\n",
            "Loss :  0.7251876513073348\n",
            "Loss :  0.7137788181955164\n",
            "Loss :  0.7028869803515707\n",
            "Loss :  0.6934796735584974\n",
            "Loss :  0.683782813078599\n",
            "Loss :  0.6741902095805234\n",
            "Loss :  0.6649867340763268\n",
            "Loss :  0.65718315843864\n",
            "Loss :  0.6493550655849748\n",
            "Loss :  0.642017930650251\n",
            "Loss :  0.634771172595544\n",
            "Loss :  0.6277699426401778\n",
            "Loss :  0.6213034062791081\n",
            "Loss :  0.6148867536643972\n",
            "Loss :  0.6086351433123908\n",
            "Loss :  0.60276378952268\n",
            "Loss :  0.5964537517135851\n",
            "Loss :  0.590879879949038\n",
            "Loss :  0.5854624561092205\n",
            "Loss :  0.5800766229194446\n",
            "Loss :  0.5750415806532473\n",
            "Loss :  0.5699751790604293\n",
            "Loss :  0.565402856692165\n",
            "Loss :  0.5606944153287723\n",
            "Loss :  0.556215103950004\n",
            "Loss :  0.5516340666136165\n",
            "Loss :  0.5476758462103886\n",
            "Loss :  0.5432978083669775\n",
            "Validation: \n",
            " Loss :  0.30649250745773315\n",
            " Loss :  0.3378171239580427\n",
            " Loss :  0.3386595416359785\n",
            " Loss :  0.33641758463421806\n",
            " Loss :  0.33542635374599034\n",
            "\n",
            "Epoch: 2\n",
            "Loss :  0.3453124165534973\n",
            "Loss :  0.3395089100707661\n",
            "Loss :  0.3359364285355523\n",
            "Loss :  0.33559507323849586\n",
            "Loss :  0.33323589912274987\n",
            "Loss :  0.33278703689575195\n",
            "Loss :  0.3289629880522118\n",
            "Loss :  0.32678995669727595\n",
            "Loss :  0.32343184727209584\n",
            "Loss :  0.32060641115838356\n",
            "Loss :  0.3180653718438479\n",
            "Loss :  0.3157094601575319\n",
            "Loss :  0.3135690538843801\n",
            "Loss :  0.3123232349184633\n",
            "Loss :  0.31033879666463704\n",
            "Loss :  0.3090059044740058\n",
            "Loss :  0.30723855778667497\n",
            "Loss :  0.30615733546471735\n",
            "Loss :  0.30530184449741193\n",
            "Loss :  0.3039969348626611\n",
            "Loss :  0.3027485862596711\n",
            "Loss :  0.30187874192027686\n",
            "Loss :  0.3004751420533495\n",
            "Loss :  0.2992846324588313\n",
            "Loss :  0.29758340893197355\n",
            "Loss :  0.29689817373971067\n",
            "Loss :  0.295948617131774\n",
            "Loss :  0.2945846964732307\n",
            "Loss :  0.2932850057226059\n",
            "Loss :  0.2925554873914653\n",
            "Loss :  0.29147096908963793\n",
            "Loss :  0.2903700003765787\n",
            "Loss :  0.2892078389167043\n",
            "Loss :  0.2882967150553476\n",
            "Loss :  0.2874440227872815\n",
            "Loss :  0.2865203149043597\n",
            "Loss :  0.28566781999001545\n",
            "Loss :  0.28465449532890574\n",
            "Loss :  0.28340626944081365\n",
            "Loss :  0.282232231381909\n",
            "Loss :  0.2812218281470629\n",
            "Loss :  0.2803345363578077\n",
            "Loss :  0.2795526964364312\n",
            "Loss :  0.2785109686395132\n",
            "Loss :  0.277784326280596\n",
            "Loss :  0.27703229719545786\n",
            "Loss :  0.27627762644642606\n",
            "Loss :  0.2753514422226357\n",
            "Loss :  0.27471211986333566\n",
            "Loss :  0.2736089423511276\n",
            "Validation: \n",
            " Loss :  0.20474176108837128\n",
            " Loss :  0.22717477026439847\n",
            " Loss :  0.22691406709391895\n",
            " Loss :  0.22566280233078315\n",
            " Loss :  0.22603972053822177\n",
            "\n",
            "Epoch: 3\n",
            "Loss :  0.2517645061016083\n",
            "Loss :  0.22620063613761554\n",
            "Loss :  0.23045008948871068\n",
            "Loss :  0.22976429039432156\n",
            "Loss :  0.2289476885301311\n",
            "Loss :  0.22875907315927393\n",
            "Loss :  0.22587587330185\n",
            "Loss :  0.22627878126124262\n",
            "Loss :  0.22464136834497805\n",
            "Loss :  0.2226736226252147\n",
            "Loss :  0.22040974695493679\n",
            "Loss :  0.2191299000033387\n",
            "Loss :  0.21863673528856484\n",
            "Loss :  0.21862557302904492\n",
            "Loss :  0.2181914688636225\n",
            "Loss :  0.21798042124075606\n",
            "Loss :  0.2171848788394691\n",
            "Loss :  0.2169962457397528\n",
            "Loss :  0.21721155653342358\n",
            "Loss :  0.2165050810856345\n",
            "Loss :  0.21610040821839327\n",
            "Loss :  0.2164073916973096\n",
            "Loss :  0.21600888696461243\n",
            "Loss :  0.2158858058772562\n",
            "Loss :  0.215392130044486\n",
            "Loss :  0.21514970111656948\n",
            "Loss :  0.21464041172316248\n",
            "Loss :  0.2141067688645472\n",
            "Loss :  0.21369533505940352\n",
            "Loss :  0.21372181694327352\n",
            "Loss :  0.213568601497384\n",
            "Loss :  0.21276638434055917\n",
            "Loss :  0.21218505873115634\n",
            "Loss :  0.21164354465879343\n",
            "Loss :  0.21120504619788563\n",
            "Loss :  0.21093432074598437\n",
            "Loss :  0.21070822251965796\n",
            "Loss :  0.2101616445738029\n",
            "Loss :  0.20954788634626884\n",
            "Loss :  0.2090496803488573\n",
            "Loss :  0.2085881682108168\n",
            "Loss :  0.20824384174497748\n",
            "Loss :  0.20806160063047024\n",
            "Loss :  0.20756165617579928\n",
            "Loss :  0.2072549958348004\n",
            "Loss :  0.20698499643221135\n",
            "Loss :  0.2066955699256082\n",
            "Loss :  0.20638451445254552\n",
            "Loss :  0.20625571853901392\n",
            "Loss :  0.2056330046682882\n",
            "Validation: \n",
            " Loss :  0.16627654433250427\n",
            " Loss :  0.17772453881445385\n",
            " Loss :  0.1773947651793317\n",
            " Loss :  0.17455215800981053\n",
            " Loss :  0.17496148506064474\n",
            "\n",
            "Epoch: 4\n",
            "Loss :  0.21935003995895386\n",
            "Loss :  0.1865910440683365\n",
            "Loss :  0.18488500444662004\n",
            "Loss :  0.18518177203593716\n",
            "Loss :  0.18538091567958273\n",
            "Loss :  0.1855055888493856\n",
            "Loss :  0.18343696071476231\n",
            "Loss :  0.18322345383570227\n",
            "Loss :  0.1828488924621064\n",
            "Loss :  0.18176368877782925\n",
            "Loss :  0.18060693572653402\n",
            "Loss :  0.1795629073639174\n",
            "Loss :  0.1792623013996881\n",
            "Loss :  0.17950734621240894\n",
            "Loss :  0.1788713018944923\n",
            "Loss :  0.1789802455941573\n",
            "Loss :  0.17889064689230474\n",
            "Loss :  0.17880696624691722\n",
            "Loss :  0.17884602286539025\n",
            "Loss :  0.17856750032664592\n",
            "Loss :  0.17821484136937268\n",
            "Loss :  0.17833745204159435\n",
            "Loss :  0.17827772231123565\n",
            "Loss :  0.17806470994051402\n",
            "Loss :  0.17770380721547296\n",
            "Loss :  0.17773440438675214\n",
            "Loss :  0.17753417281812178\n",
            "Loss :  0.17732164065776276\n",
            "Loss :  0.1770701704818583\n",
            "Loss :  0.17712825413831731\n",
            "Loss :  0.17713158095001777\n",
            "Loss :  0.17660198549940656\n",
            "Loss :  0.17630622329370255\n",
            "Loss :  0.1762032539012569\n",
            "Loss :  0.17605535266511252\n",
            "Loss :  0.17594606817787528\n",
            "Loss :  0.17578614992283056\n",
            "Loss :  0.1754848935372746\n",
            "Loss :  0.1751738318542796\n",
            "Loss :  0.1746587021957578\n",
            "Loss :  0.1745346425254446\n",
            "Loss :  0.1744564466740383\n",
            "Loss :  0.17427110441931637\n",
            "Loss :  0.1739059499618349\n",
            "Loss :  0.17376816992451544\n",
            "Loss :  0.17365499243900148\n",
            "Loss :  0.17347887446073545\n",
            "Loss :  0.17324877070013883\n",
            "Loss :  0.17322360428356084\n",
            "Loss :  0.1727492909749511\n",
            "Validation: \n",
            " Loss :  0.1424711048603058\n",
            " Loss :  0.15739396924064272\n",
            " Loss :  0.15585817687395143\n",
            " Loss :  0.1532519733319517\n",
            " Loss :  0.1537072899532907\n",
            "\n",
            "Epoch: 5\n",
            "Loss :  0.16409868001937866\n",
            "Loss :  0.16059130836616864\n",
            "Loss :  0.15991567217168354\n",
            "Loss :  0.1606549471616745\n",
            "Loss :  0.16112094245305875\n",
            "Loss :  0.16101580975102445\n",
            "Loss :  0.15973594638167835\n",
            "Loss :  0.15948726908421854\n",
            "Loss :  0.1595777330207236\n",
            "Loss :  0.15900212215198264\n",
            "Loss :  0.15725805979258944\n",
            "Loss :  0.15625733841915387\n",
            "Loss :  0.1559887137417951\n",
            "Loss :  0.1567234058530276\n",
            "Loss :  0.15609430088430432\n",
            "Loss :  0.15579261038674425\n",
            "Loss :  0.15539481083613746\n",
            "Loss :  0.15553669017135052\n",
            "Loss :  0.15571787743443283\n",
            "Loss :  0.15529670285460836\n",
            "Loss :  0.15508080069995045\n",
            "Loss :  0.15516545662382766\n",
            "Loss :  0.155306575592287\n",
            "Loss :  0.15562725802520652\n",
            "Loss :  0.15557571571644907\n",
            "Loss :  0.1555275253921866\n",
            "Loss :  0.15535833050008022\n",
            "Loss :  0.1550825900807152\n",
            "Loss :  0.15501729682671217\n",
            "Loss :  0.15522647620886051\n",
            "Loss :  0.15512931802344085\n",
            "Loss :  0.15482126703407986\n",
            "Loss :  0.15475051541380422\n",
            "Loss :  0.154660213637388\n",
            "Loss :  0.15453173397136225\n",
            "Loss :  0.15447380750352502\n",
            "Loss :  0.15429217867821537\n",
            "Loss :  0.15411954983627058\n",
            "Loss :  0.1539685192305272\n",
            "Loss :  0.153705810189552\n",
            "Loss :  0.15372302072898408\n",
            "Loss :  0.15370737063333645\n",
            "Loss :  0.15374057513447104\n",
            "Loss :  0.15354740464078853\n",
            "Loss :  0.15349682060634198\n",
            "Loss :  0.15351899754578152\n",
            "Loss :  0.15338837647903508\n",
            "Loss :  0.15328124137061416\n",
            "Loss :  0.15321606837538324\n",
            "Loss :  0.1529051237976721\n",
            "Validation: \n",
            " Loss :  0.13179747760295868\n",
            " Loss :  0.13752470449322746\n",
            " Loss :  0.1359674525333614\n",
            " Loss :  0.1341034897038194\n",
            " Loss :  0.13475935492250654\n",
            "\n",
            "Epoch: 6\n",
            "Loss :  0.14827799797058105\n",
            "Loss :  0.1437982056628574\n",
            "Loss :  0.14505051111891157\n",
            "Loss :  0.14470099994251806\n",
            "Loss :  0.1451499385804665\n",
            "Loss :  0.14495146230739706\n",
            "Loss :  0.1436963276784928\n",
            "Loss :  0.14316638831941175\n",
            "Loss :  0.1431932766680364\n",
            "Loss :  0.14282486080140858\n",
            "Loss :  0.14173391284328876\n",
            "Loss :  0.14132884225329836\n",
            "Loss :  0.14120402174793983\n",
            "Loss :  0.14162348898995014\n",
            "Loss :  0.14117331147616638\n",
            "Loss :  0.141192353916484\n",
            "Loss :  0.1407812335383818\n",
            "Loss :  0.1409287738956903\n",
            "Loss :  0.1412605203235347\n",
            "Loss :  0.14111157327266263\n",
            "Loss :  0.14082104560747669\n",
            "Loss :  0.14094954672582907\n",
            "Loss :  0.1410381220179985\n",
            "Loss :  0.14112157387521876\n",
            "Loss :  0.14101073065486686\n",
            "Loss :  0.14107714504359728\n",
            "Loss :  0.14096271143905048\n",
            "Loss :  0.14076785721242208\n",
            "Loss :  0.14065795031200523\n",
            "Loss :  0.14096498125812032\n",
            "Loss :  0.14109533745486078\n",
            "Loss :  0.14073718190672313\n",
            "Loss :  0.14060810932489198\n",
            "Loss :  0.14054650241604744\n",
            "Loss :  0.1404488789808016\n",
            "Loss :  0.140471987192787\n",
            "Loss :  0.14019374103592375\n",
            "Loss :  0.14004386288698792\n",
            "Loss :  0.13985743005992235\n",
            "Loss :  0.13959431244284295\n",
            "Loss :  0.13952624935313057\n",
            "Loss :  0.13962376665169884\n",
            "Loss :  0.13967574536446437\n",
            "Loss :  0.13941064003310458\n",
            "Loss :  0.13936562224580587\n",
            "Loss :  0.13930684503192647\n",
            "Loss :  0.13917868626415084\n",
            "Loss :  0.1391327909002132\n",
            "Loss :  0.13912163958294227\n",
            "Loss :  0.13885151684405606\n",
            "Validation: \n",
            " Loss :  0.12932391464710236\n",
            " Loss :  0.1358499512785957\n",
            " Loss :  0.1348891563531829\n",
            " Loss :  0.13253659925988462\n",
            " Loss :  0.1327569474592621\n",
            "\n",
            "Epoch: 7\n",
            "Loss :  0.13597576320171356\n",
            "Loss :  0.13024542412974618\n",
            "Loss :  0.1310151606088593\n",
            "Loss :  0.1325588291210513\n",
            "Loss :  0.13295839090899722\n",
            "Loss :  0.13311483637959348\n",
            "Loss :  0.13298852558507293\n",
            "Loss :  0.13329672383170732\n",
            "Loss :  0.13322548669429474\n",
            "Loss :  0.1323074358668956\n",
            "Loss :  0.13145954921694086\n",
            "Loss :  0.1307041459792369\n",
            "Loss :  0.13060527393394264\n",
            "Loss :  0.13104587110854288\n",
            "Loss :  0.13077264791684792\n",
            "Loss :  0.13078807294368744\n",
            "Loss :  0.13065168455890988\n",
            "Loss :  0.13069212807026523\n",
            "Loss :  0.13092404871520416\n",
            "Loss :  0.1309756896414682\n",
            "Loss :  0.13089775360787093\n",
            "Loss :  0.13119302287485926\n",
            "Loss :  0.13126077098409514\n",
            "Loss :  0.13129143491064832\n",
            "Loss :  0.13099271915389293\n",
            "Loss :  0.13095081947832943\n",
            "Loss :  0.13080127223241375\n",
            "Loss :  0.13072508276608596\n",
            "Loss :  0.13078069374018292\n",
            "Loss :  0.13101862386329888\n",
            "Loss :  0.13103461728440566\n",
            "Loss :  0.13091330544070798\n",
            "Loss :  0.1308497591321342\n",
            "Loss :  0.13077299312612442\n",
            "Loss :  0.1307375231359012\n",
            "Loss :  0.1308022104522102\n",
            "Loss :  0.13077195013494042\n",
            "Loss :  0.13060384880339682\n",
            "Loss :  0.1304738837003395\n",
            "Loss :  0.1302210011369432\n",
            "Loss :  0.13026587384523003\n",
            "Loss :  0.13025994640559754\n",
            "Loss :  0.1302757352909113\n",
            "Loss :  0.12996512397328552\n",
            "Loss :  0.1299294952869145\n",
            "Loss :  0.12986884576716604\n",
            "Loss :  0.12971255697164516\n",
            "Loss :  0.12971358896567312\n",
            "Loss :  0.12967860611957224\n",
            "Loss :  0.12947430933565812\n",
            "Validation: \n",
            " Loss :  0.11391229182481766\n",
            " Loss :  0.11811758223034087\n",
            " Loss :  0.11731488148613674\n",
            " Loss :  0.11565628134813465\n",
            " Loss :  0.11571743238119432\n",
            "\n",
            "Epoch: 8\n",
            "Loss :  0.13323912024497986\n",
            "Loss :  0.12093311074105176\n",
            "Loss :  0.12363736295983904\n",
            "Loss :  0.12420976931048978\n",
            "Loss :  0.1248838327279905\n",
            "Loss :  0.12464200179366504\n",
            "Loss :  0.1241217249485313\n",
            "Loss :  0.12407046818817166\n",
            "Loss :  0.12398980585513292\n",
            "Loss :  0.1231826350584135\n",
            "Loss :  0.12256217489738276\n",
            "Loss :  0.12176832160702697\n",
            "Loss :  0.12150856095158365\n",
            "Loss :  0.12178630415947382\n",
            "Loss :  0.12138040881630377\n",
            "Loss :  0.12166766506551907\n",
            "Loss :  0.12170328431247925\n",
            "Loss :  0.12180798448491514\n",
            "Loss :  0.12191058324845457\n",
            "Loss :  0.12174774604942162\n",
            "Loss :  0.12191334848676748\n",
            "Loss :  0.1222617561924514\n",
            "Loss :  0.12245674522349198\n",
            "Loss :  0.12273382772872975\n",
            "Loss :  0.12272571902181104\n",
            "Loss :  0.12290545719197071\n",
            "Loss :  0.12280567292966148\n",
            "Loss :  0.12269491831534902\n",
            "Loss :  0.12268715805218314\n",
            "Loss :  0.12286042959726963\n",
            "Loss :  0.12291892635267834\n",
            "Loss :  0.12269085842121835\n",
            "Loss :  0.12262530005145296\n",
            "Loss :  0.12258157654021082\n",
            "Loss :  0.12250126668872022\n",
            "Loss :  0.12258568401859696\n",
            "Loss :  0.12248538529443609\n",
            "Loss :  0.12237706069515722\n",
            "Loss :  0.12231478357096044\n",
            "Loss :  0.1222008813143996\n",
            "Loss :  0.12222296809615042\n",
            "Loss :  0.1223637917761095\n",
            "Loss :  0.1224421995057063\n",
            "Loss :  0.12225772499208384\n",
            "Loss :  0.12222888803874013\n",
            "Loss :  0.12222995747814157\n",
            "Loss :  0.12210855443927575\n",
            "Loss :  0.12207993347743515\n",
            "Loss :  0.12215269366259882\n",
            "Loss :  0.12190466624534786\n",
            "Validation: \n",
            " Loss :  0.10476729273796082\n",
            " Loss :  0.10889840126037598\n",
            " Loss :  0.10774664362756217\n",
            " Loss :  0.10608530178910396\n",
            " Loss :  0.10633854733573066\n",
            "\n",
            "Epoch: 9\n",
            "Loss :  0.11848090589046478\n",
            "Loss :  0.11764514378525993\n",
            "Loss :  0.11629418141785122\n",
            "Loss :  0.11622757584817948\n",
            "Loss :  0.11681063782151152\n",
            "Loss :  0.11721871705616221\n",
            "Loss :  0.11653888872900947\n",
            "Loss :  0.11645869879235685\n",
            "Loss :  0.11685530684980346\n",
            "Loss :  0.11623121838975739\n",
            "Loss :  0.11554212750184654\n",
            "Loss :  0.11519978987472551\n",
            "Loss :  0.11500131443512342\n",
            "Loss :  0.11555326275015605\n",
            "Loss :  0.11546383206303237\n",
            "Loss :  0.11554630337566729\n",
            "Loss :  0.11532654428148861\n",
            "Loss :  0.11561876975479182\n",
            "Loss :  0.11570192428912905\n",
            "Loss :  0.1157693386233914\n",
            "Loss :  0.11583050870480228\n",
            "Loss :  0.11605520620589008\n",
            "Loss :  0.1162050449335737\n",
            "Loss :  0.11635008831689884\n",
            "Loss :  0.1163569376552748\n",
            "Loss :  0.11639445144460496\n",
            "Loss :  0.11641852266487034\n",
            "Loss :  0.11627164829481132\n",
            "Loss :  0.11635031508699431\n",
            "Loss :  0.11649125509757766\n",
            "Loss :  0.11643568230823821\n",
            "Loss :  0.11624590204459678\n",
            "Loss :  0.11626666956044432\n",
            "Loss :  0.11628627221119728\n",
            "Loss :  0.11637957278322264\n",
            "Loss :  0.11651989280583172\n",
            "Loss :  0.11652364119515855\n",
            "Loss :  0.11646460097112424\n",
            "Loss :  0.11637278575831511\n",
            "Loss :  0.11620346142355438\n",
            "Loss :  0.11618176847696304\n",
            "Loss :  0.11622809528071805\n",
            "Loss :  0.11631317985312672\n",
            "Loss :  0.1161998267827897\n",
            "Loss :  0.11612402235502019\n",
            "Loss :  0.11615894318552081\n",
            "Loss :  0.11609638000711185\n",
            "Loss :  0.11606722519655896\n",
            "Loss :  0.1160886118385995\n",
            "Loss :  0.11589666882381905\n",
            "Validation: \n",
            " Loss :  0.1098499670624733\n",
            " Loss :  0.1123905795670691\n",
            " Loss :  0.11113504520276697\n",
            " Loss :  0.10966949103797069\n",
            " Loss :  0.11011484099758996\n",
            "\n",
            "Epoch: 10\n",
            "Loss :  0.11409494280815125\n",
            "Loss :  0.11226073584773323\n",
            "Loss :  0.11172286704892204\n",
            "Loss :  0.11031674449482272\n",
            "Loss :  0.11094995133760499\n",
            "Loss :  0.11090283022791732\n",
            "Loss :  0.11046934933936009\n",
            "Loss :  0.11027780741872922\n",
            "Loss :  0.11036868908523041\n",
            "Loss :  0.11013228798305595\n",
            "Loss :  0.10974029001623097\n",
            "Loss :  0.10949227838097392\n",
            "Loss :  0.10939839617772536\n",
            "Loss :  0.10974175551237951\n",
            "Loss :  0.1098088504786187\n",
            "Loss :  0.10993059295297458\n",
            "Loss :  0.1098590945790273\n",
            "Loss :  0.11006738319557313\n",
            "Loss :  0.11034117231546844\n",
            "Loss :  0.11050956048734525\n",
            "Loss :  0.11044743497721592\n",
            "Loss :  0.11070024850667935\n",
            "Loss :  0.11087112391696256\n",
            "Loss :  0.11108262914341765\n",
            "Loss :  0.11107646587355008\n",
            "Loss :  0.1110851814250547\n",
            "Loss :  0.11106350795290935\n",
            "Loss :  0.11095808748829408\n",
            "Loss :  0.11108023392242045\n",
            "Loss :  0.11126602924976152\n",
            "Loss :  0.11131375599838174\n",
            "Loss :  0.1111408375754617\n",
            "Loss :  0.11110779828743028\n",
            "Loss :  0.11110063192077274\n",
            "Loss :  0.11112888065601025\n",
            "Loss :  0.11120156173267935\n",
            "Loss :  0.11116459800596053\n",
            "Loss :  0.11107027122996888\n",
            "Loss :  0.11091980715514481\n",
            "Loss :  0.11077159776559571\n",
            "Loss :  0.11071180838376209\n",
            "Loss :  0.11069293331055746\n",
            "Loss :  0.11079055716830025\n",
            "Loss :  0.11067678739604153\n",
            "Loss :  0.11067709990707385\n",
            "Loss :  0.11063761411114965\n",
            "Loss :  0.11053154866439402\n",
            "Loss :  0.11050880062858517\n",
            "Loss :  0.11051436347921773\n",
            "Loss :  0.1103187920241152\n",
            "Validation: \n",
            " Loss :  0.10402581095695496\n",
            " Loss :  0.10551298303263527\n",
            " Loss :  0.10396078864975673\n",
            " Loss :  0.10258800915030182\n",
            " Loss :  0.10261549993797585\n",
            "\n",
            "Epoch: 11\n",
            "Loss :  0.11310729384422302\n",
            "Loss :  0.10588148371739821\n",
            "Loss :  0.10528435451643807\n",
            "Loss :  0.10607156469937294\n",
            "Loss :  0.10681230938289224\n",
            "Loss :  0.10713371064733057\n",
            "Loss :  0.10679354152229965\n",
            "Loss :  0.10615617099782111\n",
            "Loss :  0.1060699564807209\n",
            "Loss :  0.10549907844800216\n",
            "Loss :  0.10500273477322984\n",
            "Loss :  0.10485859779087273\n",
            "Loss :  0.10513615195662522\n",
            "Loss :  0.10576163516699813\n",
            "Loss :  0.10574171808383144\n",
            "Loss :  0.10579938436580809\n",
            "Loss :  0.10590631880375169\n",
            "Loss :  0.10623265341011404\n",
            "Loss :  0.10638441289492075\n",
            "Loss :  0.10617539606481323\n",
            "Loss :  0.1061900947520982\n",
            "Loss :  0.10633663328196766\n",
            "Loss :  0.10655550763213256\n",
            "Loss :  0.10682613118773415\n",
            "Loss :  0.10678964813096889\n",
            "Loss :  0.10684307447942605\n",
            "Loss :  0.10682833060565122\n",
            "Loss :  0.10671664548975955\n",
            "Loss :  0.10667795987527989\n",
            "Loss :  0.10689149128714788\n",
            "Loss :  0.10686891964107653\n",
            "Loss :  0.10670722589807112\n",
            "Loss :  0.1067109492801803\n",
            "Loss :  0.10664896851579948\n",
            "Loss :  0.10669127753403179\n",
            "Loss :  0.10669305190401539\n",
            "Loss :  0.10663426190697255\n",
            "Loss :  0.10651551534464417\n",
            "Loss :  0.10641184026800742\n",
            "Loss :  0.10630530246612056\n",
            "Loss :  0.10630765564721124\n",
            "Loss :  0.10642584563751871\n",
            "Loss :  0.10657905873946509\n",
            "Loss :  0.1064469813090466\n",
            "Loss :  0.10647278899103065\n",
            "Loss :  0.10644439407965033\n",
            "Loss :  0.1064384292463677\n",
            "Loss :  0.10645947831578062\n",
            "Loss :  0.10641267460435938\n",
            "Loss :  0.10626733551928565\n",
            "Validation: \n",
            " Loss :  0.10214588791131973\n",
            " Loss :  0.10314772810254778\n",
            " Loss :  0.10236600804619672\n",
            " Loss :  0.1010466068983078\n",
            " Loss :  0.10096087637874815\n",
            "\n",
            "Epoch: 12\n",
            "Loss :  0.10775481909513474\n",
            "Loss :  0.10163313759998842\n",
            "Loss :  0.10047505689518792\n",
            "Loss :  0.10115605376420482\n",
            "Loss :  0.10178683316562234\n",
            "Loss :  0.10264534430176604\n",
            "Loss :  0.10192531970192174\n",
            "Loss :  0.1018570279571372\n",
            "Loss :  0.10182068395761797\n",
            "Loss :  0.10181486115350828\n",
            "Loss :  0.10143459632550136\n",
            "Loss :  0.10090581985475781\n",
            "Loss :  0.10098464405241091\n",
            "Loss :  0.10139187658561095\n",
            "Loss :  0.1012873278653368\n",
            "Loss :  0.10153810665110088\n",
            "Loss :  0.10150838809909288\n",
            "Loss :  0.10170034245092269\n",
            "Loss :  0.10189406952475974\n",
            "Loss :  0.10186187649896632\n",
            "Loss :  0.10179175336414309\n",
            "Loss :  0.10202278871248119\n",
            "Loss :  0.10221463374422686\n",
            "Loss :  0.10256991245142826\n",
            "Loss :  0.10250960982934074\n",
            "Loss :  0.1024437543168011\n",
            "Loss :  0.10242033549994801\n",
            "Loss :  0.10235327652679599\n",
            "Loss :  0.10214950899954793\n",
            "Loss :  0.10237380741900186\n",
            "Loss :  0.10239301775381018\n",
            "Loss :  0.10225597922345833\n",
            "Loss :  0.10218124421214761\n",
            "Loss :  0.10211974092030453\n",
            "Loss :  0.102184423559572\n",
            "Loss :  0.10232922823255898\n",
            "Loss :  0.102224241168215\n",
            "Loss :  0.10214648423892148\n",
            "Loss :  0.10206235978468822\n",
            "Loss :  0.10199990035856471\n",
            "Loss :  0.1020563067380627\n",
            "Loss :  0.10221178871364199\n",
            "Loss :  0.10230286318516787\n",
            "Loss :  0.10211794277147461\n",
            "Loss :  0.10211665164721526\n",
            "Loss :  0.10213663615699353\n",
            "Loss :  0.10210117503774399\n",
            "Loss :  0.1021156610482058\n",
            "Loss :  0.10217527823673712\n",
            "Loss :  0.10198778328305592\n",
            "Validation: \n",
            " Loss :  0.09562219679355621\n",
            " Loss :  0.09895302000499907\n",
            " Loss :  0.09793091856124925\n",
            " Loss :  0.09680646987723522\n",
            " Loss :  0.09715473348343814\n",
            "\n",
            "Epoch: 13\n",
            "Loss :  0.11168307811021805\n",
            "Loss :  0.09971408884633672\n",
            "Loss :  0.0996245387054625\n",
            "Loss :  0.10031460297684516\n",
            "Loss :  0.10047508358228498\n",
            "Loss :  0.10065568662157245\n",
            "Loss :  0.10022102613918117\n",
            "Loss :  0.09996584844841085\n",
            "Loss :  0.09968176585288695\n",
            "Loss :  0.09941382697977863\n",
            "Loss :  0.09880324342463276\n",
            "Loss :  0.09842540907698709\n",
            "Loss :  0.09823511473157188\n",
            "Loss :  0.09855724757409277\n",
            "Loss :  0.09850397642622603\n",
            "Loss :  0.09858364049369926\n",
            "Loss :  0.09864963813227896\n",
            "Loss :  0.09886323843608823\n",
            "Loss :  0.09903690815795192\n",
            "Loss :  0.09895876674127828\n",
            "Loss :  0.09889340497071471\n",
            "Loss :  0.09896148529380419\n",
            "Loss :  0.09903303786640254\n",
            "Loss :  0.09955390636281018\n",
            "Loss :  0.09955003409341164\n",
            "Loss :  0.0995820760133257\n",
            "Loss :  0.09955585719410943\n",
            "Loss :  0.09949006772569184\n",
            "Loss :  0.09954800006015445\n",
            "Loss :  0.09963791072368622\n",
            "Loss :  0.09978373529408065\n",
            "Loss :  0.0997165530872115\n",
            "Loss :  0.09976293466915594\n",
            "Loss :  0.09984920185138092\n",
            "Loss :  0.09992674624552825\n",
            "Loss :  0.09998768319686253\n",
            "Loss :  0.09993275700761341\n",
            "Loss :  0.099854648595229\n",
            "Loss :  0.09974873173502799\n",
            "Loss :  0.09970016310663174\n",
            "Loss :  0.0996820380414216\n",
            "Loss :  0.09970571537595016\n",
            "Loss :  0.0997364454130662\n",
            "Loss :  0.09959009692522323\n",
            "Loss :  0.09966408964795591\n",
            "Loss :  0.09966230446972497\n",
            "Loss :  0.09951638452983472\n",
            "Loss :  0.09954575431827781\n",
            "Loss :  0.09958491544094006\n",
            "Loss :  0.09945219098125602\n",
            "Validation: \n",
            " Loss :  0.09092225879430771\n",
            " Loss :  0.09507480050836291\n",
            " Loss :  0.09418621750139608\n",
            " Loss :  0.09285164832091722\n",
            " Loss :  0.09316060682873667\n",
            "\n",
            "Epoch: 14\n",
            "Loss :  0.11124810576438904\n",
            "Loss :  0.09388496184890921\n",
            "Loss :  0.09592266096955254\n",
            "Loss :  0.09621082054030511\n",
            "Loss :  0.0966532959080324\n",
            "Loss :  0.0965125803269592\n",
            "Loss :  0.09612272582093223\n",
            "Loss :  0.09620935249496514\n",
            "Loss :  0.09587118268748861\n",
            "Loss :  0.09576422557398513\n",
            "Loss :  0.09550566063954098\n",
            "Loss :  0.09512330772908958\n",
            "Loss :  0.09509326763882124\n",
            "Loss :  0.0953652385536951\n",
            "Loss :  0.09554611500484723\n",
            "Loss :  0.09589076757628397\n",
            "Loss :  0.09607901692575549\n",
            "Loss :  0.09619107611520945\n",
            "Loss :  0.09632400522080574\n",
            "Loss :  0.09634588752429522\n",
            "Loss :  0.09617332698990456\n",
            "Loss :  0.09631116246866389\n",
            "Loss :  0.09647696687759857\n",
            "Loss :  0.09689632467764281\n",
            "Loss :  0.09673408870256787\n",
            "Loss :  0.09667580491636854\n",
            "Loss :  0.09671599046823165\n",
            "Loss :  0.0966917954907646\n",
            "Loss :  0.09666356216333939\n",
            "Loss :  0.09689590365616317\n",
            "Loss :  0.09697736151194651\n",
            "Loss :  0.0968119351618543\n",
            "Loss :  0.09680002008643106\n",
            "Loss :  0.09682408391619017\n",
            "Loss :  0.09691015381879471\n",
            "Loss :  0.09711696706351391\n",
            "Loss :  0.09711187378273776\n",
            "Loss :  0.09711100673177493\n",
            "Loss :  0.09716451048772792\n",
            "Loss :  0.09698627267957038\n",
            "Loss :  0.09697991390329347\n",
            "Loss :  0.09697396761382003\n",
            "Loss :  0.0970011059362928\n",
            "Loss :  0.0969000633659053\n",
            "Loss :  0.09694696775301784\n",
            "Loss :  0.09690052470782907\n",
            "Loss :  0.09685736605311682\n",
            "Loss :  0.0968519702648661\n",
            "Loss :  0.09690856287670235\n",
            "Loss :  0.096815204562699\n",
            "Validation: \n",
            " Loss :  0.0954829752445221\n",
            " Loss :  0.09802990122919991\n",
            " Loss :  0.09702519799877958\n",
            " Loss :  0.09591804261578889\n",
            " Loss :  0.0962790267335044\n",
            "\n",
            "Epoch: 15\n",
            "Loss :  0.10563857853412628\n",
            "Loss :  0.09243084558031776\n",
            "Loss :  0.09097885943594433\n",
            "Loss :  0.09292769984852883\n",
            "Loss :  0.09344408642954943\n",
            "Loss :  0.09372395759119707\n",
            "Loss :  0.09294453739631371\n",
            "Loss :  0.09297954752831392\n",
            "Loss :  0.09303490265651986\n",
            "Loss :  0.0929893685893698\n",
            "Loss :  0.09254539558793058\n",
            "Loss :  0.09198852769426398\n",
            "Loss :  0.09213349099986809\n",
            "Loss :  0.09236280386912003\n",
            "Loss :  0.09221892803907394\n",
            "Loss :  0.09245222180292306\n",
            "Loss :  0.09249691623523369\n",
            "Loss :  0.09269187551492836\n",
            "Loss :  0.09288467705579094\n",
            "Loss :  0.09292062223737776\n",
            "Loss :  0.0928780406861756\n",
            "Loss :  0.09291132906743135\n",
            "Loss :  0.09308303693705554\n",
            "Loss :  0.09344024794958371\n",
            "Loss :  0.09341794230754939\n",
            "Loss :  0.09334352924173096\n",
            "Loss :  0.09338058990879534\n",
            "Loss :  0.09336303985536758\n",
            "Loss :  0.09331022648412562\n",
            "Loss :  0.09353537773041382\n",
            "Loss :  0.0936036929488182\n",
            "Loss :  0.09351790229223932\n",
            "Loss :  0.09353252716153582\n",
            "Loss :  0.09363745044905614\n",
            "Loss :  0.09372256005789179\n",
            "Loss :  0.09387541299107408\n",
            "Loss :  0.09379258343222399\n",
            "Loss :  0.09369622345642259\n",
            "Loss :  0.093611832776564\n",
            "Loss :  0.09352725854767557\n",
            "Loss :  0.09349365092052189\n",
            "Loss :  0.0935426661676734\n",
            "Loss :  0.09359488797301069\n",
            "Loss :  0.093507846208017\n",
            "Loss :  0.09351313767992721\n",
            "Loss :  0.09352269025308858\n",
            "Loss :  0.09345353493333639\n",
            "Loss :  0.09349534499227621\n",
            "Loss :  0.09350562442612995\n",
            "Loss :  0.0933491883992906\n",
            "Validation: \n",
            " Loss :  0.09083449840545654\n",
            " Loss :  0.09288446073021207\n",
            " Loss :  0.09233852494053724\n",
            " Loss :  0.09133500434824678\n",
            " Loss :  0.09149585093980954\n",
            "\n",
            "Epoch: 16\n",
            "Loss :  0.10070464760065079\n",
            "Loss :  0.09381381896409122\n",
            "Loss :  0.09175448190598261\n",
            "Loss :  0.0923921360123542\n",
            "Loss :  0.09241960252203592\n",
            "Loss :  0.092754554076522\n",
            "Loss :  0.09211906754091138\n",
            "Loss :  0.09191476207383921\n",
            "Loss :  0.09192634962591124\n",
            "Loss :  0.09176270311677849\n",
            "Loss :  0.09144776875134741\n",
            "Loss :  0.0911682292416289\n",
            "Loss :  0.09090330113064159\n",
            "Loss :  0.09101334254022773\n",
            "Loss :  0.09117659741471\n",
            "Loss :  0.09122555234179591\n",
            "Loss :  0.09132802731687238\n",
            "Loss :  0.09165008928169284\n",
            "Loss :  0.09187917304302447\n",
            "Loss :  0.09180035629353599\n",
            "Loss :  0.09181694074797986\n",
            "Loss :  0.09198496815576372\n",
            "Loss :  0.09200889162078701\n",
            "Loss :  0.09219853815449265\n",
            "Loss :  0.09212681202843971\n",
            "Loss :  0.09215625093515176\n",
            "Loss :  0.09207546571091217\n",
            "Loss :  0.0919113137735212\n",
            "Loss :  0.09196700789432084\n",
            "Loss :  0.09205304764697642\n",
            "Loss :  0.09211643857021268\n",
            "Loss :  0.09196859983865088\n",
            "Loss :  0.09194556222155087\n",
            "Loss :  0.09194239465311575\n",
            "Loss :  0.09195363386110826\n",
            "Loss :  0.09200515757259141\n",
            "Loss :  0.09196327007543348\n",
            "Loss :  0.09193856176742003\n",
            "Loss :  0.09195915480532985\n",
            "Loss :  0.09189918707779911\n",
            "Loss :  0.09192536377401424\n",
            "Loss :  0.09195791904581144\n",
            "Loss :  0.09200457111885882\n",
            "Loss :  0.09190753138604685\n",
            "Loss :  0.0919466492067389\n",
            "Loss :  0.09195746706092436\n",
            "Loss :  0.09188594790316973\n",
            "Loss :  0.09192361703217662\n",
            "Loss :  0.09196110636677415\n",
            "Loss :  0.09185994281424038\n",
            "Validation: \n",
            " Loss :  0.08358583599328995\n",
            " Loss :  0.08835762561786742\n",
            " Loss :  0.08734325192323546\n",
            " Loss :  0.08633270395583793\n",
            " Loss :  0.08646364068543469\n",
            "\n",
            "Epoch: 17\n",
            "Loss :  0.09448143094778061\n",
            "Loss :  0.08685297993096439\n",
            "Loss :  0.08747539598317373\n",
            "Loss :  0.08779562120476077\n",
            "Loss :  0.08863666998903925\n",
            "Loss :  0.0888843815408501\n",
            "Loss :  0.08874520525091985\n",
            "Loss :  0.08855634287629328\n",
            "Loss :  0.08881460148611187\n",
            "Loss :  0.08875358866138773\n",
            "Loss :  0.08866502337231494\n",
            "Loss :  0.08816408124324437\n",
            "Loss :  0.08814588336905171\n",
            "Loss :  0.08839513594640121\n",
            "Loss :  0.08832340243648976\n",
            "Loss :  0.08855013381566433\n",
            "Loss :  0.08865609718775898\n",
            "Loss :  0.08890403946589308\n",
            "Loss :  0.08876845191196842\n",
            "Loss :  0.08862543675599922\n",
            "Loss :  0.08868302126873785\n",
            "Loss :  0.08873828814775458\n",
            "Loss :  0.0888299470708381\n",
            "Loss :  0.08906337744616843\n",
            "Loss :  0.08911739521625131\n",
            "Loss :  0.0890576815996987\n",
            "Loss :  0.08902251001062064\n",
            "Loss :  0.08887654595159517\n",
            "Loss :  0.08895802924747569\n",
            "Loss :  0.08912248038958848\n",
            "Loss :  0.0892266101664879\n",
            "Loss :  0.08917457763692574\n",
            "Loss :  0.08916538603302103\n",
            "Loss :  0.0892320925150393\n",
            "Loss :  0.08930274767697381\n",
            "Loss :  0.08945907849786627\n",
            "Loss :  0.08942193279966423\n",
            "Loss :  0.08934915840947082\n",
            "Loss :  0.08927876240234049\n",
            "Loss :  0.08920512291247887\n",
            "Loss :  0.08920118090071881\n",
            "Loss :  0.08925769287739357\n",
            "Loss :  0.08930174581653431\n",
            "Loss :  0.08920465516933432\n",
            "Loss :  0.08923727152298908\n",
            "Loss :  0.08925901832443119\n",
            "Loss :  0.08920252250422625\n",
            "Loss :  0.08924308327479474\n",
            "Loss :  0.08935196319761494\n",
            "Loss :  0.08925744466286327\n",
            "Validation: \n",
            " Loss :  0.08796583861112595\n",
            " Loss :  0.09163175984507516\n",
            " Loss :  0.09164272793909399\n",
            " Loss :  0.09061363107356869\n",
            " Loss :  0.09078306649570111\n",
            "\n",
            "Epoch: 18\n",
            "Loss :  0.08913469314575195\n",
            "Loss :  0.08605759319933978\n",
            "Loss :  0.0850853721300761\n",
            "Loss :  0.08583508359809075\n",
            "Loss :  0.08648070529466723\n",
            "Loss :  0.0868408649283297\n",
            "Loss :  0.0864842068709311\n",
            "Loss :  0.08626423975531484\n",
            "Loss :  0.0864046636370965\n",
            "Loss :  0.08635440542475208\n",
            "Loss :  0.08588355337039079\n",
            "Loss :  0.08563203002149994\n",
            "Loss :  0.0854298753186691\n",
            "Loss :  0.08589012158736017\n",
            "Loss :  0.08596993005233454\n",
            "Loss :  0.08641440100622493\n",
            "Loss :  0.08646989484196124\n",
            "Loss :  0.08677706531962456\n",
            "Loss :  0.08700169779319131\n",
            "Loss :  0.08690038541848746\n",
            "Loss :  0.08700955536828112\n",
            "Loss :  0.08721214818869721\n",
            "Loss :  0.08732228368790441\n",
            "Loss :  0.08766844097799037\n",
            "Loss :  0.08763899202787036\n",
            "Loss :  0.08766412114598361\n",
            "Loss :  0.08772923075147972\n",
            "Loss :  0.08760262143238004\n",
            "Loss :  0.08757298333683047\n",
            "Loss :  0.08770985947441809\n",
            "Loss :  0.08767154892021635\n",
            "Loss :  0.08761623609583477\n",
            "Loss :  0.0875746779306284\n",
            "Loss :  0.087673927249325\n",
            "Loss :  0.08765277303646038\n",
            "Loss :  0.08778607870778467\n",
            "Loss :  0.08774621281102093\n",
            "Loss :  0.08772459543859862\n",
            "Loss :  0.0876604150247386\n",
            "Loss :  0.08760468547453966\n",
            "Loss :  0.08754787092419931\n",
            "Loss :  0.08758490092127863\n",
            "Loss :  0.0876307654196746\n",
            "Loss :  0.08744743668631999\n",
            "Loss :  0.08750491173697167\n",
            "Loss :  0.08757784990738342\n",
            "Loss :  0.08754279189733001\n",
            "Loss :  0.08762643895964208\n",
            "Loss :  0.08765946942034977\n",
            "Loss :  0.08757629482238696\n",
            "Validation: \n",
            " Loss :  0.08501657098531723\n",
            " Loss :  0.0840288024573099\n",
            " Loss :  0.08307420407853476\n",
            " Loss :  0.0818054698041228\n",
            " Loss :  0.08210221567639599\n",
            "\n",
            "Epoch: 19\n",
            "Loss :  0.09487462788820267\n",
            "Loss :  0.08307448300448331\n",
            "Loss :  0.08318474995238441\n",
            "Loss :  0.08408753670031024\n",
            "Loss :  0.08516337359096945\n",
            "Loss :  0.08555452014301337\n",
            "Loss :  0.08533364851943782\n",
            "Loss :  0.08564977494763656\n",
            "Loss :  0.08575439775063667\n",
            "Loss :  0.0855621554694333\n",
            "Loss :  0.08520646135110667\n",
            "Loss :  0.08474774846622536\n",
            "Loss :  0.08442901462809113\n",
            "Loss :  0.0850257849079052\n",
            "Loss :  0.0851150837244717\n",
            "Loss :  0.08539066054173652\n",
            "Loss :  0.08559086789255557\n",
            "Loss :  0.08571281458376444\n",
            "Loss :  0.08574856126176718\n",
            "Loss :  0.08565944522931314\n",
            "Loss :  0.08575755669109857\n",
            "Loss :  0.0858967904160373\n",
            "Loss :  0.08594038570089038\n",
            "Loss :  0.08613518567441346\n",
            "Loss :  0.08617889998612067\n",
            "Loss :  0.0861472951107291\n",
            "Loss :  0.08621491734438015\n",
            "Loss :  0.08613820371262702\n",
            "Loss :  0.08612358996982676\n",
            "Loss :  0.08619180536761727\n",
            "Loss :  0.08627747345802396\n",
            "Loss :  0.08620027629987986\n",
            "Loss :  0.08614853479409143\n",
            "Loss :  0.08618591361175491\n",
            "Loss :  0.0862167696755303\n",
            "Loss :  0.08635260776067391\n",
            "Loss :  0.08634762765215374\n",
            "Loss :  0.08634827237807194\n",
            "Loss :  0.08628551941609446\n",
            "Loss :  0.08620059703622023\n",
            "Loss :  0.0861244111732949\n",
            "Loss :  0.08617936832481347\n",
            "Loss :  0.08620051535249039\n",
            "Loss :  0.08604173848012207\n",
            "Loss :  0.08602332969168687\n",
            "Loss :  0.08603264665656501\n",
            "Loss :  0.08603160466328101\n",
            "Loss :  0.08599803527132974\n",
            "Loss :  0.08604417487266405\n",
            "Loss :  0.08595607123221982\n",
            "Validation: \n",
            " Loss :  0.08526955544948578\n",
            " Loss :  0.0867279622526396\n",
            " Loss :  0.0864815304918987\n",
            " Loss :  0.08541325736241262\n",
            " Loss :  0.08580832615678693\n",
            "\n",
            "Epoch: 20\n",
            "Loss :  0.09164559841156006\n",
            "Loss :  0.08422259783202951\n",
            "Loss :  0.0844541419120062\n",
            "Loss :  0.0849248291023316\n",
            "Loss :  0.08464042587978084\n",
            "Loss :  0.08435756625498042\n",
            "Loss :  0.08421914572598505\n",
            "Loss :  0.08399781301407747\n",
            "Loss :  0.08412259725140936\n",
            "Loss :  0.08417897728773263\n",
            "Loss :  0.08389661129158323\n",
            "Loss :  0.08357103457590481\n",
            "Loss :  0.08349114933536073\n",
            "Loss :  0.08394641986557545\n",
            "Loss :  0.08383702132718783\n",
            "Loss :  0.08383160010473617\n",
            "Loss :  0.08400818733324916\n",
            "Loss :  0.08420394452517493\n",
            "Loss :  0.08435618543987117\n",
            "Loss :  0.08424234909068852\n",
            "Loss :  0.08407326865552077\n",
            "Loss :  0.08412664447209281\n",
            "Loss :  0.08415396761031173\n",
            "Loss :  0.08440797211545886\n",
            "Loss :  0.08454722274264855\n",
            "Loss :  0.0844014036762287\n",
            "Loss :  0.0843668537521271\n",
            "Loss :  0.0842446797008444\n",
            "Loss :  0.0842696285120533\n",
            "Loss :  0.08445555081789437\n",
            "Loss :  0.08439188878797614\n",
            "Loss :  0.08436537232142169\n",
            "Loss :  0.08436920036593702\n",
            "Loss :  0.08449414275654132\n",
            "Loss :  0.08459251186103066\n",
            "Loss :  0.08476196736776591\n",
            "Loss :  0.08473159044650783\n",
            "Loss :  0.08464480625249626\n",
            "Loss :  0.08461692712203724\n",
            "Loss :  0.08455894502532452\n",
            "Loss :  0.08454061468640468\n",
            "Loss :  0.08454867218532701\n",
            "Loss :  0.0845451579913674\n",
            "Loss :  0.08446374613350893\n",
            "Loss :  0.08451219494380648\n",
            "Loss :  0.08445570890496416\n",
            "Loss :  0.08439941510331864\n",
            "Loss :  0.08447021709442645\n",
            "Loss :  0.08446889541067354\n",
            "Loss :  0.084400371141929\n",
            "Validation: \n",
            " Loss :  0.08863324671983719\n",
            " Loss :  0.09235123580410368\n",
            " Loss :  0.0925036403464108\n",
            " Loss :  0.09163288735463972\n",
            " Loss :  0.09214266132057448\n",
            "\n",
            "Epoch: 21\n",
            "Loss :  0.08409183472394943\n",
            "Loss :  0.08177218247543681\n",
            "Loss :  0.08128265539805095\n",
            "Loss :  0.08263157812818404\n",
            "Loss :  0.08325800208783732\n",
            "Loss :  0.08271752750756693\n",
            "Loss :  0.08278821921739422\n",
            "Loss :  0.0828063945535203\n",
            "Loss :  0.08313064810670452\n",
            "Loss :  0.08272481603281838\n",
            "Loss :  0.08255010902291478\n",
            "Loss :  0.08224999931481508\n",
            "Loss :  0.08195336760321925\n",
            "Loss :  0.08245701740943749\n",
            "Loss :  0.0825843907205771\n",
            "Loss :  0.08286435482715139\n",
            "Loss :  0.08297857484832313\n",
            "Loss :  0.08316500049236922\n",
            "Loss :  0.08326777358904727\n",
            "Loss :  0.08313113271565961\n",
            "Loss :  0.08323253583700503\n",
            "Loss :  0.08336601593483116\n",
            "Loss :  0.08337038907125525\n",
            "Loss :  0.08361872914549592\n",
            "Loss :  0.08365254765724245\n",
            "Loss :  0.08353653977591678\n",
            "Loss :  0.08350752776495798\n",
            "Loss :  0.08339560914413516\n",
            "Loss :  0.08334910294233268\n",
            "Loss :  0.08344883182409293\n",
            "Loss :  0.08348141717059272\n",
            "Loss :  0.08338538993305715\n",
            "Loss :  0.08337637048644068\n",
            "Loss :  0.08341215523888337\n",
            "Loss :  0.08350893900978251\n",
            "Loss :  0.08365268502225223\n",
            "Loss :  0.08362476917762836\n",
            "Loss :  0.08355559117469505\n",
            "Loss :  0.08347064485465448\n",
            "Loss :  0.08342344060425869\n",
            "Loss :  0.08337346592597533\n",
            "Loss :  0.0833942148553484\n",
            "Loss :  0.0833995970864194\n",
            "Loss :  0.08326224498710057\n",
            "Loss :  0.08325270943495693\n",
            "Loss :  0.08327090138870967\n",
            "Loss :  0.08323401589261735\n",
            "Loss :  0.08323006760605835\n",
            "Loss :  0.08328668937365875\n",
            "Loss :  0.08318100344624879\n",
            "Validation: \n",
            " Loss :  0.08460782468318939\n",
            " Loss :  0.08787946651379268\n",
            " Loss :  0.0868438382337733\n",
            " Loss :  0.08627789638570098\n",
            " Loss :  0.08656421036999902\n",
            "\n",
            "Epoch: 22\n",
            "Loss :  0.0891968160867691\n",
            "Loss :  0.08325265077027408\n",
            "Loss :  0.08202553966215678\n",
            "Loss :  0.08155435684227175\n",
            "Loss :  0.08097003291292888\n",
            "Loss :  0.08103123876978369\n",
            "Loss :  0.08062809644663921\n",
            "Loss :  0.08070691517541106\n",
            "Loss :  0.08035994468279826\n",
            "Loss :  0.08024732703036004\n",
            "Loss :  0.0797878687482069\n",
            "Loss :  0.0796239174701072\n",
            "Loss :  0.0794066307096442\n",
            "Loss :  0.07980201939362606\n",
            "Loss :  0.07992671979657302\n",
            "Loss :  0.08027726959511144\n",
            "Loss :  0.08057224537645068\n",
            "Loss :  0.08074600070889233\n",
            "Loss :  0.08097416870501818\n",
            "Loss :  0.08086093156281567\n",
            "Loss :  0.08090394286818765\n",
            "Loss :  0.08091302060685451\n",
            "Loss :  0.08106844794696273\n",
            "Loss :  0.08141956781541114\n",
            "Loss :  0.08146241186689045\n",
            "Loss :  0.08130601277033171\n",
            "Loss :  0.08127419073919684\n",
            "Loss :  0.08123170007088967\n",
            "Loss :  0.0813270445077869\n",
            "Loss :  0.0814178162526429\n",
            "Loss :  0.08150584072468685\n",
            "Loss :  0.08150562579321324\n",
            "Loss :  0.08144506942074618\n",
            "Loss :  0.08153532994657844\n",
            "Loss :  0.08157712203666262\n",
            "Loss :  0.08167997435626821\n",
            "Loss :  0.08161187607413184\n",
            "Loss :  0.08156380109549212\n",
            "Loss :  0.0815295374260487\n",
            "Loss :  0.08144444204352395\n",
            "Loss :  0.08151732587680555\n",
            "Loss :  0.08162639830306789\n",
            "Loss :  0.08168861924752487\n",
            "Loss :  0.08157325760740139\n",
            "Loss :  0.08161394196708187\n",
            "Loss :  0.08161157364657608\n",
            "Loss :  0.0815195909776036\n",
            "Loss :  0.08158508601208908\n",
            "Loss :  0.08161426882362167\n",
            "Loss :  0.08154644521215054\n",
            "Validation: \n",
            " Loss :  0.08013299852609634\n",
            " Loss :  0.08723082961071105\n",
            " Loss :  0.08698170141475957\n",
            " Loss :  0.08625415297316723\n",
            " Loss :  0.08660691792582288\n",
            "\n",
            "Epoch: 23\n",
            "Loss :  0.08974447101354599\n",
            "Loss :  0.07956567677584561\n",
            "Loss :  0.07843526239906039\n",
            "Loss :  0.07980941524428706\n",
            "Loss :  0.08004729704159062\n",
            "Loss :  0.08030186681186452\n",
            "Loss :  0.07949420431109726\n",
            "Loss :  0.07938404097943239\n",
            "Loss :  0.07953670004635681\n",
            "Loss :  0.07947396405123092\n",
            "Loss :  0.07912211968462066\n",
            "Loss :  0.07879641430603491\n",
            "Loss :  0.07852531070551597\n",
            "Loss :  0.0788868413740442\n",
            "Loss :  0.07897084486399981\n",
            "Loss :  0.07925001674929992\n",
            "Loss :  0.07944450064660599\n",
            "Loss :  0.0798069900500844\n",
            "Loss :  0.07983478158712387\n",
            "Loss :  0.07980583210266073\n",
            "Loss :  0.0797968287002388\n",
            "Loss :  0.0799212282373442\n",
            "Loss :  0.08009457652250566\n",
            "Loss :  0.08034923256604702\n",
            "Loss :  0.08037107617157624\n",
            "Loss :  0.08034252621738085\n",
            "Loss :  0.08029597923445062\n",
            "Loss :  0.08020273527316062\n",
            "Loss :  0.08034479000284155\n",
            "Loss :  0.08041988830386158\n",
            "Loss :  0.08042869862923986\n",
            "Loss :  0.08039323726842641\n",
            "Loss :  0.08039360449321545\n",
            "Loss :  0.08048693243740548\n",
            "Loss :  0.08052213085798923\n",
            "Loss :  0.08065154087169897\n",
            "Loss :  0.08067298152192477\n",
            "Loss :  0.08063938743823944\n",
            "Loss :  0.08060053201133185\n",
            "Loss :  0.08052656893878032\n",
            "Loss :  0.08049764527830103\n",
            "Loss :  0.08052365515390161\n",
            "Loss :  0.08056448008024494\n",
            "Loss :  0.08043265625350436\n",
            "Loss :  0.08050569509263752\n",
            "Loss :  0.0805213317985413\n",
            "Loss :  0.08048768571846655\n",
            "Loss :  0.08042986011587384\n",
            "Loss :  0.0804586164467424\n",
            "Loss :  0.08039259439421526\n",
            "Validation: \n",
            " Loss :  0.0762125626206398\n",
            " Loss :  0.07926873720827557\n",
            " Loss :  0.07891941615721075\n",
            " Loss :  0.07833290210024255\n",
            " Loss :  0.07868301344138605\n",
            "\n",
            "Epoch: 24\n",
            "Loss :  0.08455126732587814\n",
            "Loss :  0.0788967643271793\n",
            "Loss :  0.07724528937112718\n",
            "Loss :  0.07803626430611457\n",
            "Loss :  0.0779679297673993\n",
            "Loss :  0.07820281196458667\n",
            "Loss :  0.07813157397704046\n",
            "Loss :  0.0780326077635859\n",
            "Loss :  0.07808296180065767\n",
            "Loss :  0.07795350286331805\n",
            "Loss :  0.07780831203897401\n",
            "Loss :  0.07754117740435643\n",
            "Loss :  0.07759376048795448\n",
            "Loss :  0.07791491347642346\n",
            "Loss :  0.07789868422856568\n",
            "Loss :  0.07819785187575991\n",
            "Loss :  0.07813905137851372\n",
            "Loss :  0.07843926779882253\n",
            "Loss :  0.07852879022531088\n",
            "Loss :  0.07854980366860384\n",
            "Loss :  0.07861878682131791\n",
            "Loss :  0.0787080863217042\n",
            "Loss :  0.07872490588221615\n",
            "Loss :  0.0789219963795695\n",
            "Loss :  0.07897612709103778\n",
            "Loss :  0.07890559991161186\n",
            "Loss :  0.07885699022661223\n",
            "Loss :  0.07873417225381105\n",
            "Loss :  0.07871796726332016\n",
            "Loss :  0.07884093906563991\n",
            "Loss :  0.07878784278401504\n",
            "Loss :  0.07878295116581717\n",
            "Loss :  0.07883921541807436\n",
            "Loss :  0.07888951370903373\n",
            "Loss :  0.07896926688833321\n",
            "Loss :  0.07906499137820681\n",
            "Loss :  0.07900203289747898\n",
            "Loss :  0.07898684095018957\n",
            "Loss :  0.0789591021777138\n",
            "Loss :  0.07889533418295024\n",
            "Loss :  0.07891630009745719\n",
            "Loss :  0.07891874405081835\n",
            "Loss :  0.07890735957727296\n",
            "Loss :  0.07878200243078086\n",
            "Loss :  0.07876536854946153\n",
            "Loss :  0.07878513019655867\n",
            "Loss :  0.07870575984426796\n",
            "Loss :  0.07873001980996688\n",
            "Loss :  0.07879166376318109\n",
            "Loss :  0.07868036069355283\n",
            "Validation: \n",
            " Loss :  0.08166925609111786\n",
            " Loss :  0.08407757466747648\n",
            " Loss :  0.08339841173189443\n",
            " Loss :  0.08265969987775458\n",
            " Loss :  0.08311525474727889\n",
            "\n",
            "Epoch: 25\n",
            "Loss :  0.08559553325176239\n",
            "Loss :  0.07578678564591841\n",
            "Loss :  0.07595315362725939\n",
            "Loss :  0.07643855218925784\n",
            "Loss :  0.07709665451107955\n",
            "Loss :  0.07715697279747795\n",
            "Loss :  0.07707599355060546\n",
            "Loss :  0.0773838576926312\n",
            "Loss :  0.0774732768351649\n",
            "Loss :  0.07744308945896862\n",
            "Loss :  0.07720459599306087\n",
            "Loss :  0.07691961667827658\n",
            "Loss :  0.07659765262125938\n",
            "Loss :  0.07685621580436029\n",
            "Loss :  0.07701701921879822\n",
            "Loss :  0.0771813764428066\n",
            "Loss :  0.07723175977235255\n",
            "Loss :  0.07750213632022428\n",
            "Loss :  0.07762642589639564\n",
            "Loss :  0.07759333793951578\n",
            "Loss :  0.0774590603599501\n",
            "Loss :  0.07759397173238591\n",
            "Loss :  0.07757522037665768\n",
            "Loss :  0.07776905360805007\n",
            "Loss :  0.07778012879285574\n",
            "Loss :  0.07776477101789528\n",
            "Loss :  0.07776205786914205\n",
            "Loss :  0.07764985569068866\n",
            "Loss :  0.07766285689806175\n",
            "Loss :  0.07774258883753184\n",
            "Loss :  0.07783362633267114\n",
            "Loss :  0.07789009737623466\n",
            "Loss :  0.07788983572309262\n",
            "Loss :  0.07792568812287466\n",
            "Loss :  0.07801675925419128\n",
            "Loss :  0.07817446518997181\n",
            "Loss :  0.07816487755058875\n",
            "Loss :  0.0781244698239144\n",
            "Loss :  0.07809606403738183\n",
            "Loss :  0.07802494290425345\n",
            "Loss :  0.07801756759794276\n",
            "Loss :  0.07796351585328724\n",
            "Loss :  0.0780374631728101\n",
            "Loss :  0.07800805344831915\n",
            "Loss :  0.07805675549470649\n",
            "Loss :  0.078071652364374\n",
            "Loss :  0.07797033864188609\n",
            "Loss :  0.0780120813020855\n",
            "Loss :  0.07805959964771281\n",
            "Loss :  0.07797260246336339\n",
            "Validation: \n",
            " Loss :  0.08386821299791336\n",
            " Loss :  0.08596976740019661\n",
            " Loss :  0.08505835856606321\n",
            " Loss :  0.08449612948738161\n",
            " Loss :  0.08500881014782706\n",
            "\n",
            "Epoch: 26\n",
            "Loss :  0.07582227140665054\n",
            "Loss :  0.07399629124186256\n",
            "Loss :  0.0742921130288215\n",
            "Loss :  0.07547304779291153\n",
            "Loss :  0.07611007043501226\n",
            "Loss :  0.07603286133677352\n",
            "Loss :  0.07583877248842208\n",
            "Loss :  0.0755783676786322\n",
            "Loss :  0.07570796390926396\n",
            "Loss :  0.07560150980294406\n",
            "Loss :  0.07528105569948064\n",
            "Loss :  0.07500982687279985\n",
            "Loss :  0.07497336506104667\n",
            "Loss :  0.07508598876590947\n",
            "Loss :  0.07516044853849614\n",
            "Loss :  0.07532184901616432\n",
            "Loss :  0.07547792815458701\n",
            "Loss :  0.07586992871865891\n",
            "Loss :  0.07593650350254544\n",
            "Loss :  0.07576264729674574\n",
            "Loss :  0.07584222928801579\n",
            "Loss :  0.07593841568271131\n",
            "Loss :  0.07600090079582654\n",
            "Loss :  0.07631782945487406\n",
            "Loss :  0.07646018721132358\n",
            "Loss :  0.0764924688524459\n",
            "Loss :  0.07643067953801247\n",
            "Loss :  0.07643828249615497\n",
            "Loss :  0.07645481633970322\n",
            "Loss :  0.07652833983558151\n",
            "Loss :  0.0765808227400843\n",
            "Loss :  0.07660003880883337\n",
            "Loss :  0.07659487270770414\n",
            "Loss :  0.07661323411407067\n",
            "Loss :  0.07670405111617007\n",
            "Loss :  0.07682967236918262\n",
            "Loss :  0.07676795892768289\n",
            "Loss :  0.0767355287532922\n",
            "Loss :  0.07669251937213845\n",
            "Loss :  0.07663905816843443\n",
            "Loss :  0.0766329388145794\n",
            "Loss :  0.07675767706258453\n",
            "Loss :  0.0767673961906705\n",
            "Loss :  0.0767215812392965\n",
            "Loss :  0.07674557036485802\n",
            "Loss :  0.07678254697050593\n",
            "Loss :  0.07671682659431549\n",
            "Loss :  0.0767554020204615\n",
            "Loss :  0.07684824866651249\n",
            "Loss :  0.07679703323333667\n",
            "Validation: \n",
            " Loss :  0.08294347673654556\n",
            " Loss :  0.08245222980067843\n",
            " Loss :  0.08233491985536204\n",
            " Loss :  0.08161912174498448\n",
            " Loss :  0.08167994307515061\n",
            "\n",
            "Epoch: 27\n",
            "Loss :  0.09296771883964539\n",
            "Loss :  0.07718444480137392\n",
            "Loss :  0.07580847293138504\n",
            "Loss :  0.07591409620738798\n",
            "Loss :  0.07631994402263223\n",
            "Loss :  0.07625163448791877\n",
            "Loss :  0.07600314463259744\n",
            "Loss :  0.07598577534228983\n",
            "Loss :  0.0760428473169421\n",
            "Loss :  0.07590765113031471\n",
            "Loss :  0.07566887582882796\n",
            "Loss :  0.07530116061638067\n",
            "Loss :  0.07520087007894989\n",
            "Loss :  0.07555294594236912\n",
            "Loss :  0.07557666079795107\n",
            "Loss :  0.07574701013154542\n",
            "Loss :  0.07586694124692715\n",
            "Loss :  0.0760656554709401\n",
            "Loss :  0.07609929971767394\n",
            "Loss :  0.07603735465029772\n",
            "Loss :  0.0759624342494343\n",
            "Loss :  0.0760270957945365\n",
            "Loss :  0.07612727860587215\n",
            "Loss :  0.07634845577599682\n",
            "Loss :  0.07635697597228146\n",
            "Loss :  0.07627842315580266\n",
            "Loss :  0.07622763555910853\n",
            "Loss :  0.07612665408596782\n",
            "Loss :  0.07621977344367428\n",
            "Loss :  0.0763290855406281\n",
            "Loss :  0.07636897722599514\n",
            "Loss :  0.07623374311199525\n",
            "Loss :  0.07616704335650923\n",
            "Loss :  0.07618574939915781\n",
            "Loss :  0.07618667220265285\n",
            "Loss :  0.07626003748670943\n",
            "Loss :  0.07623605857347848\n",
            "Loss :  0.07621662198211948\n",
            "Loss :  0.0761602898164997\n",
            "Loss :  0.07606470659184639\n",
            "Loss :  0.07606308440912395\n",
            "Loss :  0.07612624994667197\n",
            "Loss :  0.0761539079868312\n",
            "Loss :  0.07609088015321236\n",
            "Loss :  0.07611124508099762\n",
            "Loss :  0.07613356603090621\n",
            "Loss :  0.07606933484108484\n",
            "Loss :  0.07609043681634206\n",
            "Loss :  0.07614431724633844\n",
            "Loss :  0.07603749635111533\n",
            "Validation: \n",
            " Loss :  0.0827886164188385\n",
            " Loss :  0.0846114701458386\n",
            " Loss :  0.08446143549389956\n",
            " Loss :  0.08354986837652863\n",
            " Loss :  0.08396186642808678\n",
            "\n",
            "Epoch: 28\n",
            "Loss :  0.08918624371290207\n",
            "Loss :  0.07442126219922846\n",
            "Loss :  0.07346191292717344\n",
            "Loss :  0.07373554331641044\n",
            "Loss :  0.07430306949266573\n",
            "Loss :  0.07435522389178183\n",
            "Loss :  0.07431473521912685\n",
            "Loss :  0.07418223706559396\n",
            "Loss :  0.07383155404233638\n",
            "Loss :  0.07384609672558176\n",
            "Loss :  0.07364490971264273\n",
            "Loss :  0.07337771681649191\n",
            "Loss :  0.07315793538019677\n",
            "Loss :  0.07345808067053329\n",
            "Loss :  0.07341616928366058\n",
            "Loss :  0.07373657531493547\n",
            "Loss :  0.07380672912142291\n",
            "Loss :  0.07403978524588005\n",
            "Loss :  0.07404197808046367\n",
            "Loss :  0.07393114684217887\n",
            "Loss :  0.07386996287537452\n",
            "Loss :  0.0740194848045636\n",
            "Loss :  0.07415118242078776\n",
            "Loss :  0.07444925220820295\n",
            "Loss :  0.07454672630458947\n",
            "Loss :  0.07446580693363193\n",
            "Loss :  0.0745209133059814\n",
            "Loss :  0.07441273522717927\n",
            "Loss :  0.07444645844903705\n",
            "Loss :  0.07464301246906474\n",
            "Loss :  0.07479802413883796\n",
            "Loss :  0.07477889503457155\n",
            "Loss :  0.07482657181186096\n",
            "Loss :  0.07487034699752972\n",
            "Loss :  0.07495582224992363\n",
            "Loss :  0.07513965595184569\n",
            "Loss :  0.07515606945612754\n",
            "Loss :  0.07512443324907449\n",
            "Loss :  0.07512643779786866\n",
            "Loss :  0.07508675827432776\n",
            "Loss :  0.07507335659385916\n",
            "Loss :  0.07506859791068556\n",
            "Loss :  0.07509079231325351\n",
            "Loss :  0.07503756700113866\n",
            "Loss :  0.07503974712253157\n",
            "Loss :  0.07507455237457598\n",
            "Loss :  0.0750538233966605\n",
            "Loss :  0.07506087959413822\n",
            "Loss :  0.0751128395682437\n",
            "Loss :  0.07502348904636387\n",
            "Validation: \n",
            " Loss :  0.08130841702222824\n",
            " Loss :  0.08537928476220086\n",
            " Loss :  0.0847086506645854\n",
            " Loss :  0.08428862202362936\n",
            " Loss :  0.08463043636745876\n",
            "\n",
            "Epoch: 29\n",
            "Loss :  0.07977738976478577\n",
            "Loss :  0.07425885647535324\n",
            "Loss :  0.07337139546871185\n",
            "Loss :  0.07316236342153241\n",
            "Loss :  0.07308810985669857\n",
            "Loss :  0.07327099093327336\n",
            "Loss :  0.07328260024307204\n",
            "Loss :  0.07327273288663004\n",
            "Loss :  0.0732920859698896\n",
            "Loss :  0.07319033186841797\n",
            "Loss :  0.07317245972923714\n",
            "Loss :  0.07302481130705224\n",
            "Loss :  0.07272297670402803\n",
            "Loss :  0.07309515253846882\n",
            "Loss :  0.07310216797264756\n",
            "Loss :  0.0733441925354746\n",
            "Loss :  0.07347814417126015\n",
            "Loss :  0.07373634941483799\n",
            "Loss :  0.07385703787536911\n",
            "Loss :  0.0737611975774403\n",
            "Loss :  0.07368002641037923\n",
            "Loss :  0.07376650026977345\n",
            "Loss :  0.07391265117272533\n",
            "Loss :  0.07422320356036162\n",
            "Loss :  0.07433411619660765\n",
            "Loss :  0.07428072857488674\n",
            "Loss :  0.07425679407756904\n",
            "Loss :  0.07419135296421737\n",
            "Loss :  0.07412827167381596\n",
            "Loss :  0.07424385654916059\n",
            "Loss :  0.07427631812386734\n",
            "Loss :  0.07418015001527366\n",
            "Loss :  0.07415516854400205\n",
            "Loss :  0.07424686667917359\n",
            "Loss :  0.07430274335563707\n",
            "Loss :  0.07446813172636887\n",
            "Loss :  0.07443707237159446\n",
            "Loss :  0.07439612377324516\n",
            "Loss :  0.074353428415739\n",
            "Loss :  0.07434105397680836\n",
            "Loss :  0.07434797917778355\n",
            "Loss :  0.07443121428218491\n",
            "Loss :  0.07443756481849383\n",
            "Loss :  0.0743010186021953\n",
            "Loss :  0.07427072414363864\n",
            "Loss :  0.07426671664790145\n",
            "Loss :  0.07420786483104214\n",
            "Loss :  0.07423986496123032\n",
            "Loss :  0.07429383746241829\n",
            "Loss :  0.07418366222663227\n",
            "Validation: \n",
            " Loss :  0.08231557905673981\n",
            " Loss :  0.08371000062851679\n",
            " Loss :  0.08333573926512788\n",
            " Loss :  0.0828242521794116\n",
            " Loss :  0.08338530048911955\n",
            "\n",
            "Epoch: 30\n",
            "Loss :  0.07369528710842133\n",
            "Loss :  0.07094934718175368\n",
            "Loss :  0.07034516086181004\n",
            "Loss :  0.07170169127564277\n",
            "Loss :  0.07179012476671033\n",
            "Loss :  0.07164994583410375\n",
            "Loss :  0.07157112847341866\n",
            "Loss :  0.07155817196192876\n",
            "Loss :  0.07158804211167642\n",
            "Loss :  0.07175482477951836\n",
            "Loss :  0.0716040823055376\n",
            "Loss :  0.07150948705436948\n",
            "Loss :  0.07141247631843425\n",
            "Loss :  0.07174392459729245\n",
            "Loss :  0.07178551069599517\n",
            "Loss :  0.07195457298037232\n",
            "Loss :  0.07195394351985884\n",
            "Loss :  0.07222818949243479\n",
            "Loss :  0.0724268386047848\n",
            "Loss :  0.0722755220040913\n",
            "Loss :  0.07240739807634805\n",
            "Loss :  0.07255820866444665\n",
            "Loss :  0.07263090643542924\n",
            "Loss :  0.07280698989505892\n",
            "Loss :  0.07281500884791628\n",
            "Loss :  0.0727563879909031\n",
            "Loss :  0.07289897551995585\n",
            "Loss :  0.07292466097927182\n",
            "Loss :  0.07289453491144333\n",
            "Loss :  0.07296450829126991\n",
            "Loss :  0.0729788593774618\n",
            "Loss :  0.07298373131506695\n",
            "Loss :  0.07297668773158689\n",
            "Loss :  0.07310774846964732\n",
            "Loss :  0.07316502665450846\n",
            "Loss :  0.07328430959089868\n",
            "Loss :  0.07325377727025434\n",
            "Loss :  0.07323986756793573\n",
            "Loss :  0.07317301495064275\n",
            "Loss :  0.07314479632107802\n",
            "Loss :  0.0731476254463939\n",
            "Loss :  0.07324729708460706\n",
            "Loss :  0.07324435052211947\n",
            "Loss :  0.07311207948040796\n",
            "Loss :  0.07311385924043028\n",
            "Loss :  0.07308077549524687\n",
            "Loss :  0.07300796106037503\n",
            "Loss :  0.07304101415184147\n",
            "Loss :  0.07309807265436823\n",
            "Loss :  0.07305869179381372\n",
            "Validation: \n",
            " Loss :  0.08340305835008621\n",
            " Loss :  0.08424407527560279\n",
            " Loss :  0.08425937611155393\n",
            " Loss :  0.083634866187807\n",
            " Loss :  0.08403245249280224\n",
            "\n",
            "Epoch: 31\n",
            "Loss :  0.07915142178535461\n",
            "Loss :  0.07103596830909903\n",
            "Loss :  0.07085686017360006\n",
            "Loss :  0.071379738349107\n",
            "Loss :  0.0719851392616586\n",
            "Loss :  0.0719462178063159\n",
            "Loss :  0.07202513778551681\n",
            "Loss :  0.07215698918616267\n",
            "Loss :  0.07215331398226597\n",
            "Loss :  0.07221563811321835\n",
            "Loss :  0.0720421428432559\n",
            "Loss :  0.07170239768855206\n",
            "Loss :  0.07155494887597305\n",
            "Loss :  0.0718355784548148\n",
            "Loss :  0.07198951837230236\n",
            "Loss :  0.07214242688670064\n",
            "Loss :  0.07242289791751352\n",
            "Loss :  0.07262325661572797\n",
            "Loss :  0.07265161932831969\n",
            "Loss :  0.07261863046603677\n",
            "Loss :  0.07266027382134799\n",
            "Loss :  0.07279125920616054\n",
            "Loss :  0.07288741467023327\n",
            "Loss :  0.07303295360305609\n",
            "Loss :  0.07296471268371428\n",
            "Loss :  0.0729167517497245\n",
            "Loss :  0.07291675557407383\n",
            "Loss :  0.07283279730000179\n",
            "Loss :  0.07292828784453487\n",
            "Loss :  0.07308850218493913\n",
            "Loss :  0.07313962652041667\n",
            "Loss :  0.07311961373428057\n",
            "Loss :  0.07313272296639618\n",
            "Loss :  0.07319008859487819\n",
            "Loss :  0.07325110343063682\n",
            "Loss :  0.07330474980239175\n",
            "Loss :  0.07324737045201898\n",
            "Loss :  0.07320966355683348\n",
            "Loss :  0.0731486585333435\n",
            "Loss :  0.07306142548656525\n",
            "Loss :  0.07307472183111302\n",
            "Loss :  0.07307269778601154\n",
            "Loss :  0.07309818128898138\n",
            "Loss :  0.0729808400789159\n",
            "Loss :  0.07299504197643998\n",
            "Loss :  0.07296797053314366\n",
            "Loss :  0.07290498426672952\n",
            "Loss :  0.07289656894696746\n",
            "Loss :  0.07291712406674195\n",
            "Loss :  0.07285658880127423\n",
            "Validation: \n",
            " Loss :  0.07603345066308975\n",
            " Loss :  0.07613119163683482\n",
            " Loss :  0.07581730350488569\n",
            " Loss :  0.07535257483603525\n",
            " Loss :  0.07562508719202912\n",
            "\n",
            "Epoch: 32\n",
            "Loss :  0.07528532296419144\n",
            "Loss :  0.06933298774740913\n",
            "Loss :  0.06890518856900078\n",
            "Loss :  0.0694461427628994\n",
            "Loss :  0.06977914346427452\n",
            "Loss :  0.06995540696616266\n",
            "Loss :  0.06986370157511508\n",
            "Loss :  0.06989495996648157\n",
            "Loss :  0.06992673266817022\n",
            "Loss :  0.06986028656035989\n",
            "Loss :  0.06966914952096373\n",
            "Loss :  0.06949579568059595\n",
            "Loss :  0.06925112763342779\n",
            "Loss :  0.0695944644851994\n",
            "Loss :  0.0697705218608075\n",
            "Loss :  0.07012210704928992\n",
            "Loss :  0.07016690262818928\n",
            "Loss :  0.070465347724177\n",
            "Loss :  0.07045176903104913\n",
            "Loss :  0.07048593670207792\n",
            "Loss :  0.07060561629373636\n",
            "Loss :  0.07089524867975316\n",
            "Loss :  0.07092565582955584\n",
            "Loss :  0.0711500993183939\n",
            "Loss :  0.07119790169398814\n",
            "Loss :  0.07115033705277272\n",
            "Loss :  0.07114174433937474\n",
            "Loss :  0.07108944035874082\n",
            "Loss :  0.07115077439470223\n",
            "Loss :  0.07131758042934425\n",
            "Loss :  0.0713164806242203\n",
            "Loss :  0.07130230990780512\n",
            "Loss :  0.07128590607661696\n",
            "Loss :  0.0713297656745709\n",
            "Loss :  0.07139796931897441\n",
            "Loss :  0.07142749456343828\n",
            "Loss :  0.07144032263937419\n",
            "Loss :  0.07143712545822894\n",
            "Loss :  0.07137351377507833\n",
            "Loss :  0.07130955626516391\n",
            "Loss :  0.07130510057148494\n",
            "Loss :  0.07132939582831088\n",
            "Loss :  0.07134197706316825\n",
            "Loss :  0.07126035290638422\n",
            "Loss :  0.07126782530424547\n",
            "Loss :  0.07124560026497376\n",
            "Loss :  0.07118165065532393\n",
            "Loss :  0.07118016312930994\n",
            "Loss :  0.07123289821890189\n",
            "Loss :  0.07116699274894905\n",
            "Validation: \n",
            " Loss :  0.08003101497888565\n",
            " Loss :  0.08833418360778264\n",
            " Loss :  0.08772366784694718\n",
            " Loss :  0.08715434164785948\n",
            " Loss :  0.08753168509330278\n",
            "\n",
            "Epoch: 33\n",
            "Loss :  0.07570164650678635\n",
            "Loss :  0.07092757421461018\n",
            "Loss :  0.06995029179822831\n",
            "Loss :  0.07026349128253999\n",
            "Loss :  0.06998745751817052\n",
            "Loss :  0.07044632564864907\n",
            "Loss :  0.07034038880565127\n",
            "Loss :  0.06991725304806737\n",
            "Loss :  0.06982998407365364\n",
            "Loss :  0.06969336956575677\n",
            "Loss :  0.06953792093266355\n",
            "Loss :  0.06948204241223163\n",
            "Loss :  0.06947806403656637\n",
            "Loss :  0.06997778494166963\n",
            "Loss :  0.07015912176658076\n",
            "Loss :  0.07041143436009521\n",
            "Loss :  0.07049029836465853\n",
            "Loss :  0.07079431909131027\n",
            "Loss :  0.0706836662116301\n",
            "Loss :  0.07061087150926365\n",
            "Loss :  0.0705938686556484\n",
            "Loss :  0.0707177160334248\n",
            "Loss :  0.07073798448656479\n",
            "Loss :  0.07094600516093241\n",
            "Loss :  0.07095277402163541\n",
            "Loss :  0.07085757597212297\n",
            "Loss :  0.07082653281161155\n",
            "Loss :  0.07080461204436872\n",
            "Loss :  0.07082144641632288\n",
            "Loss :  0.07092643376836662\n",
            "Loss :  0.07091278090231443\n",
            "Loss :  0.07093759901653915\n",
            "Loss :  0.07086780478139161\n",
            "Loss :  0.0709511736616087\n",
            "Loss :  0.07099977117086435\n",
            "Loss :  0.07112925713006248\n",
            "Loss :  0.07110493078084863\n",
            "Loss :  0.07111145665422283\n",
            "Loss :  0.07109552774373001\n",
            "Loss :  0.07104237716826027\n",
            "Loss :  0.07102972365674236\n",
            "Loss :  0.07103307570564196\n",
            "Loss :  0.07101581696058112\n",
            "Loss :  0.07095517779067331\n",
            "Loss :  0.07097813303744982\n",
            "Loss :  0.07100922311191549\n",
            "Loss :  0.07093974581865065\n",
            "Loss :  0.07100146411742121\n",
            "Loss :  0.07101825701378735\n",
            "Loss :  0.070947933304759\n",
            "Validation: \n",
            " Loss :  0.0797184631228447\n",
            " Loss :  0.08657772306885038\n",
            " Loss :  0.08572421913466803\n",
            " Loss :  0.08498552319456319\n",
            " Loss :  0.08533781582926526\n",
            "\n",
            "Epoch: 34\n",
            "Loss :  0.07571611553430557\n",
            "Loss :  0.06958352367986333\n",
            "Loss :  0.06840742627779643\n",
            "Loss :  0.06908859360602594\n",
            "Loss :  0.06918081804746534\n",
            "Loss :  0.06893786846422682\n",
            "Loss :  0.06840006793375875\n",
            "Loss :  0.06831704289980338\n",
            "Loss :  0.06871033009187674\n",
            "Loss :  0.06878297489423019\n",
            "Loss :  0.06869593144643425\n",
            "Loss :  0.0685623100883252\n",
            "Loss :  0.06846632131121376\n",
            "Loss :  0.06877668543171336\n",
            "Loss :  0.06911917279163997\n",
            "Loss :  0.06920780887864283\n",
            "Loss :  0.06951642041065678\n",
            "Loss :  0.06963429547715605\n",
            "Loss :  0.0696368805693658\n",
            "Loss :  0.06956829659211698\n",
            "Loss :  0.06957893966887127\n",
            "Loss :  0.06965517777049146\n",
            "Loss :  0.0698154925524649\n",
            "Loss :  0.07004223762330039\n",
            "Loss :  0.070213531943896\n",
            "Loss :  0.07005610708815169\n",
            "Loss :  0.0700380845402164\n",
            "Loss :  0.07004792169011387\n",
            "Loss :  0.070078508285441\n",
            "Loss :  0.07020530690954313\n",
            "Loss :  0.07023330738279114\n",
            "Loss :  0.07017507061506008\n",
            "Loss :  0.07016756745328041\n",
            "Loss :  0.07021080553981833\n",
            "Loss :  0.0702671695314894\n",
            "Loss :  0.07039496311095365\n",
            "Loss :  0.07038938610136014\n",
            "Loss :  0.0703569238257376\n",
            "Loss :  0.0703228437231751\n",
            "Loss :  0.07024965238998003\n",
            "Loss :  0.07022343713446448\n",
            "Loss :  0.07017571575160154\n",
            "Loss :  0.07013195100950516\n",
            "Loss :  0.07004434324092212\n",
            "Loss :  0.07008893868110888\n",
            "Loss :  0.07008354627669783\n",
            "Loss :  0.0699983624632343\n",
            "Loss :  0.07003337342584716\n",
            "Loss :  0.07004999146330133\n",
            "Loss :  0.06997280705970563\n",
            "Validation: \n",
            " Loss :  0.08283742517232895\n",
            " Loss :  0.0861883486310641\n",
            " Loss :  0.08550560856010855\n",
            " Loss :  0.08485474803897201\n",
            " Loss :  0.08509792350692513\n",
            "\n",
            "Epoch: 35\n",
            "Loss :  0.0743994489312172\n",
            "Loss :  0.0677421350370754\n",
            "Loss :  0.06818484053725288\n",
            "Loss :  0.0687807607314279\n",
            "Loss :  0.06873465774626267\n",
            "Loss :  0.068613544471708\n",
            "Loss :  0.06853698261204313\n",
            "Loss :  0.06832183456756699\n",
            "Loss :  0.06869161657897042\n",
            "Loss :  0.06864459841297223\n",
            "Loss :  0.06838901285635363\n",
            "Loss :  0.0679987952583008\n",
            "Loss :  0.06788113907225861\n",
            "Loss :  0.06816358682308488\n",
            "Loss :  0.0682948926556195\n",
            "Loss :  0.06844585818170712\n",
            "Loss :  0.06858872867519071\n",
            "Loss :  0.06884526762000301\n",
            "Loss :  0.06891014966187556\n",
            "Loss :  0.06886069833530181\n",
            "Loss :  0.06884243778550803\n",
            "Loss :  0.0689213956525258\n",
            "Loss :  0.06905937162796837\n",
            "Loss :  0.06932339909208285\n",
            "Loss :  0.06950606849369172\n",
            "Loss :  0.06949988316254786\n",
            "Loss :  0.06947445692458828\n",
            "Loss :  0.06940969466008383\n",
            "Loss :  0.06936889185546981\n",
            "Loss :  0.06953354729051442\n",
            "Loss :  0.06952325601217359\n",
            "Loss :  0.06953810242834199\n",
            "Loss :  0.06949669151709087\n",
            "Loss :  0.06951641033648365\n",
            "Loss :  0.06951140801359482\n",
            "Loss :  0.06955168292223558\n",
            "Loss :  0.06951093161865615\n",
            "Loss :  0.06951341679636037\n",
            "Loss :  0.06952023517897749\n",
            "Loss :  0.06948224384613964\n",
            "Loss :  0.0694833042522767\n",
            "Loss :  0.06948896716836016\n",
            "Loss :  0.06947700556305696\n",
            "Loss :  0.06935103854765196\n",
            "Loss :  0.06936895431709938\n",
            "Loss :  0.06937960842562356\n",
            "Loss :  0.06929373854282363\n",
            "Loss :  0.06930892310261473\n",
            "Loss :  0.06939335700256165\n",
            "Loss :  0.06932139460399535\n",
            "Validation: \n",
            " Loss :  0.08420433849096298\n",
            " Loss :  0.0877522627512614\n",
            " Loss :  0.08661668620458464\n",
            " Loss :  0.0858353259133511\n",
            " Loss :  0.0861503349410163\n",
            "\n",
            "Epoch: 36\n",
            "Loss :  0.07030883431434631\n",
            "Loss :  0.06869870288805528\n",
            "Loss :  0.06765028390856016\n",
            "Loss :  0.06789750305394973\n",
            "Loss :  0.06759278112795294\n",
            "Loss :  0.06733620429740232\n",
            "Loss :  0.0671997172910659\n",
            "Loss :  0.06727788314013414\n",
            "Loss :  0.06755536058802664\n",
            "Loss :  0.06771472374816517\n",
            "Loss :  0.06752933923265722\n",
            "Loss :  0.06720927535547866\n",
            "Loss :  0.06710856752701042\n",
            "Loss :  0.06743841385113374\n",
            "Loss :  0.06753951194861256\n",
            "Loss :  0.06775857508182526\n",
            "Loss :  0.06793868995231131\n",
            "Loss :  0.06822218976871312\n",
            "Loss :  0.06820192309456635\n",
            "Loss :  0.06808034129713843\n",
            "Loss :  0.06814225535116979\n",
            "Loss :  0.0681420999327542\n",
            "Loss :  0.06821998638602403\n",
            "Loss :  0.06839983534567799\n",
            "Loss :  0.06850736135330933\n",
            "Loss :  0.0683584532949079\n",
            "Loss :  0.06842113477516905\n",
            "Loss :  0.06834076709248044\n",
            "Loss :  0.06834415091844641\n",
            "Loss :  0.06847240894879263\n",
            "Loss :  0.06857746273922762\n",
            "Loss :  0.06856349520265481\n",
            "Loss :  0.0685387746636927\n",
            "Loss :  0.06857377294102104\n",
            "Loss :  0.06864807932826081\n",
            "Loss :  0.06874216578242785\n",
            "Loss :  0.06876014567230547\n",
            "Loss :  0.06863837963286436\n",
            "Loss :  0.06864306441168459\n",
            "Loss :  0.06863363412068323\n",
            "Loss :  0.06860904460610297\n",
            "Loss :  0.06871061201078178\n",
            "Loss :  0.06863556378421477\n",
            "Loss :  0.06858727028226078\n",
            "Loss :  0.06857710287752065\n",
            "Loss :  0.06861553749197602\n",
            "Loss :  0.0685514264422875\n",
            "Loss :  0.06854443512303308\n",
            "Loss :  0.06859102267423439\n",
            "Loss :  0.06857520093014673\n",
            "Validation: \n",
            " Loss :  0.08282167464494705\n",
            " Loss :  0.08433806186630613\n",
            " Loss :  0.08345920574374316\n",
            " Loss :  0.08287372513384116\n",
            " Loss :  0.0829470216492076\n",
            "\n",
            "Epoch: 37\n",
            "Loss :  0.06700814515352249\n",
            "Loss :  0.06630723652514545\n",
            "Loss :  0.06610686704516411\n",
            "Loss :  0.06728210816940954\n",
            "Loss :  0.06772443043386064\n",
            "Loss :  0.06763921188665371\n",
            "Loss :  0.06773099822343373\n",
            "Loss :  0.06746932224068843\n",
            "Loss :  0.06751538537166736\n",
            "Loss :  0.06755600750937567\n",
            "Loss :  0.06747511663649342\n",
            "Loss :  0.06731878929175772\n",
            "Loss :  0.06711708695804777\n",
            "Loss :  0.06741546694445245\n",
            "Loss :  0.06752017843173751\n",
            "Loss :  0.06780392720999308\n",
            "Loss :  0.06799088140822346\n",
            "Loss :  0.0681102986049931\n",
            "Loss :  0.0681315719957839\n",
            "Loss :  0.06803744762199711\n",
            "Loss :  0.06812276932137523\n",
            "Loss :  0.06825707609195845\n",
            "Loss :  0.06836007006162971\n",
            "Loss :  0.06851466611285746\n",
            "Loss :  0.06851401370157839\n",
            "Loss :  0.06844525162382904\n",
            "Loss :  0.06842367225451487\n",
            "Loss :  0.06835812412724723\n",
            "Loss :  0.06831282190315664\n",
            "Loss :  0.06836891139751856\n",
            "Loss :  0.06839759442398319\n",
            "Loss :  0.06834103177837621\n",
            "Loss :  0.06837032446376631\n",
            "Loss :  0.06839603582942955\n",
            "Loss :  0.06846569639103503\n",
            "Loss :  0.06854927895373089\n",
            "Loss :  0.06857235590382957\n",
            "Loss :  0.06853444636309886\n",
            "Loss :  0.0684803314115901\n",
            "Loss :  0.06840299359520378\n",
            "Loss :  0.0684044186555388\n",
            "Loss :  0.06843247029866906\n",
            "Loss :  0.06846267001752049\n",
            "Loss :  0.0684044181764679\n",
            "Loss :  0.06837979603617911\n",
            "Loss :  0.06837946724997392\n",
            "Loss :  0.06828891796235666\n",
            "Loss :  0.06831263000987897\n",
            "Loss :  0.06833241940350146\n",
            "Loss :  0.06829320333609756\n",
            "Validation: \n",
            " Loss :  0.07253504544496536\n",
            " Loss :  0.07981230673335847\n",
            " Loss :  0.07911324446521155\n",
            " Loss :  0.07872889007701249\n",
            " Loss :  0.07897996930060563\n",
            "\n",
            "Epoch: 38\n",
            "Loss :  0.08097332715988159\n",
            "Loss :  0.06649654392491687\n",
            "Loss :  0.06658672151111421\n",
            "Loss :  0.0668232170564513\n",
            "Loss :  0.06688620568048663\n",
            "Loss :  0.06743053361481312\n",
            "Loss :  0.06722554955326143\n",
            "Loss :  0.06691757857169904\n",
            "Loss :  0.06703071664144963\n",
            "Loss :  0.06712393249784197\n",
            "Loss :  0.06710971061988633\n",
            "Loss :  0.06666430280552255\n",
            "Loss :  0.0663921918011894\n",
            "Loss :  0.06668522388084244\n",
            "Loss :  0.06679001931391709\n",
            "Loss :  0.06692445968950031\n",
            "Loss :  0.06699879039426028\n",
            "Loss :  0.0672882957198815\n",
            "Loss :  0.06730688147205674\n",
            "Loss :  0.06721708231181374\n",
            "Loss :  0.06730257590018694\n",
            "Loss :  0.06729360297322273\n",
            "Loss :  0.0673311657327063\n",
            "Loss :  0.06760966310124376\n",
            "Loss :  0.06761256143203415\n",
            "Loss :  0.06746915968290838\n",
            "Loss :  0.06743587850382501\n",
            "Loss :  0.06740983152191578\n",
            "Loss :  0.06740878631105626\n",
            "Loss :  0.0675199305273823\n",
            "Loss :  0.0676034851946506\n",
            "Loss :  0.06763487611073773\n",
            "Loss :  0.0676286369213991\n",
            "Loss :  0.06764810896982239\n",
            "Loss :  0.06771858886452364\n",
            "Loss :  0.06778821637827447\n",
            "Loss :  0.06778307761635807\n",
            "Loss :  0.06773811561680547\n",
            "Loss :  0.0676931672854217\n",
            "Loss :  0.06760672545608352\n",
            "Loss :  0.06761784450223024\n",
            "Loss :  0.06762154503677884\n",
            "Loss :  0.06763687897075384\n",
            "Loss :  0.06752268810110414\n",
            "Loss :  0.06753165174334769\n",
            "Loss :  0.06754122494395715\n",
            "Loss :  0.06742172285817451\n",
            "Loss :  0.06741453389263456\n",
            "Loss :  0.06743189200696975\n",
            "Loss :  0.06738850211083525\n",
            "Validation: \n",
            " Loss :  0.08193344622850418\n",
            " Loss :  0.08933911472558975\n",
            " Loss :  0.08882156850361242\n",
            " Loss :  0.08851815393713654\n",
            " Loss :  0.08909708500644307\n",
            "\n",
            "Epoch: 39\n",
            "Loss :  0.07646527141332626\n",
            "Loss :  0.06866294992240993\n",
            "Loss :  0.06687559755075545\n",
            "Loss :  0.06683253184441597\n",
            "Loss :  0.06695035081811068\n",
            "Loss :  0.06686684956737593\n",
            "Loss :  0.06654992467555844\n",
            "Loss :  0.06657331468353808\n",
            "Loss :  0.06666516760985057\n",
            "Loss :  0.0666890595476706\n",
            "Loss :  0.06656504669549441\n",
            "Loss :  0.06618016169549108\n",
            "Loss :  0.06592463680412158\n",
            "Loss :  0.06617022191977683\n",
            "Loss :  0.06633719114969808\n",
            "Loss :  0.06654159626029185\n",
            "Loss :  0.06658044485201747\n",
            "Loss :  0.0667374782830651\n",
            "Loss :  0.06684461777299149\n",
            "Loss :  0.06681062930619529\n",
            "Loss :  0.06677603680844331\n",
            "Loss :  0.0667861034625797\n",
            "Loss :  0.06688383295794957\n",
            "Loss :  0.06709418813516567\n",
            "Loss :  0.06716263430247169\n",
            "Loss :  0.0670158741216498\n",
            "Loss :  0.06693729631231662\n",
            "Loss :  0.06677157521192878\n",
            "Loss :  0.0668261953069434\n",
            "Loss :  0.06685491327413988\n",
            "Loss :  0.06690331667265623\n",
            "Loss :  0.06692442246428257\n",
            "Loss :  0.06688062883797465\n",
            "Loss :  0.06686374874858697\n",
            "Loss :  0.0669216572845087\n",
            "Loss :  0.06701568103371522\n",
            "Loss :  0.06696925979943487\n",
            "Loss :  0.06695238977791164\n",
            "Loss :  0.0669239087719617\n",
            "Loss :  0.06683421210216745\n",
            "Loss :  0.0668193743171686\n",
            "Loss :  0.06685072385735466\n",
            "Loss :  0.0668456377439431\n",
            "Loss :  0.0668031309413661\n",
            "Loss :  0.06681729332795219\n",
            "Loss :  0.06681963527149742\n",
            "Loss :  0.06677814413856012\n",
            "Loss :  0.06682932943959904\n",
            "Loss :  0.06683489983115276\n",
            "Loss :  0.06682628385586069\n",
            "Validation: \n",
            " Loss :  0.07133565098047256\n",
            " Loss :  0.07645606657578832\n",
            " Loss :  0.0761252624414316\n",
            " Loss :  0.07587842234089727\n",
            " Loss :  0.07603433999566385\n",
            "\n",
            "Epoch: 40\n",
            "Loss :  0.07586167007684708\n",
            "Loss :  0.06579279188405383\n",
            "Loss :  0.06512586222518058\n",
            "Loss :  0.06538622213467475\n",
            "Loss :  0.06591615426104243\n",
            "Loss :  0.06587045296442275\n",
            "Loss :  0.06562401519202796\n",
            "Loss :  0.06549903063077323\n",
            "Loss :  0.06563819397562816\n",
            "Loss :  0.06556572617737802\n",
            "Loss :  0.06533969573602819\n",
            "Loss :  0.0651844972828487\n",
            "Loss :  0.0649397173696313\n",
            "Loss :  0.06517732333932214\n",
            "Loss :  0.06547824285448865\n",
            "Loss :  0.06554450785482166\n",
            "Loss :  0.06549274400802133\n",
            "Loss :  0.06566175162705065\n",
            "Loss :  0.0656544077338764\n",
            "Loss :  0.06557028736743628\n",
            "Loss :  0.06566477203695335\n",
            "Loss :  0.06578588206762385\n",
            "Loss :  0.06583965844495804\n",
            "Loss :  0.066085441755655\n",
            "Loss :  0.06617059373138356\n",
            "Loss :  0.06616397387537348\n",
            "Loss :  0.06624466728890079\n",
            "Loss :  0.06619995910798052\n",
            "Loss :  0.0662425975464417\n",
            "Loss :  0.06636451101538651\n",
            "Loss :  0.0664212540089094\n",
            "Loss :  0.0663391710573454\n",
            "Loss :  0.06627668831439404\n",
            "Loss :  0.06626789651113335\n",
            "Loss :  0.06633917194220328\n",
            "Loss :  0.06643302641023598\n",
            "Loss :  0.06640332254098723\n",
            "Loss :  0.0663199458080482\n",
            "Loss :  0.06630423149763756\n",
            "Loss :  0.06620919299514397\n",
            "Loss :  0.06619888805131663\n",
            "Loss :  0.06621184570764684\n",
            "Loss :  0.06620977322555495\n",
            "Loss :  0.06614178985253565\n",
            "Loss :  0.06614529525611947\n",
            "Loss :  0.06608343228466496\n",
            "Loss :  0.06602502236882156\n",
            "Loss :  0.06605939892162184\n",
            "Loss :  0.0660805246035671\n",
            "Loss :  0.06603953233197361\n",
            "Validation: \n",
            " Loss :  0.07271931320428848\n",
            " Loss :  0.07845862458149593\n",
            " Loss :  0.07820109114414309\n",
            " Loss :  0.07756709747138571\n",
            " Loss :  0.07796547322729487\n",
            "\n",
            "Epoch: 41\n",
            "Loss :  0.07041511684656143\n",
            "Loss :  0.06418561021035368\n",
            "Loss :  0.06411368932042803\n",
            "Loss :  0.06467021893589728\n",
            "Loss :  0.06469862453821229\n",
            "Loss :  0.06526286257248298\n",
            "Loss :  0.0650860065685921\n",
            "Loss :  0.06480877746788549\n",
            "Loss :  0.06494363852673107\n",
            "Loss :  0.06504718234742081\n",
            "Loss :  0.06471208619451758\n",
            "Loss :  0.06443839765212557\n",
            "Loss :  0.06428256578558733\n",
            "Loss :  0.0643585451402282\n",
            "Loss :  0.06454371988561981\n",
            "Loss :  0.06466476962167696\n",
            "Loss :  0.06467174923753147\n",
            "Loss :  0.06474870319167773\n",
            "Loss :  0.06485364674616255\n",
            "Loss :  0.06495984156094296\n",
            "Loss :  0.06499422308224351\n",
            "Loss :  0.06504739317778163\n",
            "Loss :  0.06504758516520397\n",
            "Loss :  0.0652988874131725\n",
            "Loss :  0.0653342829082022\n",
            "Loss :  0.0652890765512607\n",
            "Loss :  0.06523044124759476\n",
            "Loss :  0.06514002966979773\n",
            "Loss :  0.06517472969255414\n",
            "Loss :  0.06527412436979334\n",
            "Loss :  0.06531274112009527\n",
            "Loss :  0.06533254468603916\n",
            "Loss :  0.06530607650723784\n",
            "Loss :  0.06528802990373168\n",
            "Loss :  0.0654344867008173\n",
            "Loss :  0.06554650495152528\n",
            "Loss :  0.06559042150665519\n",
            "Loss :  0.065587012754939\n",
            "Loss :  0.06551538664955167\n",
            "Loss :  0.0654457834980372\n",
            "Loss :  0.06541314364371455\n",
            "Loss :  0.06544144244053358\n",
            "Loss :  0.065390023723634\n",
            "Loss :  0.06532293582322149\n",
            "Loss :  0.06530981300538088\n",
            "Loss :  0.06538532272873326\n",
            "Loss :  0.06539832125872438\n",
            "Loss :  0.06539628127957606\n",
            "Loss :  0.06544592392605704\n",
            "Loss :  0.0654154587423243\n",
            "Validation: \n",
            " Loss :  0.08064386248588562\n",
            " Loss :  0.09008865058422089\n",
            " Loss :  0.08870286094706233\n",
            " Loss :  0.08850441714290713\n",
            " Loss :  0.0883986563594253\n",
            "\n",
            "Epoch: 42\n",
            "Loss :  0.0686054602265358\n",
            "Loss :  0.06415171447125348\n",
            "Loss :  0.06394490688329652\n",
            "Loss :  0.06433098102288862\n",
            "Loss :  0.06431139451338024\n",
            "Loss :  0.06475944619844942\n",
            "Loss :  0.06468912922456617\n",
            "Loss :  0.06467218025469444\n",
            "Loss :  0.06472253256741865\n",
            "Loss :  0.06466864753555465\n",
            "Loss :  0.0647001530200538\n",
            "Loss :  0.0643245822808764\n",
            "Loss :  0.06419945455902865\n",
            "Loss :  0.06435636214855063\n",
            "Loss :  0.06455387537043991\n",
            "Loss :  0.0647986437508602\n",
            "Loss :  0.06489566840833018\n",
            "Loss :  0.06513658372892274\n",
            "Loss :  0.0651698949751933\n",
            "Loss :  0.06508804048309151\n",
            "Loss :  0.06508481167071495\n",
            "Loss :  0.06514100456760392\n",
            "Loss :  0.0651739791912191\n",
            "Loss :  0.06527975124198121\n",
            "Loss :  0.06539838482853783\n",
            "Loss :  0.06535725341553232\n",
            "Loss :  0.06532580085755307\n",
            "Loss :  0.06524864475610512\n",
            "Loss :  0.0652591953869392\n",
            "Loss :  0.06534572270830062\n",
            "Loss :  0.06535762663546987\n",
            "Loss :  0.06534374082443031\n",
            "Loss :  0.06531449540799653\n",
            "Loss :  0.06530640168492527\n",
            "Loss :  0.06534186068955056\n",
            "Loss :  0.06543076394969581\n",
            "Loss :  0.06538319346234409\n",
            "Loss :  0.06538670321278817\n",
            "Loss :  0.06540036295342633\n",
            "Loss :  0.065313903181373\n",
            "Loss :  0.06532850341309336\n",
            "Loss :  0.06532384919279104\n",
            "Loss :  0.0652655323971762\n",
            "Loss :  0.06518791685974239\n",
            "Loss :  0.06521136760542723\n",
            "Loss :  0.06524695198313889\n",
            "Loss :  0.06517425695042289\n",
            "Loss :  0.06517006422050946\n",
            "Loss :  0.06521946968599814\n",
            "Loss :  0.06519245223546222\n",
            "Validation: \n",
            " Loss :  0.08482123911380768\n",
            " Loss :  0.0883462056517601\n",
            " Loss :  0.08746573201766829\n",
            " Loss :  0.08685100811426757\n",
            " Loss :  0.08718054346096368\n",
            "\n",
            "Epoch: 43\n",
            "Loss :  0.06363862007856369\n",
            "Loss :  0.06483078002929688\n",
            "Loss :  0.06368013968070348\n",
            "Loss :  0.06384977626223717\n",
            "Loss :  0.06404853139708681\n",
            "Loss :  0.06391751839249742\n",
            "Loss :  0.06381639942038254\n",
            "Loss :  0.06368362310696656\n",
            "Loss :  0.06364835913718482\n",
            "Loss :  0.06377328928191583\n",
            "Loss :  0.06378011239489706\n",
            "Loss :  0.06341218814119562\n",
            "Loss :  0.06324031752003126\n",
            "Loss :  0.06353996563276262\n",
            "Loss :  0.06377896280787515\n",
            "Loss :  0.06396893920093183\n",
            "Loss :  0.06412440637901703\n",
            "Loss :  0.06430509045981524\n",
            "Loss :  0.06436725487695873\n",
            "Loss :  0.06417727979455942\n",
            "Loss :  0.0641993629880509\n",
            "Loss :  0.06419742525824439\n",
            "Loss :  0.06423034662237534\n",
            "Loss :  0.06444671417856629\n",
            "Loss :  0.06450772126247774\n",
            "Loss :  0.06449660587595754\n",
            "Loss :  0.06447375148991516\n",
            "Loss :  0.0644627037660882\n",
            "Loss :  0.06453626305925464\n",
            "Loss :  0.06465532168541167\n",
            "Loss :  0.06468700539580612\n",
            "Loss :  0.06464424608365132\n",
            "Loss :  0.06464526138480207\n",
            "Loss :  0.06465170342261338\n",
            "Loss :  0.06471543563557161\n",
            "Loss :  0.06473835845321332\n",
            "Loss :  0.06469396006450098\n",
            "Loss :  0.06467315216229932\n",
            "Loss :  0.06466793539760307\n",
            "Loss :  0.06456721906581193\n",
            "Loss :  0.06454658831592808\n",
            "Loss :  0.06453406194410764\n",
            "Loss :  0.06449296279432089\n",
            "Loss :  0.06442598315720768\n",
            "Loss :  0.06440982390659737\n",
            "Loss :  0.06440038319900136\n",
            "Loss :  0.0643132725354777\n",
            "Loss :  0.0643613840000518\n",
            "Loss :  0.06440014103253269\n",
            "Loss :  0.06433096266296634\n",
            "Validation: \n",
            " Loss :  0.07865402847528458\n",
            " Loss :  0.08120575405302502\n",
            " Loss :  0.08071929979615095\n",
            " Loss :  0.08041639613812088\n",
            " Loss :  0.08060559446429029\n",
            "\n",
            "Epoch: 44\n",
            "Loss :  0.06969815492630005\n",
            "Loss :  0.06217741695317355\n",
            "Loss :  0.06242312863469124\n",
            "Loss :  0.0629498505544278\n",
            "Loss :  0.06291723587527508\n",
            "Loss :  0.06298044404270602\n",
            "Loss :  0.06282213989828454\n",
            "Loss :  0.06284002513742783\n",
            "Loss :  0.06313824552444765\n",
            "Loss :  0.06336610608703487\n",
            "Loss :  0.06320546228106659\n",
            "Loss :  0.06314816859525603\n",
            "Loss :  0.06311653766873454\n",
            "Loss :  0.06344788669402363\n",
            "Loss :  0.06346746431069171\n",
            "Loss :  0.06355411697499799\n",
            "Loss :  0.063585763438518\n",
            "Loss :  0.06379482274254163\n",
            "Loss :  0.06379716442023194\n",
            "Loss :  0.06381781286126031\n",
            "Loss :  0.06384060357637074\n",
            "Loss :  0.06394842265270897\n",
            "Loss :  0.06398988494910805\n",
            "Loss :  0.06416730340812113\n",
            "Loss :  0.06423148903918464\n",
            "Loss :  0.06410558227880067\n",
            "Loss :  0.06408877720720924\n",
            "Loss :  0.06397127144286113\n",
            "Loss :  0.06394848940741549\n",
            "Loss :  0.06405046017667682\n",
            "Loss :  0.06403583653295951\n",
            "Loss :  0.06401741394467676\n",
            "Loss :  0.0639983344695464\n",
            "Loss :  0.06407047643852377\n",
            "Loss :  0.06416287140980843\n",
            "Loss :  0.06424194251709854\n",
            "Loss :  0.064260254389609\n",
            "Loss :  0.06425988027509653\n",
            "Loss :  0.06420839706000693\n",
            "Loss :  0.06418589033815257\n",
            "Loss :  0.06413365671053492\n",
            "Loss :  0.06416552339809654\n",
            "Loss :  0.06415800022894866\n",
            "Loss :  0.06410599181000705\n",
            "Loss :  0.06410624017588405\n",
            "Loss :  0.06410323151133278\n",
            "Loss :  0.06403335140222065\n",
            "Loss :  0.06402198702191851\n",
            "Loss :  0.06407798208652564\n",
            "Loss :  0.06404067253477705\n",
            "Validation: \n",
            " Loss :  0.08052976429462433\n",
            " Loss :  0.08063127597173055\n",
            " Loss :  0.08018687094857053\n",
            " Loss :  0.07995509624969764\n",
            " Loss :  0.08044543236861994\n",
            "\n",
            "Epoch: 45\n",
            "Loss :  0.06985964626073837\n",
            "Loss :  0.06284679912708023\n",
            "Loss :  0.06175516989259493\n",
            "Loss :  0.06180152681566054\n",
            "Loss :  0.06232056030776442\n",
            "Loss :  0.06253810847798984\n",
            "Loss :  0.06232600152248242\n",
            "Loss :  0.06204585416216246\n",
            "Loss :  0.06224629365735584\n",
            "Loss :  0.062296434745683776\n",
            "Loss :  0.06226776536590982\n",
            "Loss :  0.062109751039528635\n",
            "Loss :  0.06212860847677081\n",
            "Loss :  0.06253483385302638\n",
            "Loss :  0.06267150531106806\n",
            "Loss :  0.06282711371976808\n",
            "Loss :  0.06298920433528675\n",
            "Loss :  0.06317849866828026\n",
            "Loss :  0.06313035188458901\n",
            "Loss :  0.06308358090476215\n",
            "Loss :  0.06303028341623682\n",
            "Loss :  0.06311569110406519\n",
            "Loss :  0.06309954161287973\n",
            "Loss :  0.06321339734963008\n",
            "Loss :  0.06337673332990452\n",
            "Loss :  0.06343738394845054\n",
            "Loss :  0.06342689229096946\n",
            "Loss :  0.06333401287833702\n",
            "Loss :  0.06327902179648867\n",
            "Loss :  0.0634161594909491\n",
            "Loss :  0.06340539519770994\n",
            "Loss :  0.06336362741384476\n",
            "Loss :  0.06339981362827099\n",
            "Loss :  0.06339101973932315\n",
            "Loss :  0.06346062492153162\n",
            "Loss :  0.06350492891932485\n",
            "Loss :  0.06347515470144491\n",
            "Loss :  0.06342198389439249\n",
            "Loss :  0.0634223482329545\n",
            "Loss :  0.06334289271965661\n",
            "Loss :  0.06330553162610739\n",
            "Loss :  0.06332484782285934\n",
            "Loss :  0.06333772791217739\n",
            "Loss :  0.06324581183026395\n",
            "Loss :  0.0632671023594414\n",
            "Loss :  0.06324284027295472\n",
            "Loss :  0.06322879713521604\n",
            "Loss :  0.06323668183541349\n",
            "Loss :  0.06324983832741973\n",
            "Loss :  0.06317193003451023\n",
            "Validation: \n",
            " Loss :  0.0766502246260643\n",
            " Loss :  0.08255001263959068\n",
            " Loss :  0.0823402604678782\n",
            " Loss :  0.08193093489428036\n",
            " Loss :  0.08196891209593525\n",
            "\n",
            "Epoch: 46\n",
            "Loss :  0.07431937009096146\n",
            "Loss :  0.06218510967763988\n",
            "Loss :  0.06216879774417196\n",
            "Loss :  0.06222710217679701\n",
            "Loss :  0.0625811685330984\n",
            "Loss :  0.06264897231377807\n",
            "Loss :  0.06254304401942941\n",
            "Loss :  0.06224492890104442\n",
            "Loss :  0.062236902936373226\n",
            "Loss :  0.062234226696111346\n",
            "Loss :  0.06211624312961456\n",
            "Loss :  0.06181359911958376\n",
            "Loss :  0.0614927789458066\n",
            "Loss :  0.06178067567694278\n",
            "Loss :  0.06190079278874059\n",
            "Loss :  0.06205107034831647\n",
            "Loss :  0.062235859286340865\n",
            "Loss :  0.06241081413208393\n",
            "Loss :  0.06249259914482496\n",
            "Loss :  0.062346331170560175\n",
            "Loss :  0.062401397644880396\n",
            "Loss :  0.06264671137759471\n",
            "Loss :  0.06270256110917928\n",
            "Loss :  0.06295730638039576\n",
            "Loss :  0.06301913147645373\n",
            "Loss :  0.06299528991618004\n",
            "Loss :  0.06300751685068526\n",
            "Loss :  0.06298656926603775\n",
            "Loss :  0.06290356508618572\n",
            "Loss :  0.06300890978259319\n",
            "Loss :  0.0631152567178308\n",
            "Loss :  0.06310822551177629\n",
            "Loss :  0.06309572400705094\n",
            "Loss :  0.06311947293016845\n",
            "Loss :  0.06322960443030005\n",
            "Loss :  0.06337675069089968\n",
            "Loss :  0.06335364272843767\n",
            "Loss :  0.06332427554455086\n",
            "Loss :  0.06327562348970904\n",
            "Loss :  0.06318645067798817\n",
            "Loss :  0.063196761733353\n",
            "Loss :  0.06324989324177269\n",
            "Loss :  0.06326034205861726\n",
            "Loss :  0.06319952330912902\n",
            "Loss :  0.06319260678323758\n",
            "Loss :  0.06320501554203932\n",
            "Loss :  0.06314604990426734\n",
            "Loss :  0.0631257808448909\n",
            "Loss :  0.06316948926033696\n",
            "Loss :  0.06309929216193327\n",
            "Validation: \n",
            " Loss :  0.08147046715021133\n",
            " Loss :  0.08376903548127129\n",
            " Loss :  0.0832309595695356\n",
            " Loss :  0.08293139946753861\n",
            " Loss :  0.08303670539164248\n",
            "\n",
            "Epoch: 47\n",
            "Loss :  0.07280800491571426\n",
            "Loss :  0.06282267821106044\n",
            "Loss :  0.0613771735557488\n",
            "Loss :  0.062167494768096555\n",
            "Loss :  0.0622675041781693\n",
            "Loss :  0.06207308896324214\n",
            "Loss :  0.06188810549554278\n",
            "Loss :  0.061807702964460344\n",
            "Loss :  0.0621380703408777\n",
            "Loss :  0.06209465787633435\n",
            "Loss :  0.06216558275541457\n",
            "Loss :  0.062142746606925585\n",
            "Loss :  0.06184092841365121\n",
            "Loss :  0.062018976866743944\n",
            "Loss :  0.062215480213681014\n",
            "Loss :  0.06234141066670418\n",
            "Loss :  0.06244552850445605\n",
            "Loss :  0.06253120775895508\n",
            "Loss :  0.06254049481113971\n",
            "Loss :  0.06244724635912486\n",
            "Loss :  0.062450820014844484\n",
            "Loss :  0.06249786860428715\n",
            "Loss :  0.06259299915840183\n",
            "Loss :  0.06283433674088804\n",
            "Loss :  0.06286834699731644\n",
            "Loss :  0.06282765479142448\n",
            "Loss :  0.0627737210000155\n",
            "Loss :  0.0626314704571043\n",
            "Loss :  0.06258485950131858\n",
            "Loss :  0.06267228023600332\n",
            "Loss :  0.0626988724814697\n",
            "Loss :  0.06271477013682629\n",
            "Loss :  0.06271468642576833\n",
            "Loss :  0.06271035247834067\n",
            "Loss :  0.06279384275991197\n",
            "Loss :  0.06288224973442548\n",
            "Loss :  0.06287823103554031\n",
            "Loss :  0.0628316332548455\n",
            "Loss :  0.06274640827080397\n",
            "Loss :  0.06270149072913257\n",
            "Loss :  0.06268377429306358\n",
            "Loss :  0.06269936618868742\n",
            "Loss :  0.06267975583991076\n",
            "Loss :  0.06259664201681132\n",
            "Loss :  0.06258889460036544\n",
            "Loss :  0.06260543290170492\n",
            "Loss :  0.06251609195306607\n",
            "Loss :  0.062536917903684\n",
            "Loss :  0.06253126198561425\n",
            "Loss :  0.062485110045025646\n",
            "Validation: \n",
            " Loss :  0.07678256928920746\n",
            " Loss :  0.07946072022120158\n",
            " Loss :  0.0788998156785965\n",
            " Loss :  0.07888466009839637\n",
            " Loss :  0.07908000502689386\n",
            "\n",
            "Epoch: 48\n",
            "Loss :  0.07267727702856064\n",
            "Loss :  0.06216052987358787\n",
            "Loss :  0.06099817121312732\n",
            "Loss :  0.06201472337688169\n",
            "Loss :  0.061823384427442785\n",
            "Loss :  0.06229493547888363\n",
            "Loss :  0.061891959033540035\n",
            "Loss :  0.061761344507546494\n",
            "Loss :  0.0617034318914384\n",
            "Loss :  0.06181049829983449\n",
            "Loss :  0.0618226356287994\n",
            "Loss :  0.061596965299801784\n",
            "Loss :  0.06151809527977439\n",
            "Loss :  0.061738584901540335\n",
            "Loss :  0.06189244414897675\n",
            "Loss :  0.06194618556475797\n",
            "Loss :  0.062027247356516976\n",
            "Loss :  0.06223696971323058\n",
            "Loss :  0.06226886942653366\n",
            "Loss :  0.062194312059598446\n",
            "Loss :  0.062254955704828994\n",
            "Loss :  0.06239367144014598\n",
            "Loss :  0.06253567895940526\n",
            "Loss :  0.06275897939161305\n",
            "Loss :  0.06274876646171962\n",
            "Loss :  0.06268023755920836\n",
            "Loss :  0.06263058813881144\n",
            "Loss :  0.06259882887241146\n",
            "Loss :  0.0625589854511502\n",
            "Loss :  0.06255735637447268\n",
            "Loss :  0.06251430056023835\n",
            "Loss :  0.062490939327374914\n",
            "Loss :  0.062478666933618976\n",
            "Loss :  0.06242396015099889\n",
            "Loss :  0.06251564837140072\n",
            "Loss :  0.06265039607203245\n",
            "Loss :  0.06263834787042517\n",
            "Loss :  0.06256442092298818\n",
            "Loss :  0.06249491569251213\n",
            "Loss :  0.06243489605500875\n",
            "Loss :  0.06240933335518599\n",
            "Loss :  0.062410030332722514\n",
            "Loss :  0.0624139731984382\n",
            "Loss :  0.06235357642761516\n",
            "Loss :  0.062344413723737474\n",
            "Loss :  0.062369197904402825\n",
            "Loss :  0.06228370241287474\n",
            "Loss :  0.062321606042721724\n",
            "Loss :  0.06240259745233768\n",
            "Loss :  0.062336500969292434\n",
            "Validation: \n",
            " Loss :  0.07554326951503754\n",
            " Loss :  0.0831608594883056\n",
            " Loss :  0.08205293137125852\n",
            " Loss :  0.08184684263389619\n",
            " Loss :  0.08181488421964056\n",
            "\n",
            "Epoch: 49\n",
            "Loss :  0.06645891070365906\n",
            "Loss :  0.059276332570747894\n",
            "Loss :  0.05918520440657934\n",
            "Loss :  0.06045086261245512\n",
            "Loss :  0.06053031235933304\n",
            "Loss :  0.06070742383599281\n",
            "Loss :  0.060742767558234635\n",
            "Loss :  0.06032572161983436\n",
            "Loss :  0.06058962712133372\n",
            "Loss :  0.06079143838404299\n",
            "Loss :  0.06061053670721479\n",
            "Loss :  0.060344232430866174\n",
            "Loss :  0.060247370711535464\n",
            "Loss :  0.060515934871580766\n",
            "Loss :  0.06081838982430755\n",
            "Loss :  0.06085997015632541\n",
            "Loss :  0.06098864451976296\n",
            "Loss :  0.061231497239473964\n",
            "Loss :  0.06119425502188956\n",
            "Loss :  0.06135451038860526\n",
            "Loss :  0.06135104978410759\n",
            "Loss :  0.06143141306618943\n",
            "Loss :  0.061478634950666945\n",
            "Loss :  0.0616583481463261\n",
            "Loss :  0.0617132247947311\n",
            "Loss :  0.0617460794153204\n",
            "Loss :  0.061697494581170464\n",
            "Loss :  0.06166245441925042\n",
            "Loss :  0.06169418964097508\n",
            "Loss :  0.061750408671668304\n",
            "Loss :  0.061805997712172545\n",
            "Loss :  0.06177138647370017\n",
            "Loss :  0.06174756133946303\n",
            "Loss :  0.06179827767961695\n",
            "Loss :  0.061800800100012605\n",
            "Loss :  0.061875973331962215\n",
            "Loss :  0.061896459733515236\n",
            "Loss :  0.06186320990163361\n",
            "Loss :  0.06183795891995505\n",
            "Loss :  0.061791519584405756\n",
            "Loss :  0.06181859450781732\n",
            "Loss :  0.06181215221139346\n",
            "Loss :  0.06178430144867535\n",
            "Loss :  0.06173425981139639\n",
            "Loss :  0.061743277921225206\n",
            "Loss :  0.06177471300582928\n",
            "Loss :  0.06173337600042401\n",
            "Loss :  0.061752052491257904\n",
            "Loss :  0.06177961454569922\n",
            "Loss :  0.061715133351609566\n",
            "Validation: \n",
            " Loss :  0.07193418592214584\n",
            " Loss :  0.07929912404645056\n",
            " Loss :  0.07868061605386617\n",
            " Loss :  0.07824637832455948\n",
            " Loss :  0.07829943386676871\n",
            "\n",
            "Epoch: 50\n",
            "Loss :  0.06673481315374374\n",
            "Loss :  0.05902540616013787\n",
            "Loss :  0.05965711921453476\n",
            "Loss :  0.06030485911234733\n",
            "Loss :  0.060426277812661196\n",
            "Loss :  0.06035890707782671\n",
            "Loss :  0.06046626576390423\n",
            "Loss :  0.060191065492764324\n",
            "Loss :  0.06039433503224526\n",
            "Loss :  0.06051962593427071\n",
            "Loss :  0.06052815939972896\n",
            "Loss :  0.06036700670783584\n",
            "Loss :  0.06020025890474477\n",
            "Loss :  0.06043576983777621\n",
            "Loss :  0.060583523137772335\n",
            "Loss :  0.06080198243556433\n",
            "Loss :  0.06091859541046694\n",
            "Loss :  0.06098519729679091\n",
            "Loss :  0.06106996283099796\n",
            "Loss :  0.06113299782051466\n",
            "Loss :  0.061185986174279776\n",
            "Loss :  0.06123677104429046\n",
            "Loss :  0.06126492558156743\n",
            "Loss :  0.06140265249328696\n",
            "Loss :  0.061415881335858985\n",
            "Loss :  0.06121695197495331\n",
            "Loss :  0.0611884748969955\n",
            "Loss :  0.0611368364266144\n",
            "Loss :  0.061112622402316734\n",
            "Loss :  0.0611901832712475\n",
            "Loss :  0.06122359898300266\n",
            "Loss :  0.06124700181833034\n",
            "Loss :  0.06121220800596234\n",
            "Loss :  0.06122678160397308\n",
            "Loss :  0.061274781954253524\n",
            "Loss :  0.06133172335235821\n",
            "Loss :  0.06131861235048632\n",
            "Loss :  0.061301528403379844\n",
            "Loss :  0.06126551050448355\n",
            "Loss :  0.06126490509723458\n",
            "Loss :  0.0612307883000136\n",
            "Loss :  0.06124913715134282\n",
            "Loss :  0.06123156589843032\n",
            "Loss :  0.06115418949149324\n",
            "Loss :  0.061159983446268266\n",
            "Loss :  0.06118735709039705\n",
            "Loss :  0.061125902479227605\n",
            "Loss :  0.061128223042009745\n",
            "Loss :  0.06119864541657749\n",
            "Loss :  0.06114506592515044\n",
            "Validation: \n",
            " Loss :  0.07326560467481613\n",
            " Loss :  0.07818414164440972\n",
            " Loss :  0.07794152545492823\n",
            " Loss :  0.07748992716679808\n",
            " Loss :  0.07767526409876199\n",
            "\n",
            "Epoch: 51\n",
            "Loss :  0.05889204889535904\n",
            "Loss :  0.058606517924503845\n",
            "Loss :  0.058739040401719865\n",
            "Loss :  0.05918894816310175\n",
            "Loss :  0.05977459415429976\n",
            "Loss :  0.06013831810331812\n",
            "Loss :  0.06007758882202086\n",
            "Loss :  0.05966215727614685\n",
            "Loss :  0.059957204225622576\n",
            "Loss :  0.06016064848709893\n",
            "Loss :  0.06015086716206947\n",
            "Loss :  0.05995301700927116\n",
            "Loss :  0.05986816688510012\n",
            "Loss :  0.060151212628333624\n",
            "Loss :  0.060270501287482306\n",
            "Loss :  0.06042095542644823\n",
            "Loss :  0.06055676805214112\n",
            "Loss :  0.060721698995919254\n",
            "Loss :  0.06083072302091187\n",
            "Loss :  0.06077967931306799\n",
            "Loss :  0.060744771872883414\n",
            "Loss :  0.06074742531465693\n",
            "Loss :  0.06075661017546826\n",
            "Loss :  0.060912575475968324\n",
            "Loss :  0.06101494324219672\n",
            "Loss :  0.060944804528438715\n",
            "Loss :  0.06092791541897018\n",
            "Loss :  0.06088230259908961\n",
            "Loss :  0.06087441180312336\n",
            "Loss :  0.06098638361006258\n",
            "Loss :  0.0609795613913639\n",
            "Loss :  0.06101659764430914\n",
            "Loss :  0.061032076514212885\n",
            "Loss :  0.0611059718576803\n",
            "Loss :  0.06118515810090775\n",
            "Loss :  0.06120200492335521\n",
            "Loss :  0.06120548067082989\n",
            "Loss :  0.061201177246246054\n",
            "Loss :  0.06117249150171367\n",
            "Loss :  0.0611188604837031\n",
            "Loss :  0.061080951782459036\n",
            "Loss :  0.06107709517842952\n",
            "Loss :  0.06102251016364528\n",
            "Loss :  0.06094843409350466\n",
            "Loss :  0.06094293919753055\n",
            "Loss :  0.06097771718007762\n",
            "Loss :  0.060948082421645684\n",
            "Loss :  0.060937528880095536\n",
            "Loss :  0.060971226612892074\n",
            "Loss :  0.06089119352172689\n",
            "Validation: \n",
            " Loss :  0.07207517325878143\n",
            " Loss :  0.07998184398526237\n",
            " Loss :  0.07926152065032865\n",
            " Loss :  0.07929178376178272\n",
            " Loss :  0.07951823851944488\n",
            "\n",
            "Epoch: 52\n",
            "Loss :  0.06371849775314331\n",
            "Loss :  0.06026010316881267\n",
            "Loss :  0.05949334906680243\n",
            "Loss :  0.059975300224558\n",
            "Loss :  0.059891147402728474\n",
            "Loss :  0.059628052308278924\n",
            "Loss :  0.06001316994184353\n",
            "Loss :  0.05975823329997734\n",
            "Loss :  0.059767382978289214\n",
            "Loss :  0.05994745962076135\n",
            "Loss :  0.05982268446742898\n",
            "Loss :  0.05984192942311098\n",
            "Loss :  0.0597573694857684\n",
            "Loss :  0.0600197704238746\n",
            "Loss :  0.06003695036819641\n",
            "Loss :  0.06011920037451169\n",
            "Loss :  0.06024678856689737\n",
            "Loss :  0.060252973559307074\n",
            "Loss :  0.060224958361018426\n",
            "Loss :  0.06016447607681389\n",
            "Loss :  0.0601944577560496\n",
            "Loss :  0.06027879744273791\n",
            "Loss :  0.060398245828723475\n",
            "Loss :  0.060485727552856715\n",
            "Loss :  0.060479889287617196\n",
            "Loss :  0.060479881338389274\n",
            "Loss :  0.06052333917254689\n",
            "Loss :  0.06048209826389802\n",
            "Loss :  0.060481136578576\n",
            "Loss :  0.06053159720043546\n",
            "Loss :  0.06062414739714112\n",
            "Loss :  0.06058381305007306\n",
            "Loss :  0.06052719470914279\n",
            "Loss :  0.06053696603168174\n",
            "Loss :  0.0605705796557962\n",
            "Loss :  0.060627034692852584\n",
            "Loss :  0.06062231386219696\n",
            "Loss :  0.06056580724661562\n",
            "Loss :  0.06052071125957909\n",
            "Loss :  0.060472186921578845\n",
            "Loss :  0.060443578459824114\n",
            "Loss :  0.06042184334456776\n",
            "Loss :  0.06039031132206214\n",
            "Loss :  0.06030288131220435\n",
            "Loss :  0.060360144806894854\n",
            "Loss :  0.060362560108733546\n",
            "Loss :  0.06028157870998352\n",
            "Loss :  0.06028392646786007\n",
            "Loss :  0.06030021842545878\n",
            "Loss :  0.06026502534018757\n",
            "Validation: \n",
            " Loss :  0.07955572754144669\n",
            " Loss :  0.08507920979034334\n",
            " Loss :  0.08464853810827906\n",
            " Loss :  0.08439190624678722\n",
            " Loss :  0.08454943494296369\n",
            "\n",
            "Epoch: 53\n",
            "Loss :  0.06244907155632973\n",
            "Loss :  0.05969602479176088\n",
            "Loss :  0.05929256478945414\n",
            "Loss :  0.059812184783720204\n",
            "Loss :  0.059608897421418165\n",
            "Loss :  0.059468139036028995\n",
            "Loss :  0.059218185114078836\n",
            "Loss :  0.0596061707277533\n",
            "Loss :  0.059668914035514546\n",
            "Loss :  0.05973643154560865\n",
            "Loss :  0.05957516673767921\n",
            "Loss :  0.05928291121984387\n",
            "Loss :  0.05906305890068535\n",
            "Loss :  0.0592104499406032\n",
            "Loss :  0.059226146350938376\n",
            "Loss :  0.05937166911679388\n",
            "Loss :  0.05956826315070531\n",
            "Loss :  0.059849843731400565\n",
            "Loss :  0.059822702255532226\n",
            "Loss :  0.05974070281180412\n",
            "Loss :  0.05968566813427417\n",
            "Loss :  0.05971423112809376\n",
            "Loss :  0.05989739138683582\n",
            "Loss :  0.06005608701667228\n",
            "Loss :  0.060124782451208204\n",
            "Loss :  0.05999208562283877\n",
            "Loss :  0.060019185159969145\n",
            "Loss :  0.05992748272155044\n",
            "Loss :  0.059867466246424195\n",
            "Loss :  0.059944064730835946\n",
            "Loss :  0.060006834665604206\n",
            "Loss :  0.059954898521735356\n",
            "Loss :  0.05994828503776191\n",
            "Loss :  0.059954534777161575\n",
            "Loss :  0.05995426487581821\n",
            "Loss :  0.060041645005812334\n",
            "Loss :  0.06000026411420751\n",
            "Loss :  0.05998315004446114\n",
            "Loss :  0.059981438979076276\n",
            "Loss :  0.059968959864066994\n",
            "Loss :  0.05995340758651272\n",
            "Loss :  0.05997170009389701\n",
            "Loss :  0.05997425539865913\n",
            "Loss :  0.059914989591405454\n",
            "Loss :  0.05994621583190905\n",
            "Loss :  0.059941331498704835\n",
            "Loss :  0.05988151020897702\n",
            "Loss :  0.05988747194694106\n",
            "Loss :  0.05990535889804487\n",
            "Loss :  0.059821539036853735\n",
            "Validation: \n",
            " Loss :  0.07001801580190659\n",
            " Loss :  0.07752219339211781\n",
            " Loss :  0.07708821700113576\n",
            " Loss :  0.07660254665085527\n",
            " Loss :  0.07659421796783988\n",
            "\n",
            "Epoch: 54\n",
            "Loss :  0.0693332850933075\n",
            "Loss :  0.060062272982163864\n",
            "Loss :  0.05850391923671677\n",
            "Loss :  0.059028991407925085\n",
            "Loss :  0.059451833094765504\n",
            "Loss :  0.05931144215020479\n",
            "Loss :  0.05916225152914641\n",
            "Loss :  0.05900359017328477\n",
            "Loss :  0.05899224112983103\n",
            "Loss :  0.058864511467598295\n",
            "Loss :  0.05883008640001316\n",
            "Loss :  0.05852118228469883\n",
            "Loss :  0.05831498723507913\n",
            "Loss :  0.058595254107286\n",
            "Loss :  0.058677585148219524\n",
            "Loss :  0.05886933020013847\n",
            "Loss :  0.05894573130037473\n",
            "Loss :  0.05904642331321337\n",
            "Loss :  0.059067474194488476\n",
            "Loss :  0.0590743121949478\n",
            "Loss :  0.05902604780980011\n",
            "Loss :  0.05913345773496899\n",
            "Loss :  0.059308008286613145\n",
            "Loss :  0.05946826871919941\n",
            "Loss :  0.05952644311046205\n",
            "Loss :  0.05948377365016368\n",
            "Loss :  0.059431230499484074\n",
            "Loss :  0.059346069846425986\n",
            "Loss :  0.059333966600831296\n",
            "Loss :  0.059401044334025724\n",
            "Loss :  0.05945726222918675\n",
            "Loss :  0.059362447003076316\n",
            "Loss :  0.05940575435804058\n",
            "Loss :  0.05943639700236277\n",
            "Loss :  0.0594963079586057\n",
            "Loss :  0.05951094678324512\n",
            "Loss :  0.05948549770128364\n",
            "Loss :  0.05949425541687847\n",
            "Loss :  0.059456734519618075\n",
            "Loss :  0.05940807141992442\n",
            "Loss :  0.05939536988549399\n",
            "Loss :  0.05938573418198711\n",
            "Loss :  0.0593797629013339\n",
            "Loss :  0.05932712824604627\n",
            "Loss :  0.05931256875361986\n",
            "Loss :  0.05935096661426011\n",
            "Loss :  0.05933242536871138\n",
            "Loss :  0.05936451828119102\n",
            "Loss :  0.05942126951352722\n",
            "Loss :  0.059381553345566614\n",
            "Validation: \n",
            " Loss :  0.07348376512527466\n",
            " Loss :  0.07958054844112623\n",
            " Loss :  0.07906072850270969\n",
            " Loss :  0.07878142503685638\n",
            " Loss :  0.07885957725438071\n",
            "\n",
            "Epoch: 55\n",
            "Loss :  0.06093607470393181\n",
            "Loss :  0.05925774303349582\n",
            "Loss :  0.05755300873092243\n",
            "Loss :  0.05812262335131245\n",
            "Loss :  0.05834873720276647\n",
            "Loss :  0.05819205742548494\n",
            "Loss :  0.058072104134031986\n",
            "Loss :  0.057737189069600174\n",
            "Loss :  0.05779944879957187\n",
            "Loss :  0.057882290824756516\n",
            "Loss :  0.057906197549978104\n",
            "Loss :  0.057860591121622035\n",
            "Loss :  0.05770879559034158\n",
            "Loss :  0.057905599133658954\n",
            "Loss :  0.05797319824919633\n",
            "Loss :  0.05814787207652401\n",
            "Loss :  0.05825788951160745\n",
            "Loss :  0.05846336616845856\n",
            "Loss :  0.058470721052990435\n",
            "Loss :  0.05840109489585093\n",
            "Loss :  0.05853876291741779\n",
            "Loss :  0.05875641284113247\n",
            "Loss :  0.058906787644009785\n",
            "Loss :  0.05909460834610514\n",
            "Loss :  0.05911944352121274\n",
            "Loss :  0.05908188643861577\n",
            "Loss :  0.05897661625368385\n",
            "Loss :  0.058894393271405755\n",
            "Loss :  0.05889853242506336\n",
            "Loss :  0.058912653040742544\n",
            "Loss :  0.0589147520768286\n",
            "Loss :  0.058945082643120233\n",
            "Loss :  0.05894799742882497\n",
            "Loss :  0.05897626701708647\n",
            "Loss :  0.05900715763443027\n",
            "Loss :  0.05907277122904093\n",
            "Loss :  0.05906418608785336\n",
            "Loss :  0.05905251135683124\n",
            "Loss :  0.05903905701488647\n",
            "Loss :  0.058993252399175064\n",
            "Loss :  0.05901336141320833\n",
            "Loss :  0.058987799270527205\n",
            "Loss :  0.058949247798981974\n",
            "Loss :  0.058888223221019914\n",
            "Loss :  0.05887687525578907\n",
            "Loss :  0.05888599283712402\n",
            "Loss :  0.05882781642184082\n",
            "Loss :  0.05884949652569056\n",
            "Loss :  0.05887752368643477\n",
            "Loss :  0.05883073543488615\n",
            "Validation: \n",
            " Loss :  0.07911770045757294\n",
            " Loss :  0.0855942712653251\n",
            " Loss :  0.0850142603967248\n",
            " Loss :  0.08434321914539962\n",
            " Loss :  0.08458456728193495\n",
            "\n",
            "Epoch: 56\n",
            "Loss :  0.060372285544872284\n",
            "Loss :  0.05580813369967721\n",
            "Loss :  0.056749657683429267\n",
            "Loss :  0.057520470071223476\n",
            "Loss :  0.05759804050733403\n",
            "Loss :  0.05725873641523661\n",
            "Loss :  0.05704114146408488\n",
            "Loss :  0.05710445407410743\n",
            "Loss :  0.05751842582299386\n",
            "Loss :  0.05756401217409542\n",
            "Loss :  0.057698341768861995\n",
            "Loss :  0.0575559352365163\n",
            "Loss :  0.05735065615620495\n",
            "Loss :  0.05756715261412941\n",
            "Loss :  0.0578634131514857\n",
            "Loss :  0.058072774936229185\n",
            "Loss :  0.058208933898380825\n",
            "Loss :  0.05821708825073744\n",
            "Loss :  0.0581910792758781\n",
            "Loss :  0.05810357462517254\n",
            "Loss :  0.05813124243966976\n",
            "Loss :  0.058226397781857946\n",
            "Loss :  0.05830241872916394\n",
            "Loss :  0.05845831513598368\n",
            "Loss :  0.05855438023558296\n",
            "Loss :  0.058516847377873035\n",
            "Loss :  0.0585761658447684\n",
            "Loss :  0.058529753100498134\n",
            "Loss :  0.05851190351612627\n",
            "Loss :  0.058482822709272\n",
            "Loss :  0.058536604083950734\n",
            "Loss :  0.05854935084029409\n",
            "Loss :  0.05853751355149664\n",
            "Loss :  0.05862301355087145\n",
            "Loss :  0.05866855146947844\n",
            "Loss :  0.058735564849909896\n",
            "Loss :  0.05867905636160658\n",
            "Loss :  0.058638047336568085\n",
            "Loss :  0.05858530811085476\n",
            "Loss :  0.05853315535218209\n",
            "Loss :  0.05848921298460473\n",
            "Loss :  0.05849167578593078\n",
            "Loss :  0.05844609815441514\n",
            "Loss :  0.05842898690126611\n",
            "Loss :  0.05840456208674545\n",
            "Loss :  0.05842838849368487\n",
            "Loss :  0.05840281432660179\n",
            "Loss :  0.058394250200938774\n",
            "Loss :  0.058404373752414064\n",
            "Loss :  0.05836670551365602\n",
            "Validation: \n",
            " Loss :  0.07652238756418228\n",
            " Loss :  0.08096530714205333\n",
            " Loss :  0.0803944790145246\n",
            " Loss :  0.08000298357400738\n",
            " Loss :  0.08022667550378376\n",
            "\n",
            "Epoch: 57\n",
            "Loss :  0.06429212540388107\n",
            "Loss :  0.05669154870239171\n",
            "Loss :  0.05684882455638477\n",
            "Loss :  0.05744878563188737\n",
            "Loss :  0.05699993897138572\n",
            "Loss :  0.056965828234074166\n",
            "Loss :  0.057104700657187916\n",
            "Loss :  0.05705239476872162\n",
            "Loss :  0.05719316736967475\n",
            "Loss :  0.0571116050193598\n",
            "Loss :  0.05715149338587676\n",
            "Loss :  0.05692572881643836\n",
            "Loss :  0.05683500615279537\n",
            "Loss :  0.05703088857290399\n",
            "Loss :  0.0573028842291088\n",
            "Loss :  0.05741032496686803\n",
            "Loss :  0.057543361547941006\n",
            "Loss :  0.05765739954703036\n",
            "Loss :  0.05772331182057686\n",
            "Loss :  0.05760014525258728\n",
            "Loss :  0.05766386462755464\n",
            "Loss :  0.057755660089130086\n",
            "Loss :  0.057762205196182116\n",
            "Loss :  0.05802200304029824\n",
            "Loss :  0.05804725608081244\n",
            "Loss :  0.05804665512832038\n",
            "Loss :  0.058062104729514466\n",
            "Loss :  0.057974263159661275\n",
            "Loss :  0.05802576791402284\n",
            "Loss :  0.0581480436718341\n",
            "Loss :  0.05824565651062319\n",
            "Loss :  0.05823678929370699\n",
            "Loss :  0.05822895389526061\n",
            "Loss :  0.05821097429470353\n",
            "Loss :  0.05829238137803819\n",
            "Loss :  0.05835701205749118\n",
            "Loss :  0.05833346509165711\n",
            "Loss :  0.05836378669521879\n",
            "Loss :  0.058412801161447536\n",
            "Loss :  0.05840145382086944\n",
            "Loss :  0.05841072612532654\n",
            "Loss :  0.058400717294274165\n",
            "Loss :  0.05836179236618187\n",
            "Loss :  0.05830268182396336\n",
            "Loss :  0.05832384250500575\n",
            "Loss :  0.05835538336846094\n",
            "Loss :  0.05831077129230582\n",
            "Loss :  0.0583195625596745\n",
            "Loss :  0.058346434986207195\n",
            "Loss :  0.05828730284773162\n",
            "Validation: \n",
            " Loss :  0.07748492062091827\n",
            " Loss :  0.07922624548276265\n",
            " Loss :  0.07839377715093333\n",
            " Loss :  0.07811969176667635\n",
            " Loss :  0.07804014653335382\n",
            "\n",
            "Epoch: 58\n",
            "Loss :  0.05859695374965668\n",
            "Loss :  0.056692360138351265\n",
            "Loss :  0.05571597300115086\n",
            "Loss :  0.0561500929536358\n",
            "Loss :  0.05597222560062641\n",
            "Loss :  0.05628523223248182\n",
            "Loss :  0.056248203408522685\n",
            "Loss :  0.056232516666952996\n",
            "Loss :  0.05630414799591641\n",
            "Loss :  0.056388397342883624\n",
            "Loss :  0.056411239549075023\n",
            "Loss :  0.0562384332286882\n",
            "Loss :  0.05618924111866754\n",
            "Loss :  0.05643149115543329\n",
            "Loss :  0.056661830673403774\n",
            "Loss :  0.05673510657734429\n",
            "Loss :  0.05691780874170132\n",
            "Loss :  0.05718679859014283\n",
            "Loss :  0.05721593421796409\n",
            "Loss :  0.057220528477617584\n",
            "Loss :  0.057233478503292474\n",
            "Loss :  0.057354649996729255\n",
            "Loss :  0.057431002817542307\n",
            "Loss :  0.05768667879107194\n",
            "Loss :  0.057815137616826294\n",
            "Loss :  0.057789458429433435\n",
            "Loss :  0.05776784446276011\n",
            "Loss :  0.057739063767489475\n",
            "Loss :  0.05773898552161943\n",
            "Loss :  0.0578224091467374\n",
            "Loss :  0.057853586204424254\n",
            "Loss :  0.057816146259043376\n",
            "Loss :  0.05782697853455291\n",
            "Loss :  0.05786024283876592\n",
            "Loss :  0.05795644276128137\n",
            "Loss :  0.058037935466932776\n",
            "Loss :  0.05803367932045889\n",
            "Loss :  0.05806979526968979\n",
            "Loss :  0.05805662436669893\n",
            "Loss :  0.0580178129360499\n",
            "Loss :  0.05801143777786645\n",
            "Loss :  0.05799372072746284\n",
            "Loss :  0.0579777888759015\n",
            "Loss :  0.057919442463501974\n",
            "Loss :  0.057915319762532676\n",
            "Loss :  0.057900404933418775\n",
            "Loss :  0.05784574930036921\n",
            "Loss :  0.057828781829707944\n",
            "Loss :  0.05785530316110956\n",
            "Loss :  0.057824760371944325\n",
            "Validation: \n",
            " Loss :  0.08194702863693237\n",
            " Loss :  0.08728968174684615\n",
            " Loss :  0.08681955152168506\n",
            " Loss :  0.0867576683398153\n",
            " Loss :  0.08677850930411139\n",
            "\n",
            "Epoch: 59\n",
            "Loss :  0.0613485611975193\n",
            "Loss :  0.05710194869474931\n",
            "Loss :  0.05648813645044962\n",
            "Loss :  0.05714664716393717\n",
            "Loss :  0.05727773641304272\n",
            "Loss :  0.05716419862765892\n",
            "Loss :  0.05712439699983988\n",
            "Loss :  0.0568739277586131\n",
            "Loss :  0.0572016512354215\n",
            "Loss :  0.057250319847038815\n",
            "Loss :  0.05711741513102361\n",
            "Loss :  0.05706060130719666\n",
            "Loss :  0.05699190996156251\n",
            "Loss :  0.05736275096886031\n",
            "Loss :  0.05740334378912094\n",
            "Loss :  0.05758361607197894\n",
            "Loss :  0.05761224459509672\n",
            "Loss :  0.05771240642109112\n",
            "Loss :  0.05768290069468772\n",
            "Loss :  0.05767649895857766\n",
            "Loss :  0.05772799436948193\n",
            "Loss :  0.05782198152059062\n",
            "Loss :  0.0578484168250906\n",
            "Loss :  0.05795069666200386\n",
            "Loss :  0.0579743334613895\n",
            "Loss :  0.057879722495359255\n",
            "Loss :  0.05793243940648448\n",
            "Loss :  0.057896323873218136\n",
            "Loss :  0.05790898415965966\n",
            "Loss :  0.057961034022041204\n",
            "Loss :  0.05796442721719758\n",
            "Loss :  0.05797309216435316\n",
            "Loss :  0.05798190074314209\n",
            "Loss :  0.05798982666788505\n",
            "Loss :  0.057982108449219256\n",
            "Loss :  0.058022619641659265\n",
            "Loss :  0.05796954919096506\n",
            "Loss :  0.05795616210711934\n",
            "Loss :  0.05785244105877526\n",
            "Loss :  0.05781633255388731\n",
            "Loss :  0.05777904436809761\n",
            "Loss :  0.0577721963398648\n",
            "Loss :  0.057752042911468945\n",
            "Loss :  0.05767119318903737\n",
            "Loss :  0.05767076947386303\n",
            "Loss :  0.05763477409989501\n",
            "Loss :  0.05756663956119802\n",
            "Loss :  0.05755344160581344\n",
            "Loss :  0.057556592152606924\n",
            "Loss :  0.05751264413966666\n",
            "Validation: \n",
            " Loss :  0.0772971361875534\n",
            " Loss :  0.08232522436550685\n",
            " Loss :  0.08159851410040041\n",
            " Loss :  0.08172662995877814\n",
            " Loss :  0.08200494475938656\n",
            "\n",
            "Epoch: 60\n",
            "Loss :  0.06250016391277313\n",
            "Loss :  0.055087213489142334\n",
            "Loss :  0.055307810327836444\n",
            "Loss :  0.05591589524861305\n",
            "Loss :  0.05606709157184857\n",
            "Loss :  0.05620090385862425\n",
            "Loss :  0.056378592721751476\n",
            "Loss :  0.05610567955693729\n",
            "Loss :  0.05619988070777905\n",
            "Loss :  0.05651773299489703\n",
            "Loss :  0.05643323296219996\n",
            "Loss :  0.05635807082593978\n",
            "Loss :  0.05627159016930367\n",
            "Loss :  0.05662991004135772\n",
            "Loss :  0.05679886753783158\n",
            "Loss :  0.05699940442742891\n",
            "Loss :  0.05705802815948954\n",
            "Loss :  0.057234405708766124\n",
            "Loss :  0.057295625298721356\n",
            "Loss :  0.05720687480107028\n",
            "Loss :  0.05720861495208385\n",
            "Loss :  0.05737143214703736\n",
            "Loss :  0.05742351704053749\n",
            "Loss :  0.057602745871910284\n",
            "Loss :  0.05770213257413188\n",
            "Loss :  0.05765917012653503\n",
            "Loss :  0.05761483214595765\n",
            "Loss :  0.057525272408974565\n",
            "Loss :  0.05748761504941564\n",
            "Loss :  0.05750945899187494\n",
            "Loss :  0.057498953308278936\n",
            "Loss :  0.05742784855450081\n",
            "Loss :  0.05739495391461337\n",
            "Loss :  0.057379940901728196\n",
            "Loss :  0.057463913773598906\n",
            "Loss :  0.05753063576089011\n",
            "Loss :  0.057504348002807584\n",
            "Loss :  0.05751131843484315\n",
            "Loss :  0.05746609196457963\n",
            "Loss :  0.057436468246419105\n",
            "Loss :  0.05739999582010909\n",
            "Loss :  0.057374725956696376\n",
            "Loss :  0.057363428078113995\n",
            "Loss :  0.057296596519355154\n",
            "Loss :  0.0572864567256974\n",
            "Loss :  0.05729469837013739\n",
            "Loss :  0.05719881144366399\n",
            "Loss :  0.05716784451860784\n",
            "Loss :  0.05715609129649934\n",
            "Loss :  0.05710230719927858\n",
            "Validation: \n",
            " Loss :  0.07652295380830765\n",
            " Loss :  0.08058544070947737\n",
            " Loss :  0.08002650556040973\n",
            " Loss :  0.07992396491472839\n",
            " Loss :  0.07989441787387118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Finetuning student on crossentropy\n",
        "Now the student is trained. In this cell we need to replace the classifier(i.e: Fully Connected layer) of student network from one with output shape of dense feature to one with shape of classes. e.g: nn.Linear(256,512) to nn.Linear(256,10). After this we need to freez Conv layers in the network and finetune the network using orignal dataset. "
      ],
      "metadata": {
        "id": "N5rEK4VmiN_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s1.classifier = nn.Linear(512, 10)\n",
        "for m in s1.modules():\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        m.weight.requires_grad = False\n",
        "        if m.bias is not None:\n",
        "            m.bias.requires_grad = False\n",
        "s1 = s1.to(device)\n",
        "summary(s1, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv_cGp5aiSbS",
        "outputId": "c11acb9c-f22e-4721-ddb9-31e711179ae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "           Conv2d-29            [-1, 512, 2, 2]       1,180,160\n",
            "      BatchNorm2d-30            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-31            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-32            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-33            [-1, 512, 1, 1]               0\n",
            "           Linear-34                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 2,360,490\n",
            "Trainable params: 8,074\n",
            "Non-trainable params: 2,352,416\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.98\n",
            "Params size (MB): 9.00\n",
            "Estimated Total Size (MB): 12.00\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(s1.parameters(), lr=0.0001)\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    s1.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        s1.zero_grad()\n",
        "        outputs = s1(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if(batch_idx % 200 == 0):\n",
        "          print(\"Accuracy : \",100.*correct/total,\" Loss : \", train_loss/(batch_idx+1))\n",
        "def test(epoch):\n",
        "    s1.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = s1(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\"Accuracy : \",100.*correct/total,\" Loss : \", test_loss/(batch_idx+1))"
      ],
      "metadata": {
        "id": "t7gwqasZiseD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "    train(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGgfdwA-iue-",
        "outputId": "042f3ea0-48aa-4d54-9cb0-acbe9756a4b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Accuracy :  13.0  Loss :  2.3406362533569336\n",
            "Accuracy :  69.44278606965175  Loss :  1.4717139343717205\n",
            "Accuracy :  77.3640897755611  Loss :  1.1176168737268806\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.5915505886077881\n",
            "Accuracy :  84.14285714285714  Loss :  0.5783692726067134\n",
            "Accuracy :  83.82926829268293  Loss :  0.5829475471159307\n",
            "Accuracy :  84.09836065573771  Loss :  0.5820076895541832\n",
            "Accuracy :  84.11111111111111  Loss :  0.5806706735381374\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy :  82.0  Loss :  0.5544735193252563\n",
            "Accuracy :  85.71641791044776  Loss :  0.5207162867138042\n",
            "Accuracy :  86.07481296758105  Loss :  0.49043705888519856\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.4678022861480713\n",
            "Accuracy :  85.0  Loss :  0.4648697092419579\n",
            "Accuracy :  84.60975609756098  Loss :  0.4711946045480123\n",
            "Accuracy :  84.75409836065573  Loss :  0.46949492419352296\n",
            "Accuracy :  84.85185185185185  Loss :  0.4675458275977476\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy :  87.0  Loss :  0.40074536204338074\n",
            "Accuracy :  86.33830845771145  Loss :  0.4234069590248279\n",
            "Accuracy :  86.60598503740648  Loss :  0.41434788826546465\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.4359573721885681\n",
            "Accuracy :  85.04761904761905  Loss :  0.4403695875690097\n",
            "Accuracy :  84.85365853658537  Loss :  0.4470213804303146\n",
            "Accuracy :  85.06557377049181  Loss :  0.4447761070532877\n",
            "Accuracy :  85.1604938271605  Loss :  0.4426679692150634\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy :  89.0  Loss :  0.36340785026550293\n",
            "Accuracy :  86.72636815920399  Loss :  0.39733397708603396\n",
            "Accuracy :  86.88778054862843  Loss :  0.39068682811355354\n",
            "Validation: \n",
            "Accuracy :  88.0  Loss :  0.4188281297683716\n",
            "Accuracy :  85.23809523809524  Loss :  0.43245758754866465\n",
            "Accuracy :  85.09756097560975  Loss :  0.4392263460450056\n",
            "Accuracy :  85.21311475409836  Loss :  0.43702534378552044\n",
            "Accuracy :  85.32098765432099  Loss :  0.43483704917224836\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy :  88.0  Loss :  0.3661419153213501\n",
            "Accuracy :  86.8905472636816  Loss :  0.38450111826853967\n",
            "Accuracy :  87.01995012468828  Loss :  0.38024007675802324\n",
            "Validation: \n",
            "Accuracy :  88.0  Loss :  0.40844354033470154\n",
            "Accuracy :  85.42857142857143  Loss :  0.4295568572623389\n",
            "Accuracy :  85.1951219512195  Loss :  0.43673316044051474\n",
            "Accuracy :  85.22950819672131  Loss :  0.43440292824487214\n",
            "Accuracy :  85.33333333333333  Loss :  0.4321039092761499\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy :  92.0  Loss :  0.33555862307548523\n",
            "Accuracy :  87.12437810945273  Loss :  0.380191825960406\n",
            "Accuracy :  87.24688279301746  Loss :  0.375374685573459\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4065747857093811\n",
            "Accuracy :  85.23809523809524  Loss :  0.4284237083934602\n",
            "Accuracy :  85.1219512195122  Loss :  0.4354355538763651\n",
            "Accuracy :  85.26229508196721  Loss :  0.4327308882455357\n",
            "Accuracy :  85.4074074074074  Loss :  0.4303741723666956\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy :  91.0  Loss :  0.36680299043655396\n",
            "Accuracy :  86.96517412935323  Loss :  0.37606080691909316\n",
            "Accuracy :  87.23690773067332  Loss :  0.372890130354282\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4015743136405945\n",
            "Accuracy :  85.28571428571429  Loss :  0.427399259238016\n",
            "Accuracy :  85.17073170731707  Loss :  0.43431568727260683\n",
            "Accuracy :  85.31147540983606  Loss :  0.43208492583915836\n",
            "Accuracy :  85.46913580246914  Loss :  0.4296861316686795\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy :  89.0  Loss :  0.3054676055908203\n",
            "Accuracy :  87.13930348258707  Loss :  0.3697983798251223\n",
            "Accuracy :  87.28678304239402  Loss :  0.368000478741534\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.40024876594543457\n",
            "Accuracy :  85.38095238095238  Loss :  0.42641933049474445\n",
            "Accuracy :  85.21951219512195  Loss :  0.43326170197347313\n",
            "Accuracy :  85.39344262295081  Loss :  0.43038281651793936\n",
            "Accuracy :  85.50617283950618  Loss :  0.4277764750115665\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy :  90.0  Loss :  0.2875247299671173\n",
            "Accuracy :  87.28358208955224  Loss :  0.368846304603477\n",
            "Accuracy :  87.3790523690773  Loss :  0.3667525038531891\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.3947552442550659\n",
            "Accuracy :  85.28571428571429  Loss :  0.42599410599186305\n",
            "Accuracy :  85.29268292682927  Loss :  0.43284123009297903\n",
            "Accuracy :  85.42622950819673  Loss :  0.4302906450189528\n",
            "Accuracy :  85.60493827160494  Loss :  0.4276731468645143\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy :  88.0  Loss :  0.30968278646469116\n",
            "Accuracy :  87.45273631840796  Loss :  0.36800066370572615\n",
            "Accuracy :  87.53615960099751  Loss :  0.36524396882092863\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.3901783227920532\n",
            "Accuracy :  85.42857142857143  Loss :  0.42506549117111025\n",
            "Accuracy :  85.29268292682927  Loss :  0.4321807902760622\n",
            "Accuracy :  85.49180327868852  Loss :  0.42954785730995115\n",
            "Accuracy :  85.60493827160494  Loss :  0.4268613422726407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_save_name = '1student.pt'\n",
        "# path = F\"/content/gdrive/MyDrive/{model_save_name}\" \n",
        "# torch.save(s1.state_dict(), path)"
      ],
      "metadata": {
        "id": "285Zfq11iwx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_name = '1student.pt'\n",
        "path = F\"/content/gdrive/MyDrive/{model_save_name}\" \n",
        "s1.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "eeygMyzeeKF2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90332f16-cb6d-486a-a332-e471ac3bb3c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extract denseFeatures from s1"
      ],
      "metadata": {
        "id": "SsHS6Sb4k4t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "S1_AS_TA_WOH = nn.Sequential(*list(s1.children())[:-1],nn.Flatten())\n",
        "summary(s1, (3, 32, 32))\n",
        "summary(S1_AS_TA_WOH, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rSy2A3uk8cF",
        "outputId": "d0f98756-ae0f-4ef1-a312-b1e9d92092dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "           Conv2d-29            [-1, 512, 2, 2]       1,180,160\n",
            "      BatchNorm2d-30            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-31            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-32            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-33            [-1, 512, 1, 1]               0\n",
            "           Linear-34                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 2,360,490\n",
            "Trainable params: 8,074\n",
            "Non-trainable params: 2,352,416\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.98\n",
            "Params size (MB): 9.00\n",
            "Estimated Total Size (MB): 12.00\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "           Conv2d-29            [-1, 512, 2, 2]       1,180,160\n",
            "      BatchNorm2d-30            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-31            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-32            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-33            [-1, 512, 1, 1]               0\n",
            "          Flatten-34                  [-1, 512]               0\n",
            "================================================================\n",
            "Total params: 2,355,360\n",
            "Trainable params: 2,944\n",
            "Non-trainable params: 2,352,416\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.99\n",
            "Params size (MB): 8.98\n",
            "Estimated Total Size (MB): 11.98\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "S1_AS_TA_WOH.eval()\n",
        "S1DenseTrain = None\n",
        "s1DenseTest = None\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = S1_AS_TA_WOH(inputs)\n",
        "        if(S1DenseTrain == None):\n",
        "            S1DenseTrain = outputs\n",
        "        else:\n",
        "            S1DenseTrain = torch.cat((S1DenseTrain,outputs))\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = S1_AS_TA_WOH(inputs)\n",
        "        if(s1DenseTest == None):\n",
        "            s1DenseTest = outputs\n",
        "        else:\n",
        "            s1DenseTest = torch.cat((s1DenseTest,outputs))"
      ],
      "metadata": {
        "id": "4w6BzS6LlRvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating 2 more students "
      ],
      "metadata": {
        "id": "guODYUgtigU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "    'VGGS': [32, 32, 'M', 64, 64, 'M', 128, 128, 'M', 256, 256, 'M','M'],\n",
        "    \n",
        "\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(256, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "s01 = VGG('VGGS')\n",
        "s01 = s01.to(device)\n",
        "summary(s01, (3, 32, 32))\n",
        "s2 = VGG('VGGS')\n",
        "s2 = s2.to(device)\n",
        "summary(s2, (3, 32, 32))"
      ],
      "metadata": {
        "id": "5TbTN57Ke7ZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30358c91-ed06-4c29-b0c8-1d816adbfd0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 256, 1, 1]               0\n",
            "           Linear-31                  [-1, 256]          65,792\n",
            "================================================================\n",
            "Total params: 1,239,968\n",
            "Trainable params: 1,239,968\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.94\n",
            "Params size (MB): 4.73\n",
            "Estimated Total Size (MB): 7.68\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 256, 1, 1]               0\n",
            "           Linear-31                  [-1, 256]          65,792\n",
            "================================================================\n",
            "Total params: 1,239,968\n",
            "Trainable params: 1,239,968\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.94\n",
            "Params size (MB): 4.73\n",
            "Estimated Total Size (MB): 7.68\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Multi Students\n",
        "1.6 In this step you will train two students instead of one. In the training loop you will pass the input from both students and then backwark both the losses. "
      ],
      "metadata": {
        "id": "ZYIy2HShm_Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer1 = optim.Adam(s01.parameters(), lr=0.0001)\n",
        "optimizer2 = optim.Adam(s2.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    s01.train()\n",
        "    train_loss1 = 0\n",
        "    train_loss2 = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        #code here\n",
        "        targets = S1DenseTrain[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "        targets.to(device)\n",
        "        s01.zero_grad()\n",
        "        s2.zero_grad()\n",
        "        output1 = s01(inputs)\n",
        "        output2 = s2(inputs)\n",
        "        loss1 = criterion(output1, targets[:,:256])\n",
        "        loss2 = criterion(output2, targets[:,256:])\n",
        "        loss1.backward()\n",
        "        loss2.backward()\n",
        "        optimizer1.step()\n",
        "        optimizer2.step()\n",
        "\n",
        "        train_loss1 += loss1.item()\n",
        "        train_loss2 += loss2.item()\n",
        "        if(batch_idx % 10 == 0):\n",
        "          print(\"Loss S01: \", train_loss1/(batch_idx+1))\n",
        "          print(\"Loss S2: \", train_loss2/(batch_idx+1))\n",
        "def test(epoch):\n",
        "    s01.eval()\n",
        "    test_loss1 = 0\n",
        "    test_loss2 = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            #code here\n",
        "            targets = s1DenseTest[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "            targets.to(device)\n",
        "            output1 = s01(inputs)\n",
        "            output2 = s2(inputs)\n",
        "            loss1 = criterion(output1, targets[:,:256])\n",
        "            loss2 = criterion(output2, targets[:,256:])\n",
        "            test_loss1 += loss1.item()\n",
        "            test_loss2 += loss2.item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\" Loss S01: \", test_loss1/(batch_idx+1))\n",
        "              print(\" Loss S2: \", test_loss2/(batch_idx+1))"
      ],
      "metadata": {
        "id": "kAcYq4NrnBBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+60):\n",
        "    train(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test(epoch)"
      ],
      "metadata": {
        "id": "RQJvw4e4nDwb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20daa530-8f5c-42e7-d477-c6830c43c77c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Loss S2:  0.04375377510404645\n",
            "Loss S01:  0.038112056333208875\n",
            "Loss S2:  0.04377395882671633\n",
            "Loss S01:  0.03808712256583979\n",
            "Loss S2:  0.043727828505213465\n",
            "Loss S01:  0.038085202637916246\n",
            "Loss S2:  0.04373429564425464\n",
            "Loss S01:  0.03805583996281127\n",
            "Loss S2:  0.04373177210335721\n",
            "Loss S01:  0.0380508425017241\n",
            "Loss S2:  0.04374178717167414\n",
            "Loss S01:  0.03805911801201776\n",
            "Loss S2:  0.043720615580836175\n",
            "Loss S01:  0.038063195307512544\n",
            "Loss S2:  0.04373016571645429\n",
            "Loss S01:  0.03802304793824489\n",
            "Loss S2:  0.04369718822709412\n",
            "Validation: \n",
            " Loss S01:  0.03389983996748924\n",
            " Loss S2:  0.04940960928797722\n",
            " Loss S01:  0.0355720402938979\n",
            " Loss S2:  0.05561993909733636\n",
            " Loss S01:  0.035594048750836674\n",
            " Loss S2:  0.05614946591781407\n",
            " Loss S01:  0.03548704863327448\n",
            " Loss S2:  0.05598935301675171\n",
            " Loss S01:  0.03547275038780989\n",
            " Loss S2:  0.05580454098957556\n",
            "\n",
            "Epoch: 17\n",
            "Loss S01:  0.04076644405722618\n",
            "Loss S2:  0.0450172983109951\n",
            "Loss S01:  0.037159763276576996\n",
            "Loss S2:  0.042032941498539665\n",
            "Loss S01:  0.03769184560293243\n",
            "Loss S2:  0.042202010041191465\n",
            "Loss S01:  0.037644308901602225\n",
            "Loss S2:  0.042378940289058996\n",
            "Loss S01:  0.03786230632444707\n",
            "Loss S2:  0.042460024629424255\n",
            "Loss S01:  0.037912379292880785\n",
            "Loss S2:  0.04285842070684714\n",
            "Loss S01:  0.03764648619489592\n",
            "Loss S2:  0.04280998014280053\n",
            "Loss S01:  0.0375610883282104\n",
            "Loss S2:  0.042833467132188906\n",
            "Loss S01:  0.03739904805465981\n",
            "Loss S2:  0.04267110781353197\n",
            "Loss S01:  0.03724976727268198\n",
            "Loss S2:  0.04257061328370493\n",
            "Loss S01:  0.037147260501538173\n",
            "Loss S2:  0.04246061073966546\n",
            "Loss S01:  0.0370935465435724\n",
            "Loss S2:  0.042373569396970505\n",
            "Loss S01:  0.03711435722171768\n",
            "Loss S2:  0.042393899930656447\n",
            "Loss S01:  0.037116384147915224\n",
            "Loss S2:  0.04248495664419109\n",
            "Loss S01:  0.03710951568915489\n",
            "Loss S2:  0.04247912598100114\n",
            "Loss S01:  0.037173682095988696\n",
            "Loss S2:  0.042516753620264545\n",
            "Loss S01:  0.03714987566197141\n",
            "Loss S2:  0.04255898238986916\n",
            "Loss S01:  0.037299102945634495\n",
            "Loss S2:  0.04266383736367114\n",
            "Loss S01:  0.037349626236692975\n",
            "Loss S2:  0.042724085899677064\n",
            "Loss S01:  0.03729396859544734\n",
            "Loss S2:  0.042747609063748915\n",
            "Loss S01:  0.03723080458110245\n",
            "Loss S2:  0.04273641893445556\n",
            "Loss S01:  0.03724677731852396\n",
            "Loss S2:  0.04277139693781098\n",
            "Loss S01:  0.0372513355307989\n",
            "Loss S2:  0.04279319965596652\n",
            "Loss S01:  0.03727063409242279\n",
            "Loss S2:  0.04275621001274039\n",
            "Loss S01:  0.03733870326792551\n",
            "Loss S2:  0.04276501296157659\n",
            "Loss S01:  0.037352710813877595\n",
            "Loss S2:  0.04281600594401835\n",
            "Loss S01:  0.03735620544160006\n",
            "Loss S2:  0.04284818760222859\n",
            "Loss S01:  0.03729243884447316\n",
            "Loss S2:  0.04280682550090265\n",
            "Loss S01:  0.03725459013014926\n",
            "Loss S2:  0.04277489755925759\n",
            "Loss S01:  0.037232268243208785\n",
            "Loss S2:  0.042780716177012096\n",
            "Loss S01:  0.037247959388252906\n",
            "Loss S2:  0.042747249387427425\n",
            "Loss S01:  0.03720137773986031\n",
            "Loss S2:  0.04271065842635762\n",
            "Loss S01:  0.03719070478345375\n",
            "Loss S2:  0.04274212365906187\n",
            "Loss S01:  0.037194883276418615\n",
            "Loss S2:  0.04274813385168231\n",
            "Loss S01:  0.03721629560561823\n",
            "Loss S2:  0.04278333865302749\n",
            "Loss S01:  0.03723351500312827\n",
            "Loss S2:  0.04279285531353067\n",
            "Loss S01:  0.03722473901601049\n",
            "Loss S2:  0.04278620085846684\n",
            "Loss S01:  0.03719899772272277\n",
            "Loss S2:  0.04274572450115353\n",
            "Loss S01:  0.03722685496286145\n",
            "Loss S2:  0.042705499114874465\n",
            "Loss S01:  0.03719299260879417\n",
            "Loss S2:  0.042658618001072\n",
            "Loss S01:  0.037178784385880924\n",
            "Loss S2:  0.042628563782595046\n",
            "Loss S01:  0.037180774765873184\n",
            "Loss S2:  0.04260432280581943\n",
            "Loss S01:  0.03720026593345644\n",
            "Loss S2:  0.04262115511680442\n",
            "Loss S01:  0.03716581846369115\n",
            "Loss S2:  0.04257212026074978\n",
            "Loss S01:  0.03716289788450784\n",
            "Loss S2:  0.042568431275974865\n",
            "Loss S01:  0.03714111004594689\n",
            "Loss S2:  0.042554796766283244\n",
            "Loss S01:  0.03715289294428784\n",
            "Loss S2:  0.04256028273908539\n",
            "Loss S01:  0.037150008860983415\n",
            "Loss S2:  0.04255680891738576\n",
            "Loss S01:  0.03713888696660123\n",
            "Loss S2:  0.04255151132164279\n",
            "Loss S01:  0.03708889354361779\n",
            "Loss S2:  0.04250951990667283\n",
            "Validation: \n",
            " Loss S01:  0.032552897930145264\n",
            " Loss S2:  0.04981746897101402\n",
            " Loss S01:  0.03525014008794512\n",
            " Loss S2:  0.05568536938655944\n",
            " Loss S01:  0.03515737485594866\n",
            " Loss S2:  0.056094068034393034\n",
            " Loss S01:  0.03500081140731202\n",
            " Loss S2:  0.05599319641707373\n",
            " Loss S01:  0.03498907624111499\n",
            " Loss S2:  0.055747966201585016\n",
            "\n",
            "Epoch: 18\n",
            "Loss S01:  0.043653521686792374\n",
            "Loss S2:  0.045387573540210724\n",
            "Loss S01:  0.03600378842516379\n",
            "Loss S2:  0.04134950719096444\n",
            "Loss S01:  0.036277021325769876\n",
            "Loss S2:  0.041671112711940496\n",
            "Loss S01:  0.03622503686816462\n",
            "Loss S2:  0.04152372179012145\n",
            "Loss S01:  0.03664853778190729\n",
            "Loss S2:  0.041651844705750306\n",
            "Loss S01:  0.03677444663994452\n",
            "Loss S2:  0.041845990965763726\n",
            "Loss S01:  0.03658743251542576\n",
            "Loss S2:  0.0418135120121182\n",
            "Loss S01:  0.03642960708640831\n",
            "Loss S2:  0.041802468539123804\n",
            "Loss S01:  0.03632682072067702\n",
            "Loss S2:  0.04152301326394081\n",
            "Loss S01:  0.03619255516473409\n",
            "Loss S2:  0.041357824967785194\n",
            "Loss S01:  0.03612893128232791\n",
            "Loss S2:  0.04117595341683614\n",
            "Loss S01:  0.036135305675703125\n",
            "Loss S2:  0.04114695500817385\n",
            "Loss S01:  0.03620180793968607\n",
            "Loss S2:  0.041232059594274555\n",
            "Loss S01:  0.036278521243966265\n",
            "Loss S2:  0.0413198284804821\n",
            "Loss S01:  0.036306195291327246\n",
            "Loss S2:  0.041351167022759185\n",
            "Loss S01:  0.03632011458228361\n",
            "Loss S2:  0.041414622562807916\n",
            "Loss S01:  0.03622602325464998\n",
            "Loss S2:  0.04144932146860946\n",
            "Loss S01:  0.036299852131490125\n",
            "Loss S2:  0.04160556968366891\n",
            "Loss S01:  0.036343904932477196\n",
            "Loss S2:  0.041659995913505554\n",
            "Loss S01:  0.03627680545350956\n",
            "Loss S2:  0.04162036890836911\n",
            "Loss S01:  0.03626188703474417\n",
            "Loss S2:  0.04158974937464467\n",
            "Loss S01:  0.03632454214820647\n",
            "Loss S2:  0.041615573415682776\n",
            "Loss S01:  0.03641077888733391\n",
            "Loss S2:  0.0416631705013875\n",
            "Loss S01:  0.036416029694663496\n",
            "Loss S2:  0.041635608518278446\n",
            "Loss S01:  0.03646266706195115\n",
            "Loss S2:  0.04168516627794974\n",
            "Loss S01:  0.036454712608420994\n",
            "Loss S2:  0.04173842253556764\n",
            "Loss S01:  0.03647286861678193\n",
            "Loss S2:  0.041774738594261625\n",
            "Loss S01:  0.036404086201283326\n",
            "Loss S2:  0.04171769855702055\n",
            "Loss S01:  0.03640332588680698\n",
            "Loss S2:  0.04169744043693848\n",
            "Loss S01:  0.03641255605722621\n",
            "Loss S2:  0.041715513057110645\n",
            "Loss S01:  0.03640965584578902\n",
            "Loss S2:  0.04172254760944566\n",
            "Loss S01:  0.03637244346943891\n",
            "Loss S2:  0.041647846747632965\n",
            "Loss S01:  0.03639312072822424\n",
            "Loss S2:  0.041675072465080336\n",
            "Loss S01:  0.03639319045553575\n",
            "Loss S2:  0.041690236964794805\n",
            "Loss S01:  0.03641870745138164\n",
            "Loss S2:  0.041739268288497\n",
            "Loss S01:  0.03644056145388346\n",
            "Loss S2:  0.04175960320310715\n",
            "Loss S01:  0.03641495354721255\n",
            "Loss S2:  0.04174679878923702\n",
            "Loss S01:  0.03637386446271463\n",
            "Loss S2:  0.041722426597641485\n",
            "Loss S01:  0.03638154312753145\n",
            "Loss S2:  0.04169914304349679\n",
            "Loss S01:  0.03634877491961507\n",
            "Loss S2:  0.04165823967255595\n",
            "Loss S01:  0.036330023134810076\n",
            "Loss S2:  0.041661990848265384\n",
            "Loss S01:  0.036317803998933224\n",
            "Loss S2:  0.0416414819549035\n",
            "Loss S01:  0.036333826253292394\n",
            "Loss S2:  0.041655786415106325\n",
            "Loss S01:  0.036299001559476844\n",
            "Loss S2:  0.04162498411404838\n",
            "Loss S01:  0.036309017856814424\n",
            "Loss S2:  0.04164411674956886\n",
            "Loss S01:  0.03629128266943664\n",
            "Loss S2:  0.041617069104294024\n",
            "Loss S01:  0.03629400195662991\n",
            "Loss S2:  0.041616276278283744\n",
            "Loss S01:  0.036306248665362154\n",
            "Loss S2:  0.04160675306798546\n",
            "Loss S01:  0.03631277939231133\n",
            "Loss S2:  0.04162417141538648\n",
            "Loss S01:  0.03626428797173403\n",
            "Loss S2:  0.041588931145286855\n",
            "Validation: \n",
            " Loss S01:  0.030818337574601173\n",
            " Loss S2:  0.04865409806370735\n",
            " Loss S01:  0.03291507641829196\n",
            " Loss S2:  0.05416299065663701\n",
            " Loss S01:  0.032918173274615915\n",
            " Loss S2:  0.05448517921130832\n",
            " Loss S01:  0.03298381105309627\n",
            " Loss S2:  0.0544540634775748\n",
            " Loss S01:  0.03303811505988792\n",
            " Loss S2:  0.05420815057040733\n",
            "\n",
            "Epoch: 19\n",
            "Loss S01:  0.039005961269140244\n",
            "Loss S2:  0.04267936944961548\n",
            "Loss S01:  0.03508283197879791\n",
            "Loss S2:  0.039382832971486176\n",
            "Loss S01:  0.03514405410914194\n",
            "Loss S2:  0.04043975827239808\n",
            "Loss S01:  0.03531689076654373\n",
            "Loss S2:  0.04051668689616265\n",
            "Loss S01:  0.035646240340500346\n",
            "Loss S2:  0.04058470967702749\n",
            "Loss S01:  0.03576441423273554\n",
            "Loss S2:  0.04074893234407201\n",
            "Loss S01:  0.035576061635720926\n",
            "Loss S2:  0.04066636382800634\n",
            "Loss S01:  0.03556901961565018\n",
            "Loss S2:  0.040613527711428385\n",
            "Loss S01:  0.035426557408990686\n",
            "Loss S2:  0.04039213737403905\n",
            "Loss S01:  0.035304461943579245\n",
            "Loss S2:  0.040305780594820505\n",
            "Loss S01:  0.035245074125209656\n",
            "Loss S2:  0.040200655671334504\n",
            "Loss S01:  0.035263200097524365\n",
            "Loss S2:  0.04014440676247751\n",
            "Loss S01:  0.03527549971356865\n",
            "Loss S2:  0.04021299368709572\n",
            "Loss S01:  0.0352941045665559\n",
            "Loss S2:  0.04026570085805791\n",
            "Loss S01:  0.03532420447532167\n",
            "Loss S2:  0.04031410979780745\n",
            "Loss S01:  0.03536937588098033\n",
            "Loss S2:  0.04040678671949747\n",
            "Loss S01:  0.03533193523422902\n",
            "Loss S2:  0.04041411808651427\n",
            "Loss S01:  0.035429248717008976\n",
            "Loss S2:  0.04055175779942881\n",
            "Loss S01:  0.03546314076288958\n",
            "Loss S2:  0.04060893571673177\n",
            "Loss S01:  0.035412828696413816\n",
            "Loss S2:  0.04060630406696759\n",
            "Loss S01:  0.03542333895071822\n",
            "Loss S2:  0.04054532624521659\n",
            "Loss S01:  0.035438273463980846\n",
            "Loss S2:  0.04056196942289859\n",
            "Loss S01:  0.03546310178261267\n",
            "Loss S2:  0.04055619216208005\n",
            "Loss S01:  0.03544422271318766\n",
            "Loss S2:  0.04052017717843964\n",
            "Loss S01:  0.035514866492683955\n",
            "Loss S2:  0.040538050929911404\n",
            "Loss S01:  0.035508292174315545\n",
            "Loss S2:  0.040578053173315955\n",
            "Loss S01:  0.035527954099278794\n",
            "Loss S2:  0.04059402153638131\n",
            "Loss S01:  0.03550451351982641\n",
            "Loss S2:  0.040546559003556346\n",
            "Loss S01:  0.03550335312812354\n",
            "Loss S2:  0.040535816770325354\n",
            "Loss S01:  0.03550520159073712\n",
            "Loss S2:  0.04056906396734346\n",
            "Loss S01:  0.0355173894683784\n",
            "Loss S2:  0.04058528648212899\n",
            "Loss S01:  0.03550216851462505\n",
            "Loss S2:  0.04056624072780563\n",
            "Loss S01:  0.03551268312975625\n",
            "Loss S2:  0.040583485093350725\n",
            "Loss S01:  0.03552470475525294\n",
            "Loss S2:  0.04061601884955726\n",
            "Loss S01:  0.0355424143352117\n",
            "Loss S2:  0.0406384557332636\n",
            "Loss S01:  0.0355561845047012\n",
            "Loss S2:  0.040646668757044015\n",
            "Loss S01:  0.035552560447243114\n",
            "Loss S2:  0.04063617304958135\n",
            "Loss S01:  0.03552779944919833\n",
            "Loss S2:  0.040616411976817486\n",
            "Loss S01:  0.03553584220958507\n",
            "Loss S2:  0.04060198451707682\n",
            "Loss S01:  0.03550209787190723\n",
            "Loss S2:  0.04056307134192313\n",
            "Loss S01:  0.03549985684212901\n",
            "Loss S2:  0.040562839802363866\n",
            "Loss S01:  0.035509787481776696\n",
            "Loss S2:  0.04056499069080736\n",
            "Loss S01:  0.03555758975251554\n",
            "Loss S2:  0.04060498438333106\n",
            "Loss S01:  0.03554103870523916\n",
            "Loss S2:  0.04057695363970201\n",
            "Loss S01:  0.035540836649923635\n",
            "Loss S2:  0.040593615802777865\n",
            "Loss S01:  0.035535363141785965\n",
            "Loss S2:  0.04059133491997179\n",
            "Loss S01:  0.03552369025501538\n",
            "Loss S2:  0.040590814939031376\n",
            "Loss S01:  0.03552478889988107\n",
            "Loss S2:  0.04058575845321526\n",
            "Loss S01:  0.03551300768033754\n",
            "Loss S2:  0.04059663539647808\n",
            "Loss S01:  0.03547403106727692\n",
            "Loss S2:  0.04056752729561567\n",
            "Validation: \n",
            " Loss S01:  0.03035239689052105\n",
            " Loss S2:  0.05029965192079544\n",
            " Loss S01:  0.032798215125997864\n",
            " Loss S2:  0.054528319055125826\n",
            " Loss S01:  0.03291083068200728\n",
            " Loss S2:  0.0547930224821335\n",
            " Loss S01:  0.0328324343093106\n",
            " Loss S2:  0.054571011882336415\n",
            " Loss S01:  0.03289847505957256\n",
            " Loss S2:  0.05437081468142109\n",
            "\n",
            "Epoch: 20\n",
            "Loss S01:  0.03961551561951637\n",
            "Loss S2:  0.041084032505750656\n",
            "Loss S01:  0.034836665811863815\n",
            "Loss S2:  0.0392993458292701\n",
            "Loss S01:  0.035128306242681685\n",
            "Loss S2:  0.03966924814241273\n",
            "Loss S01:  0.03485534448296793\n",
            "Loss S2:  0.03955694396168955\n",
            "Loss S01:  0.03510162488716405\n",
            "Loss S2:  0.039916186525327406\n",
            "Loss S01:  0.035203818450955784\n",
            "Loss S2:  0.04014893460507486\n",
            "Loss S01:  0.03506770833838181\n",
            "Loss S2:  0.04005764522513405\n",
            "Loss S01:  0.03513707506509734\n",
            "Loss S2:  0.04008965131262658\n",
            "Loss S01:  0.034938863237146976\n",
            "Loss S2:  0.039896594981352486\n",
            "Loss S01:  0.03484918828030209\n",
            "Loss S2:  0.039804747635191616\n",
            "Loss S01:  0.03478875143988298\n",
            "Loss S2:  0.0397313125062697\n",
            "Loss S01:  0.03475994212334758\n",
            "Loss S2:  0.03962606624574275\n",
            "Loss S01:  0.03478045504509417\n",
            "Loss S2:  0.03972190923311494\n",
            "Loss S01:  0.03479935053935033\n",
            "Loss S2:  0.0397475648187954\n",
            "Loss S01:  0.03479765626397116\n",
            "Loss S2:  0.03980230563815604\n",
            "Loss S01:  0.03481592901593802\n",
            "Loss S2:  0.03988784978425266\n",
            "Loss S01:  0.0347773074085668\n",
            "Loss S2:  0.039863145869711174\n",
            "Loss S01:  0.03482424487408839\n",
            "Loss S2:  0.03996686415191282\n",
            "Loss S01:  0.034820589262642254\n",
            "Loss S2:  0.03999707071903002\n",
            "Loss S01:  0.03476023455564888\n",
            "Loss S2:  0.04000453857695245\n",
            "Loss S01:  0.03471878612997817\n",
            "Loss S2:  0.03998441541965921\n",
            "Loss S01:  0.03471229341040008\n",
            "Loss S2:  0.03998717879302694\n",
            "Loss S01:  0.034726112398289447\n",
            "Loss S2:  0.03999599359289014\n",
            "Loss S01:  0.03474053450219043\n",
            "Loss S2:  0.03995559277472558\n",
            "Loss S01:  0.034794440724852174\n",
            "Loss S2:  0.039962891087368814\n",
            "Loss S01:  0.03479811673115687\n",
            "Loss S2:  0.039987011675340724\n",
            "Loss S01:  0.03479814272502373\n",
            "Loss S2:  0.04004137230844333\n",
            "Loss S01:  0.03475557422313523\n",
            "Loss S2:  0.040025128347187466\n",
            "Loss S01:  0.03475993551525993\n",
            "Loss S2:  0.039979048802441124\n",
            "Loss S01:  0.0347681927952365\n",
            "Loss S2:  0.040028280076087545\n",
            "Loss S01:  0.03478502086750495\n",
            "Loss S2:  0.040054825465643526\n",
            "Loss S01:  0.034755830745844596\n",
            "Loss S2:  0.04003334058586424\n",
            "Loss S01:  0.034760987502160105\n",
            "Loss S2:  0.04005400056593886\n",
            "Loss S01:  0.03478368130312765\n",
            "Loss S2:  0.04006829676626312\n",
            "Loss S01:  0.03482010348254809\n",
            "Loss S2:  0.04010535513332163\n",
            "Loss S01:  0.0348473502179751\n",
            "Loss S2:  0.04013476695897233\n",
            "Loss S01:  0.03484178839607417\n",
            "Loss S2:  0.04011798835774868\n",
            "Loss S01:  0.034810022814977526\n",
            "Loss S2:  0.040074156441617846\n",
            "Loss S01:  0.03480374956811507\n",
            "Loss S2:  0.04003806046535337\n",
            "Loss S01:  0.03476542677930401\n",
            "Loss S2:  0.03999296663438573\n",
            "Loss S01:  0.034749846413099854\n",
            "Loss S2:  0.03998176799041969\n",
            "Loss S01:  0.03474346438644866\n",
            "Loss S2:  0.03996196212450953\n",
            "Loss S01:  0.0347750817741889\n",
            "Loss S2:  0.039983371861119735\n",
            "Loss S01:  0.03474724917670414\n",
            "Loss S2:  0.0399336523339671\n",
            "Loss S01:  0.03475177931745036\n",
            "Loss S2:  0.03994776079612795\n",
            "Loss S01:  0.034741702512270066\n",
            "Loss S2:  0.03993343913766603\n",
            "Loss S01:  0.03476114251091268\n",
            "Loss S2:  0.03993783242305551\n",
            "Loss S01:  0.03475754300721638\n",
            "Loss S2:  0.039927072214029906\n",
            "Loss S01:  0.03476316804790447\n",
            "Loss S2:  0.039937893389540256\n",
            "Loss S01:  0.034721155493480364\n",
            "Loss S2:  0.03990702391763326\n",
            "Validation: \n",
            " Loss S01:  0.03031899780035019\n",
            " Loss S2:  0.049786295741796494\n",
            " Loss S01:  0.03194586444823515\n",
            " Loss S2:  0.054381168491783594\n",
            " Loss S01:  0.03181664850108507\n",
            " Loss S2:  0.05463321897678259\n",
            " Loss S01:  0.03174936053816412\n",
            " Loss S2:  0.05448676731254234\n",
            " Loss S01:  0.03174715907669362\n",
            " Loss S2:  0.05428834204320555\n",
            "\n",
            "Epoch: 21\n",
            "Loss S01:  0.03867018222808838\n",
            "Loss S2:  0.04295136779546738\n",
            "Loss S01:  0.03403721665116874\n",
            "Loss S2:  0.03878402269699357\n",
            "Loss S01:  0.03398340709862255\n",
            "Loss S2:  0.03917097371248972\n",
            "Loss S01:  0.03410216012308674\n",
            "Loss S2:  0.039203462941992666\n",
            "Loss S01:  0.03441433335949735\n",
            "Loss S2:  0.03950596773406354\n",
            "Loss S01:  0.034474426113507324\n",
            "Loss S2:  0.039569212233319\n",
            "Loss S01:  0.03436292561351276\n",
            "Loss S2:  0.03956808424631103\n",
            "Loss S01:  0.034325002286006025\n",
            "Loss S2:  0.03955943824749597\n",
            "Loss S01:  0.034069724242996285\n",
            "Loss S2:  0.03927266183826658\n",
            "Loss S01:  0.033903595919792466\n",
            "Loss S2:  0.03911159695177288\n",
            "Loss S01:  0.033832980541161975\n",
            "Loss S2:  0.03893827115840251\n",
            "Loss S01:  0.03379765282141733\n",
            "Loss S2:  0.03888365867975596\n",
            "Loss S01:  0.03381794192327941\n",
            "Loss S2:  0.038922801305932446\n",
            "Loss S01:  0.03391261444291996\n",
            "Loss S2:  0.039035984777084747\n",
            "Loss S01:  0.03394915600776249\n",
            "Loss S2:  0.039062492126691425\n",
            "Loss S01:  0.033980276127228676\n",
            "Loss S2:  0.03910200105400275\n",
            "Loss S01:  0.03397922586904179\n",
            "Loss S2:  0.03908382494997534\n",
            "Loss S01:  0.03404248284709732\n",
            "Loss S2:  0.03919782520037646\n",
            "Loss S01:  0.03406058852843817\n",
            "Loss S2:  0.0392347389616031\n",
            "Loss S01:  0.0340295205799697\n",
            "Loss S2:  0.03922580539478057\n",
            "Loss S01:  0.034021530615453106\n",
            "Loss S2:  0.03914614160781476\n",
            "Loss S01:  0.034052824725118856\n",
            "Loss S2:  0.03917388513825516\n",
            "Loss S01:  0.03409235190381022\n",
            "Loss S2:  0.039197759896651654\n",
            "Loss S01:  0.034183618963345305\n",
            "Loss S2:  0.03919291976855431\n",
            "Loss S01:  0.034265536018859795\n",
            "Loss S2:  0.0392385821835876\n",
            "Loss S01:  0.034305097357151045\n",
            "Loss S2:  0.03927508521958651\n",
            "Loss S01:  0.03435100873368453\n",
            "Loss S2:  0.03933921126627374\n",
            "Loss S01:  0.03429967858502126\n",
            "Loss S2:  0.03930813574483034\n",
            "Loss S01:  0.03426877115571414\n",
            "Loss S2:  0.03927040188921304\n",
            "Loss S01:  0.034255985612260925\n",
            "Loss S2:  0.039284764043346715\n",
            "Loss S01:  0.03427455989501603\n",
            "Loss S2:  0.03930637796109301\n",
            "Loss S01:  0.0342558127830937\n",
            "Loss S2:  0.03926494630155456\n",
            "Loss S01:  0.03427278516796705\n",
            "Loss S2:  0.039287728231365435\n",
            "Loss S01:  0.034263852162123444\n",
            "Loss S2:  0.03929570851864411\n",
            "Loss S01:  0.03428928297329858\n",
            "Loss S2:  0.039347313617593735\n",
            "Loss S01:  0.03430476028504025\n",
            "Loss S2:  0.03936929997598004\n",
            "Loss S01:  0.034285061701182847\n",
            "Loss S2:  0.03935718072005586\n",
            "Loss S01:  0.034252229826951607\n",
            "Loss S2:  0.03933625776732386\n",
            "Loss S01:  0.034270877434121653\n",
            "Loss S2:  0.03932387288904253\n",
            "Loss S01:  0.034234630353653525\n",
            "Loss S2:  0.03927317134978826\n",
            "Loss S01:  0.03420847342198626\n",
            "Loss S2:  0.03925391473340572\n",
            "Loss S01:  0.03419729691098496\n",
            "Loss S2:  0.03922980130987736\n",
            "Loss S01:  0.034230094779096984\n",
            "Loss S2:  0.039249497644974894\n",
            "Loss S01:  0.034203388423522774\n",
            "Loss S2:  0.03921744172956163\n",
            "Loss S01:  0.034194386908425495\n",
            "Loss S2:  0.03921391795518177\n",
            "Loss S01:  0.03419052425796732\n",
            "Loss S2:  0.03921233776733775\n",
            "Loss S01:  0.03417534643376081\n",
            "Loss S2:  0.03921416481863909\n",
            "Loss S01:  0.034180748543352076\n",
            "Loss S2:  0.039201937054309625\n",
            "Loss S01:  0.03417975240214699\n",
            "Loss S2:  0.03921701520557463\n",
            "Loss S01:  0.034143442272866815\n",
            "Loss S2:  0.03918622250530239\n",
            "Validation: \n",
            " Loss S01:  0.03088916279375553\n",
            " Loss S2:  0.04842846468091011\n",
            " Loss S01:  0.032943133353477434\n",
            " Loss S2:  0.0533141688931556\n",
            " Loss S01:  0.03283223219034148\n",
            " Loss S2:  0.05369689342815701\n",
            " Loss S01:  0.03269862456888449\n",
            " Loss S2:  0.05331477993091599\n",
            " Loss S01:  0.03266614611134117\n",
            " Loss S2:  0.053105595257179235\n",
            "\n",
            "Epoch: 22\n",
            "Loss S01:  0.038597188889980316\n",
            "Loss S2:  0.0420803502202034\n",
            "Loss S01:  0.033389397812160576\n",
            "Loss S2:  0.038440129635008896\n",
            "Loss S01:  0.03330493611948831\n",
            "Loss S2:  0.03827116354590371\n",
            "Loss S01:  0.03337832684478452\n",
            "Loss S2:  0.03852925882224114\n",
            "Loss S01:  0.03358538407923245\n",
            "Loss S2:  0.038594419636377476\n",
            "Loss S01:  0.033734649456307\n",
            "Loss S2:  0.03872072134240001\n",
            "Loss S01:  0.03353174229259374\n",
            "Loss S2:  0.038655543791466074\n",
            "Loss S01:  0.03355406030592784\n",
            "Loss S2:  0.03864320241649386\n",
            "Loss S01:  0.033442692578206826\n",
            "Loss S2:  0.03842872128258517\n",
            "Loss S01:  0.03338264714885544\n",
            "Loss S2:  0.03833257525668039\n",
            "Loss S01:  0.03331943885377138\n",
            "Loss S2:  0.0381469846066862\n",
            "Loss S01:  0.03328270716844378\n",
            "Loss S2:  0.03804006185886022\n",
            "Loss S01:  0.033290462245133295\n",
            "Loss S2:  0.03806880501306747\n",
            "Loss S01:  0.03331260073162217\n",
            "Loss S2:  0.03808415058346195\n",
            "Loss S01:  0.033351052235415644\n",
            "Loss S2:  0.03817904946651864\n",
            "Loss S01:  0.033365068846191004\n",
            "Loss S2:  0.038232749093644665\n",
            "Loss S01:  0.033330193784388694\n",
            "Loss S2:  0.03828256639727154\n",
            "Loss S01:  0.03341791657894327\n",
            "Loss S2:  0.038424341498236904\n",
            "Loss S01:  0.03342685408025815\n",
            "Loss S2:  0.038478946014827126\n",
            "Loss S01:  0.033382571190205546\n",
            "Loss S2:  0.03844555845747443\n",
            "Loss S01:  0.03335879592048885\n",
            "Loss S2:  0.03837712385242258\n",
            "Loss S01:  0.03337165802470033\n",
            "Loss S2:  0.0384149747896251\n",
            "Loss S01:  0.0334270761276667\n",
            "Loss S2:  0.03844526273565055\n",
            "Loss S01:  0.03345933626708272\n",
            "Loss S2:  0.038452756949962474\n",
            "Loss S01:  0.03352419764607279\n",
            "Loss S2:  0.03850874244486643\n",
            "Loss S01:  0.03356159767840963\n",
            "Loss S2:  0.038551623484052985\n",
            "Loss S01:  0.033587910449025274\n",
            "Loss S2:  0.03859639326217531\n",
            "Loss S01:  0.0335195899710774\n",
            "Loss S2:  0.03856285090980934\n",
            "Loss S01:  0.033521692066377166\n",
            "Loss S2:  0.03852388592854514\n",
            "Loss S01:  0.0335233539880551\n",
            "Loss S2:  0.0385775735595382\n",
            "Loss S01:  0.03356033859409367\n",
            "Loss S2:  0.03859933329579046\n",
            "Loss S01:  0.0335487813488176\n",
            "Loss S2:  0.03858602494959663\n",
            "Loss S01:  0.03355721678480367\n",
            "Loss S2:  0.03860552641461571\n",
            "Loss S01:  0.033566394545070356\n",
            "Loss S2:  0.03861116678693143\n",
            "Loss S01:  0.03357359303210663\n",
            "Loss S2:  0.038647327309916796\n",
            "Loss S01:  0.03358767468726363\n",
            "Loss S2:  0.0386774844975553\n",
            "Loss S01:  0.03359417084325879\n",
            "Loss S2:  0.03866244960896196\n",
            "Loss S01:  0.033569608641803424\n",
            "Loss S2:  0.03863099710438451\n",
            "Loss S01:  0.03358697369596933\n",
            "Loss S2:  0.03862896916551853\n",
            "Loss S01:  0.03355471320602747\n",
            "Loss S2:  0.038582684334053104\n",
            "Loss S01:  0.03356510233087581\n",
            "Loss S2:  0.03860267336268972\n",
            "Loss S01:  0.033575448712873344\n",
            "Loss S2:  0.038573690375127354\n",
            "Loss S01:  0.03359637548815185\n",
            "Loss S2:  0.03858499206832639\n",
            "Loss S01:  0.033574688198636966\n",
            "Loss S2:  0.03853923430679292\n",
            "Loss S01:  0.033571481362593414\n",
            "Loss S2:  0.03854516351304087\n",
            "Loss S01:  0.03355052588536443\n",
            "Loss S2:  0.03853155945958425\n",
            "Loss S01:  0.03353546925066867\n",
            "Loss S2:  0.038513274948211655\n",
            "Loss S01:  0.033551256771490075\n",
            "Loss S2:  0.03850858335851864\n",
            "Loss S01:  0.03355346808670836\n",
            "Loss S2:  0.03850727210018838\n",
            "Loss S01:  0.033525946658188116\n",
            "Loss S2:  0.038472185370392806\n",
            "Validation: \n",
            " Loss S01:  0.029615888372063637\n",
            " Loss S2:  0.048343218863010406\n",
            " Loss S01:  0.03168859483585471\n",
            " Loss S2:  0.05226686951659974\n",
            " Loss S01:  0.03158153752546485\n",
            " Loss S2:  0.05246762831400081\n",
            " Loss S01:  0.03150250497045087\n",
            " Loss S2:  0.05206722008888839\n",
            " Loss S01:  0.03150025527510378\n",
            " Loss S2:  0.05189695181670012\n",
            "\n",
            "Epoch: 23\n",
            "Loss S01:  0.034202445298433304\n",
            "Loss S2:  0.03866814821958542\n",
            "Loss S01:  0.03245496224950661\n",
            "Loss S2:  0.03718114610422741\n",
            "Loss S01:  0.03290373219975403\n",
            "Loss S2:  0.037477578435625346\n",
            "Loss S01:  0.03296774139087046\n",
            "Loss S2:  0.037343249085449406\n",
            "Loss S01:  0.03321616256200686\n",
            "Loss S2:  0.03753384584333838\n",
            "Loss S01:  0.03323802950919843\n",
            "Loss S2:  0.03786021156930456\n",
            "Loss S01:  0.03311158887675551\n",
            "Loss S2:  0.03780867037225942\n",
            "Loss S01:  0.03320212995397373\n",
            "Loss S2:  0.0379038033875781\n",
            "Loss S01:  0.033016079806803184\n",
            "Loss S2:  0.037833891303083046\n",
            "Loss S01:  0.03294386180465693\n",
            "Loss S2:  0.037692283249490866\n",
            "Loss S01:  0.032904421062310146\n",
            "Loss S2:  0.0376112643297356\n",
            "Loss S01:  0.03291624079684954\n",
            "Loss S2:  0.03763281070702785\n",
            "Loss S01:  0.03289312450600065\n",
            "Loss S2:  0.03770532545225679\n",
            "Loss S01:  0.03293196183010822\n",
            "Loss S2:  0.03777249470239377\n",
            "Loss S01:  0.03297000978115602\n",
            "Loss S2:  0.037831433038128186\n",
            "Loss S01:  0.03302930684032424\n",
            "Loss S2:  0.03790125894724138\n",
            "Loss S01:  0.033055627498752585\n",
            "Loss S2:  0.037925030249431266\n",
            "Loss S01:  0.0331579558115605\n",
            "Loss S2:  0.038056952149024485\n",
            "Loss S01:  0.033177228716526244\n",
            "Loss S2:  0.03806686862397589\n",
            "Loss S01:  0.03311177788799658\n",
            "Loss S2:  0.038062758782771246\n",
            "Loss S01:  0.03304972252182996\n",
            "Loss S2:  0.03802380686746308\n",
            "Loss S01:  0.03309013749256518\n",
            "Loss S2:  0.03805999664404381\n",
            "Loss S01:  0.033119051915285816\n",
            "Loss S2:  0.03805788865995623\n",
            "Loss S01:  0.03313190087643795\n",
            "Loss S2:  0.038043208371896245\n",
            "Loss S01:  0.0331461497145817\n",
            "Loss S2:  0.03809347730207245\n",
            "Loss S01:  0.03315190405425322\n",
            "Loss S2:  0.038133995392289295\n",
            "Loss S01:  0.033147809398985\n",
            "Loss S2:  0.03818625818324272\n",
            "Loss S01:  0.033094767192413006\n",
            "Loss S2:  0.03816334665426469\n",
            "Loss S01:  0.03309598539249965\n",
            "Loss S2:  0.03815246303151511\n",
            "Loss S01:  0.03310873425370434\n",
            "Loss S2:  0.03817444635215903\n",
            "Loss S01:  0.03311003669725106\n",
            "Loss S2:  0.0381853475151862\n",
            "Loss S01:  0.03309120923592155\n",
            "Loss S2:  0.038174737544305073\n",
            "Loss S01:  0.03309192083126102\n",
            "Loss S2:  0.03817589254904759\n",
            "Loss S01:  0.03308328524727115\n",
            "Loss S2:  0.03818186281355846\n",
            "Loss S01:  0.03312083310789027\n",
            "Loss S2:  0.03822083897954208\n",
            "Loss S01:  0.03313264631244362\n",
            "Loss S2:  0.0382342693048325\n",
            "Loss S01:  0.033132980394478984\n",
            "Loss S2:  0.038213916907185\n",
            "Loss S01:  0.03311379779521828\n",
            "Loss S2:  0.03817831569243313\n",
            "Loss S01:  0.03311696459399903\n",
            "Loss S2:  0.03813355059174728\n",
            "Loss S01:  0.03308940637866249\n",
            "Loss S2:  0.03808045486355072\n",
            "Loss S01:  0.03308673781758532\n",
            "Loss S2:  0.038072526538535545\n",
            "Loss S01:  0.033096023660760435\n",
            "Loss S2:  0.0380648728596033\n",
            "Loss S01:  0.033096395424100394\n",
            "Loss S2:  0.038059306647318845\n",
            "Loss S01:  0.03307245791891888\n",
            "Loss S2:  0.03803666726861089\n",
            "Loss S01:  0.03307809300146271\n",
            "Loss S2:  0.038051986674062246\n",
            "Loss S01:  0.03307170109893557\n",
            "Loss S2:  0.03806020317809809\n",
            "Loss S01:  0.03307130493207505\n",
            "Loss S2:  0.03807284752484775\n",
            "Loss S01:  0.03308483932934496\n",
            "Loss S2:  0.03805930546293087\n",
            "Loss S01:  0.03307617141278519\n",
            "Loss S2:  0.03806166988245663\n",
            "Loss S01:  0.033052866125209995\n",
            "Loss S2:  0.03803235143726567\n",
            "Validation: \n",
            " Loss S01:  0.02960575744509697\n",
            " Loss S2:  0.048886898905038834\n",
            " Loss S01:  0.03166037877755506\n",
            " Loss S2:  0.052492836578970865\n",
            " Loss S01:  0.03163344372154736\n",
            " Loss S2:  0.05281871384600314\n",
            " Loss S01:  0.03143668632771148\n",
            " Loss S2:  0.052477504508417164\n",
            " Loss S01:  0.031399031112223495\n",
            " Loss S2:  0.05231768251569183\n",
            "\n",
            "Epoch: 24\n",
            "Loss S01:  0.03680983558297157\n",
            "Loss S2:  0.041061125695705414\n",
            "Loss S01:  0.03244110721756111\n",
            "Loss S2:  0.037010866132649506\n",
            "Loss S01:  0.032436122674317586\n",
            "Loss S2:  0.03733989525408972\n",
            "Loss S01:  0.032412484648727605\n",
            "Loss S2:  0.03758433280933288\n",
            "Loss S01:  0.032617549979832114\n",
            "Loss S2:  0.03770573437213898\n",
            "Loss S01:  0.032821034829990534\n",
            "Loss S2:  0.03781617622749478\n",
            "Loss S01:  0.032664889530813104\n",
            "Loss S2:  0.03760664940613215\n",
            "Loss S01:  0.03266173923834109\n",
            "Loss S2:  0.03767533421936169\n",
            "Loss S01:  0.032532487685481705\n",
            "Loss S2:  0.037449256229547805\n",
            "Loss S01:  0.03243906437777556\n",
            "Loss S2:  0.03742094751406502\n",
            "Loss S01:  0.03241082505867033\n",
            "Loss S2:  0.0372552573312037\n",
            "Loss S01:  0.032375696375294846\n",
            "Loss S2:  0.0372193860108251\n",
            "Loss S01:  0.03238511635067542\n",
            "Loss S2:  0.03724092408275801\n",
            "Loss S01:  0.032416564533273684\n",
            "Loss S2:  0.03724308823811189\n",
            "Loss S01:  0.03245923077647991\n",
            "Loss S2:  0.037294432494445896\n",
            "Loss S01:  0.03246279783694949\n",
            "Loss S2:  0.0374206274205091\n",
            "Loss S01:  0.032465887747658705\n",
            "Loss S2:  0.03744438230436041\n",
            "Loss S01:  0.03254824082710241\n",
            "Loss S2:  0.037572868647631145\n",
            "Loss S01:  0.03257209629602525\n",
            "Loss S2:  0.037604991511251386\n",
            "Loss S01:  0.03251062126839972\n",
            "Loss S2:  0.0375566620048116\n",
            "Loss S01:  0.032473220661356675\n",
            "Loss S2:  0.03751142176600238\n",
            "Loss S01:  0.03246070442818352\n",
            "Loss S2:  0.03754097025541333\n",
            "Loss S01:  0.03248996232800624\n",
            "Loss S2:  0.0375397463088931\n",
            "Loss S01:  0.03249353363916471\n",
            "Loss S2:  0.037473760029196224\n",
            "Loss S01:  0.0325437554108033\n",
            "Loss S2:  0.037496666767908825\n",
            "Loss S01:  0.032548530146004666\n",
            "Loss S2:  0.03750569045365569\n",
            "Loss S01:  0.03257215938662889\n",
            "Loss S2:  0.03752036699561324\n",
            "Loss S01:  0.03254822770140488\n",
            "Loss S2:  0.03748934405316286\n",
            "Loss S01:  0.032526242768796315\n",
            "Loss S2:  0.03744217301337745\n",
            "Loss S01:  0.0325219077138147\n",
            "Loss S2:  0.037495219766376764\n",
            "Loss S01:  0.03255152904685352\n",
            "Loss S2:  0.03750705871223611\n",
            "Loss S01:  0.03253458645733798\n",
            "Loss S2:  0.03749353425439531\n",
            "Loss S01:  0.032544766288726504\n",
            "Loss S2:  0.03749251429490583\n",
            "Loss S01:  0.032553531949793824\n",
            "Loss S2:  0.037513226011818626\n",
            "Loss S01:  0.03257643353314169\n",
            "Loss S2:  0.03754713177637271\n",
            "Loss S01:  0.03258361845699131\n",
            "Loss S2:  0.03755816631549783\n",
            "Loss S01:  0.03257692562947643\n",
            "Loss S2:  0.03754412835861177\n",
            "Loss S01:  0.03254524153479026\n",
            "Loss S2:  0.0374944498536561\n",
            "Loss S01:  0.03255453213971118\n",
            "Loss S2:  0.03746573105845551\n",
            "Loss S01:  0.03251747946585993\n",
            "Loss S2:  0.03740650520224095\n",
            "Loss S01:  0.03252553448378297\n",
            "Loss S2:  0.03741080054729954\n",
            "Loss S01:  0.032523503395617735\n",
            "Loss S2:  0.037406471496733436\n",
            "Loss S01:  0.03254590507357817\n",
            "Loss S2:  0.037420347496654814\n",
            "Loss S01:  0.03250120838200825\n",
            "Loss S2:  0.0373793872518426\n",
            "Loss S01:  0.03249334936323755\n",
            "Loss S2:  0.037363969621643456\n",
            "Loss S01:  0.03248222040512213\n",
            "Loss S2:  0.037367446001148275\n",
            "Loss S01:  0.03247852168508569\n",
            "Loss S2:  0.03738104544353718\n",
            "Loss S01:  0.03249225822809269\n",
            "Loss S2:  0.03739139313894599\n",
            "Loss S01:  0.032498954406120426\n",
            "Loss S2:  0.037406660178811786\n",
            "Loss S01:  0.032463228375539034\n",
            "Loss S2:  0.037378597530382716\n",
            "Validation: \n",
            " Loss S01:  0.029722336679697037\n",
            " Loss S2:  0.047082118690013885\n",
            " Loss S01:  0.030952352232166698\n",
            " Loss S2:  0.05122661413181396\n",
            " Loss S01:  0.030917054178511223\n",
            " Loss S2:  0.05153170991234663\n",
            " Loss S01:  0.03090000622829453\n",
            " Loss S2:  0.05140999521388382\n",
            " Loss S01:  0.030875576614045802\n",
            " Loss S2:  0.05131015130951081\n",
            "\n",
            "Epoch: 25\n",
            "Loss S01:  0.036909401416778564\n",
            "Loss S2:  0.043725233525037766\n",
            "Loss S01:  0.03177794881842353\n",
            "Loss S2:  0.03658179972659458\n",
            "Loss S01:  0.03202554707725843\n",
            "Loss S2:  0.03671631962060928\n",
            "Loss S01:  0.03220514226103983\n",
            "Loss S2:  0.03688141042667051\n",
            "Loss S01:  0.032294258884176974\n",
            "Loss S2:  0.03706182076073274\n",
            "Loss S01:  0.03243318382723659\n",
            "Loss S2:  0.03734839722221973\n",
            "Loss S01:  0.03233446538081912\n",
            "Loss S2:  0.03724537875320091\n",
            "Loss S01:  0.03231407838388228\n",
            "Loss S2:  0.03720716817278258\n",
            "Loss S01:  0.03213286519418528\n",
            "Loss S2:  0.03705877916496477\n",
            "Loss S01:  0.032005797109597334\n",
            "Loss S2:  0.036858350734461795\n",
            "Loss S01:  0.03197020220358183\n",
            "Loss S2:  0.03671726654514228\n",
            "Loss S01:  0.03196203218655543\n",
            "Loss S2:  0.03669726768055478\n",
            "Loss S01:  0.031972755424001\n",
            "Loss S2:  0.036676753928099784\n",
            "Loss S01:  0.03196132579775712\n",
            "Loss S2:  0.03673760200502308\n",
            "Loss S01:  0.032003739901573944\n",
            "Loss S2:  0.0367460421266708\n",
            "Loss S01:  0.03204207524459883\n",
            "Loss S2:  0.036797741662390186\n",
            "Loss S01:  0.03197153400838005\n",
            "Loss S2:  0.03680962324142456\n",
            "Loss S01:  0.03203929727624732\n",
            "Loss S2:  0.03690689331606815\n",
            "Loss S01:  0.03201968276368979\n",
            "Loss S2:  0.036916436164418635\n",
            "Loss S01:  0.031983840791264755\n",
            "Loss S2:  0.036914857757809276\n",
            "Loss S01:  0.03194881981210922\n",
            "Loss S2:  0.036888866319288664\n",
            "Loss S01:  0.031966908547974314\n",
            "Loss S2:  0.036882732805058854\n",
            "Loss S01:  0.032010161861507604\n",
            "Loss S2:  0.03690329967783047\n",
            "Loss S01:  0.032008963350803304\n",
            "Loss S2:  0.03687480096886685\n",
            "Loss S01:  0.03205453296079923\n",
            "Loss S2:  0.03689568563986616\n",
            "Loss S01:  0.03207325268225129\n",
            "Loss S2:  0.0369369223773242\n",
            "Loss S01:  0.032081413514541024\n",
            "Loss S2:  0.03695280378919908\n",
            "Loss S01:  0.032036662452280304\n",
            "Loss S2:  0.0369284667514567\n",
            "Loss S01:  0.03203850995842036\n",
            "Loss S2:  0.03689280713102996\n",
            "Loss S01:  0.032062975145697184\n",
            "Loss S2:  0.03694067470396507\n",
            "Loss S01:  0.032089621626905034\n",
            "Loss S2:  0.036941572466858995\n",
            "Loss S01:  0.032077844664933595\n",
            "Loss S2:  0.036922730649207564\n",
            "Loss S01:  0.03207126512798565\n",
            "Loss S2:  0.036942395605030834\n",
            "Loss S01:  0.03206413580819379\n",
            "Loss S2:  0.036946061953587114\n",
            "Loss S01:  0.03207934346279441\n",
            "Loss S2:  0.03696695711911598\n",
            "Loss S01:  0.03207547735539597\n",
            "Loss S2:  0.03697604865387634\n",
            "Loss S01:  0.03208068174689265\n",
            "Loss S2:  0.03694713951560599\n",
            "Loss S01:  0.032058007909723046\n",
            "Loss S2:  0.03693198047156604\n",
            "Loss S01:  0.032069127322260166\n",
            "Loss S2:  0.03690359842159304\n",
            "Loss S01:  0.03206328403614366\n",
            "Loss S2:  0.036886132034042\n",
            "Loss S01:  0.032062411155003565\n",
            "Loss S2:  0.03688950912836484\n",
            "Loss S01:  0.03206771034339484\n",
            "Loss S2:  0.03689326124759776\n",
            "Loss S01:  0.03209385391758343\n",
            "Loss S2:  0.036903437899509686\n",
            "Loss S01:  0.03206518999454317\n",
            "Loss S2:  0.0368655158893968\n",
            "Loss S01:  0.03206655465471907\n",
            "Loss S2:  0.03686011490536655\n",
            "Loss S01:  0.0320432132518873\n",
            "Loss S2:  0.0368307616041267\n",
            "Loss S01:  0.032023674705411245\n",
            "Loss S2:  0.03681271391518721\n",
            "Loss S01:  0.03204361149633893\n",
            "Loss S2:  0.03681903100728735\n",
            "Loss S01:  0.03203501186397492\n",
            "Loss S2:  0.036822294673094384\n",
            "Loss S01:  0.032013059565285315\n",
            "Loss S2:  0.03680162945887465\n",
            "Validation: \n",
            " Loss S01:  0.02922731824219227\n",
            " Loss S2:  0.04800880327820778\n",
            " Loss S01:  0.031582788253823914\n",
            " Loss S2:  0.051791134157351086\n",
            " Loss S01:  0.03159784484745526\n",
            " Loss S2:  0.052079268163297235\n",
            " Loss S01:  0.03155511941333286\n",
            " Loss S2:  0.05174456231418203\n",
            " Loss S01:  0.03155236944188306\n",
            " Loss S2:  0.051585182225630605\n",
            "\n",
            "Epoch: 26\n",
            "Loss S01:  0.03338300809264183\n",
            "Loss S2:  0.0379459485411644\n",
            "Loss S01:  0.031948618252169\n",
            "Loss S2:  0.03685124015266245\n",
            "Loss S01:  0.03163093523610206\n",
            "Loss S2:  0.036666477719942726\n",
            "Loss S01:  0.031701273435065826\n",
            "Loss S2:  0.03670941413410248\n",
            "Loss S01:  0.03195489379690915\n",
            "Loss S2:  0.03681927574117009\n",
            "Loss S01:  0.03196960892163071\n",
            "Loss S2:  0.03677417578942636\n",
            "Loss S01:  0.031775688691461676\n",
            "Loss S2:  0.03670946013976316\n",
            "Loss S01:  0.031771294169232876\n",
            "Loss S2:  0.03671348446481664\n",
            "Loss S01:  0.03171781266545072\n",
            "Loss S2:  0.0365923500539344\n",
            "Loss S01:  0.03162871301174164\n",
            "Loss S2:  0.03642635681963229\n",
            "Loss S01:  0.03155759965429212\n",
            "Loss S2:  0.03628953103676881\n",
            "Loss S01:  0.031478071048318804\n",
            "Loss S2:  0.03613943440487256\n",
            "Loss S01:  0.03146617995067076\n",
            "Loss S2:  0.03614684956317598\n",
            "Loss S01:  0.03146052255316545\n",
            "Loss S2:  0.03612807823899593\n",
            "Loss S01:  0.03144418242129874\n",
            "Loss S2:  0.0361192016234846\n",
            "Loss S01:  0.03146998569467999\n",
            "Loss S2:  0.03621900800788245\n",
            "Loss S01:  0.031435034405555784\n",
            "Loss S2:  0.036212306226725166\n",
            "Loss S01:  0.0315202979360059\n",
            "Loss S2:  0.03635292995873599\n",
            "Loss S01:  0.03157281789167151\n",
            "Loss S2:  0.036420862267606824\n",
            "Loss S01:  0.03150933022313405\n",
            "Loss S2:  0.036386911557372954\n",
            "Loss S01:  0.03146748104832362\n",
            "Loss S2:  0.036345458862867525\n",
            "Loss S01:  0.031481281284825496\n",
            "Loss S2:  0.03637667080625821\n",
            "Loss S01:  0.031493672502661184\n",
            "Loss S2:  0.03637892966120761\n",
            "Loss S01:  0.031520220229313486\n",
            "Loss S2:  0.03637277551833944\n",
            "Loss S01:  0.031607994206898936\n",
            "Loss S2:  0.0364721509941015\n",
            "Loss S01:  0.0316304857749863\n",
            "Loss S2:  0.036520577455125006\n",
            "Loss S01:  0.03165639801329123\n",
            "Loss S2:  0.0365910530589892\n",
            "Loss S01:  0.03161517912143931\n",
            "Loss S2:  0.03657168762352854\n",
            "Loss S01:  0.031596852625551175\n",
            "Loss S2:  0.03656663703881337\n",
            "Loss S01:  0.031607127380381336\n",
            "Loss S2:  0.03657228259619364\n",
            "Loss S01:  0.03160720311748823\n",
            "Loss S2:  0.03656365241595479\n",
            "Loss S01:  0.03158003467934691\n",
            "Loss S2:  0.036549607806889955\n",
            "Loss S01:  0.03157915699519101\n",
            "Loss S2:  0.036532199103095075\n",
            "Loss S01:  0.03157734546213892\n",
            "Loss S2:  0.03653641558868467\n",
            "Loss S01:  0.03161699056647215\n",
            "Loss S2:  0.03655755190490103\n",
            "Loss S01:  0.03163774702644925\n",
            "Loss S2:  0.036575912086669875\n",
            "Loss S01:  0.03161567427899038\n",
            "Loss S2:  0.03656238172502564\n",
            "Loss S01:  0.031594071446885315\n",
            "Loss S2:  0.036527731473434646\n",
            "Loss S01:  0.03159754820109353\n",
            "Loss S2:  0.03650827876480389\n",
            "Loss S01:  0.03157521230752206\n",
            "Loss S2:  0.036469838031760564\n",
            "Loss S01:  0.03157460743426682\n",
            "Loss S2:  0.03649408208145911\n",
            "Loss S01:  0.03157765944454357\n",
            "Loss S2:  0.036511559052950275\n",
            "Loss S01:  0.03159836569973522\n",
            "Loss S2:  0.03650854533259489\n",
            "Loss S01:  0.031571079654891365\n",
            "Loss S2:  0.036469938540334215\n",
            "Loss S01:  0.03156336153225969\n",
            "Loss S2:  0.03647610222468841\n",
            "Loss S01:  0.03156406587266473\n",
            "Loss S2:  0.03647291296039875\n",
            "Loss S01:  0.03156356581783734\n",
            "Loss S2:  0.0364811714989115\n",
            "Loss S01:  0.031568813064548874\n",
            "Loss S2:  0.03648100683422099\n",
            "Loss S01:  0.03156671122398049\n",
            "Loss S2:  0.0364886352370286\n",
            "Loss S01:  0.0315539075399307\n",
            "Loss S2:  0.03647289850015019\n",
            "Validation: \n",
            " Loss S01:  0.028309477493166924\n",
            " Loss S2:  0.04895646870136261\n",
            " Loss S01:  0.030497363812866666\n",
            " Loss S2:  0.05199609980696723\n",
            " Loss S01:  0.030503818129257457\n",
            " Loss S2:  0.052062580861696385\n",
            " Loss S01:  0.030426434897741335\n",
            " Loss S2:  0.05179070418731111\n",
            " Loss S01:  0.030431164750530395\n",
            " Loss S2:  0.051654635434165416\n",
            "\n",
            "Epoch: 27\n",
            "Loss S01:  0.03549724817276001\n",
            "Loss S2:  0.036994073539972305\n",
            "Loss S01:  0.030875146050344814\n",
            "Loss S2:  0.035341584885662254\n",
            "Loss S01:  0.030816220722737767\n",
            "Loss S2:  0.035488857399849666\n",
            "Loss S01:  0.031131842203678622\n",
            "Loss S2:  0.035803835959203785\n",
            "Loss S01:  0.03137820259463496\n",
            "Loss S2:  0.03592516136605565\n",
            "Loss S01:  0.03150405899128493\n",
            "Loss S2:  0.0361424655306573\n",
            "Loss S01:  0.03139260209730414\n",
            "Loss S2:  0.03604019207299733\n",
            "Loss S01:  0.03142526921566943\n",
            "Loss S2:  0.03601449659802544\n",
            "Loss S01:  0.03126450779813307\n",
            "Loss S2:  0.035837680055403415\n",
            "Loss S01:  0.031206083248604785\n",
            "Loss S2:  0.03576633035317882\n",
            "Loss S01:  0.03115137524460212\n",
            "Loss S2:  0.03564308941511825\n",
            "Loss S01:  0.03115197798019057\n",
            "Loss S2:  0.03559620564316844\n",
            "Loss S01:  0.031136948692281383\n",
            "Loss S2:  0.03563382471950093\n",
            "Loss S01:  0.03113011282136422\n",
            "Loss S2:  0.035672841195500536\n",
            "Loss S01:  0.031144674352191863\n",
            "Loss S2:  0.03566237035742466\n",
            "Loss S01:  0.03122584308368086\n",
            "Loss S2:  0.035766364044404975\n",
            "Loss S01:  0.031167897343728112\n",
            "Loss S2:  0.03575591565983266\n",
            "Loss S01:  0.031227944734675144\n",
            "Loss S2:  0.03587795753707314\n",
            "Loss S01:  0.03125000058657886\n",
            "Loss S2:  0.03588368919631724\n",
            "Loss S01:  0.031220737454622827\n",
            "Loss S2:  0.035850371570565316\n",
            "Loss S01:  0.031195062133877433\n",
            "Loss S2:  0.03582549091447052\n",
            "Loss S01:  0.03119682758093163\n",
            "Loss S2:  0.03583867443610707\n",
            "Loss S01:  0.031245600524868362\n",
            "Loss S2:  0.035830978759273685\n",
            "Loss S01:  0.031272859453703415\n",
            "Loss S2:  0.03580165339948295\n",
            "Loss S01:  0.031319475667048784\n",
            "Loss S2:  0.03587436628106719\n",
            "Loss S01:  0.03131854575706193\n",
            "Loss S2:  0.035912786703185734\n",
            "Loss S01:  0.03131282383083612\n",
            "Loss S2:  0.03593579287216125\n",
            "Loss S01:  0.03126291306586283\n",
            "Loss S2:  0.03592069937789132\n",
            "Loss S01:  0.031249121335265476\n",
            "Loss S2:  0.03587740405174337\n",
            "Loss S01:  0.03126010640533929\n",
            "Loss S2:  0.03590559251478447\n",
            "Loss S01:  0.031273769295195805\n",
            "Loss S2:  0.035886091226557164\n",
            "Loss S01:  0.03125474545685425\n",
            "Loss S2:  0.03587412811408472\n",
            "Loss S01:  0.03126600165372697\n",
            "Loss S2:  0.0358985820640099\n",
            "Loss S01:  0.03128625717565552\n",
            "Loss S2:  0.035905437819727236\n",
            "Loss S01:  0.031306387425080774\n",
            "Loss S2:  0.035929423262559074\n",
            "Loss S01:  0.03132268873990601\n",
            "Loss S2:  0.03594549357933536\n",
            "Loss S01:  0.031291318200838206\n",
            "Loss S2:  0.03596155797428041\n",
            "Loss S01:  0.03128321067181559\n",
            "Loss S2:  0.0359497530651944\n",
            "Loss S01:  0.03130246379126714\n",
            "Loss S2:  0.03594579089560065\n",
            "Loss S01:  0.03127278976828393\n",
            "Loss S2:  0.03590581874789484\n",
            "Loss S01:  0.03127133348338919\n",
            "Loss S2:  0.03590293450657269\n",
            "Loss S01:  0.03126992526806329\n",
            "Loss S2:  0.03589689618298317\n",
            "Loss S01:  0.031280990911209665\n",
            "Loss S2:  0.03591410071542597\n",
            "Loss S01:  0.03126057703264075\n",
            "Loss S2:  0.03587329498299866\n",
            "Loss S01:  0.031247789724444856\n",
            "Loss S2:  0.03587791570325017\n",
            "Loss S01:  0.031250939202678706\n",
            "Loss S2:  0.035884054257904086\n",
            "Loss S01:  0.031235743645344278\n",
            "Loss S2:  0.0358915092694268\n",
            "Loss S01:  0.03125671778198513\n",
            "Loss S2:  0.035880018468558664\n",
            "Loss S01:  0.03126190225990671\n",
            "Loss S2:  0.0358995120902691\n",
            "Loss S01:  0.0312334809437121\n",
            "Loss S2:  0.03587612498346754\n",
            "Validation: \n",
            " Loss S01:  0.027152013033628464\n",
            " Loss S2:  0.04881269857287407\n",
            " Loss S01:  0.029797711631371862\n",
            " Loss S2:  0.051545489934228715\n",
            " Loss S01:  0.029846561391179156\n",
            " Loss S2:  0.05154359367925946\n",
            " Loss S01:  0.02972456571630767\n",
            " Loss S2:  0.05116282812640315\n",
            " Loss S01:  0.02978413660125232\n",
            " Loss S2:  0.05102808341199969\n",
            "\n",
            "Epoch: 28\n",
            "Loss S01:  0.03314559534192085\n",
            "Loss S2:  0.03606466203927994\n",
            "Loss S01:  0.030598975718021393\n",
            "Loss S2:  0.034584910185499626\n",
            "Loss S01:  0.030571232594194867\n",
            "Loss S2:  0.03471514999511696\n",
            "Loss S01:  0.030513903907229824\n",
            "Loss S2:  0.035063767084671606\n",
            "Loss S01:  0.030734395971748887\n",
            "Loss S2:  0.03526283968694326\n",
            "Loss S01:  0.030902452584283025\n",
            "Loss S2:  0.03536232352695044\n",
            "Loss S01:  0.030781998650216667\n",
            "Loss S2:  0.03527498223864641\n",
            "Loss S01:  0.03078823783238169\n",
            "Loss S2:  0.035371611262081375\n",
            "Loss S01:  0.030631175103746813\n",
            "Loss S2:  0.0351865922825204\n",
            "Loss S01:  0.03055062142947873\n",
            "Loss S2:  0.03507413590265499\n",
            "Loss S01:  0.030593667326882335\n",
            "Loss S2:  0.03504756921072408\n",
            "Loss S01:  0.030616847445835937\n",
            "Loss S2:  0.034975939438686716\n",
            "Loss S01:  0.030599123682857544\n",
            "Loss S2:  0.03499030497325353\n",
            "Loss S01:  0.030613621341363164\n",
            "Loss S2:  0.03508900566637971\n",
            "Loss S01:  0.030627852569974905\n",
            "Loss S2:  0.03513686084155495\n",
            "Loss S01:  0.030663330860386622\n",
            "Loss S2:  0.03523785270602498\n",
            "Loss S01:  0.030645955805175054\n",
            "Loss S2:  0.03526150897978256\n",
            "Loss S01:  0.030703120314848353\n",
            "Loss S2:  0.035356688939514214\n",
            "Loss S01:  0.030754651997794105\n",
            "Loss S2:  0.035383435120569406\n",
            "Loss S01:  0.03071639933550233\n",
            "Loss S2:  0.03537081880251151\n",
            "Loss S01:  0.030718878550983188\n",
            "Loss S2:  0.03535290876060576\n",
            "Loss S01:  0.03075041937955183\n",
            "Loss S2:  0.03541876173520823\n",
            "Loss S01:  0.030770623019782666\n",
            "Loss S2:  0.03542608072416545\n",
            "Loss S01:  0.03077265933897846\n",
            "Loss S2:  0.035416671119275545\n",
            "Loss S01:  0.03083969381371227\n",
            "Loss S2:  0.03547702786122368\n",
            "Loss S01:  0.030842305869754567\n",
            "Loss S2:  0.03550102072675152\n",
            "Loss S01:  0.030887419614812422\n",
            "Loss S2:  0.035530194690115605\n",
            "Loss S01:  0.030827955744913142\n",
            "Loss S2:  0.035487347430243704\n",
            "Loss S01:  0.03081235266373463\n",
            "Loss S2:  0.03549435372162756\n",
            "Loss S01:  0.03079962389242813\n",
            "Loss S2:  0.03550548786841512\n",
            "Loss S01:  0.030806849286752286\n",
            "Loss S2:  0.035515692474883655\n",
            "Loss S01:  0.030772547737674315\n",
            "Loss S2:  0.035485922577536376\n",
            "Loss S01:  0.03077694503851583\n",
            "Loss S2:  0.03549549206975279\n",
            "Loss S01:  0.030776868785346023\n",
            "Loss S2:  0.03553073064865843\n",
            "Loss S01:  0.030799606749432877\n",
            "Loss S2:  0.03556299623091025\n",
            "Loss S01:  0.030823127915843938\n",
            "Loss S2:  0.035577889186916525\n",
            "Loss S01:  0.030853233689705422\n",
            "Loss S2:  0.03558490851022035\n",
            "Loss S01:  0.03083967145361669\n",
            "Loss S2:  0.03555080708307076\n",
            "Loss S01:  0.03083711435769017\n",
            "Loss S2:  0.03553722574915786\n",
            "Loss S01:  0.030819666841069755\n",
            "Loss S2:  0.035501097502839536\n",
            "Loss S01:  0.030839816381472006\n",
            "Loss S2:  0.035497803138824474\n",
            "Loss S01:  0.030843030099378596\n",
            "Loss S2:  0.0354940414138664\n",
            "Loss S01:  0.03086363628170411\n",
            "Loss S2:  0.03551792153854127\n",
            "Loss S01:  0.030846600941535768\n",
            "Loss S2:  0.03549026508152208\n",
            "Loss S01:  0.030852237728988232\n",
            "Loss S2:  0.035507033407992245\n",
            "Loss S01:  0.030845217943654094\n",
            "Loss S2:  0.03550322185747507\n",
            "Loss S01:  0.030837802095197285\n",
            "Loss S2:  0.03549605899302535\n",
            "Loss S01:  0.030845930716794007\n",
            "Loss S2:  0.03550270412931903\n",
            "Loss S01:  0.030859942828729097\n",
            "Loss S2:  0.03551910013316699\n",
            "Loss S01:  0.030845202562492152\n",
            "Loss S2:  0.03549952912021072\n",
            "Validation: \n",
            " Loss S01:  0.02751813270151615\n",
            " Loss S2:  0.04808192327618599\n",
            " Loss S01:  0.029870865955239252\n",
            " Loss S2:  0.05059493581453959\n",
            " Loss S01:  0.02990301934684195\n",
            " Loss S2:  0.05061568410658255\n",
            " Loss S01:  0.029781140142776927\n",
            " Loss S2:  0.05038682218702113\n",
            " Loss S01:  0.029804966469973694\n",
            " Loss S2:  0.05024482967492975\n",
            "\n",
            "Epoch: 29\n",
            "Loss S01:  0.03311016038060188\n",
            "Loss S2:  0.03867756575345993\n",
            "Loss S01:  0.030543900179592045\n",
            "Loss S2:  0.03513681380586191\n",
            "Loss S01:  0.030424663974415688\n",
            "Loss S2:  0.03523243467013041\n",
            "Loss S01:  0.030451692340354764\n",
            "Loss S2:  0.03521653222701242\n",
            "Loss S01:  0.03069289619239365\n",
            "Loss S2:  0.03519295860172772\n",
            "Loss S01:  0.03074698099026493\n",
            "Loss S2:  0.03534408114558341\n",
            "Loss S01:  0.030553998578278743\n",
            "Loss S2:  0.03523353789551336\n",
            "Loss S01:  0.03062813771022877\n",
            "Loss S2:  0.035277680375836264\n",
            "Loss S01:  0.030566721119814448\n",
            "Loss S2:  0.03510553946281657\n",
            "Loss S01:  0.030502327543857333\n",
            "Loss S2:  0.03500580345536326\n",
            "Loss S01:  0.030484428776815387\n",
            "Loss S2:  0.03491922235577413\n",
            "Loss S01:  0.0304256551101938\n",
            "Loss S2:  0.03490061101478499\n",
            "Loss S01:  0.030428028934873827\n",
            "Loss S2:  0.03490073618686889\n",
            "Loss S01:  0.030473047654137355\n",
            "Loss S2:  0.03492099410704984\n",
            "Loss S01:  0.03056717406403511\n",
            "Loss S2:  0.0349792462661334\n",
            "Loss S01:  0.03060299502253927\n",
            "Loss S2:  0.03507221613498713\n",
            "Loss S01:  0.030575893865609022\n",
            "Loss S2:  0.03505864469855098\n",
            "Loss S01:  0.030675202444718594\n",
            "Loss S2:  0.03515075226668377\n",
            "Loss S01:  0.03068761300110356\n",
            "Loss S2:  0.03515803403329125\n",
            "Loss S01:  0.03058334570670627\n",
            "Loss S2:  0.03511594252475581\n",
            "Loss S01:  0.030527907706315246\n",
            "Loss S2:  0.03507216677848083\n",
            "Loss S01:  0.03052400931786587\n",
            "Loss S2:  0.03506027152364571\n",
            "Loss S01:  0.030581834730249724\n",
            "Loss S2:  0.035065286662176724\n",
            "Loss S01:  0.03058226209020976\n",
            "Loss S2:  0.03504339149987801\n",
            "Loss S01:  0.030634803233737767\n",
            "Loss S2:  0.035096888867094805\n",
            "Loss S01:  0.0306472700178267\n",
            "Loss S2:  0.03512497833258365\n",
            "Loss S01:  0.030651898105482488\n",
            "Loss S2:  0.03513357810447727\n",
            "Loss S01:  0.03060734876847355\n",
            "Loss S2:  0.03507888858031303\n",
            "Loss S01:  0.030580046875037757\n",
            "Loss S2:  0.03501304901234832\n",
            "Loss S01:  0.030575137847836074\n",
            "Loss S2:  0.03502982258924709\n",
            "Loss S01:  0.030580462071042123\n",
            "Loss S2:  0.03504425056427222\n",
            "Loss S01:  0.030545821754782914\n",
            "Loss S2:  0.03500506341145928\n",
            "Loss S01:  0.03053003737000847\n",
            "Loss S2:  0.035025093959777896\n",
            "Loss S01:  0.030522982288838513\n",
            "Loss S2:  0.03504004134737473\n",
            "Loss S01:  0.030542718628940344\n",
            "Loss S2:  0.03506760073066457\n",
            "Loss S01:  0.030550622601497206\n",
            "Loss S2:  0.03507698467929988\n",
            "Loss S01:  0.030545814948316426\n",
            "Loss S2:  0.03507300649467763\n",
            "Loss S01:  0.030525570906035984\n",
            "Loss S2:  0.0350371006113098\n",
            "Loss S01:  0.030522076559700365\n",
            "Loss S2:  0.03501732455287862\n",
            "Loss S01:  0.03049109733241903\n",
            "Loss S2:  0.0349947969426813\n",
            "Loss S01:  0.030498551720395648\n",
            "Loss S2:  0.034993024231712716\n",
            "Loss S01:  0.030499994967794476\n",
            "Loss S2:  0.034983349297821084\n",
            "Loss S01:  0.030500702029027736\n",
            "Loss S2:  0.0349864353627108\n",
            "Loss S01:  0.030477480859152126\n",
            "Loss S2:  0.034949841142011905\n",
            "Loss S01:  0.030464118629558827\n",
            "Loss S2:  0.03495862654277256\n",
            "Loss S01:  0.03044893056517694\n",
            "Loss S2:  0.03494210276100165\n",
            "Loss S01:  0.030441087737291598\n",
            "Loss S2:  0.03495264068698418\n",
            "Loss S01:  0.03046396597349846\n",
            "Loss S2:  0.03495994442777269\n",
            "Loss S01:  0.03047090092685515\n",
            "Loss S2:  0.03495503592608872\n",
            "Loss S01:  0.030442342704613438\n",
            "Loss S2:  0.034938039709557826\n",
            "Validation: \n",
            " Loss S01:  0.027112165465950966\n",
            " Loss S2:  0.04854342341423035\n",
            " Loss S01:  0.029049068068464596\n",
            " Loss S2:  0.051569709643012\n",
            " Loss S01:  0.02904560267016655\n",
            " Loss S2:  0.05171151232065224\n",
            " Loss S01:  0.028984428459747892\n",
            " Loss S2:  0.05128979554674665\n",
            " Loss S01:  0.029014436942007806\n",
            " Loss S2:  0.05113748277043119\n",
            "\n",
            "Epoch: 30\n",
            "Loss S01:  0.030832108110189438\n",
            "Loss S2:  0.03730374574661255\n",
            "Loss S01:  0.029274029487913304\n",
            "Loss S2:  0.034323083236813545\n",
            "Loss S01:  0.02985017037107831\n",
            "Loss S2:  0.03434329612978867\n",
            "Loss S01:  0.030131670796582775\n",
            "Loss S2:  0.03470551456895567\n",
            "Loss S01:  0.030437698905787815\n",
            "Loss S2:  0.03481937018109531\n",
            "Loss S01:  0.030472348279812756\n",
            "Loss S2:  0.034869828060561534\n",
            "Loss S01:  0.030410620429721036\n",
            "Loss S2:  0.0347424895792711\n",
            "Loss S01:  0.03040631668983211\n",
            "Loss S2:  0.03471161795019264\n",
            "Loss S01:  0.030188888587333537\n",
            "Loss S2:  0.03454681276631208\n",
            "Loss S01:  0.03010433371905442\n",
            "Loss S2:  0.034457128310760296\n",
            "Loss S01:  0.03009706470045713\n",
            "Loss S2:  0.03431527707541343\n",
            "Loss S01:  0.030093109503656894\n",
            "Loss S2:  0.034320402437367954\n",
            "Loss S01:  0.03012037615884434\n",
            "Loss S2:  0.03434902053972906\n",
            "Loss S01:  0.030084330233112546\n",
            "Loss S2:  0.034379593089564155\n",
            "Loss S01:  0.030152531842707744\n",
            "Loss S2:  0.03440006822347641\n",
            "Loss S01:  0.030193667402429295\n",
            "Loss S2:  0.03446616460600042\n",
            "Loss S01:  0.03016708746135975\n",
            "Loss S2:  0.034438657071094336\n",
            "Loss S01:  0.030222820205210943\n",
            "Loss S2:  0.03455580903859864\n",
            "Loss S01:  0.03022673424559733\n",
            "Loss S2:  0.03460950237693708\n",
            "Loss S01:  0.03015314194700481\n",
            "Loss S2:  0.03456647153846256\n",
            "Loss S01:  0.030120397765022604\n",
            "Loss S2:  0.03455209361380013\n",
            "Loss S01:  0.03013938525912321\n",
            "Loss S2:  0.034594129523818525\n",
            "Loss S01:  0.030183863805025413\n",
            "Loss S2:  0.03465337956922626\n",
            "Loss S01:  0.03020356452794044\n",
            "Loss S2:  0.034653644194399126\n",
            "Loss S01:  0.03023544974099551\n",
            "Loss S2:  0.03467005692205983\n",
            "Loss S01:  0.03025056757923379\n",
            "Loss S2:  0.034685669259068026\n",
            "Loss S01:  0.030267659631839656\n",
            "Loss S2:  0.03472651098051976\n",
            "Loss S01:  0.030234429064272074\n",
            "Loss S2:  0.034699723674362436\n",
            "Loss S01:  0.03020762540797746\n",
            "Loss S2:  0.034653519900031786\n",
            "Loss S01:  0.030201174216376955\n",
            "Loss S2:  0.034675511336306114\n",
            "Loss S01:  0.0302056089427285\n",
            "Loss S2:  0.034662117333804254\n",
            "Loss S01:  0.03017966673856188\n",
            "Loss S2:  0.03459908753944938\n",
            "Loss S01:  0.030189714588897992\n",
            "Loss S2:  0.03460367656431837\n",
            "Loss S01:  0.03019654645030232\n",
            "Loss S2:  0.03462255638644234\n",
            "Loss S01:  0.030210781066823216\n",
            "Loss S2:  0.03466505602635771\n",
            "Loss S01:  0.03023387264046404\n",
            "Loss S2:  0.03468475230250433\n",
            "Loss S01:  0.030237313550884045\n",
            "Loss S2:  0.034675302716627346\n",
            "Loss S01:  0.030230772287416652\n",
            "Loss S2:  0.034655702594075244\n",
            "Loss S01:  0.030236306177388653\n",
            "Loss S2:  0.03464648825681116\n",
            "Loss S01:  0.03020893404608035\n",
            "Loss S2:  0.034618566755939016\n",
            "Loss S01:  0.030231120134529627\n",
            "Loss S2:  0.03462088631089786\n",
            "Loss S01:  0.030237764015866312\n",
            "Loss S2:  0.034620140436719515\n",
            "Loss S01:  0.03024759917407166\n",
            "Loss S2:  0.03462085141988378\n",
            "Loss S01:  0.030216588956366558\n",
            "Loss S2:  0.03459961573202206\n",
            "Loss S01:  0.03022074154124103\n",
            "Loss S2:  0.03462146587517797\n",
            "Loss S01:  0.030223314900223803\n",
            "Loss S2:  0.03463768019049765\n",
            "Loss S01:  0.030213909090341044\n",
            "Loss S2:  0.03464928635450609\n",
            "Loss S01:  0.030219079819834155\n",
            "Loss S2:  0.03464846239838504\n",
            "Loss S01:  0.030214448604663047\n",
            "Loss S2:  0.03465811978336308\n",
            "Loss S01:  0.03018134205304314\n",
            "Loss S2:  0.03463400034290831\n",
            "Validation: \n",
            " Loss S01:  0.027301620692014694\n",
            " Loss S2:  0.04678000509738922\n",
            " Loss S01:  0.02927859349264985\n",
            " Loss S2:  0.05095541122413817\n",
            " Loss S01:  0.029338271879568334\n",
            " Loss S2:  0.05113828900020297\n",
            " Loss S01:  0.029190258779486673\n",
            " Loss S2:  0.05067051195951759\n",
            " Loss S01:  0.029226475130812623\n",
            " Loss S2:  0.050597441371208356\n",
            "\n",
            "Epoch: 31\n",
            "Loss S01:  0.033726178109645844\n",
            "Loss S2:  0.0384998545050621\n",
            "Loss S01:  0.02994053861634298\n",
            "Loss S2:  0.03397744623097507\n",
            "Loss S01:  0.030146577173755282\n",
            "Loss S2:  0.03437659978156998\n",
            "Loss S01:  0.03013866064288924\n",
            "Loss S2:  0.03446769113502195\n",
            "Loss S01:  0.030211746238353775\n",
            "Loss S2:  0.034511766964342536\n",
            "Loss S01:  0.030251061616867195\n",
            "Loss S2:  0.03455620291916763\n",
            "Loss S01:  0.030116574075378356\n",
            "Loss S2:  0.03441698699578887\n",
            "Loss S01:  0.030100110524766882\n",
            "Loss S2:  0.03447102773672258\n",
            "Loss S01:  0.02992676361751409\n",
            "Loss S2:  0.03423633297652374\n",
            "Loss S01:  0.029885112870853027\n",
            "Loss S2:  0.03409136375310002\n",
            "Loss S01:  0.02982954380314539\n",
            "Loss S2:  0.03399473832606679\n",
            "Loss S01:  0.029772407853522816\n",
            "Loss S2:  0.033879457813528205\n",
            "Loss S01:  0.029720276855855935\n",
            "Loss S2:  0.03388350464649929\n",
            "Loss S01:  0.029734253087116563\n",
            "Loss S2:  0.03394052322299881\n",
            "Loss S01:  0.02976767510070023\n",
            "Loss S2:  0.03399683726647644\n",
            "Loss S01:  0.02976186295958939\n",
            "Loss S2:  0.03405491096146454\n",
            "Loss S01:  0.02972731772833359\n",
            "Loss S2:  0.034005429896508683\n",
            "Loss S01:  0.029790704133130654\n",
            "Loss S2:  0.03412570832678449\n",
            "Loss S01:  0.029776104576679882\n",
            "Loss S2:  0.03412464226066078\n",
            "Loss S01:  0.029747327233796344\n",
            "Loss S2:  0.0341310585551549\n",
            "Loss S01:  0.029702527717274812\n",
            "Loss S2:  0.03409758205548744\n",
            "Loss S01:  0.029735229971213927\n",
            "Loss S2:  0.034132704942076694\n",
            "Loss S01:  0.029793413726789918\n",
            "Loss S2:  0.034165708853850535\n",
            "Loss S01:  0.02979116357895203\n",
            "Loss S2:  0.03415306108435253\n",
            "Loss S01:  0.029835303974844112\n",
            "Loss S2:  0.03418534424511476\n",
            "Loss S01:  0.029818448887403266\n",
            "Loss S2:  0.034178988599919705\n",
            "Loss S01:  0.02984252270896316\n",
            "Loss S2:  0.03421865922480936\n",
            "Loss S01:  0.029797271746331035\n",
            "Loss S2:  0.034181962786345464\n",
            "Loss S01:  0.029784276902516543\n",
            "Loss S2:  0.034169950035278056\n",
            "Loss S01:  0.02981301962279577\n",
            "Loss S2:  0.034188276707255556\n",
            "Loss S01:  0.0298188970396388\n",
            "Loss S2:  0.03419170864668993\n",
            "Loss S01:  0.029769401021086134\n",
            "Loss S2:  0.034146852749384865\n",
            "Loss S01:  0.029773920437907133\n",
            "Loss S2:  0.034164118148224\n",
            "Loss S01:  0.029781910599268456\n",
            "Loss S2:  0.03418636426127029\n",
            "Loss S01:  0.029806214649656302\n",
            "Loss S2:  0.03422451917835869\n",
            "Loss S01:  0.029837994669110347\n",
            "Loss S2:  0.03423667249622678\n",
            "Loss S01:  0.02982366615983753\n",
            "Loss S2:  0.03421753655262601\n",
            "Loss S01:  0.02981332028309772\n",
            "Loss S2:  0.034182160585676885\n",
            "Loss S01:  0.029829341483123974\n",
            "Loss S2:  0.034182723548038425\n",
            "Loss S01:  0.02980123343103377\n",
            "Loss S2:  0.034151310158317044\n",
            "Loss S01:  0.029805343876827387\n",
            "Loss S2:  0.034166584961915256\n",
            "Loss S01:  0.02980965088119327\n",
            "Loss S2:  0.03416245875295496\n",
            "Loss S01:  0.029843613323965717\n",
            "Loss S2:  0.034178098053129435\n",
            "Loss S01:  0.029830496304445918\n",
            "Loss S2:  0.03414361101949021\n",
            "Loss S01:  0.029839615660662552\n",
            "Loss S2:  0.03415861425080807\n",
            "Loss S01:  0.029841664802704313\n",
            "Loss S2:  0.034173694300876224\n",
            "Loss S01:  0.02983739972922652\n",
            "Loss S2:  0.03418349070624002\n",
            "Loss S01:  0.029851753185572392\n",
            "Loss S2:  0.03418184288099187\n",
            "Loss S01:  0.029863712406889564\n",
            "Loss S2:  0.03419114171049079\n",
            "Loss S01:  0.02984109580091935\n",
            "Loss S2:  0.03417424200189939\n",
            "Validation: \n",
            " Loss S01:  0.02693512663245201\n",
            " Loss S2:  0.04768884554505348\n",
            " Loss S01:  0.028934981673955917\n",
            " Loss S2:  0.050060500701268516\n",
            " Loss S01:  0.029086981241295978\n",
            " Loss S2:  0.05015832122142722\n",
            " Loss S01:  0.029015662850903682\n",
            " Loss S2:  0.049785553126550114\n",
            " Loss S01:  0.029032991969107108\n",
            " Loss S2:  0.04963784467106984\n",
            "\n",
            "Epoch: 32\n",
            "Loss S01:  0.03396263346076012\n",
            "Loss S2:  0.037951063364744186\n",
            "Loss S01:  0.029192317446524448\n",
            "Loss S2:  0.03323996100913395\n",
            "Loss S01:  0.029329659328574224\n",
            "Loss S2:  0.033225154947666896\n",
            "Loss S01:  0.029383290198541457\n",
            "Loss S2:  0.03340164950538066\n",
            "Loss S01:  0.029669108459862266\n",
            "Loss S2:  0.03369865543776896\n",
            "Loss S01:  0.029811426266735674\n",
            "Loss S2:  0.033839790515747724\n",
            "Loss S01:  0.029671129022465378\n",
            "Loss S2:  0.03369031990038567\n",
            "Loss S01:  0.02957609062358527\n",
            "Loss S2:  0.0336685038735749\n",
            "Loss S01:  0.029416761395555956\n",
            "Loss S2:  0.03347408288606891\n",
            "Loss S01:  0.02935107551760726\n",
            "Loss S2:  0.03333332672060191\n",
            "Loss S01:  0.029292755953893804\n",
            "Loss S2:  0.033261347008813724\n",
            "Loss S01:  0.029274379029064566\n",
            "Loss S2:  0.0332635051092586\n",
            "Loss S01:  0.02931166023077551\n",
            "Loss S2:  0.03333590957066737\n",
            "Loss S01:  0.02931056195827386\n",
            "Loss S2:  0.03338904707258894\n",
            "Loss S01:  0.029353948944109552\n",
            "Loss S2:  0.03349944649277427\n",
            "Loss S01:  0.029395051448550445\n",
            "Loss S2:  0.033631082389824436\n",
            "Loss S01:  0.029366909358534754\n",
            "Loss S2:  0.033659441963486046\n",
            "Loss S01:  0.029475594789051172\n",
            "Loss S2:  0.03374642896199087\n",
            "Loss S01:  0.029452965826372416\n",
            "Loss S2:  0.0337476204689695\n",
            "Loss S01:  0.029438023967424613\n",
            "Loss S2:  0.03375807513741298\n",
            "Loss S01:  0.029392891670044382\n",
            "Loss S2:  0.03371474700076366\n",
            "Loss S01:  0.029390845288880064\n",
            "Loss S2:  0.03371120458777764\n",
            "Loss S01:  0.02941441727271177\n",
            "Loss S2:  0.033766819754135015\n",
            "Loss S01:  0.029452950536430655\n",
            "Loss S2:  0.03377578277705294\n",
            "Loss S01:  0.02950274489078037\n",
            "Loss S2:  0.033837524014699014\n",
            "Loss S01:  0.029500358066413983\n",
            "Loss S2:  0.03388262095590274\n",
            "Loss S01:  0.02950759285568506\n",
            "Loss S2:  0.033918232039701894\n",
            "Loss S01:  0.029472032094518638\n",
            "Loss S2:  0.033915430616683624\n",
            "Loss S01:  0.029447820137032836\n",
            "Loss S2:  0.03388567503501936\n",
            "Loss S01:  0.029461136753462843\n",
            "Loss S2:  0.0339033331888443\n",
            "Loss S01:  0.029474706577749744\n",
            "Loss S2:  0.03390695233875731\n",
            "Loss S01:  0.029473636976152753\n",
            "Loss S2:  0.033891524873985356\n",
            "Loss S01:  0.02947512257414815\n",
            "Loss S2:  0.033901719353234286\n",
            "Loss S01:  0.02946486340391132\n",
            "Loss S2:  0.0338987251152805\n",
            "Loss S01:  0.029492414941536017\n",
            "Loss S2:  0.03392395592373837\n",
            "Loss S01:  0.029512122008111062\n",
            "Loss S2:  0.03395149748018834\n",
            "Loss S01:  0.029503047074142256\n",
            "Loss S2:  0.0339418024939183\n",
            "Loss S01:  0.029485636463544438\n",
            "Loss S2:  0.033928078807629346\n",
            "Loss S01:  0.02950418252605466\n",
            "Loss S2:  0.03392287846801319\n",
            "Loss S01:  0.029490695532668584\n",
            "Loss S2:  0.033898344215796426\n",
            "Loss S01:  0.02949052222741958\n",
            "Loss S2:  0.033896029933803994\n",
            "Loss S01:  0.029498470716015267\n",
            "Loss S2:  0.03388609198305004\n",
            "Loss S01:  0.029532900580034688\n",
            "Loss S2:  0.03389461941999381\n",
            "Loss S01:  0.0295109063810442\n",
            "Loss S2:  0.03386507737785232\n",
            "Loss S01:  0.029513604474378553\n",
            "Loss S2:  0.03387232076233071\n",
            "Loss S01:  0.029512015202324302\n",
            "Loss S2:  0.033896105562206645\n",
            "Loss S01:  0.029505981285925024\n",
            "Loss S2:  0.03388991729717968\n",
            "Loss S01:  0.02950190638454983\n",
            "Loss S2:  0.033888902689178026\n",
            "Loss S01:  0.02950116607850406\n",
            "Loss S2:  0.03388204947965928\n",
            "Loss S01:  0.0294729233197792\n",
            "Loss S2:  0.03385074652518978\n",
            "Validation: \n",
            " Loss S01:  0.027011211961507797\n",
            " Loss S2:  0.047823887318372726\n",
            " Loss S01:  0.028966200227538746\n",
            " Loss S2:  0.04952038097239676\n",
            " Loss S01:  0.028970173173924772\n",
            " Loss S2:  0.04968769557592345\n",
            " Loss S01:  0.028881573561029356\n",
            " Loss S2:  0.04927614019786725\n",
            " Loss S01:  0.02889502257752566\n",
            " Loss S2:  0.04911273428135448\n",
            "\n",
            "Epoch: 33\n",
            "Loss S01:  0.03267071768641472\n",
            "Loss S2:  0.03824860230088234\n",
            "Loss S01:  0.028814347291534596\n",
            "Loss S2:  0.03345642966980284\n",
            "Loss S01:  0.029000272353490193\n",
            "Loss S2:  0.033497833158998264\n",
            "Loss S01:  0.02892085699544799\n",
            "Loss S2:  0.03330570128896544\n",
            "Loss S01:  0.029035761089223188\n",
            "Loss S2:  0.033432510340722595\n",
            "Loss S01:  0.02924238547098403\n",
            "Loss S2:  0.033587719478151375\n",
            "Loss S01:  0.029237624471549126\n",
            "Loss S2:  0.033605210941101686\n",
            "Loss S01:  0.02927030554749596\n",
            "Loss S2:  0.03369561837277782\n",
            "Loss S01:  0.02907108655774299\n",
            "Loss S2:  0.03347534741516466\n",
            "Loss S01:  0.02901007537985896\n",
            "Loss S2:  0.03342882807847563\n",
            "Loss S01:  0.028954744707829883\n",
            "Loss S2:  0.03329538336337203\n",
            "Loss S01:  0.028937381566376298\n",
            "Loss S2:  0.033236967718547526\n",
            "Loss S01:  0.02902595366328216\n",
            "Loss S2:  0.033275168389081955\n",
            "Loss S01:  0.029034084235211366\n",
            "Loss S2:  0.033312637060311914\n",
            "Loss S01:  0.02905637611047173\n",
            "Loss S2:  0.033350685611367226\n",
            "Loss S01:  0.029116275849819972\n",
            "Loss S2:  0.03343608310532491\n",
            "Loss S01:  0.029097920572905805\n",
            "Loss S2:  0.033410175489435284\n",
            "Loss S01:  0.029160557325646194\n",
            "Loss S2:  0.03350239024873365\n",
            "Loss S01:  0.029152180701082584\n",
            "Loss S2:  0.03353745122928975\n",
            "Loss S01:  0.029136886907965724\n",
            "Loss S2:  0.033505654869435346\n",
            "Loss S01:  0.029127337084851455\n",
            "Loss S2:  0.03348938189446926\n",
            "Loss S01:  0.029142759625592502\n",
            "Loss S2:  0.033473440768170694\n",
            "Loss S01:  0.029181616465093322\n",
            "Loss S2:  0.03349112275971007\n",
            "Loss S01:  0.02920841843341336\n",
            "Loss S2:  0.03348770264326494\n",
            "Loss S01:  0.029249662355639628\n",
            "Loss S2:  0.03351195758649175\n",
            "Loss S01:  0.029276141889423013\n",
            "Loss S2:  0.03354551266151595\n",
            "Loss S01:  0.02928609881992541\n",
            "Loss S2:  0.033563888021584214\n",
            "Loss S01:  0.02924068324350343\n",
            "Loss S2:  0.03354479949852637\n",
            "Loss S01:  0.029227540977431785\n",
            "Loss S2:  0.03352299595775341\n",
            "Loss S01:  0.029223754534760293\n",
            "Loss S2:  0.03353446097308418\n",
            "Loss S01:  0.02924010303007606\n",
            "Loss S2:  0.03355655395905441\n",
            "Loss S01:  0.029209494800381722\n",
            "Loss S2:  0.0335216167116376\n",
            "Loss S01:  0.029205006225226083\n",
            "Loss S2:  0.033542257516044324\n",
            "Loss S01:  0.029209033285022862\n",
            "Loss S2:  0.033549408903880064\n",
            "Loss S01:  0.029237209237522044\n",
            "Loss S2:  0.03357874957675284\n",
            "Loss S01:  0.029266600913641458\n",
            "Loss S2:  0.03361853644082009\n",
            "Loss S01:  0.029268743073015663\n",
            "Loss S2:  0.03362837526689276\n",
            "Loss S01:  0.029249059001229843\n",
            "Loss S2:  0.033605110311925895\n",
            "Loss S01:  0.029253247968872076\n",
            "Loss S2:  0.03358732263149396\n",
            "Loss S01:  0.029229655523624873\n",
            "Loss S2:  0.03356869689300847\n",
            "Loss S01:  0.029223203998142645\n",
            "Loss S2:  0.033575747710213696\n",
            "Loss S01:  0.029234549021597615\n",
            "Loss S2:  0.03356676230120978\n",
            "Loss S01:  0.02926174529126874\n",
            "Loss S2:  0.03357317452617609\n",
            "Loss S01:  0.02924230342985859\n",
            "Loss S2:  0.033526912461231594\n",
            "Loss S01:  0.02923825401770555\n",
            "Loss S2:  0.03353730297294739\n",
            "Loss S01:  0.029235697357591135\n",
            "Loss S2:  0.03353811085158071\n",
            "Loss S01:  0.029219845891677636\n",
            "Loss S2:  0.03353866071737252\n",
            "Loss S01:  0.029218702883540698\n",
            "Loss S2:  0.03353497295021505\n",
            "Loss S01:  0.029225727047562104\n",
            "Loss S2:  0.03355587469781759\n",
            "Loss S01:  0.029205261576017875\n",
            "Loss S2:  0.0335337603711305\n",
            "Validation: \n",
            " Loss S01:  0.026170963421463966\n",
            " Loss S2:  0.04825384169816971\n",
            " Loss S01:  0.028817985206842422\n",
            " Loss S2:  0.05123708734200114\n",
            " Loss S01:  0.028838452269754757\n",
            " Loss S2:  0.05129172816509154\n",
            " Loss S01:  0.028787551325608472\n",
            " Loss S2:  0.05082637236499395\n",
            " Loss S01:  0.02880119180513753\n",
            " Loss S2:  0.05070639907577892\n",
            "\n",
            "Epoch: 34\n",
            "Loss S01:  0.030837630853056908\n",
            "Loss S2:  0.032975759357213974\n",
            "Loss S01:  0.028595680032264103\n",
            "Loss S2:  0.032833157276565376\n",
            "Loss S01:  0.028833849266881033\n",
            "Loss S2:  0.03301087429835683\n",
            "Loss S01:  0.028859139930817387\n",
            "Loss S2:  0.03303169260822957\n",
            "Loss S01:  0.029010549551103174\n",
            "Loss S2:  0.03328007009879845\n",
            "Loss S01:  0.029033036281665165\n",
            "Loss S2:  0.033455686826331946\n",
            "Loss S01:  0.0288995728202042\n",
            "Loss S2:  0.033262329878377135\n",
            "Loss S01:  0.02888226944585921\n",
            "Loss S2:  0.03321971106802074\n",
            "Loss S01:  0.028729459638764828\n",
            "Loss S2:  0.03303150563604302\n",
            "Loss S01:  0.02867909342787423\n",
            "Loss S2:  0.03297102099264061\n",
            "Loss S01:  0.028685809140748315\n",
            "Loss S2:  0.03287489481712922\n",
            "Loss S01:  0.02866091483549492\n",
            "Loss S2:  0.032856729133306325\n",
            "Loss S01:  0.028658904658615095\n",
            "Loss S2:  0.03284864158423479\n",
            "Loss S01:  0.02869016355799355\n",
            "Loss S2:  0.03293259531430161\n",
            "Loss S01:  0.028704857889642107\n",
            "Loss S2:  0.032959899488598746\n",
            "Loss S01:  0.028774742345442837\n",
            "Loss S2:  0.033010377875523064\n",
            "Loss S01:  0.02871093871245473\n",
            "Loss S2:  0.03301491988649279\n",
            "Loss S01:  0.028783076524473074\n",
            "Loss S2:  0.033100823038502744\n",
            "Loss S01:  0.028824584826168436\n",
            "Loss S2:  0.03310395931588352\n",
            "Loss S01:  0.02878382369017726\n",
            "Loss S2:  0.03309577274384923\n",
            "Loss S01:  0.028747615844604387\n",
            "Loss S2:  0.03305439739047888\n",
            "Loss S01:  0.028762315113044463\n",
            "Loss S2:  0.03306055569500437\n",
            "Loss S01:  0.02879443340745177\n",
            "Loss S2:  0.03307969649891238\n",
            "Loss S01:  0.0288325192092301\n",
            "Loss S2:  0.033078856814604304\n",
            "Loss S01:  0.02889760449157711\n",
            "Loss S2:  0.03312223993581855\n",
            "Loss S01:  0.028892628084200313\n",
            "Loss S2:  0.03314837916527849\n",
            "Loss S01:  0.028910724075075767\n",
            "Loss S2:  0.033173634531511655\n",
            "Loss S01:  0.02889335192602499\n",
            "Loss S2:  0.03314867313965\n",
            "Loss S01:  0.02887969365959914\n",
            "Loss S2:  0.03313715816922883\n",
            "Loss S01:  0.028868248987034013\n",
            "Loss S2:  0.033165028670809116\n",
            "Loss S01:  0.02887910227616166\n",
            "Loss S2:  0.033167160242993966\n",
            "Loss S01:  0.028854067476280634\n",
            "Loss S2:  0.033141031154578136\n",
            "Loss S01:  0.028864949152477063\n",
            "Loss S2:  0.03315674128863856\n",
            "Loss S01:  0.02888294432053393\n",
            "Loss S2:  0.033169218516286764\n",
            "Loss S01:  0.02892002671454484\n",
            "Loss S2:  0.033198643710649955\n",
            "Loss S01:  0.02895288576406461\n",
            "Loss S2:  0.03322326345767221\n",
            "Loss S01:  0.028959106715844937\n",
            "Loss S2:  0.03322374681913787\n",
            "Loss S01:  0.028946199120496802\n",
            "Loss S2:  0.0332379169219909\n",
            "Loss S01:  0.02894739299680148\n",
            "Loss S2:  0.03321647674842613\n",
            "Loss S01:  0.0289132857976286\n",
            "Loss S2:  0.03318336850408551\n",
            "Loss S01:  0.028919856405429115\n",
            "Loss S2:  0.033194457345695566\n",
            "Loss S01:  0.02893060274023354\n",
            "Loss S2:  0.033198691895480865\n",
            "Loss S01:  0.028943351534086847\n",
            "Loss S2:  0.03322719875059473\n",
            "Loss S01:  0.028924222110733908\n",
            "Loss S2:  0.03320153370897206\n",
            "Loss S01:  0.028909906339483198\n",
            "Loss S2:  0.03320867787886098\n",
            "Loss S01:  0.02890429748614585\n",
            "Loss S2:  0.03320120441873956\n",
            "Loss S01:  0.028912457833450426\n",
            "Loss S2:  0.03320583753255469\n",
            "Loss S01:  0.028914637697540272\n",
            "Loss S2:  0.03320490498878774\n",
            "Loss S01:  0.028922842141713274\n",
            "Loss S2:  0.03320885165797821\n",
            "Loss S01:  0.028913858139891245\n",
            "Loss S2:  0.033196895180342155\n",
            "Validation: \n",
            " Loss S01:  0.026593178510665894\n",
            " Loss S2:  0.046577081084251404\n",
            " Loss S01:  0.02844199768844105\n",
            " Loss S2:  0.04832850095062029\n",
            " Loss S01:  0.02850352636561161\n",
            " Loss S2:  0.04841138458833462\n",
            " Loss S01:  0.02840653351950841\n",
            " Loss S2:  0.047899651112126525\n",
            " Loss S01:  0.02842849327458276\n",
            " Loss S2:  0.047880844523509346\n",
            "\n",
            "Epoch: 35\n",
            "Loss S01:  0.031768057495355606\n",
            "Loss S2:  0.03405993804335594\n",
            "Loss S01:  0.02853121557696299\n",
            "Loss S2:  0.03314605236730792\n",
            "Loss S01:  0.02841134121020635\n",
            "Loss S2:  0.03295703294376532\n",
            "Loss S01:  0.028663436792069865\n",
            "Loss S2:  0.03316796877451481\n",
            "Loss S01:  0.028840940491091913\n",
            "Loss S2:  0.033066788915453886\n",
            "Loss S01:  0.028872062360831334\n",
            "Loss S2:  0.03301494199709565\n",
            "Loss S01:  0.028605566955492146\n",
            "Loss S2:  0.03279984946988645\n",
            "Loss S01:  0.028622673535850685\n",
            "Loss S2:  0.032791741926905135\n",
            "Loss S01:  0.028483445859617658\n",
            "Loss S2:  0.03257442794648217\n",
            "Loss S01:  0.028401992580556607\n",
            "Loss S2:  0.03248645082771123\n",
            "Loss S01:  0.02840303338252672\n",
            "Loss S2:  0.03236368387052328\n",
            "Loss S01:  0.028352146179557922\n",
            "Loss S2:  0.0323411336688845\n",
            "Loss S01:  0.028365014551961717\n",
            "Loss S2:  0.03244039842720367\n",
            "Loss S01:  0.028376782497831883\n",
            "Loss S2:  0.03250738748043548\n",
            "Loss S01:  0.028442970140183224\n",
            "Loss S2:  0.03255060275501393\n",
            "Loss S01:  0.028474758237304276\n",
            "Loss S2:  0.03261470802078974\n",
            "Loss S01:  0.028478797687136608\n",
            "Loss S2:  0.03265158207335087\n",
            "Loss S01:  0.028546685418277457\n",
            "Loss S2:  0.032770263074695716\n",
            "Loss S01:  0.02857480364856799\n",
            "Loss S2:  0.03282019458916965\n",
            "Loss S01:  0.02856946727797311\n",
            "Loss S2:  0.03282033464827463\n",
            "Loss S01:  0.02856275388294488\n",
            "Loss S2:  0.03277110497453319\n",
            "Loss S01:  0.028574220747885546\n",
            "Loss S2:  0.03277890003723273\n",
            "Loss S01:  0.02859427929093126\n",
            "Loss S2:  0.032780479302638256\n",
            "Loss S01:  0.028618747481342518\n",
            "Loss S2:  0.032769558580342306\n",
            "Loss S01:  0.0286785310203851\n",
            "Loss S2:  0.032805992970513606\n",
            "Loss S01:  0.028694452317171836\n",
            "Loss S2:  0.03283444303737219\n",
            "Loss S01:  0.02870718882172957\n",
            "Loss S2:  0.032849357524080296\n",
            "Loss S01:  0.028678308471322498\n",
            "Loss S2:  0.032819278936874384\n",
            "Loss S01:  0.02867053165448518\n",
            "Loss S2:  0.03278748079259412\n",
            "Loss S01:  0.02866380951044076\n",
            "Loss S2:  0.03281934519019938\n",
            "Loss S01:  0.028673753061910404\n",
            "Loss S2:  0.03281314044794767\n",
            "Loss S01:  0.028639233919681078\n",
            "Loss S2:  0.03279183204869749\n",
            "Loss S01:  0.028628790698875892\n",
            "Loss S2:  0.032816061761659625\n",
            "Loss S01:  0.02864656771260266\n",
            "Loss S2:  0.03283644948728675\n",
            "Loss S01:  0.02866799031490804\n",
            "Loss S2:  0.03285688885605056\n",
            "Loss S01:  0.02867516400849717\n",
            "Loss S2:  0.03287378696464745\n",
            "Loss S01:  0.028661417086038563\n",
            "Loss S2:  0.03289140835713977\n",
            "Loss S01:  0.02864793725211344\n",
            "Loss S2:  0.03289786186521105\n",
            "Loss S01:  0.028652892308676337\n",
            "Loss S2:  0.03290550707499656\n",
            "Loss S01:  0.028634607653750484\n",
            "Loss S2:  0.03289037011087398\n",
            "Loss S01:  0.02864563688542926\n",
            "Loss S2:  0.03290198910563367\n",
            "Loss S01:  0.028643026286305592\n",
            "Loss S2:  0.03290323963855595\n",
            "Loss S01:  0.028675642010541257\n",
            "Loss S2:  0.03292165418330253\n",
            "Loss S01:  0.02864445971104634\n",
            "Loss S2:  0.03289790281252352\n",
            "Loss S01:  0.028650304302573204\n",
            "Loss S2:  0.03291543995087244\n",
            "Loss S01:  0.02864654362614694\n",
            "Loss S2:  0.0329117203242829\n",
            "Loss S01:  0.02863968560500414\n",
            "Loss S2:  0.03292170910007137\n",
            "Loss S01:  0.028649195838885703\n",
            "Loss S2:  0.03293251584732861\n",
            "Loss S01:  0.028650906490715774\n",
            "Loss S2:  0.03294202185810975\n",
            "Loss S01:  0.02862415045818462\n",
            "Loss S2:  0.03290796417613986\n",
            "Validation: \n",
            " Loss S01:  0.026297718286514282\n",
            " Loss S2:  0.04575080797076225\n",
            " Loss S01:  0.028135076874778384\n",
            " Loss S2:  0.04907867099557604\n",
            " Loss S01:  0.028158658842851476\n",
            " Loss S2:  0.04921228974694159\n",
            " Loss S01:  0.02810555281209164\n",
            " Loss S2:  0.048844031378871104\n",
            " Loss S01:  0.028082859736901743\n",
            " Loss S2:  0.04872563798670416\n",
            "\n",
            "Epoch: 36\n",
            "Loss S01:  0.030516643077135086\n",
            "Loss S2:  0.03715171664953232\n",
            "Loss S01:  0.028564441271803596\n",
            "Loss S2:  0.03242705085060813\n",
            "Loss S01:  0.02855869107657955\n",
            "Loss S2:  0.03247457467729137\n",
            "Loss S01:  0.028553517234902227\n",
            "Loss S2:  0.03234645610134448\n",
            "Loss S01:  0.028612443223232177\n",
            "Loss S2:  0.03237238835270812\n",
            "Loss S01:  0.02864423693687308\n",
            "Loss S2:  0.03245107771134844\n",
            "Loss S01:  0.02861359376521384\n",
            "Loss S2:  0.03252548031264641\n",
            "Loss S01:  0.02866541108929775\n",
            "Loss S2:  0.032645952087682735\n",
            "Loss S01:  0.028574423818492595\n",
            "Loss S2:  0.03248715425990982\n",
            "Loss S01:  0.028479424827210196\n",
            "Loss S2:  0.032416851638437626\n",
            "Loss S01:  0.028435431918737913\n",
            "Loss S2:  0.03233056294150872\n",
            "Loss S01:  0.028420250443322165\n",
            "Loss S2:  0.03230439824563963\n",
            "Loss S01:  0.028346873850615555\n",
            "Loss S2:  0.0323112092080934\n",
            "Loss S01:  0.028362168027585698\n",
            "Loss S2:  0.03235372374872215\n",
            "Loss S01:  0.028390098548104578\n",
            "Loss S2:  0.032431516492514745\n",
            "Loss S01:  0.028439295200618688\n",
            "Loss S2:  0.03253723451534644\n",
            "Loss S01:  0.028404885255124256\n",
            "Loss S2:  0.032578525995338184\n",
            "Loss S01:  0.028489788721876536\n",
            "Loss S2:  0.03267109044419046\n",
            "Loss S01:  0.028476038103851166\n",
            "Loss S2:  0.0327228519921474\n",
            "Loss S01:  0.028433673152046677\n",
            "Loss S2:  0.03273256810586802\n",
            "Loss S01:  0.028403081713401855\n",
            "Loss S2:  0.03267590968466517\n",
            "Loss S01:  0.028400312561841938\n",
            "Loss S2:  0.032676242618552315\n",
            "Loss S01:  0.02842875723453129\n",
            "Loss S2:  0.03267776873384126\n",
            "Loss S01:  0.028442040376074902\n",
            "Loss S2:  0.03266182485403436\n",
            "Loss S01:  0.028488160895485105\n",
            "Loss S2:  0.03269606005570453\n",
            "Loss S01:  0.028496621087549693\n",
            "Loss S2:  0.03273327487606214\n",
            "Loss S01:  0.028504763690619175\n",
            "Loss S2:  0.03275478653381382\n",
            "Loss S01:  0.02846382378267186\n",
            "Loss S2:  0.03272882720448654\n",
            "Loss S01:  0.028458383464622327\n",
            "Loss S2:  0.03272849863587325\n",
            "Loss S01:  0.028466572024926695\n",
            "Loss S2:  0.03275110972629175\n",
            "Loss S01:  0.02846719038075585\n",
            "Loss S2:  0.032741035962867183\n",
            "Loss S01:  0.02841669188099659\n",
            "Loss S2:  0.03269830710251615\n",
            "Loss S01:  0.028429168198153238\n",
            "Loss S2:  0.032695685806350544\n",
            "Loss S01:  0.028440032801464246\n",
            "Loss S2:  0.032696969733992734\n",
            "Loss S01:  0.028447452031316295\n",
            "Loss S2:  0.03270291546332347\n",
            "Loss S01:  0.02848662489605935\n",
            "Loss S2:  0.03272645854712212\n",
            "Loss S01:  0.02849156299600806\n",
            "Loss S2:  0.03272582316084912\n",
            "Loss S01:  0.028485490260218994\n",
            "Loss S2:  0.03271889102185833\n",
            "Loss S01:  0.028480506714756094\n",
            "Loss S2:  0.03270141173672332\n",
            "Loss S01:  0.028452860224811014\n",
            "Loss S2:  0.032675470127855114\n",
            "Loss S01:  0.02843813546596471\n",
            "Loss S2:  0.032674048657986586\n",
            "Loss S01:  0.028436459727820978\n",
            "Loss S2:  0.03265380223537739\n",
            "Loss S01:  0.028439624876562604\n",
            "Loss S2:  0.03266944795528387\n",
            "Loss S01:  0.028418864402271756\n",
            "Loss S2:  0.032636825107643055\n",
            "Loss S01:  0.02841036540377978\n",
            "Loss S2:  0.03263609440756493\n",
            "Loss S01:  0.02841029319903935\n",
            "Loss S2:  0.0326259266626412\n",
            "Loss S01:  0.028399162770222686\n",
            "Loss S2:  0.032622445148462326\n",
            "Loss S01:  0.028406765612510612\n",
            "Loss S2:  0.032612951243031305\n",
            "Loss S01:  0.02841061587579037\n",
            "Loss S2:  0.03261736914004333\n",
            "Loss S01:  0.028380191279859746\n",
            "Loss S2:  0.032590736166522846\n",
            "Validation: \n",
            " Loss S01:  0.02563823200762272\n",
            " Loss S2:  0.04720186069607735\n",
            " Loss S01:  0.02767026699369862\n",
            " Loss S2:  0.05055334844759533\n",
            " Loss S01:  0.02782158380964907\n",
            " Loss S2:  0.05052363163814312\n",
            " Loss S01:  0.027684719286492614\n",
            " Loss S2:  0.050107616137285704\n",
            " Loss S01:  0.02769461977812979\n",
            " Loss S2:  0.0499637269495446\n",
            "\n",
            "Epoch: 37\n",
            "Loss S01:  0.032259829342365265\n",
            "Loss S2:  0.03543644770979881\n",
            "Loss S01:  0.027961891821839592\n",
            "Loss S2:  0.031755571168932045\n",
            "Loss S01:  0.02799608629374277\n",
            "Loss S2:  0.03219352423080376\n",
            "Loss S01:  0.028106464974341855\n",
            "Loss S2:  0.032154140032587514\n",
            "Loss S01:  0.02827883198312143\n",
            "Loss S2:  0.032180844192824715\n",
            "Loss S01:  0.028408867946150256\n",
            "Loss S2:  0.03234360900287535\n",
            "Loss S01:  0.028325359535510422\n",
            "Loss S2:  0.03223501029806059\n",
            "Loss S01:  0.028245023491097167\n",
            "Loss S2:  0.032248159424519876\n",
            "Loss S01:  0.028122589498022457\n",
            "Loss S2:  0.032076492468699994\n",
            "Loss S01:  0.02804138812308128\n",
            "Loss S2:  0.03199455784736099\n",
            "Loss S01:  0.028027771538732076\n",
            "Loss S2:  0.031896274961014784\n",
            "Loss S01:  0.028026978745385334\n",
            "Loss S2:  0.03184012402486694\n",
            "Loss S01:  0.028007036983227925\n",
            "Loss S2:  0.03187284715596802\n",
            "Loss S01:  0.028019281715609645\n",
            "Loss S2:  0.03191592493130051\n",
            "Loss S01:  0.02805541367237027\n",
            "Loss S2:  0.03198401600862226\n",
            "Loss S01:  0.028146816121525323\n",
            "Loss S2:  0.03208055901063594\n",
            "Loss S01:  0.02816755263285237\n",
            "Loss S2:  0.03208686500463797\n",
            "Loss S01:  0.02822280496533154\n",
            "Loss S2:  0.032175062300517546\n",
            "Loss S01:  0.02823764976957885\n",
            "Loss S2:  0.03217434499268703\n",
            "Loss S01:  0.028214313011831013\n",
            "Loss S2:  0.03216515366397603\n",
            "Loss S01:  0.0281888598585455\n",
            "Loss S2:  0.03214218532342223\n",
            "Loss S01:  0.028205141698791517\n",
            "Loss S2:  0.03215879289283289\n",
            "Loss S01:  0.02820385711865997\n",
            "Loss S2:  0.032190697402503694\n",
            "Loss S01:  0.028219553654070025\n",
            "Loss S2:  0.032169920049640005\n",
            "Loss S01:  0.028262108224308836\n",
            "Loss S2:  0.03223409925691814\n",
            "Loss S01:  0.0282746906952554\n",
            "Loss S2:  0.032248661144677386\n",
            "Loss S01:  0.028292397022875334\n",
            "Loss S2:  0.032286680578272005\n",
            "Loss S01:  0.028246573382143165\n",
            "Loss S2:  0.03225910405858636\n",
            "Loss S01:  0.028243652561487254\n",
            "Loss S2:  0.03225746169500716\n",
            "Loss S01:  0.02825096531857535\n",
            "Loss S2:  0.032283284147734084\n",
            "Loss S01:  0.028260183474203678\n",
            "Loss S2:  0.03229263312248296\n",
            "Loss S01:  0.028250160384503974\n",
            "Loss S2:  0.032290013620803595\n",
            "Loss S01:  0.028256446929689136\n",
            "Loss S2:  0.03231710995724454\n",
            "Loss S01:  0.028283535466273386\n",
            "Loss S2:  0.032331918943054365\n",
            "Loss S01:  0.028302487543076713\n",
            "Loss S2:  0.032363014066280504\n",
            "Loss S01:  0.028308562005729416\n",
            "Loss S2:  0.0323994378174557\n",
            "Loss S01:  0.028315296128309665\n",
            "Loss S2:  0.03240350210249754\n",
            "Loss S01:  0.028306721677034692\n",
            "Loss S2:  0.032394308195200894\n",
            "Loss S01:  0.028314427039989338\n",
            "Loss S2:  0.03237989284293702\n",
            "Loss S01:  0.02829808437877604\n",
            "Loss S2:  0.03234585028741976\n",
            "Loss S01:  0.02830853834376668\n",
            "Loss S2:  0.03235272213147763\n",
            "Loss S01:  0.028314244790668904\n",
            "Loss S2:  0.03234244409468197\n",
            "Loss S01:  0.028326320818087832\n",
            "Loss S2:  0.03234792745842503\n",
            "Loss S01:  0.028310913216590603\n",
            "Loss S2:  0.03231776955595841\n",
            "Loss S01:  0.028291788053553122\n",
            "Loss S2:  0.032314344383184875\n",
            "Loss S01:  0.028291339736159254\n",
            "Loss S2:  0.032319955395621366\n",
            "Loss S01:  0.028284821449223414\n",
            "Loss S2:  0.032344415432363424\n",
            "Loss S01:  0.028281963788241844\n",
            "Loss S2:  0.03234915814566131\n",
            "Loss S01:  0.028283659202238378\n",
            "Loss S2:  0.03235810096307876\n",
            "Loss S01:  0.028260950970249117\n",
            "Loss S2:  0.03233852086858084\n",
            "Validation: \n",
            " Loss S01:  0.026560606434941292\n",
            " Loss S2:  0.046094853430986404\n",
            " Loss S01:  0.029527365185675166\n",
            " Loss S2:  0.04894831900795301\n",
            " Loss S01:  0.029501863687140185\n",
            " Loss S2:  0.04887689958985259\n",
            " Loss S01:  0.029337362767975838\n",
            " Loss S2:  0.0484562345826235\n",
            " Loss S01:  0.029372118252479\n",
            " Loss S2:  0.04832563898813577\n",
            "\n",
            "Epoch: 38\n",
            "Loss S01:  0.030438605695962906\n",
            "Loss S2:  0.03531736135482788\n",
            "Loss S01:  0.02811044336042621\n",
            "Loss S2:  0.03171586059033871\n",
            "Loss S01:  0.02832708623082865\n",
            "Loss S2:  0.03195198314885298\n",
            "Loss S01:  0.028276796302487774\n",
            "Loss S2:  0.03212570921788292\n",
            "Loss S01:  0.028313022606619973\n",
            "Loss S2:  0.03208384072271789\n",
            "Loss S01:  0.028371538346012432\n",
            "Loss S2:  0.032182588223733155\n",
            "Loss S01:  0.0281844431626015\n",
            "Loss S2:  0.032095191023144565\n",
            "Loss S01:  0.028165883214121133\n",
            "Loss S2:  0.0321830072978013\n",
            "Loss S01:  0.02795356144139796\n",
            "Loss S2:  0.03201580629396586\n",
            "Loss S01:  0.027875524914854174\n",
            "Loss S2:  0.031973731939445485\n",
            "Loss S01:  0.027855253695408897\n",
            "Loss S2:  0.03190822410627757\n",
            "Loss S01:  0.027825712687797374\n",
            "Loss S2:  0.03183536406035896\n",
            "Loss S01:  0.0278375647531068\n",
            "Loss S2:  0.031818240188246916\n",
            "Loss S01:  0.02785789849990197\n",
            "Loss S2:  0.031838207598064695\n",
            "Loss S01:  0.02786696601844003\n",
            "Loss S2:  0.03189280100748049\n",
            "Loss S01:  0.027940834607213538\n",
            "Loss S2:  0.03197369672703427\n",
            "Loss S01:  0.02790323545381149\n",
            "Loss S2:  0.03199313694368238\n",
            "Loss S01:  0.027964205355846393\n",
            "Loss S2:  0.0320494789521248\n",
            "Loss S01:  0.02797842613364781\n",
            "Loss S2:  0.032076974147261836\n",
            "Loss S01:  0.027931688577716888\n",
            "Loss S2:  0.032035815212583044\n",
            "Loss S01:  0.02788812029569303\n",
            "Loss S2:  0.032021685100313446\n",
            "Loss S01:  0.027918342553891275\n",
            "Loss S2:  0.032051532101150935\n",
            "Loss S01:  0.02795995598262791\n",
            "Loss S2:  0.032068617675638844\n",
            "Loss S01:  0.027978884531951055\n",
            "Loss S2:  0.032037056014909374\n",
            "Loss S01:  0.028033716310418493\n",
            "Loss S2:  0.03210408219688908\n",
            "Loss S01:  0.02800462422410093\n",
            "Loss S2:  0.032101033121881735\n",
            "Loss S01:  0.02800485800052511\n",
            "Loss S2:  0.03212366488644446\n",
            "Loss S01:  0.027964639632079433\n",
            "Loss S2:  0.03209937412662919\n",
            "Loss S01:  0.027969713969109744\n",
            "Loss S2:  0.032097278719641985\n",
            "Loss S01:  0.02796820507631269\n",
            "Loss S2:  0.03212295158980638\n",
            "Loss S01:  0.027963600433942092\n",
            "Loss S2:  0.032110568358751625\n",
            "Loss S01:  0.027939699290745513\n",
            "Loss S2:  0.03207064497844583\n",
            "Loss S01:  0.027933026782913\n",
            "Loss S2:  0.032073494731637175\n",
            "Loss S01:  0.027945419030652306\n",
            "Loss S2:  0.03208467185924961\n",
            "Loss S01:  0.027966208433694153\n",
            "Loss S2:  0.03209842781910449\n",
            "Loss S01:  0.027975548849443765\n",
            "Loss S2:  0.03213146991199917\n",
            "Loss S01:  0.02797694777724155\n",
            "Loss S2:  0.03213647292756638\n",
            "Loss S01:  0.02796151966981168\n",
            "Loss S2:  0.032120425379621696\n",
            "Loss S01:  0.02797202119637975\n",
            "Loss S2:  0.0321261205336475\n",
            "Loss S01:  0.027949921448555445\n",
            "Loss S2:  0.032110085553677795\n",
            "Loss S01:  0.02794884090448853\n",
            "Loss S2:  0.03211742338545602\n",
            "Loss S01:  0.027955178388931454\n",
            "Loss S2:  0.032133694158055774\n",
            "Loss S01:  0.02796692609556922\n",
            "Loss S2:  0.03215803779803234\n",
            "Loss S01:  0.02793947452994759\n",
            "Loss S2:  0.03213821709363477\n",
            "Loss S01:  0.027938499320256195\n",
            "Loss S2:  0.03213913671148607\n",
            "Loss S01:  0.02794034391674657\n",
            "Loss S2:  0.03213772445735408\n",
            "Loss S01:  0.027934271430115898\n",
            "Loss S2:  0.03214471022358039\n",
            "Loss S01:  0.027939658410325172\n",
            "Loss S2:  0.03214446889066519\n",
            "Loss S01:  0.027943797285616273\n",
            "Loss S2:  0.03216162766184728\n",
            "Loss S01:  0.027937272267677875\n",
            "Loss S2:  0.03215421635710659\n",
            "Validation: \n",
            " Loss S01:  0.025139112025499344\n",
            " Loss S2:  0.0464063435792923\n",
            " Loss S01:  0.027307721741852305\n",
            " Loss S2:  0.04887851008347103\n",
            " Loss S01:  0.027316049967960613\n",
            " Loss S2:  0.04884345302494561\n",
            " Loss S01:  0.0272040491404592\n",
            " Loss S2:  0.0484972778768813\n",
            " Loss S01:  0.027166556245014992\n",
            " Loss S2:  0.04831633574248832\n",
            "\n",
            "Epoch: 39\n",
            "Loss S01:  0.030529052019119263\n",
            "Loss S2:  0.03297819197177887\n",
            "Loss S01:  0.02762168578126214\n",
            "Loss S2:  0.03143114715137265\n",
            "Loss S01:  0.027556109109095166\n",
            "Loss S2:  0.03175030497922784\n",
            "Loss S01:  0.027610233594332973\n",
            "Loss S2:  0.03171940153885272\n",
            "Loss S01:  0.027755112864258812\n",
            "Loss S2:  0.0318588869328179\n",
            "Loss S01:  0.027824934900683516\n",
            "Loss S2:  0.03188741444518753\n",
            "Loss S01:  0.027645382511078335\n",
            "Loss S2:  0.031828524728046086\n",
            "Loss S01:  0.02757843591692582\n",
            "Loss S2:  0.03181994824447262\n",
            "Loss S01:  0.027489816631983827\n",
            "Loss S2:  0.03156703308132695\n",
            "Loss S01:  0.027411233879380173\n",
            "Loss S2:  0.03144616222234218\n",
            "Loss S01:  0.027379433430805063\n",
            "Loss S2:  0.03129543391711051\n",
            "Loss S01:  0.0273603291527645\n",
            "Loss S2:  0.031282990415756766\n",
            "Loss S01:  0.027362593380380267\n",
            "Loss S2:  0.0312750260845935\n",
            "Loss S01:  0.027387803775891093\n",
            "Loss S2:  0.03131780285701042\n",
            "Loss S01:  0.02740600545302773\n",
            "Loss S2:  0.03133103415264305\n",
            "Loss S01:  0.027439969338920733\n",
            "Loss S2:  0.031420024666959874\n",
            "Loss S01:  0.027418063849395848\n",
            "Loss S2:  0.03145565107603621\n",
            "Loss S01:  0.02748837193463281\n",
            "Loss S2:  0.03155323062418846\n",
            "Loss S01:  0.027531306161570943\n",
            "Loss S2:  0.031595051926637883\n",
            "Loss S01:  0.027522349905671248\n",
            "Loss S2:  0.031569864347343046\n",
            "Loss S01:  0.027532216939908354\n",
            "Loss S2:  0.03154842196894226\n",
            "Loss S01:  0.02756808637266193\n",
            "Loss S2:  0.03157694139010251\n",
            "Loss S01:  0.02757755893691363\n",
            "Loss S2:  0.031591603001453215\n",
            "Loss S01:  0.02760761361811068\n",
            "Loss S2:  0.03159952882951472\n",
            "Loss S01:  0.02766151975886703\n",
            "Loss S2:  0.031669598636970976\n",
            "Loss S01:  0.027665850613637275\n",
            "Loss S2:  0.03169093605323854\n",
            "Loss S01:  0.027687775370775512\n",
            "Loss S2:  0.03173934255808706\n",
            "Loss S01:  0.027632480452861295\n",
            "Loss S2:  0.031707776600419374\n",
            "Loss S01:  0.027617397264070358\n",
            "Loss S2:  0.03170426809676389\n",
            "Loss S01:  0.027627757510419973\n",
            "Loss S2:  0.031741621542427545\n",
            "Loss S01:  0.027637875512044295\n",
            "Loss S2:  0.03177470768309907\n",
            "Loss S01:  0.027617489847195878\n",
            "Loss S2:  0.03175321298013546\n",
            "Loss S01:  0.027608113206714115\n",
            "Loss S2:  0.031774013973424366\n",
            "Loss S01:  0.027615479050131002\n",
            "Loss S2:  0.031770794930955075\n",
            "Loss S01:  0.027632328351178476\n",
            "Loss S2:  0.03179345817968817\n",
            "Loss S01:  0.027647040821505747\n",
            "Loss S2:  0.03178529223252056\n",
            "Loss S01:  0.027647278356750255\n",
            "Loss S2:  0.031780097772911645\n",
            "Loss S01:  0.02763551574531752\n",
            "Loss S2:  0.03178541399940166\n",
            "Loss S01:  0.02764238100841133\n",
            "Loss S2:  0.03177736334874248\n",
            "Loss S01:  0.027623250546967588\n",
            "Loss S2:  0.03176059701558574\n",
            "Loss S01:  0.02763691454884269\n",
            "Loss S2:  0.03177346356510373\n",
            "Loss S01:  0.027640922035831603\n",
            "Loss S2:  0.03177110451543273\n",
            "Loss S01:  0.027652348088644745\n",
            "Loss S2:  0.031789652361687175\n",
            "Loss S01:  0.0276317316174438\n",
            "Loss S2:  0.03175684876670832\n",
            "Loss S01:  0.027620420413188924\n",
            "Loss S2:  0.03175360423485303\n",
            "Loss S01:  0.027623622449003408\n",
            "Loss S2:  0.03175988067313203\n",
            "Loss S01:  0.027623018573104948\n",
            "Loss S2:  0.03177224997018915\n",
            "Loss S01:  0.02763449461169683\n",
            "Loss S2:  0.031768746618695574\n",
            "Loss S01:  0.027634940476092877\n",
            "Loss S2:  0.031758767663038444\n",
            "Loss S01:  0.027625680189814927\n",
            "Loss S2:  0.031748175416179684\n",
            "Validation: \n",
            " Loss S01:  0.026271691545844078\n",
            " Loss S2:  0.044847987592220306\n",
            " Loss S01:  0.02882274967573938\n",
            " Loss S2:  0.048103573421637215\n",
            " Loss S01:  0.02867815498171783\n",
            " Loss S2:  0.047928567067152116\n",
            " Loss S01:  0.028603215045371992\n",
            " Loss S2:  0.047546991009692674\n",
            " Loss S01:  0.02865691680783107\n",
            " Loss S2:  0.04747693217646928\n",
            "\n",
            "Epoch: 40\n",
            "Loss S01:  0.03209163248538971\n",
            "Loss S2:  0.03491515666246414\n",
            "Loss S01:  0.027550864964723587\n",
            "Loss S2:  0.03126814080910249\n",
            "Loss S01:  0.02751999916065307\n",
            "Loss S2:  0.03128178701514289\n",
            "Loss S01:  0.027395462917704737\n",
            "Loss S2:  0.03140986993187858\n",
            "Loss S01:  0.027493381554760585\n",
            "Loss S2:  0.03154902132909473\n",
            "Loss S01:  0.02759506107837546\n",
            "Loss S2:  0.03158765344643125\n",
            "Loss S01:  0.027551446720713475\n",
            "Loss S2:  0.031522740319859785\n",
            "Loss S01:  0.027570350856428415\n",
            "Loss S2:  0.03153226047124661\n",
            "Loss S01:  0.02747794254510491\n",
            "Loss S2:  0.031390603769708564\n",
            "Loss S01:  0.027433851590523355\n",
            "Loss S2:  0.031339395873166705\n",
            "Loss S01:  0.027437259987144188\n",
            "Loss S2:  0.03126115466255953\n",
            "Loss S01:  0.027361826182485702\n",
            "Loss S2:  0.031229595326491305\n",
            "Loss S01:  0.027374641954406233\n",
            "Loss S2:  0.03124104748087481\n",
            "Loss S01:  0.027338889249749768\n",
            "Loss S2:  0.031249174677574907\n",
            "Loss S01:  0.027400030391224732\n",
            "Loss S2:  0.03129007416308349\n",
            "Loss S01:  0.027440419963357464\n",
            "Loss S2:  0.031332570714074255\n",
            "Loss S01:  0.027405217949370418\n",
            "Loss S2:  0.03137025499704832\n",
            "Loss S01:  0.0274641361109346\n",
            "Loss S2:  0.031511734016457495\n",
            "Loss S01:  0.02747573983438766\n",
            "Loss S2:  0.031509487966888516\n",
            "Loss S01:  0.027457888915900786\n",
            "Loss S2:  0.03153970501146704\n",
            "Loss S01:  0.027462927114904223\n",
            "Loss S2:  0.03149820784505327\n",
            "Loss S01:  0.027487477817312235\n",
            "Loss S2:  0.03148959954026469\n",
            "Loss S01:  0.0275124125519759\n",
            "Loss S2:  0.03148729183892319\n",
            "Loss S01:  0.027532376002246166\n",
            "Loss S2:  0.031490026606303276\n",
            "Loss S01:  0.027541531118490885\n",
            "Loss S2:  0.031508959141942475\n",
            "Loss S01:  0.027548580842307838\n",
            "Loss S2:  0.03153340183050034\n",
            "Loss S01:  0.027537351609760775\n",
            "Loss S2:  0.03154699986807688\n",
            "Loss S01:  0.027510410868923602\n",
            "Loss S2:  0.03154388117531789\n",
            "Loss S01:  0.027526222567117085\n",
            "Loss S2:  0.031549744009706476\n",
            "Loss S01:  0.02752480612382856\n",
            "Loss S2:  0.031564931804474276\n",
            "Loss S01:  0.0275278523737608\n",
            "Loss S2:  0.031556741220016414\n",
            "Loss S01:  0.027509567480522336\n",
            "Loss S2:  0.03153404803090157\n",
            "Loss S01:  0.027526665252642097\n",
            "Loss S2:  0.03155192085237146\n",
            "Loss S01:  0.027547316406248558\n",
            "Loss S2:  0.03156726105065504\n",
            "Loss S01:  0.02757245729352372\n",
            "Loss S2:  0.03158846431441845\n",
            "Loss S01:  0.02760118299675973\n",
            "Loss S2:  0.03161206792116675\n",
            "Loss S01:  0.02758769677059307\n",
            "Loss S2:  0.03161587421666222\n",
            "Loss S01:  0.02758266397744337\n",
            "Loss S2:  0.03159882339606227\n",
            "Loss S01:  0.027575915786889907\n",
            "Loss S2:  0.03157025022221988\n",
            "Loss S01:  0.027555800293145888\n",
            "Loss S2:  0.03154593030624377\n",
            "Loss S01:  0.027556316532250356\n",
            "Loss S2:  0.03155901300676743\n",
            "Loss S01:  0.02754620867815331\n",
            "Loss S2:  0.03156181544953744\n",
            "Loss S01:  0.027562437994923274\n",
            "Loss S2:  0.03156688697901588\n",
            "Loss S01:  0.027541872036152815\n",
            "Loss S2:  0.03152462967569895\n",
            "Loss S01:  0.027540846307531775\n",
            "Loss S2:  0.03152867896554151\n",
            "Loss S01:  0.027539444145665993\n",
            "Loss S2:  0.03153464286082203\n",
            "Loss S01:  0.027528090931782236\n",
            "Loss S2:  0.031533855581231854\n",
            "Loss S01:  0.027539316189105597\n",
            "Loss S2:  0.03153889188879108\n",
            "Loss S01:  0.02755630451589513\n",
            "Loss S2:  0.03154059657885106\n",
            "Loss S01:  0.02754388771981547\n",
            "Loss S2:  0.0315371564534434\n",
            "Validation: \n",
            " Loss S01:  0.025756381452083588\n",
            " Loss S2:  0.045732948929071426\n",
            " Loss S01:  0.027753805298180806\n",
            " Loss S2:  0.04888674865166346\n",
            " Loss S01:  0.027735176609783637\n",
            " Loss S2:  0.04884708736364434\n",
            " Loss S01:  0.027726232486425854\n",
            " Loss S2:  0.04839210993930942\n",
            " Loss S01:  0.0277303290311937\n",
            " Loss S2:  0.04827941760972694\n",
            "\n",
            "Epoch: 41\n",
            "Loss S01:  0.02866019681096077\n",
            "Loss S2:  0.03231212869286537\n",
            "Loss S01:  0.02728352780369195\n",
            "Loss S2:  0.0312614171681079\n",
            "Loss S01:  0.026960231718562898\n",
            "Loss S2:  0.03114818444564229\n",
            "Loss S01:  0.027148809764654405\n",
            "Loss S2:  0.031342442598073714\n",
            "Loss S01:  0.027290280953776544\n",
            "Loss S2:  0.03132957624407803\n",
            "Loss S01:  0.027268065833577923\n",
            "Loss S2:  0.031368591767900134\n",
            "Loss S01:  0.027206694462993106\n",
            "Loss S2:  0.03129571517471407\n",
            "Loss S01:  0.027223781407089302\n",
            "Loss S2:  0.031354682966017386\n",
            "Loss S01:  0.02709451531646428\n",
            "Loss S2:  0.031169175802741523\n",
            "Loss S01:  0.02706676084998545\n",
            "Loss S2:  0.031153309332964185\n",
            "Loss S01:  0.027044634141101694\n",
            "Loss S2:  0.031034288946354745\n",
            "Loss S01:  0.02703313537881718\n",
            "Loss S2:  0.031027661753935856\n",
            "Loss S01:  0.02704378642326544\n",
            "Loss S2:  0.031032342540700572\n",
            "Loss S01:  0.027059309198769903\n",
            "Loss S2:  0.031051084939532608\n",
            "Loss S01:  0.02710948012610699\n",
            "Loss S2:  0.03112128863097928\n",
            "Loss S01:  0.027163385284065412\n",
            "Loss S2:  0.031146203642649365\n",
            "Loss S01:  0.027148907471980368\n",
            "Loss S2:  0.031133452129567633\n",
            "Loss S01:  0.027223708075389527\n",
            "Loss S2:  0.03122485705722145\n",
            "Loss S01:  0.027230779439704852\n",
            "Loss S2:  0.031234336399488687\n",
            "Loss S01:  0.02720350445448104\n",
            "Loss S2:  0.03122915863912767\n",
            "Loss S01:  0.027186975744900418\n",
            "Loss S2:  0.031199574359317323\n",
            "Loss S01:  0.02721836931685701\n",
            "Loss S2:  0.031268038438253494\n",
            "Loss S01:  0.027268438528363522\n",
            "Loss S2:  0.03128825015982621\n",
            "Loss S01:  0.027315131150734372\n",
            "Loss S2:  0.031291854878266655\n",
            "Loss S01:  0.027348156995236628\n",
            "Loss S2:  0.03134483629953565\n",
            "Loss S01:  0.027376347870762603\n",
            "Loss S2:  0.031370746120216834\n",
            "Loss S01:  0.02739906834859501\n",
            "Loss S2:  0.03138316217881281\n",
            "Loss S01:  0.02733608271335544\n",
            "Loss S2:  0.03135144095386746\n",
            "Loss S01:  0.02732674762261932\n",
            "Loss S2:  0.03134711717795647\n",
            "Loss S01:  0.027320980820868843\n",
            "Loss S2:  0.031368649183833315\n",
            "Loss S01:  0.02731974336915832\n",
            "Loss S2:  0.031368627154153846\n",
            "Loss S01:  0.02729095370056545\n",
            "Loss S2:  0.03134389162973959\n",
            "Loss S01:  0.02729693482806935\n",
            "Loss S2:  0.031342641644852925\n",
            "Loss S01:  0.027293649538676183\n",
            "Loss S2:  0.03135209150184677\n",
            "Loss S01:  0.02730443038570741\n",
            "Loss S2:  0.031355197396664675\n",
            "Loss S01:  0.027320532733176507\n",
            "Loss S2:  0.03137763943045567\n",
            "Loss S01:  0.02731640823185444\n",
            "Loss S2:  0.03138451272018068\n",
            "Loss S01:  0.027283052620139085\n",
            "Loss S2:  0.031366470568584943\n",
            "Loss S01:  0.02728273561639892\n",
            "Loss S2:  0.03134378333260694\n",
            "Loss S01:  0.027261472285708504\n",
            "Loss S2:  0.031316374959733786\n",
            "Loss S01:  0.027268435968611008\n",
            "Loss S2:  0.03132120720075996\n",
            "Loss S01:  0.027260951380587553\n",
            "Loss S2:  0.03132412322046838\n",
            "Loss S01:  0.027284794874188452\n",
            "Loss S2:  0.031336022514380925\n",
            "Loss S01:  0.027280247828592556\n",
            "Loss S2:  0.03130434369646756\n",
            "Loss S01:  0.02728523382000507\n",
            "Loss S2:  0.03130723318494772\n",
            "Loss S01:  0.027292355097945937\n",
            "Loss S2:  0.031306771231100726\n",
            "Loss S01:  0.02728231796699853\n",
            "Loss S2:  0.031311830639063405\n",
            "Loss S01:  0.027289793721841146\n",
            "Loss S2:  0.031321280641825336\n",
            "Loss S01:  0.027289359785786785\n",
            "Loss S2:  0.03132714037618865\n",
            "Loss S01:  0.027272493018941336\n",
            "Loss S2:  0.03131157225813497\n",
            "Validation: \n",
            " Loss S01:  0.0251705814152956\n",
            " Loss S2:  0.04582803696393967\n",
            " Loss S01:  0.026947788007202603\n",
            " Loss S2:  0.048014199982086815\n",
            " Loss S01:  0.02696208755781011\n",
            " Loss S2:  0.04796333620097579\n",
            " Loss S01:  0.026958185141203832\n",
            " Loss S2:  0.047559753854255205\n",
            " Loss S01:  0.02697756398975113\n",
            " Loss S2:  0.04739108175775151\n",
            "\n",
            "Epoch: 42\n",
            "Loss S01:  0.030438456684350967\n",
            "Loss S2:  0.034184955060482025\n",
            "Loss S01:  0.027179525318470867\n",
            "Loss S2:  0.031151994385502556\n",
            "Loss S01:  0.026732340455055237\n",
            "Loss S2:  0.030816207684221723\n",
            "Loss S01:  0.02690874791193393\n",
            "Loss S2:  0.030786396154472904\n",
            "Loss S01:  0.02705258703449877\n",
            "Loss S2:  0.031040847982938696\n",
            "Loss S01:  0.02721808996855044\n",
            "Loss S2:  0.03121431924256624\n",
            "Loss S01:  0.027076429641637645\n",
            "Loss S2:  0.03119175853665735\n",
            "Loss S01:  0.027092388476914083\n",
            "Loss S2:  0.0312113972302054\n",
            "Loss S01:  0.02696741241271849\n",
            "Loss S2:  0.03113605675322038\n",
            "Loss S01:  0.02690158688186944\n",
            "Loss S2:  0.031093348165626054\n",
            "Loss S01:  0.026892929974168835\n",
            "Loss S2:  0.03098183794703224\n",
            "Loss S01:  0.026869123365293752\n",
            "Loss S2:  0.030898748066376994\n",
            "Loss S01:  0.026858944923055073\n",
            "Loss S2:  0.03090851693914449\n",
            "Loss S01:  0.02684745855117572\n",
            "Loss S2:  0.0309158097668231\n",
            "Loss S01:  0.026881151680722304\n",
            "Loss S2:  0.03093005913633404\n",
            "Loss S01:  0.026955422630767948\n",
            "Loss S2:  0.031042056972321295\n",
            "Loss S01:  0.026950744842612964\n",
            "Loss S2:  0.031071974866971466\n",
            "Loss S01:  0.027024026448179406\n",
            "Loss S2:  0.03113844155263134\n",
            "Loss S01:  0.027076638679313397\n",
            "Loss S2:  0.03119779000501277\n",
            "Loss S01:  0.02704638222242213\n",
            "Loss S2:  0.031207664981874494\n",
            "Loss S01:  0.027026595121518296\n",
            "Loss S2:  0.031187066530336194\n",
            "Loss S01:  0.027038886463402006\n",
            "Loss S2:  0.03120025823784383\n",
            "Loss S01:  0.027054807322934204\n",
            "Loss S2:  0.03121616780690478\n",
            "Loss S01:  0.027060880555541483\n",
            "Loss S2:  0.031201072359755957\n",
            "Loss S01:  0.027087369222673144\n",
            "Loss S2:  0.031241934398707018\n",
            "Loss S01:  0.027081188482354836\n",
            "Loss S2:  0.031264655277192355\n",
            "Loss S01:  0.027086744941342836\n",
            "Loss S2:  0.03125050520502973\n",
            "Loss S01:  0.027050401919222405\n",
            "Loss S2:  0.03123035788453593\n",
            "Loss S01:  0.027047219265292122\n",
            "Loss S2:  0.031203701336506847\n",
            "Loss S01:  0.02705938345992688\n",
            "Loss S2:  0.03121746012051286\n",
            "Loss S01:  0.027059677233331622\n",
            "Loss S2:  0.031220252740125718\n",
            "Loss S01:  0.027038979700428113\n",
            "Loss S2:  0.031224538694911447\n",
            "Loss S01:  0.027019563284804145\n",
            "Loss S2:  0.031213231959214834\n",
            "Loss S01:  0.02702261445610545\n",
            "Loss S2:  0.03122770049105598\n",
            "Loss S01:  0.027044687529375826\n",
            "Loss S2:  0.03124497935743975\n",
            "Loss S01:  0.02705599416970697\n",
            "Loss S2:  0.03126862993267527\n",
            "Loss S01:  0.02705810784550585\n",
            "Loss S2:  0.031274683211112285\n",
            "Loss S01:  0.027049546107408815\n",
            "Loss S2:  0.03126233035944543\n",
            "Loss S01:  0.02705220926034795\n",
            "Loss S2:  0.031243083656342638\n",
            "Loss S01:  0.027032961292416238\n",
            "Loss S2:  0.031227865373082173\n",
            "Loss S01:  0.027045730947333382\n",
            "Loss S2:  0.03122940469077995\n",
            "Loss S01:  0.027051386801829592\n",
            "Loss S2:  0.031225478616509126\n",
            "Loss S01:  0.027075608656907592\n",
            "Loss S2:  0.031244899272352387\n",
            "Loss S01:  0.027062347487237504\n",
            "Loss S2:  0.031217241811019363\n",
            "Loss S01:  0.02706753494230663\n",
            "Loss S2:  0.03121282746952002\n",
            "Loss S01:  0.027073540206494724\n",
            "Loss S2:  0.031226666045360712\n",
            "Loss S01:  0.02706229689565372\n",
            "Loss S2:  0.031225812267875724\n",
            "Loss S01:  0.02707142004003697\n",
            "Loss S2:  0.031214936859826625\n",
            "Loss S01:  0.02706327900027163\n",
            "Loss S2:  0.03121317813091615\n",
            "Loss S01:  0.027045106371132516\n",
            "Loss S2:  0.031201390325719617\n",
            "Validation: \n",
            " Loss S01:  0.02494632825255394\n",
            " Loss S2:  0.043560437858104706\n",
            " Loss S01:  0.027191542177682833\n",
            " Loss S2:  0.04712361221512159\n",
            " Loss S01:  0.027210921639712844\n",
            " Loss S2:  0.047248780091361305\n",
            " Loss S01:  0.027161832593503545\n",
            " Loss S2:  0.04677950358781658\n",
            " Loss S01:  0.02716834236074377\n",
            " Loss S2:  0.04663193713367721\n",
            "\n",
            "Epoch: 43\n",
            "Loss S01:  0.02799053117632866\n",
            "Loss S2:  0.030739014968276024\n",
            "Loss S01:  0.026473818516189403\n",
            "Loss S2:  0.030741596763784237\n",
            "Loss S01:  0.026688744358363607\n",
            "Loss S2:  0.031026535711827733\n",
            "Loss S01:  0.026707711419270884\n",
            "Loss S2:  0.03095257925170083\n",
            "Loss S01:  0.026722122029196924\n",
            "Loss S2:  0.031023115873700234\n",
            "Loss S01:  0.026885481295632382\n",
            "Loss S2:  0.031039355943600338\n",
            "Loss S01:  0.026861838294101544\n",
            "Loss S2:  0.03108829888896864\n",
            "Loss S01:  0.02691016138964136\n",
            "Loss S2:  0.031076880355536098\n",
            "Loss S01:  0.02684185837889895\n",
            "Loss S2:  0.030930557276731656\n",
            "Loss S01:  0.026770400784009104\n",
            "Loss S2:  0.030831927351735452\n",
            "Loss S01:  0.02675805692699286\n",
            "Loss S2:  0.03071889168776498\n",
            "Loss S01:  0.026750606929396722\n",
            "Loss S2:  0.030614711696634423\n",
            "Loss S01:  0.026774509078707577\n",
            "Loss S2:  0.030585690532341475\n",
            "Loss S01:  0.02672439569052849\n",
            "Loss S2:  0.03054198335486514\n",
            "Loss S01:  0.026748879449375977\n",
            "Loss S2:  0.03056765670049275\n",
            "Loss S01:  0.02679178687367613\n",
            "Loss S2:  0.030677582476510118\n",
            "Loss S01:  0.026768031906082023\n",
            "Loss S2:  0.03063922243551438\n",
            "Loss S01:  0.026816135413988293\n",
            "Loss S2:  0.03072779248768126\n",
            "Loss S01:  0.02683673796979762\n",
            "Loss S2:  0.030762727274585164\n",
            "Loss S01:  0.02680588817877295\n",
            "Loss S2:  0.030768208186195783\n",
            "Loss S01:  0.0267831531552533\n",
            "Loss S2:  0.030752981878557607\n",
            "Loss S01:  0.026788004563671152\n",
            "Loss S2:  0.0307878715149458\n",
            "Loss S01:  0.026831254245064377\n",
            "Loss S2:  0.030838525832980468\n",
            "Loss S01:  0.026862231950029666\n",
            "Loss S2:  0.030833221794593902\n",
            "Loss S01:  0.026917907846048167\n",
            "Loss S2:  0.030867287077597068\n",
            "Loss S01:  0.02691386287579261\n",
            "Loss S2:  0.030887617250956387\n",
            "Loss S01:  0.026898462446656262\n",
            "Loss S2:  0.03088814637470291\n",
            "Loss S01:  0.026879642942294862\n",
            "Loss S2:  0.030897514069817163\n",
            "Loss S01:  0.02687480975459479\n",
            "Loss S2:  0.03088013677993703\n",
            "Loss S01:  0.026878083515208203\n",
            "Loss S2:  0.030889554806949757\n",
            "Loss S01:  0.026887225587403654\n",
            "Loss S2:  0.03088893760552636\n",
            "Loss S01:  0.02686363802414232\n",
            "Loss S2:  0.03087245156072152\n",
            "Loss S01:  0.026865948885215034\n",
            "Loss S2:  0.030878171713506323\n",
            "Loss S01:  0.02685566291528166\n",
            "Loss S2:  0.030877148119164376\n",
            "Loss S01:  0.026880827003166824\n",
            "Loss S2:  0.030890712328801407\n",
            "Loss S01:  0.026893136720372062\n",
            "Loss S2:  0.0309186203622835\n",
            "Loss S01:  0.02688325945690398\n",
            "Loss S2:  0.030910478528186555\n",
            "Loss S01:  0.026869514439947843\n",
            "Loss S2:  0.03090528664342316\n",
            "Loss S01:  0.02687118203914541\n",
            "Loss S2:  0.030910081574766653\n",
            "Loss S01:  0.026848587671966503\n",
            "Loss S2:  0.03089233831314327\n",
            "Loss S01:  0.02685679107941594\n",
            "Loss S2:  0.03089617483986732\n",
            "Loss S01:  0.026867779443117534\n",
            "Loss S2:  0.03089147229699323\n",
            "Loss S01:  0.02688817297619482\n",
            "Loss S2:  0.03091817008079231\n",
            "Loss S01:  0.02686333635134791\n",
            "Loss S2:  0.030898360639222813\n",
            "Loss S01:  0.02686941794324624\n",
            "Loss S2:  0.030906557572198835\n",
            "Loss S01:  0.026881246309223568\n",
            "Loss S2:  0.03091094845662228\n",
            "Loss S01:  0.02686812314497804\n",
            "Loss S2:  0.03090697219049853\n",
            "Loss S01:  0.026888415854099956\n",
            "Loss S2:  0.030919149173957525\n",
            "Loss S01:  0.02688626933401439\n",
            "Loss S2:  0.030914037153434604\n",
            "Loss S01:  0.026854884676073817\n",
            "Loss S2:  0.030896091177422258\n",
            "Validation: \n",
            " Loss S01:  0.024764755740761757\n",
            " Loss S2:  0.04406985267996788\n",
            " Loss S01:  0.027237925767188982\n",
            " Loss S2:  0.04695119158852668\n",
            " Loss S01:  0.027256083924595904\n",
            " Loss S2:  0.047060803214951256\n",
            " Loss S01:  0.02716061341591546\n",
            " Loss S2:  0.04662218512814553\n",
            " Loss S01:  0.02720737392887657\n",
            " Loss S2:  0.04655339173328729\n",
            "\n",
            "Epoch: 44\n",
            "Loss S01:  0.031032275408506393\n",
            "Loss S2:  0.03509984165430069\n",
            "Loss S01:  0.026589028198610653\n",
            "Loss S2:  0.0310144131495194\n",
            "Loss S01:  0.02676029529954706\n",
            "Loss S2:  0.030895700589531942\n",
            "Loss S01:  0.026841214888038173\n",
            "Loss S2:  0.030766600081997532\n",
            "Loss S01:  0.026972806717200978\n",
            "Loss S2:  0.030894517762268463\n",
            "Loss S01:  0.026950857693365977\n",
            "Loss S2:  0.030877714100129464\n",
            "Loss S01:  0.026916722141084124\n",
            "Loss S2:  0.030865160626221876\n",
            "Loss S01:  0.026903651051328217\n",
            "Loss S2:  0.030758043357603987\n",
            "Loss S01:  0.026772657800235865\n",
            "Loss S2:  0.03058095653483897\n",
            "Loss S01:  0.026740969660190437\n",
            "Loss S2:  0.030500085983466315\n",
            "Loss S01:  0.02676230646891169\n",
            "Loss S2:  0.030491479976785064\n",
            "Loss S01:  0.026647640358623083\n",
            "Loss S2:  0.030387964989016723\n",
            "Loss S01:  0.026607903228564697\n",
            "Loss S2:  0.030391575660833642\n",
            "Loss S01:  0.026601237678573333\n",
            "Loss S2:  0.03040713852427843\n",
            "Loss S01:  0.026597614869052637\n",
            "Loss S2:  0.030432547436009906\n",
            "Loss S01:  0.026631485695475774\n",
            "Loss S2:  0.030474042347231448\n",
            "Loss S01:  0.026594035443126784\n",
            "Loss S2:  0.03047497494157797\n",
            "Loss S01:  0.026641476932063438\n",
            "Loss S2:  0.030567895228925505\n",
            "Loss S01:  0.02665349545770258\n",
            "Loss S2:  0.03058430567918891\n",
            "Loss S01:  0.02662350821518461\n",
            "Loss S2:  0.03061932084487059\n",
            "Loss S01:  0.02660640227186739\n",
            "Loss S2:  0.030585505800386565\n",
            "Loss S01:  0.026623504324617544\n",
            "Loss S2:  0.030580785663983832\n",
            "Loss S01:  0.026629508022067233\n",
            "Loss S2:  0.030580193478597237\n",
            "Loss S01:  0.0266511497043428\n",
            "Loss S2:  0.030567816446547384\n",
            "Loss S01:  0.02667967432576591\n",
            "Loss S2:  0.03060651000944658\n",
            "Loss S01:  0.02667306125371817\n",
            "Loss S2:  0.030615547278606558\n",
            "Loss S01:  0.026694852402041242\n",
            "Loss S2:  0.030639576208260324\n",
            "Loss S01:  0.02665126367001296\n",
            "Loss S2:  0.03064860261261903\n",
            "Loss S01:  0.026663799753604833\n",
            "Loss S2:  0.03065095548236285\n",
            "Loss S01:  0.02666150622991557\n",
            "Loss S2:  0.03066435933778786\n",
            "Loss S01:  0.02667360233631839\n",
            "Loss S2:  0.030679857444971106\n",
            "Loss S01:  0.026650236350547078\n",
            "Loss S2:  0.030661612581401775\n",
            "Loss S01:  0.026649126109907932\n",
            "Loss S2:  0.030675733084804917\n",
            "Loss S01:  0.026655964699666666\n",
            "Loss S2:  0.030695309423562984\n",
            "Loss S01:  0.026668060609191396\n",
            "Loss S2:  0.030703381512346854\n",
            "Loss S01:  0.026691618532027273\n",
            "Loss S2:  0.030723430635540235\n",
            "Loss S01:  0.02670811923669646\n",
            "Loss S2:  0.030734980489202153\n",
            "Loss S01:  0.02670338198120864\n",
            "Loss S2:  0.030728637122842822\n",
            "Loss S01:  0.026710689126506565\n",
            "Loss S2:  0.030714116169242408\n",
            "Loss S01:  0.02670328400533675\n",
            "Loss S2:  0.030697784060255036\n",
            "Loss S01:  0.0267013892800805\n",
            "Loss S2:  0.030709783660429077\n",
            "Loss S01:  0.02670859757566104\n",
            "Loss S2:  0.03070239563202916\n",
            "Loss S01:  0.026730244585496513\n",
            "Loss S2:  0.030701840451239408\n",
            "Loss S01:  0.026729717383943412\n",
            "Loss S2:  0.030683996030529247\n",
            "Loss S01:  0.026731155466398145\n",
            "Loss S2:  0.03069419459843176\n",
            "Loss S01:  0.02673714164801421\n",
            "Loss S2:  0.03069486843624427\n",
            "Loss S01:  0.02673602731264077\n",
            "Loss S2:  0.03069599663309446\n",
            "Loss S01:  0.02674349175564423\n",
            "Loss S2:  0.030691935924018265\n",
            "Loss S01:  0.026739763978891957\n",
            "Loss S2:  0.030695821296059665\n",
            "Loss S01:  0.026725069454694717\n",
            "Loss S2:  0.030672600735623824\n",
            "Validation: \n",
            " Loss S01:  0.02489147149026394\n",
            " Loss S2:  0.043747663497924805\n",
            " Loss S01:  0.02690931232202621\n",
            " Loss S2:  0.04771999447118668\n",
            " Loss S01:  0.026950062002714086\n",
            " Loss S2:  0.04774860465308515\n",
            " Loss S01:  0.026877099281696022\n",
            " Loss S2:  0.047312924607855376\n",
            " Loss S01:  0.026901713445966625\n",
            " Loss S2:  0.04716776504560753\n",
            "\n",
            "Epoch: 45\n",
            "Loss S01:  0.03060460090637207\n",
            "Loss S2:  0.03427843004465103\n",
            "Loss S01:  0.026455856521021236\n",
            "Loss S2:  0.030826355415311726\n",
            "Loss S01:  0.02654737030111608\n",
            "Loss S2:  0.030775442719459534\n",
            "Loss S01:  0.02652377262711525\n",
            "Loss S2:  0.030789250507950783\n",
            "Loss S01:  0.026648991127930035\n",
            "Loss S2:  0.030971953281905593\n",
            "Loss S01:  0.02671087551496777\n",
            "Loss S2:  0.03094824619006877\n",
            "Loss S01:  0.02662507227820451\n",
            "Loss S2:  0.030816763547844576\n",
            "Loss S01:  0.026614380685586324\n",
            "Loss S2:  0.030834151405683706\n",
            "Loss S01:  0.026483866006687836\n",
            "Loss S2:  0.030540212115019928\n",
            "Loss S01:  0.02648856630037119\n",
            "Loss S2:  0.0305426804205546\n",
            "Loss S01:  0.02649356002485988\n",
            "Loss S2:  0.03048702176328343\n",
            "Loss S01:  0.026458322431321617\n",
            "Loss S2:  0.030397805846757716\n",
            "Loss S01:  0.0264211735295609\n",
            "Loss S2:  0.03040549151353107\n",
            "Loss S01:  0.026433471462312546\n",
            "Loss S2:  0.030409068452151678\n",
            "Loss S01:  0.026494402068515197\n",
            "Loss S2:  0.030424491297585746\n",
            "Loss S01:  0.026559127482357402\n",
            "Loss S2:  0.030455932995637522\n",
            "Loss S01:  0.02651119948312733\n",
            "Loss S2:  0.0304132113869516\n",
            "Loss S01:  0.02657192568594252\n",
            "Loss S2:  0.030510866012402445\n",
            "Loss S01:  0.02656136940938333\n",
            "Loss S2:  0.030537439746915966\n",
            "Loss S01:  0.026519348015959975\n",
            "Loss S2:  0.030519414566340247\n",
            "Loss S01:  0.026517142082995444\n",
            "Loss S2:  0.030527058822005543\n",
            "Loss S01:  0.026512636242531487\n",
            "Loss S2:  0.030517453636744577\n",
            "Loss S01:  0.0265305740379505\n",
            "Loss S2:  0.03050378516181562\n",
            "Loss S01:  0.026529069893158876\n",
            "Loss S2:  0.03047461671456372\n",
            "Loss S01:  0.026573087200831576\n",
            "Loss S2:  0.030521143544697166\n",
            "Loss S01:  0.026581828946255118\n",
            "Loss S2:  0.030533493396295495\n",
            "Loss S01:  0.026600862178823043\n",
            "Loss S2:  0.030545926556505006\n",
            "Loss S01:  0.026572837339666058\n",
            "Loss S2:  0.030509881379694518\n",
            "Loss S01:  0.026579459976683307\n",
            "Loss S2:  0.03049023121303723\n",
            "Loss S01:  0.026586104862878414\n",
            "Loss S2:  0.030512477274459254\n",
            "Loss S01:  0.026590321293453442\n",
            "Loss S2:  0.030517580618840516\n",
            "Loss S01:  0.026546541373829367\n",
            "Loss S2:  0.030498913743823672\n",
            "Loss S01:  0.026545777394671306\n",
            "Loss S2:  0.03050016115689389\n",
            "Loss S01:  0.026554515309564295\n",
            "Loss S2:  0.030515911350212428\n",
            "Loss S01:  0.026577518919695856\n",
            "Loss S2:  0.03053584175397399\n",
            "Loss S01:  0.026587030916684372\n",
            "Loss S2:  0.030555788751298885\n",
            "Loss S01:  0.026593243347228068\n",
            "Loss S2:  0.030564084308546995\n",
            "Loss S01:  0.026578846957404017\n",
            "Loss S2:  0.030554352754932852\n",
            "Loss S01:  0.026588132252960694\n",
            "Loss S2:  0.03053724596135103\n",
            "Loss S01:  0.02655996487992804\n",
            "Loss S2:  0.030516258691011183\n",
            "Loss S01:  0.026571254670954406\n",
            "Loss S2:  0.03053010956493399\n",
            "Loss S01:  0.026574502130313222\n",
            "Loss S2:  0.0305216498718282\n",
            "Loss S01:  0.02658296664048544\n",
            "Loss S2:  0.030521920579098465\n",
            "Loss S01:  0.026557725048224224\n",
            "Loss S2:  0.03050919418631601\n",
            "Loss S01:  0.02655079105526817\n",
            "Loss S2:  0.030508513412440444\n",
            "Loss S01:  0.026544517364575963\n",
            "Loss S2:  0.03049605967943658\n",
            "Loss S01:  0.026532047563545616\n",
            "Loss S2:  0.03047997701646449\n",
            "Loss S01:  0.026540057721790994\n",
            "Loss S2:  0.030478414824339235\n",
            "Loss S01:  0.026542398780429438\n",
            "Loss S2:  0.03048887035253762\n",
            "Loss S01:  0.026524135987075186\n",
            "Loss S2:  0.03047102128293995\n",
            "Validation: \n",
            " Loss S01:  0.02517780289053917\n",
            " Loss S2:  0.04591025784611702\n",
            " Loss S01:  0.02744086574585665\n",
            " Loss S2:  0.048736191753830226\n",
            " Loss S01:  0.027416677554933037\n",
            " Loss S2:  0.04866822690861981\n",
            " Loss S01:  0.027404548172823718\n",
            " Loss S2:  0.04823469230141796\n",
            " Loss S01:  0.027446845293412975\n",
            " Loss S2:  0.04812049139061092\n",
            "\n",
            "Epoch: 46\n",
            "Loss S01:  0.030720414593815804\n",
            "Loss S2:  0.03283348307013512\n",
            "Loss S01:  0.026669274168935688\n",
            "Loss S2:  0.03043586997823282\n",
            "Loss S01:  0.026510721870831082\n",
            "Loss S2:  0.03041370851652963\n",
            "Loss S01:  0.026425885997952953\n",
            "Loss S2:  0.030346892774105072\n",
            "Loss S01:  0.026563526381079745\n",
            "Loss S2:  0.03043437422048755\n",
            "Loss S01:  0.026758907253251356\n",
            "Loss S2:  0.03038952331624779\n",
            "Loss S01:  0.026641810954105657\n",
            "Loss S2:  0.030432545870053965\n",
            "Loss S01:  0.026582069668761442\n",
            "Loss S2:  0.03042374037101235\n",
            "Loss S01:  0.026424301857197727\n",
            "Loss S2:  0.03023273900243235\n",
            "Loss S01:  0.02637759299314284\n",
            "Loss S2:  0.030186186759517744\n",
            "Loss S01:  0.02637514652739657\n",
            "Loss S2:  0.030189427722356107\n",
            "Loss S01:  0.026358026021101454\n",
            "Loss S2:  0.030162361660250672\n",
            "Loss S01:  0.0263101750723094\n",
            "Loss S2:  0.030131399462168865\n",
            "Loss S01:  0.02632435977117706\n",
            "Loss S2:  0.03015431731882441\n",
            "Loss S01:  0.0263536514595468\n",
            "Loss S2:  0.03015661806343718\n",
            "Loss S01:  0.026394734405819943\n",
            "Loss S2:  0.03022245571806731\n",
            "Loss S01:  0.026342420485911903\n",
            "Loss S2:  0.03024538731352883\n",
            "Loss S01:  0.026385784759158978\n",
            "Loss S2:  0.03031471232224626\n",
            "Loss S01:  0.026399210661402724\n",
            "Loss S2:  0.03035419240840891\n",
            "Loss S01:  0.026367156819991418\n",
            "Loss S2:  0.030342223415278015\n",
            "Loss S01:  0.02633292558811494\n",
            "Loss S2:  0.030317083232809062\n",
            "Loss S01:  0.026364751407320464\n",
            "Loss S2:  0.030316361477730962\n",
            "Loss S01:  0.02641410391558619\n",
            "Loss S2:  0.030347294287916222\n",
            "Loss S01:  0.02644537056260037\n",
            "Loss S2:  0.030332788978239676\n",
            "Loss S01:  0.026477102932222652\n",
            "Loss S2:  0.030372141130052165\n",
            "Loss S01:  0.026484189985460968\n",
            "Loss S2:  0.030372956537630454\n",
            "Loss S01:  0.02649291634730909\n",
            "Loss S2:  0.03036588262695234\n",
            "Loss S01:  0.026475988239104898\n",
            "Loss S2:  0.03034084662107744\n",
            "Loss S01:  0.02647475857831193\n",
            "Loss S2:  0.030320560228198873\n",
            "Loss S01:  0.026478892601898445\n",
            "Loss S2:  0.030327615171312466\n",
            "Loss S01:  0.02649165644771435\n",
            "Loss S2:  0.03035617065909891\n",
            "Loss S01:  0.026470866371724768\n",
            "Loss S2:  0.030345624797884673\n",
            "Loss S01:  0.0264738309849924\n",
            "Loss S2:  0.030380660853699733\n",
            "Loss S01:  0.02648431883725335\n",
            "Loss S2:  0.030402482219963275\n",
            "Loss S01:  0.02650076655610915\n",
            "Loss S2:  0.030419319879012374\n",
            "Loss S01:  0.026518750665152174\n",
            "Loss S2:  0.030420868018074254\n",
            "Loss S01:  0.026525856581338554\n",
            "Loss S2:  0.03041669522564332\n",
            "Loss S01:  0.026517677314157755\n",
            "Loss S2:  0.030402330455754325\n",
            "Loss S01:  0.026518106392127636\n",
            "Loss S2:  0.030399280602694184\n",
            "Loss S01:  0.02649482234340647\n",
            "Loss S2:  0.030382588667714077\n",
            "Loss S01:  0.026493378331945126\n",
            "Loss S2:  0.030392407042501575\n",
            "Loss S01:  0.026485333690496837\n",
            "Loss S2:  0.03040323251010873\n",
            "Loss S01:  0.026496209235167278\n",
            "Loss S2:  0.030403760856906763\n",
            "Loss S01:  0.02647583823642432\n",
            "Loss S2:  0.030379595507256118\n",
            "Loss S01:  0.026483458194400178\n",
            "Loss S2:  0.030389010327574618\n",
            "Loss S01:  0.02648363936023411\n",
            "Loss S2:  0.030388943342999977\n",
            "Loss S01:  0.026468553129834104\n",
            "Loss S2:  0.030381410299858923\n",
            "Loss S01:  0.0264757179878573\n",
            "Loss S2:  0.030388392994453194\n",
            "Loss S01:  0.0264731369595084\n",
            "Loss S2:  0.030390035012531676\n",
            "Loss S01:  0.026461245025491034\n",
            "Loss S2:  0.030379668329586322\n",
            "Validation: \n",
            " Loss S01:  0.02445909008383751\n",
            " Loss S2:  0.04250138998031616\n",
            " Loss S01:  0.026870892604901678\n",
            " Loss S2:  0.04664261443983941\n",
            " Loss S01:  0.02691212573611155\n",
            " Loss S2:  0.046712031633388704\n",
            " Loss S01:  0.026857936296795237\n",
            " Loss S2:  0.046252939850091934\n",
            " Loss S01:  0.02689017592296924\n",
            " Loss S2:  0.0462249331176281\n",
            "\n",
            "Epoch: 47\n",
            "Loss S01:  0.02858811244368553\n",
            "Loss S2:  0.0312177836894989\n",
            "Loss S01:  0.026428411968729713\n",
            "Loss S2:  0.029565034603530712\n",
            "Loss S01:  0.026493626602348826\n",
            "Loss S2:  0.029903880630930264\n",
            "Loss S01:  0.02638433206706278\n",
            "Loss S2:  0.030110979032131932\n",
            "Loss S01:  0.026535117090111825\n",
            "Loss S2:  0.03015120491022017\n",
            "Loss S01:  0.026644439308666716\n",
            "Loss S2:  0.030211093196389722\n",
            "Loss S01:  0.026508753905530837\n",
            "Loss S2:  0.03013654114281545\n",
            "Loss S01:  0.0264492794136766\n",
            "Loss S2:  0.030086428845222567\n",
            "Loss S01:  0.02628065877950486\n",
            "Loss S2:  0.02989772714122578\n",
            "Loss S01:  0.026218186572685348\n",
            "Loss S2:  0.02988135163764377\n",
            "Loss S01:  0.026232122213091\n",
            "Loss S2:  0.02980598871882009\n",
            "Loss S01:  0.026222407213739446\n",
            "Loss S2:  0.02978993628408995\n",
            "Loss S01:  0.02619183319043522\n",
            "Loss S2:  0.029776578969206692\n",
            "Loss S01:  0.026146887026670326\n",
            "Loss S2:  0.02981623054574464\n",
            "Loss S01:  0.026160774963861662\n",
            "Loss S2:  0.029882155675837335\n",
            "Loss S01:  0.02616889699553417\n",
            "Loss S2:  0.02994607999575454\n",
            "Loss S01:  0.026151425708432374\n",
            "Loss S2:  0.029994755879229642\n",
            "Loss S01:  0.02622697966402037\n",
            "Loss S2:  0.03008849529974293\n",
            "Loss S01:  0.026223056131551938\n",
            "Loss S2:  0.030117382012431134\n",
            "Loss S01:  0.026175090516502946\n",
            "Loss S2:  0.030120650608658168\n",
            "Loss S01:  0.026165289118589452\n",
            "Loss S2:  0.030108607452900255\n",
            "Loss S01:  0.026160122500070465\n",
            "Loss S2:  0.030108618519105616\n",
            "Loss S01:  0.026169195653221726\n",
            "Loss S2:  0.030113689989121253\n",
            "Loss S01:  0.026188985956631182\n",
            "Loss S2:  0.030121628420822548\n",
            "Loss S01:  0.02621858801955504\n",
            "Loss S2:  0.030127671883437644\n",
            "Loss S01:  0.026230374863185732\n",
            "Loss S2:  0.03014035852425602\n",
            "Loss S01:  0.02623006213327934\n",
            "Loss S2:  0.03017358469038174\n",
            "Loss S01:  0.026218449614419707\n",
            "Loss S2:  0.030138419762363734\n",
            "Loss S01:  0.02621389345474727\n",
            "Loss S2:  0.03011571352114881\n",
            "Loss S01:  0.026208791907403068\n",
            "Loss S2:  0.030124154844388518\n",
            "Loss S01:  0.02621641295321558\n",
            "Loss S2:  0.030133280606067854\n",
            "Loss S01:  0.026195532669256356\n",
            "Loss S2:  0.030110811341517992\n",
            "Loss S01:  0.026196277088186823\n",
            "Loss S2:  0.03013391856603162\n",
            "Loss S01:  0.026210715442760834\n",
            "Loss S2:  0.03015563084977451\n",
            "Loss S01:  0.026234506517430204\n",
            "Loss S2:  0.03019076697200624\n",
            "Loss S01:  0.02624003771703128\n",
            "Loss S2:  0.0301953580421515\n",
            "Loss S01:  0.02623668960566501\n",
            "Loss S2:  0.03019261468005808\n",
            "Loss S01:  0.026226489591309323\n",
            "Loss S2:  0.030179294750014084\n",
            "Loss S01:  0.026242544556774806\n",
            "Loss S2:  0.030182332685339482\n",
            "Loss S01:  0.026230880704796528\n",
            "Loss S2:  0.0301668503800469\n",
            "Loss S01:  0.026230498637418796\n",
            "Loss S2:  0.030173912314591263\n",
            "Loss S01:  0.026227626908760873\n",
            "Loss S2:  0.030168157138854918\n",
            "Loss S01:  0.02622849721629659\n",
            "Loss S2:  0.030174302680510807\n",
            "Loss S01:  0.02621903819336692\n",
            "Loss S2:  0.030155318376463416\n",
            "Loss S01:  0.026217606826738165\n",
            "Loss S2:  0.03015380634877687\n",
            "Loss S01:  0.026221016372353696\n",
            "Loss S2:  0.030152244063841795\n",
            "Loss S01:  0.026212036488436306\n",
            "Loss S2:  0.030147806464526763\n",
            "Loss S01:  0.026218928724938643\n",
            "Loss S2:  0.03015277987136456\n",
            "Loss S01:  0.026229443315130014\n",
            "Loss S2:  0.03014312153624026\n",
            "Loss S01:  0.02621197159157387\n",
            "Loss S2:  0.030127489848286216\n",
            "Validation: \n",
            " Loss S01:  0.023979181423783302\n",
            " Loss S2:  0.043579429388046265\n",
            " Loss S01:  0.026625766019736017\n",
            " Loss S2:  0.047747968030827384\n",
            " Loss S01:  0.02651562732530803\n",
            " Loss S2:  0.04774146917753103\n",
            " Loss S01:  0.02642531894513818\n",
            " Loss S2:  0.04715530281184149\n",
            " Loss S01:  0.026443548385559776\n",
            " Loss S2:  0.04705863317221771\n",
            "\n",
            "Epoch: 48\n",
            "Loss S01:  0.02909008041024208\n",
            "Loss S2:  0.03073212318122387\n",
            "Loss S01:  0.025796046819199215\n",
            "Loss S2:  0.030253693800080906\n",
            "Loss S01:  0.025924710379469963\n",
            "Loss S2:  0.03042043417337395\n",
            "Loss S01:  0.02593736933364022\n",
            "Loss S2:  0.03016458854319588\n",
            "Loss S01:  0.026049814482287662\n",
            "Loss S2:  0.03012753023606975\n",
            "Loss S01:  0.026058302790510888\n",
            "Loss S2:  0.030021514232252158\n",
            "Loss S01:  0.025977743606342644\n",
            "Loss S2:  0.030009074838923625\n",
            "Loss S01:  0.026045502097883696\n",
            "Loss S2:  0.030031621534849555\n",
            "Loss S01:  0.025884509086608887\n",
            "Loss S2:  0.029850274232434637\n",
            "Loss S01:  0.025806576095439574\n",
            "Loss S2:  0.02979393741422957\n",
            "Loss S01:  0.025798295707543297\n",
            "Loss S2:  0.029728346679470326\n",
            "Loss S01:  0.025809874288267916\n",
            "Loss S2:  0.029681696514557075\n",
            "Loss S01:  0.02581951776442449\n",
            "Loss S2:  0.029653040753785245\n",
            "Loss S01:  0.025816101083204947\n",
            "Loss S2:  0.029673945286688003\n",
            "Loss S01:  0.025843414088301625\n",
            "Loss S2:  0.029705565369932366\n",
            "Loss S01:  0.02586943388527987\n",
            "Loss S2:  0.029780584665027674\n",
            "Loss S01:  0.02583922940936888\n",
            "Loss S2:  0.02978539701713168\n",
            "Loss S01:  0.02590036238625384\n",
            "Loss S2:  0.029879405756878575\n",
            "Loss S01:  0.025916894815491708\n",
            "Loss S2:  0.02992132255673738\n",
            "Loss S01:  0.02589126743610305\n",
            "Loss S2:  0.029889742448847956\n",
            "Loss S01:  0.02587732052736318\n",
            "Loss S2:  0.029870046543615376\n",
            "Loss S01:  0.025899759648253\n",
            "Loss S2:  0.029862936921594266\n",
            "Loss S01:  0.02592479925585818\n",
            "Loss S2:  0.029861937331802705\n",
            "Loss S01:  0.02593373175855581\n",
            "Loss S2:  0.029847687118368233\n",
            "Loss S01:  0.025972146888677015\n",
            "Loss S2:  0.029899200262246784\n",
            "Loss S01:  0.026000268111725254\n",
            "Loss S2:  0.02993013937605092\n",
            "Loss S01:  0.026016262135115164\n",
            "Loss S2:  0.029937730238583808\n",
            "Loss S01:  0.0260041428736765\n",
            "Loss S2:  0.02994103233477726\n",
            "Loss S01:  0.026002875095054348\n",
            "Loss S2:  0.02992198166845108\n",
            "Loss S01:  0.025999494191399963\n",
            "Loss S2:  0.02993615775583536\n",
            "Loss S01:  0.02601023659456608\n",
            "Loss S2:  0.029962246346463793\n",
            "Loss S01:  0.02599666326926644\n",
            "Loss S2:  0.02994106715347414\n",
            "Loss S01:  0.02600144739393319\n",
            "Loss S2:  0.029945172021322162\n",
            "Loss S01:  0.02602576973687666\n",
            "Loss S2:  0.02997029776434524\n",
            "Loss S01:  0.02604889125178112\n",
            "Loss S2:  0.029997317470020213\n",
            "Loss S01:  0.02605726566706967\n",
            "Loss S2:  0.030013752687308524\n",
            "Loss S01:  0.026040097105205884\n",
            "Loss S2:  0.030002024561331872\n",
            "Loss S01:  0.026027684147668012\n",
            "Loss S2:  0.029990778658589262\n",
            "Loss S01:  0.02602483131226123\n",
            "Loss S2:  0.029986400553208635\n",
            "Loss S01:  0.0260066079866627\n",
            "Loss S2:  0.029966351807193682\n",
            "Loss S01:  0.026024429540979\n",
            "Loss S2:  0.029989419272750097\n",
            "Loss S01:  0.026029620278817025\n",
            "Loss S2:  0.029984322147684084\n",
            "Loss S01:  0.02605524570670094\n",
            "Loss S2:  0.030005024834547075\n",
            "Loss S01:  0.026042460375346608\n",
            "Loss S2:  0.029973620332760215\n",
            "Loss S01:  0.02603810898486584\n",
            "Loss S2:  0.029978416104063967\n",
            "Loss S01:  0.026043481691151924\n",
            "Loss S2:  0.029967044213757812\n",
            "Loss S01:  0.02602405533792528\n",
            "Loss S2:  0.02995621012584377\n",
            "Loss S01:  0.026029600048014565\n",
            "Loss S2:  0.029954599521089257\n",
            "Loss S01:  0.02603550822611534\n",
            "Loss S2:  0.02996827847481146\n",
            "Loss S01:  0.026021144411653704\n",
            "Loss S2:  0.029959281538508818\n",
            "Validation: \n",
            " Loss S01:  0.024624161422252655\n",
            " Loss S2:  0.042699184268713\n",
            " Loss S01:  0.026923083478496188\n",
            " Loss S2:  0.0460330985841297\n",
            " Loss S01:  0.026861024929619417\n",
            " Loss S2:  0.04608769542196902\n",
            " Loss S01:  0.02682121858367177\n",
            " Loss S2:  0.045558494500449444\n",
            " Loss S01:  0.026860251585826462\n",
            " Loss S2:  0.04542858820454574\n",
            "\n",
            "Epoch: 49\n",
            "Loss S01:  0.028175391256809235\n",
            "Loss S2:  0.03266109526157379\n",
            "Loss S01:  0.026001108302311463\n",
            "Loss S2:  0.03007706000723622\n",
            "Loss S01:  0.02601846405083225\n",
            "Loss S2:  0.029947786104111446\n",
            "Loss S01:  0.02604292080767693\n",
            "Loss S2:  0.02989269738956805\n",
            "Loss S01:  0.026142317438270987\n",
            "Loss S2:  0.03005384608376317\n",
            "Loss S01:  0.026140945251373685\n",
            "Loss S2:  0.030074603052115907\n",
            "Loss S01:  0.02595901797662993\n",
            "Loss S2:  0.030071665822971064\n",
            "Loss S01:  0.025988312306958184\n",
            "Loss S2:  0.030079989890817185\n",
            "Loss S01:  0.02593781166698462\n",
            "Loss S2:  0.029911761367578567\n",
            "Loss S01:  0.02589819752253019\n",
            "Loss S2:  0.029848825473051805\n",
            "Loss S01:  0.02591713152752064\n",
            "Loss S2:  0.029781780565286627\n",
            "Loss S01:  0.025915342508941085\n",
            "Loss S2:  0.029753281659371144\n",
            "Loss S01:  0.0258885502722884\n",
            "Loss S2:  0.029737133009374635\n",
            "Loss S01:  0.025839718030268\n",
            "Loss S2:  0.029761699035422494\n",
            "Loss S01:  0.025904973836760995\n",
            "Loss S2:  0.02981491522955979\n",
            "Loss S01:  0.02592162306833741\n",
            "Loss S2:  0.02980974849486193\n",
            "Loss S01:  0.025903328350797204\n",
            "Loss S2:  0.02976081160516102\n",
            "Loss S01:  0.025970341430770025\n",
            "Loss S2:  0.029797767795491637\n",
            "Loss S01:  0.025988886225454057\n",
            "Loss S2:  0.029795289924625535\n",
            "Loss S01:  0.025944243941007486\n",
            "Loss S2:  0.029772388617171668\n",
            "Loss S01:  0.02592236071417284\n",
            "Loss S2:  0.029745901040547522\n",
            "Loss S01:  0.02592836668211702\n",
            "Loss S2:  0.029740621402012227\n",
            "Loss S01:  0.025959072558723425\n",
            "Loss S2:  0.029750526254802807\n",
            "Loss S01:  0.02598048374863156\n",
            "Loss S2:  0.029722008588058606\n",
            "Loss S01:  0.025983397087615556\n",
            "Loss S2:  0.029753774334966395\n",
            "Loss S01:  0.025985866028117945\n",
            "Loss S2:  0.029769575416270004\n",
            "Loss S01:  0.025997555275872292\n",
            "Loss S2:  0.02980521935755494\n",
            "Loss S01:  0.025971486250357875\n",
            "Loss S2:  0.029784376894525937\n",
            "Loss S01:  0.025981501440825835\n",
            "Loss S2:  0.029761659949222495\n",
            "Loss S01:  0.025974093965187517\n",
            "Loss S2:  0.02977155987493361\n",
            "Loss S01:  0.025981567725786733\n",
            "Loss S2:  0.029760376398646555\n",
            "Loss S01:  0.025955657016521864\n",
            "Loss S2:  0.029743107736493996\n",
            "Loss S01:  0.025953145732528694\n",
            "Loss S2:  0.02975927228891404\n",
            "Loss S01:  0.025958249615533474\n",
            "Loss S2:  0.029781528008578407\n",
            "Loss S01:  0.025965682626906728\n",
            "Loss S2:  0.029814525149609687\n",
            "Loss S01:  0.025975650521340193\n",
            "Loss S2:  0.029849329289377925\n",
            "Loss S01:  0.025975067983703932\n",
            "Loss S2:  0.029864708998774556\n",
            "Loss S01:  0.025966031752988655\n",
            "Loss S2:  0.029861175322106907\n",
            "Loss S01:  0.025971134185908348\n",
            "Loss S2:  0.02985348279179707\n",
            "Loss S01:  0.0259508146949665\n",
            "Loss S2:  0.029825846371633928\n",
            "Loss S01:  0.02595827180864359\n",
            "Loss S2:  0.029841048076302928\n",
            "Loss S01:  0.02595077892380184\n",
            "Loss S2:  0.029819311968384\n",
            "Loss S01:  0.025956551927073267\n",
            "Loss S2:  0.02982761617086279\n",
            "Loss S01:  0.025937723237891484\n",
            "Loss S2:  0.029804919956558817\n",
            "Loss S01:  0.025930073006657246\n",
            "Loss S2:  0.02981295536299682\n",
            "Loss S01:  0.025934692137686746\n",
            "Loss S2:  0.02981638735337823\n",
            "Loss S01:  0.02591549678263204\n",
            "Loss S2:  0.02981554617430538\n",
            "Loss S01:  0.02591825318532638\n",
            "Loss S2:  0.029821071330845484\n",
            "Loss S01:  0.025918774790829532\n",
            "Loss S2:  0.029834051157605377\n",
            "Loss S01:  0.02589770967933651\n",
            "Loss S2:  0.029823095763439802\n",
            "Validation: \n",
            " Loss S01:  0.024485964328050613\n",
            " Loss S2:  0.04531772434711456\n",
            " Loss S01:  0.026058324124841464\n",
            " Loss S2:  0.048238569604499\n",
            " Loss S01:  0.026091612966322317\n",
            " Loss S2:  0.04819302706093323\n",
            " Loss S01:  0.026099730069275764\n",
            " Loss S2:  0.047624091449819626\n",
            " Loss S01:  0.026108234384913505\n",
            " Loss S2:  0.04750338219750075\n",
            "\n",
            "Epoch: 50\n",
            "Loss S01:  0.028404366225004196\n",
            "Loss S2:  0.03433150053024292\n",
            "Loss S01:  0.025748005814173004\n",
            "Loss S2:  0.029670069840821354\n",
            "Loss S01:  0.025797706718246143\n",
            "Loss S2:  0.029842012517509005\n",
            "Loss S01:  0.02591171908763147\n",
            "Loss S2:  0.029956540753764492\n",
            "Loss S01:  0.025948120099378795\n",
            "Loss S2:  0.02984441030861401\n",
            "Loss S01:  0.026171125800294036\n",
            "Loss S2:  0.029836246999455432\n",
            "Loss S01:  0.026014090805757242\n",
            "Loss S2:  0.029721606370122708\n",
            "Loss S01:  0.025972627706720795\n",
            "Loss S2:  0.02968437744068428\n",
            "Loss S01:  0.025802924102287232\n",
            "Loss S2:  0.02957616072653988\n",
            "Loss S01:  0.025758758514792056\n",
            "Loss S2:  0.02955660074539892\n",
            "Loss S01:  0.025759548373003996\n",
            "Loss S2:  0.029465844393661705\n",
            "Loss S01:  0.025721566281742877\n",
            "Loss S2:  0.029367488381024952\n",
            "Loss S01:  0.025725413344738896\n",
            "Loss S2:  0.02939053726467219\n",
            "Loss S01:  0.02568704966199762\n",
            "Loss S2:  0.02940354309976101\n",
            "Loss S01:  0.0257033118515784\n",
            "Loss S2:  0.029420767375763428\n",
            "Loss S01:  0.02573947385200207\n",
            "Loss S2:  0.029471394229704972\n",
            "Loss S01:  0.02572152314719206\n",
            "Loss S2:  0.02945106025345577\n",
            "Loss S01:  0.025759707243121856\n",
            "Loss S2:  0.029536105104182895\n",
            "Loss S01:  0.02576724053952246\n",
            "Loss S2:  0.02954869256329141\n",
            "Loss S01:  0.025747569726477744\n",
            "Loss S2:  0.029542103107453015\n",
            "Loss S01:  0.025736146342398514\n",
            "Loss S2:  0.029536529532193545\n",
            "Loss S01:  0.025725915533671446\n",
            "Loss S2:  0.029529031772184147\n",
            "Loss S01:  0.025771299174805574\n",
            "Loss S2:  0.02955112135147347\n",
            "Loss S01:  0.025802833765938685\n",
            "Loss S2:  0.029561441182058095\n",
            "Loss S01:  0.025820958887949524\n",
            "Loss S2:  0.029574907094982155\n",
            "Loss S01:  0.025819046689991458\n",
            "Loss S2:  0.02960668310552717\n",
            "Loss S01:  0.02585148854873418\n",
            "Loss S2:  0.029641084080397853\n",
            "Loss S01:  0.025836483056474877\n",
            "Loss S2:  0.02963253300348331\n",
            "Loss S01:  0.025821980261293596\n",
            "Loss S2:  0.02964215601704935\n",
            "Loss S01:  0.02581947956784689\n",
            "Loss S2:  0.0296434280606582\n",
            "Loss S01:  0.02582789495884382\n",
            "Loss S2:  0.02964130526662269\n",
            "Loss S01:  0.025815477594733238\n",
            "Loss S2:  0.029624700288728503\n",
            "Loss S01:  0.025811753534592943\n",
            "Loss S2:  0.029632362371710973\n",
            "Loss S01:  0.025827162327903275\n",
            "Loss S2:  0.02962920557583747\n",
            "Loss S01:  0.02584362929670517\n",
            "Loss S2:  0.029642433075567496\n",
            "Loss S01:  0.02585301202777614\n",
            "Loss S2:  0.029666800773636228\n",
            "Loss S01:  0.025864582472684642\n",
            "Loss S2:  0.029666215423855755\n",
            "Loss S01:  0.025848190714366674\n",
            "Loss S2:  0.029651649497107996\n",
            "Loss S01:  0.02584526435107853\n",
            "Loss S2:  0.029646939028474914\n",
            "Loss S01:  0.02583838927338038\n",
            "Loss S2:  0.029637967476911862\n",
            "Loss S01:  0.025842587874640253\n",
            "Loss S2:  0.029650832003823242\n",
            "Loss S01:  0.02584451962253328\n",
            "Loss S2:  0.029629155100660894\n",
            "Loss S01:  0.025850429400943238\n",
            "Loss S2:  0.029628986944035228\n",
            "Loss S01:  0.025837429096866097\n",
            "Loss S2:  0.02959355158329425\n",
            "Loss S01:  0.02582660418789403\n",
            "Loss S2:  0.029597528143669744\n",
            "Loss S01:  0.02582773926797965\n",
            "Loss S2:  0.029596364790544807\n",
            "Loss S01:  0.025822073447471585\n",
            "Loss S2:  0.029597393950192893\n",
            "Loss S01:  0.025821653857131165\n",
            "Loss S2:  0.02959407492285701\n",
            "Loss S01:  0.025814835969629753\n",
            "Loss S2:  0.0296003123795664\n",
            "Loss S01:  0.025800100157830234\n",
            "Loss S2:  0.029588780443652585\n",
            "Validation: \n",
            " Loss S01:  0.024957431480288506\n",
            " Loss S2:  0.046887218952178955\n",
            " Loss S01:  0.02670411268870036\n",
            " Loss S2:  0.04913800458113352\n",
            " Loss S01:  0.026567270615842284\n",
            " Loss S2:  0.04878052905565355\n",
            " Loss S01:  0.026506200066355408\n",
            " Loss S2:  0.04819592430454786\n",
            " Loss S01:  0.026474607395537107\n",
            " Loss S2:  0.04798568969155535\n",
            "\n",
            "Epoch: 51\n",
            "Loss S01:  0.027813326567411423\n",
            "Loss S2:  0.029513750225305557\n",
            "Loss S01:  0.025401678782972423\n",
            "Loss S2:  0.02915513227609071\n",
            "Loss S01:  0.02550339929404713\n",
            "Loss S2:  0.029384226582589604\n",
            "Loss S01:  0.02551445322892358\n",
            "Loss S2:  0.029308981832958037\n",
            "Loss S01:  0.025641033244205684\n",
            "Loss S2:  0.029299886079459655\n",
            "Loss S01:  0.025735706383106755\n",
            "Loss S2:  0.0293491841019953\n",
            "Loss S01:  0.025637502308751715\n",
            "Loss S2:  0.029366709841567962\n",
            "Loss S01:  0.02563822214347376\n",
            "Loss S2:  0.029391029823414037\n",
            "Loss S01:  0.025517647074144563\n",
            "Loss S2:  0.02922397053995986\n",
            "Loss S01:  0.025512559789713923\n",
            "Loss S2:  0.02921378258411046\n",
            "Loss S01:  0.025493521667500532\n",
            "Loss S2:  0.02918030539214021\n",
            "Loss S01:  0.025475244314686674\n",
            "Loss S2:  0.029158207915119222\n",
            "Loss S01:  0.025458189220098425\n",
            "Loss S2:  0.029153780072681174\n",
            "Loss S01:  0.025462362815746823\n",
            "Loss S2:  0.029224370599134276\n",
            "Loss S01:  0.025509374533244904\n",
            "Loss S2:  0.02926474910044501\n",
            "Loss S01:  0.02550280477777617\n",
            "Loss S2:  0.02932400413054899\n",
            "Loss S01:  0.025497998163011504\n",
            "Loss S2:  0.029346343129873276\n",
            "Loss S01:  0.025538656424883514\n",
            "Loss S2:  0.029401166615081808\n",
            "Loss S01:  0.025519404879344103\n",
            "Loss S2:  0.029390762801904703\n",
            "Loss S01:  0.02552916127115644\n",
            "Loss S2:  0.029396515749199853\n",
            "Loss S01:  0.025531166432360513\n",
            "Loss S2:  0.02940211754607324\n",
            "Loss S01:  0.02551722812567842\n",
            "Loss S2:  0.029390227782288433\n",
            "Loss S01:  0.02554157824672725\n",
            "Loss S2:  0.02940653245137558\n",
            "Loss S01:  0.025568734088436865\n",
            "Loss S2:  0.029405221488181647\n",
            "Loss S01:  0.02560363389953538\n",
            "Loss S2:  0.02944289492559136\n",
            "Loss S01:  0.025602667952379383\n",
            "Loss S2:  0.02944141659009979\n",
            "Loss S01:  0.02562463928684878\n",
            "Loss S2:  0.029442642155278233\n",
            "Loss S01:  0.02559701956093751\n",
            "Loss S2:  0.02941781082413953\n",
            "Loss S01:  0.02558334821377785\n",
            "Loss S2:  0.02941585865759977\n",
            "Loss S01:  0.025590594326661213\n",
            "Loss S2:  0.029431546023723595\n",
            "Loss S01:  0.02560757357392002\n",
            "Loss S2:  0.029425391141461376\n",
            "Loss S01:  0.02559409494185371\n",
            "Loss S2:  0.0294127982361332\n",
            "Loss S01:  0.025596862542805642\n",
            "Loss S2:  0.029424193921583092\n",
            "Loss S01:  0.025610426198859948\n",
            "Loss S2:  0.02944414270338335\n",
            "Loss S01:  0.02563697621881787\n",
            "Loss S2:  0.02947107106556466\n",
            "Loss S01:  0.0256488090885161\n",
            "Loss S2:  0.029477339407742192\n",
            "Loss S01:  0.025644154244017402\n",
            "Loss S2:  0.02949178194591048\n",
            "Loss S01:  0.02562547842105926\n",
            "Loss S2:  0.02947557821990345\n",
            "Loss S01:  0.025637172768748027\n",
            "Loss S2:  0.029466750061144353\n",
            "Loss S01:  0.025619271075557868\n",
            "Loss S2:  0.029431348938085234\n",
            "Loss S01:  0.025623225539625434\n",
            "Loss S2:  0.029450037082346003\n",
            "Loss S01:  0.02563747109215335\n",
            "Loss S2:  0.029444192959915693\n",
            "Loss S01:  0.025644661270241273\n",
            "Loss S2:  0.029455555067563567\n",
            "Loss S01:  0.025624328571654805\n",
            "Loss S2:  0.029433560313488382\n",
            "Loss S01:  0.025630681439727344\n",
            "Loss S2:  0.029451547862112928\n",
            "Loss S01:  0.02563988937771241\n",
            "Loss S2:  0.029452950588351078\n",
            "Loss S01:  0.02563365102783977\n",
            "Loss S2:  0.029457531719915005\n",
            "Loss S01:  0.025643659712086313\n",
            "Loss S2:  0.02946301430637796\n",
            "Loss S01:  0.02564324627547155\n",
            "Loss S2:  0.029473169882958\n",
            "Loss S01:  0.025641621714514774\n",
            "Loss S2:  0.029471090370732266\n",
            "Validation: \n",
            " Loss S01:  0.02569148875772953\n",
            " Loss S2:  0.0461827851831913\n",
            " Loss S01:  0.02668234280177525\n",
            " Loss S2:  0.048739260860851834\n",
            " Loss S01:  0.0267724387075116\n",
            " Loss S2:  0.04862232642566285\n",
            " Loss S01:  0.026826913483807297\n",
            " Loss S2:  0.047984656373985475\n",
            " Loss S01:  0.026851203637542547\n",
            " Loss S2:  0.04783601088472354\n",
            "\n",
            "Epoch: 52\n",
            "Loss S01:  0.026172246783971786\n",
            "Loss S2:  0.030003199353814125\n",
            "Loss S01:  0.02532084184614095\n",
            "Loss S2:  0.029371650889515877\n",
            "Loss S01:  0.025356470740267208\n",
            "Loss S2:  0.02939768144417377\n",
            "Loss S01:  0.02519139737611817\n",
            "Loss S2:  0.029286742510814822\n",
            "Loss S01:  0.02540897169127697\n",
            "Loss S2:  0.029293703142462706\n",
            "Loss S01:  0.0255489032715559\n",
            "Loss S2:  0.029269355477071275\n",
            "Loss S01:  0.025547461156718066\n",
            "Loss S2:  0.029264324146216034\n",
            "Loss S01:  0.02553901071069946\n",
            "Loss S2:  0.02929261053951693\n",
            "Loss S01:  0.02542946634837139\n",
            "Loss S2:  0.02908617506424586\n",
            "Loss S01:  0.0254190481871694\n",
            "Loss S2:  0.029055626039485354\n",
            "Loss S01:  0.025453772008566574\n",
            "Loss S2:  0.028994151179005605\n",
            "Loss S01:  0.025425167720731313\n",
            "Loss S2:  0.028950069989035796\n",
            "Loss S01:  0.02540359159639059\n",
            "Loss S2:  0.028932826885999727\n",
            "Loss S01:  0.025401592368387994\n",
            "Loss S2:  0.028979314747088737\n",
            "Loss S01:  0.025445432342747425\n",
            "Loss S2:  0.029048196350534756\n",
            "Loss S01:  0.02547808551531754\n",
            "Loss S2:  0.029101658217756954\n",
            "Loss S01:  0.025462083564781995\n",
            "Loss S2:  0.02911049264095585\n",
            "Loss S01:  0.025524265584889908\n",
            "Loss S2:  0.029200895988976048\n",
            "Loss S01:  0.025516648586448386\n",
            "Loss S2:  0.029215507785589\n",
            "Loss S01:  0.025483250656989233\n",
            "Loss S2:  0.029201241795227165\n",
            "Loss S01:  0.0254643080264923\n",
            "Loss S2:  0.02919477203962815\n",
            "Loss S01:  0.02547933366944157\n",
            "Loss S2:  0.029209422574365308\n",
            "Loss S01:  0.025510958804668884\n",
            "Loss S2:  0.02922086324121348\n",
            "Loss S01:  0.02553456301651734\n",
            "Loss S2:  0.02921442727022099\n",
            "Loss S01:  0.025544683553309362\n",
            "Loss S2:  0.029220766213981442\n",
            "Loss S01:  0.025548418575667767\n",
            "Loss S2:  0.029235492471561014\n",
            "Loss S01:  0.025559053046207775\n",
            "Loss S2:  0.02924682109320529\n",
            "Loss S01:  0.02553025879158305\n",
            "Loss S2:  0.029230132073548888\n",
            "Loss S01:  0.025542482461952655\n",
            "Loss S2:  0.029244690722524058\n",
            "Loss S01:  0.02554622482623636\n",
            "Loss S2:  0.029256916633739916\n",
            "Loss S01:  0.025549287830426842\n",
            "Loss S2:  0.029258283709123284\n",
            "Loss S01:  0.025537530852667387\n",
            "Loss S2:  0.029259941513707016\n",
            "Loss S01:  0.0255267326647526\n",
            "Loss S2:  0.029272829121518358\n",
            "Loss S01:  0.025525824327213165\n",
            "Loss S2:  0.029296086639661083\n",
            "Loss S01:  0.02554463634480479\n",
            "Loss S2:  0.0293116111177241\n",
            "Loss S01:  0.025554169898062012\n",
            "Loss S2:  0.02933342849830447\n",
            "Loss S01:  0.025561758144203976\n",
            "Loss S2:  0.029340002350405972\n",
            "Loss S01:  0.025562015962046433\n",
            "Loss S2:  0.02931940622346581\n",
            "Loss S01:  0.02557641774259527\n",
            "Loss S2:  0.029304017261491986\n",
            "Loss S01:  0.02556067194951617\n",
            "Loss S2:  0.029299036935543466\n",
            "Loss S01:  0.025581163988260557\n",
            "Loss S2:  0.02930003585772324\n",
            "Loss S01:  0.025581860103601375\n",
            "Loss S2:  0.029286885484509223\n",
            "Loss S01:  0.025592643059701648\n",
            "Loss S2:  0.02929414160142602\n",
            "Loss S01:  0.02557876000824771\n",
            "Loss S2:  0.029277360632911913\n",
            "Loss S01:  0.02558171585658375\n",
            "Loss S2:  0.029289739472525462\n",
            "Loss S01:  0.025593960356223345\n",
            "Loss S2:  0.02929015482410359\n",
            "Loss S01:  0.02558366665030767\n",
            "Loss S2:  0.029292679729178515\n",
            "Loss S01:  0.025588245492926827\n",
            "Loss S2:  0.02929722011927713\n",
            "Loss S01:  0.025586506301486813\n",
            "Loss S2:  0.02929983777913755\n",
            "Loss S01:  0.02557226029545251\n",
            "Loss S2:  0.029284473174761853\n",
            "Validation: \n",
            " Loss S01:  0.02523604966700077\n",
            " Loss S2:  0.04507129639387131\n",
            " Loss S01:  0.026610518229149636\n",
            " Loss S2:  0.04805287505899157\n",
            " Loss S01:  0.02669042630529985\n",
            " Loss S2:  0.04798645226330292\n",
            " Loss S01:  0.02664378260979887\n",
            " Loss S2:  0.047329980391459386\n",
            " Loss S01:  0.026666165570969933\n",
            " Loss S2:  0.04718161687070941\n",
            "\n",
            "Epoch: 53\n",
            "Loss S01:  0.02751900441944599\n",
            "Loss S2:  0.032717764377593994\n",
            "Loss S01:  0.025627878748557785\n",
            "Loss S2:  0.02943046831271865\n",
            "Loss S01:  0.025363359600305557\n",
            "Loss S2:  0.02919473587757065\n",
            "Loss S01:  0.025128478845280987\n",
            "Loss S2:  0.02910896817282323\n",
            "Loss S01:  0.02527773103154287\n",
            "Loss S2:  0.029155106897034298\n",
            "Loss S01:  0.025453994050621986\n",
            "Loss S2:  0.029214980372903394\n",
            "Loss S01:  0.025419783335728724\n",
            "Loss S2:  0.02915431262894732\n",
            "Loss S01:  0.025368885386367918\n",
            "Loss S2:  0.029182386099242827\n",
            "Loss S01:  0.025274904605783063\n",
            "Loss S2:  0.029054373341761988\n",
            "Loss S01:  0.025196396715038424\n",
            "Loss S2:  0.02895960037770507\n",
            "Loss S01:  0.025201355429864167\n",
            "Loss S2:  0.028892502317776774\n",
            "Loss S01:  0.02516761091579725\n",
            "Loss S2:  0.02883563522954245\n",
            "Loss S01:  0.025227483131053035\n",
            "Loss S2:  0.028836343668339665\n",
            "Loss S01:  0.02522778367438844\n",
            "Loss S2:  0.028833417719557084\n",
            "Loss S01:  0.025248268790912968\n",
            "Loss S2:  0.028877474301567313\n",
            "Loss S01:  0.02530154761000974\n",
            "Loss S2:  0.028922997365724172\n",
            "Loss S01:  0.02533195562290479\n",
            "Loss S2:  0.02894423262256643\n",
            "Loss S01:  0.02535562646406436\n",
            "Loss S2:  0.02898585692875915\n",
            "Loss S01:  0.025357968045285395\n",
            "Loss S2:  0.02903027832508087\n",
            "Loss S01:  0.025346657573318606\n",
            "Loss S2:  0.029031659890921952\n",
            "Loss S01:  0.025328646381547797\n",
            "Loss S2:  0.029001598686572923\n",
            "Loss S01:  0.025327251102969545\n",
            "Loss S2:  0.029004270714041182\n",
            "Loss S01:  0.0253432190880112\n",
            "Loss S2:  0.02901910471295879\n",
            "Loss S01:  0.02536976340658221\n",
            "Loss S2:  0.02902718922082996\n",
            "Loss S01:  0.025414985044244907\n",
            "Loss S2:  0.029071332469458897\n",
            "Loss S01:  0.025424997697016633\n",
            "Loss S2:  0.029103442559026152\n",
            "Loss S01:  0.025440902117340045\n",
            "Loss S2:  0.029116983250696997\n",
            "Loss S01:  0.025418941254600386\n",
            "Loss S2:  0.02909265332803735\n",
            "Loss S01:  0.02540680647453167\n",
            "Loss S2:  0.029091060473136206\n",
            "Loss S01:  0.025394898016786658\n",
            "Loss S2:  0.029083429346072304\n",
            "Loss S01:  0.02540123960207467\n",
            "Loss S2:  0.02908081268006781\n",
            "Loss S01:  0.02540119522972889\n",
            "Loss S2:  0.029044598065484376\n",
            "Loss S01:  0.025399011191317225\n",
            "Loss S2:  0.029051184915354317\n",
            "Loss S01:  0.025409099710626906\n",
            "Loss S2:  0.0290686376955574\n",
            "Loss S01:  0.02541991109110393\n",
            "Loss S2:  0.029080616457721007\n",
            "Loss S01:  0.02542238021295974\n",
            "Loss S2:  0.029101618499285477\n",
            "Loss S01:  0.02542995241499013\n",
            "Loss S2:  0.029107143622603773\n",
            "Loss S01:  0.025420054401750512\n",
            "Loss S2:  0.029105819450475457\n",
            "Loss S01:  0.025417378543829666\n",
            "Loss S2:  0.02910354569691216\n",
            "Loss S01:  0.02541838273348863\n",
            "Loss S2:  0.029082892510249182\n",
            "Loss S01:  0.02542787206886415\n",
            "Loss S2:  0.029083123672149724\n",
            "Loss S01:  0.02542768330415235\n",
            "Loss S2:  0.02907331203112782\n",
            "Loss S01:  0.025443848943660776\n",
            "Loss S2:  0.029096976418110085\n",
            "Loss S01:  0.025434758332655214\n",
            "Loss S2:  0.029070720988435977\n",
            "Loss S01:  0.02542805827123508\n",
            "Loss S2:  0.029086414559864673\n",
            "Loss S01:  0.025435586020086134\n",
            "Loss S2:  0.029090462876132744\n",
            "Loss S01:  0.02541979086340896\n",
            "Loss S2:  0.029098060034587172\n",
            "Loss S01:  0.025413364318304255\n",
            "Loss S2:  0.029094920084354985\n",
            "Loss S01:  0.02541412753895266\n",
            "Loss S2:  0.029102905635991116\n",
            "Loss S01:  0.02540230440723192\n",
            "Loss S2:  0.02909477388148638\n",
            "Validation: \n",
            " Loss S01:  0.02436152845621109\n",
            " Loss S2:  0.043796759098768234\n",
            " Loss S01:  0.025961053602042653\n",
            " Loss S2:  0.04743619351869538\n",
            " Loss S01:  0.02599295351381709\n",
            " Loss S2:  0.047160582208051915\n",
            " Loss S01:  0.0259386381348137\n",
            " Loss S2:  0.04670956706414457\n",
            " Loss S01:  0.025923361518868693\n",
            " Loss S2:  0.04656329084141755\n",
            "\n",
            "Epoch: 54\n",
            "Loss S01:  0.025968309491872787\n",
            "Loss S2:  0.030856890603899956\n",
            "Loss S01:  0.02513753351840106\n",
            "Loss S2:  0.028842199424451046\n",
            "Loss S01:  0.025228247135168032\n",
            "Loss S2:  0.029017791684184755\n",
            "Loss S01:  0.025207652740420833\n",
            "Loss S2:  0.029055768924374736\n",
            "Loss S01:  0.02527094296202427\n",
            "Loss S2:  0.029042944948120816\n",
            "Loss S01:  0.025293360446013657\n",
            "Loss S2:  0.029064306441475365\n",
            "Loss S01:  0.02514941245317459\n",
            "Loss S2:  0.029025821473266256\n",
            "Loss S01:  0.02519448495037119\n",
            "Loss S2:  0.02908878330088837\n",
            "Loss S01:  0.025056307330543613\n",
            "Loss S2:  0.02887307887000066\n",
            "Loss S01:  0.024993409228685138\n",
            "Loss S2:  0.02877809979267173\n",
            "Loss S01:  0.024989085161302348\n",
            "Loss S2:  0.028700933579613667\n",
            "Loss S01:  0.024927152606012585\n",
            "Loss S2:  0.028605371663296544\n",
            "Loss S01:  0.024925784400302516\n",
            "Loss S2:  0.02856684058966223\n",
            "Loss S01:  0.02489051788704086\n",
            "Loss S2:  0.028575381932367805\n",
            "Loss S01:  0.024955036435672578\n",
            "Loss S2:  0.028630307591553276\n",
            "Loss S01:  0.024986957761132165\n",
            "Loss S2:  0.02869452780346997\n",
            "Loss S01:  0.025002213414781583\n",
            "Loss S2:  0.028686691399501717\n",
            "Loss S01:  0.025073496951607235\n",
            "Loss S2:  0.028747314881337315\n",
            "Loss S01:  0.02509312391363455\n",
            "Loss S2:  0.028769168107466804\n",
            "Loss S01:  0.025051748428825307\n",
            "Loss S2:  0.02874010147926695\n",
            "Loss S01:  0.02504255988667557\n",
            "Loss S2:  0.028737455050447093\n",
            "Loss S01:  0.025051384175558227\n",
            "Loss S2:  0.028751585155866722\n",
            "Loss S01:  0.02509142149121783\n",
            "Loss S2:  0.02874533403917675\n",
            "Loss S01:  0.02513470302348013\n",
            "Loss S2:  0.028747424378475068\n",
            "Loss S01:  0.02516752102748487\n",
            "Loss S2:  0.028794465176915727\n",
            "Loss S01:  0.025158403582306973\n",
            "Loss S2:  0.02882164290761093\n",
            "Loss S01:  0.025170697149074855\n",
            "Loss S2:  0.028823846151831048\n",
            "Loss S01:  0.025158787393646926\n",
            "Loss S2:  0.028799049185100956\n",
            "Loss S01:  0.02515217541216531\n",
            "Loss S2:  0.028804596586424685\n",
            "Loss S01:  0.025161667122054344\n",
            "Loss S2:  0.028818445107371537\n",
            "Loss S01:  0.025170927953928016\n",
            "Loss S2:  0.02881994764274141\n",
            "Loss S01:  0.025137482103187937\n",
            "Loss S2:  0.028814185578435948\n",
            "Loss S01:  0.02513197618551158\n",
            "Loss S2:  0.028823300685466634\n",
            "Loss S01:  0.02514791052529639\n",
            "Loss S2:  0.028833143024781318\n",
            "Loss S01:  0.025164408358005135\n",
            "Loss S2:  0.02885463998224204\n",
            "Loss S01:  0.025177258334820426\n",
            "Loss S2:  0.028874866648489594\n",
            "Loss S01:  0.025186778735008266\n",
            "Loss S2:  0.028877321491404914\n",
            "Loss S01:  0.02517880859842519\n",
            "Loss S2:  0.028881698661054563\n",
            "Loss S01:  0.02517447656807624\n",
            "Loss S2:  0.028869876506056373\n",
            "Loss S01:  0.025147871554964948\n",
            "Loss S2:  0.028855605179543994\n",
            "Loss S01:  0.02515229981792092\n",
            "Loss S2:  0.028867616947563807\n",
            "Loss S01:  0.025142118484325654\n",
            "Loss S2:  0.028854589886655194\n",
            "Loss S01:  0.025152061669921365\n",
            "Loss S2:  0.028855423794754045\n",
            "Loss S01:  0.02514584508579734\n",
            "Loss S2:  0.028829432379998906\n",
            "Loss S01:  0.025138012946598114\n",
            "Loss S2:  0.028831395990048407\n",
            "Loss S01:  0.025148794513418775\n",
            "Loss S2:  0.02883449420250308\n",
            "Loss S01:  0.025148604493466994\n",
            "Loss S2:  0.028834131540257865\n",
            "Loss S01:  0.025157805144660286\n",
            "Loss S2:  0.028839783553200164\n",
            "Loss S01:  0.025163856414369388\n",
            "Loss S2:  0.02885525828525579\n",
            "Loss S01:  0.025150091237485288\n",
            "Loss S2:  0.028853317999609135\n",
            "Validation: \n",
            " Loss S01:  0.023969832807779312\n",
            " Loss S2:  0.04376121982932091\n",
            " Loss S01:  0.02553456649184227\n",
            " Loss S2:  0.04735432405556951\n",
            " Loss S01:  0.025626126949380084\n",
            " Loss S2:  0.04723667925814303\n",
            " Loss S01:  0.02555794662750158\n",
            " Loss S2:  0.04660453586304774\n",
            " Loss S01:  0.02556246599573412\n",
            " Loss S2:  0.04643421261398881\n",
            "\n",
            "Epoch: 55\n",
            "Loss S01:  0.028092555701732635\n",
            "Loss S2:  0.02955080382525921\n",
            "Loss S01:  0.024896345693956722\n",
            "Loss S2:  0.028279070149768482\n",
            "Loss S01:  0.024923397760306085\n",
            "Loss S2:  0.0284455344080925\n",
            "Loss S01:  0.024991550512852206\n",
            "Loss S2:  0.028496594258373784\n",
            "Loss S01:  0.025100382135772124\n",
            "Loss S2:  0.02874671572410479\n",
            "Loss S01:  0.025112273828948244\n",
            "Loss S2:  0.02878688206421394\n",
            "Loss S01:  0.025097513113354075\n",
            "Loss S2:  0.0288287237530849\n",
            "Loss S01:  0.025101366728334358\n",
            "Loss S2:  0.02880207186853382\n",
            "Loss S01:  0.024964822203288845\n",
            "Loss S2:  0.02861002315249708\n",
            "Loss S01:  0.0249616205487605\n",
            "Loss S2:  0.028621775701969533\n",
            "Loss S01:  0.024961545307299878\n",
            "Loss S2:  0.02859281783573108\n",
            "Loss S01:  0.024939677736780665\n",
            "Loss S2:  0.028520743505240562\n",
            "Loss S01:  0.024943125395735434\n",
            "Loss S2:  0.02850765280794999\n",
            "Loss S01:  0.024961509251298794\n",
            "Loss S2:  0.028532714305715707\n",
            "Loss S01:  0.02498026879121226\n",
            "Loss S2:  0.02855829469172667\n",
            "Loss S01:  0.02500397190204914\n",
            "Loss S2:  0.028603806413266042\n",
            "Loss S01:  0.024973660175289427\n",
            "Loss S2:  0.028632076295173687\n",
            "Loss S01:  0.02501859727223017\n",
            "Loss S2:  0.028676555530108207\n",
            "Loss S01:  0.025018131895536217\n",
            "Loss S2:  0.028702763624448145\n",
            "Loss S01:  0.024990313964364415\n",
            "Loss S2:  0.028680731449763814\n",
            "Loss S01:  0.024967668883836093\n",
            "Loss S2:  0.028669113024550292\n",
            "Loss S01:  0.024986409295297347\n",
            "Loss S2:  0.028674680307083786\n",
            "Loss S01:  0.024997471136407615\n",
            "Loss S2:  0.028700037020768517\n",
            "Loss S01:  0.025017875855490243\n",
            "Loss S2:  0.02867438413557552\n",
            "Loss S01:  0.025032598703048536\n",
            "Loss S2:  0.028706653669662\n",
            "Loss S01:  0.025039830894406097\n",
            "Loss S2:  0.028715804553186276\n",
            "Loss S01:  0.025054569638038048\n",
            "Loss S2:  0.028736894415027792\n",
            "Loss S01:  0.025030545134276043\n",
            "Loss S2:  0.02872495604094764\n",
            "Loss S01:  0.025038351394315633\n",
            "Loss S2:  0.02871428953079354\n",
            "Loss S01:  0.02502713760175451\n",
            "Loss S2:  0.02871920643982404\n",
            "Loss S01:  0.025047364264132572\n",
            "Loss S2:  0.028712294011862573\n",
            "Loss S01:  0.025033804579036027\n",
            "Loss S2:  0.0287047713408037\n",
            "Loss S01:  0.025013963468488873\n",
            "Loss S2:  0.02870748737519404\n",
            "Loss S01:  0.025032701100197445\n",
            "Loss S2:  0.028707601647722757\n",
            "Loss S01:  0.025031613011208098\n",
            "Loss S2:  0.028718680181592442\n",
            "Loss S01:  0.025055760390356056\n",
            "Loss S2:  0.028746365073357212\n",
            "Loss S01:  0.025069101932884252\n",
            "Loss S2:  0.028750466678157408\n",
            "Loss S01:  0.02505963751067209\n",
            "Loss S2:  0.028737320486867845\n",
            "Loss S01:  0.025071561214296524\n",
            "Loss S2:  0.028727748355488452\n",
            "Loss S01:  0.025059798260783906\n",
            "Loss S2:  0.028711367041215567\n",
            "Loss S01:  0.025065385771362563\n",
            "Loss S2:  0.02872582796282899\n",
            "Loss S01:  0.025071429845552953\n",
            "Loss S2:  0.02872568748238748\n",
            "Loss S01:  0.025087775843939137\n",
            "Loss S2:  0.028737355041319853\n",
            "Loss S01:  0.02507259390314743\n",
            "Loss S2:  0.028713631562525602\n",
            "Loss S01:  0.025068036172868444\n",
            "Loss S2:  0.028715015188400167\n",
            "Loss S01:  0.02507117602916356\n",
            "Loss S2:  0.028718750502608567\n",
            "Loss S01:  0.02505310369481751\n",
            "Loss S2:  0.02872190388917535\n",
            "Loss S01:  0.025052171212377822\n",
            "Loss S2:  0.028722737130813792\n",
            "Loss S01:  0.025044016354173235\n",
            "Loss S2:  0.0287212349412471\n",
            "Loss S01:  0.025034315330251414\n",
            "Loss S2:  0.028722562142429672\n",
            "Validation: \n",
            " Loss S01:  0.024096891283988953\n",
            " Loss S2:  0.043275438249111176\n",
            " Loss S01:  0.02556778561501276\n",
            " Loss S2:  0.04672143572852725\n",
            " Loss S01:  0.025648703526069478\n",
            " Loss S2:  0.04668840838641655\n",
            " Loss S01:  0.025642300085698972\n",
            " Loss S2:  0.046197374336055065\n",
            " Loss S01:  0.025675759708255898\n",
            " Loss S2:  0.046084229546932524\n",
            "\n",
            "Epoch: 56\n",
            "Loss S01:  0.026527447625994682\n",
            "Loss S2:  0.03301989659667015\n",
            "Loss S01:  0.024888393892483276\n",
            "Loss S2:  0.028378539803353222\n",
            "Loss S01:  0.024937744030640238\n",
            "Loss S2:  0.02871352274503027\n",
            "Loss S01:  0.024934374276668794\n",
            "Loss S2:  0.02879760528524076\n",
            "Loss S01:  0.02488669489578503\n",
            "Loss S2:  0.02887110780106812\n",
            "Loss S01:  0.024944207831925035\n",
            "Loss S2:  0.028897733007575952\n",
            "Loss S01:  0.024924081154778354\n",
            "Loss S2:  0.02889052772375404\n",
            "Loss S01:  0.02495428967014165\n",
            "Loss S2:  0.02887796951641499\n",
            "Loss S01:  0.024900773333178625\n",
            "Loss S2:  0.028701946070348774\n",
            "Loss S01:  0.024841876495834234\n",
            "Loss S2:  0.028639051264950206\n",
            "Loss S01:  0.02489264237482359\n",
            "Loss S2:  0.028620193389677764\n",
            "Loss S01:  0.024833930337482744\n",
            "Loss S2:  0.028563073962121398\n",
            "Loss S01:  0.024823240689502275\n",
            "Loss S2:  0.028549125879879827\n",
            "Loss S01:  0.024795974853612086\n",
            "Loss S2:  0.028522159292838956\n",
            "Loss S01:  0.02481718321746968\n",
            "Loss S2:  0.028545037810261367\n",
            "Loss S01:  0.024850890644832164\n",
            "Loss S2:  0.02857976792910636\n",
            "Loss S01:  0.024809833002849394\n",
            "Loss S2:  0.0285681756502538\n",
            "Loss S01:  0.024860544827336458\n",
            "Loss S2:  0.028631363630469083\n",
            "Loss S01:  0.02486986633987058\n",
            "Loss S2:  0.02861996400504481\n",
            "Loss S01:  0.024851345452260597\n",
            "Loss S2:  0.028599816671918823\n",
            "Loss S01:  0.024833874026341223\n",
            "Loss S2:  0.028564890679806026\n",
            "Loss S01:  0.024843777278305796\n",
            "Loss S2:  0.028553861798056496\n",
            "Loss S01:  0.024864117998410675\n",
            "Loss S2:  0.02856348694314784\n",
            "Loss S01:  0.02488638140519202\n",
            "Loss S2:  0.028573913144923392\n",
            "Loss S01:  0.02493132256682483\n",
            "Loss S2:  0.02859504291347201\n",
            "Loss S01:  0.024963203268459595\n",
            "Loss S2:  0.028594799228457816\n",
            "Loss S01:  0.024970840826858964\n",
            "Loss S2:  0.028607197698219983\n",
            "Loss S01:  0.024976462407580603\n",
            "Loss S2:  0.028608238571959228\n",
            "Loss S01:  0.02497573259197096\n",
            "Loss S2:  0.028605068752084763\n",
            "Loss S01:  0.024992515547425067\n",
            "Loss S2:  0.02861752015413697\n",
            "Loss S01:  0.02499817230079657\n",
            "Loss S2:  0.028610068098007643\n",
            "Loss S01:  0.024988558251183134\n",
            "Loss S2:  0.02860929851099801\n",
            "Loss S01:  0.024980795178040166\n",
            "Loss S2:  0.028624490135555326\n",
            "Loss S01:  0.024986689876528665\n",
            "Loss S2:  0.0286446147207227\n",
            "Loss S01:  0.025005300002188976\n",
            "Loss S2:  0.028667545716259138\n",
            "Loss S01:  0.02502005952715534\n",
            "Loss S2:  0.028685667697987666\n",
            "Loss S01:  0.025018225859984797\n",
            "Loss S2:  0.028697895159599193\n",
            "Loss S01:  0.02500849809608048\n",
            "Loss S2:  0.02870351661026799\n",
            "Loss S01:  0.02499849038819472\n",
            "Loss S2:  0.028694194137776304\n",
            "Loss S01:  0.024977883428830623\n",
            "Loss S2:  0.028678521275748988\n",
            "Loss S01:  0.02499659639548631\n",
            "Loss S2:  0.028688442208188728\n",
            "Loss S01:  0.02499439950733289\n",
            "Loss S2:  0.028682232859324655\n",
            "Loss S01:  0.025000567364367055\n",
            "Loss S2:  0.028689921866757288\n",
            "Loss S01:  0.024989236366617432\n",
            "Loss S2:  0.028664614603338273\n",
            "Loss S01:  0.024991999293704964\n",
            "Loss S2:  0.02867604692439103\n",
            "Loss S01:  0.024996712812100703\n",
            "Loss S2:  0.028669159409245207\n",
            "Loss S01:  0.02498412335901612\n",
            "Loss S2:  0.028658931747628395\n",
            "Loss S01:  0.02497612926199431\n",
            "Loss S2:  0.02866788164587917\n",
            "Loss S01:  0.024979645754220334\n",
            "Loss S2:  0.028674254104835824\n",
            "Loss S01:  0.024969387241845945\n",
            "Loss S2:  0.02866855946932815\n",
            "Validation: \n",
            " Loss S01:  0.024381760507822037\n",
            " Loss S2:  0.042669277638196945\n",
            " Loss S01:  0.02540512445072333\n",
            " Loss S2:  0.04587832181936219\n",
            " Loss S01:  0.025460560011064132\n",
            " Loss S2:  0.04567479851042352\n",
            " Loss S01:  0.025396699727070135\n",
            " Loss S2:  0.04520927033707744\n",
            " Loss S01:  0.025376815808775984\n",
            " Loss S2:  0.04512827254739808\n",
            "\n",
            "Epoch: 57\n",
            "Loss S01:  0.027425486594438553\n",
            "Loss S2:  0.030958546325564384\n",
            "Loss S01:  0.025016940622167153\n",
            "Loss S2:  0.028242489153688603\n",
            "Loss S01:  0.025024437567307836\n",
            "Loss S2:  0.028524488033283325\n",
            "Loss S01:  0.024900977289484393\n",
            "Loss S2:  0.028530946662349087\n",
            "Loss S01:  0.024986336416587596\n",
            "Loss S2:  0.028517974994894935\n",
            "Loss S01:  0.025040683793086632\n",
            "Loss S2:  0.028484770042054793\n",
            "Loss S01:  0.02489111129744131\n",
            "Loss S2:  0.028446438737580033\n",
            "Loss S01:  0.02485363146054073\n",
            "Loss S2:  0.028455902782964036\n",
            "Loss S01:  0.024740494810688643\n",
            "Loss S2:  0.028368518147387622\n",
            "Loss S01:  0.024714482538811453\n",
            "Loss S2:  0.028361241056860147\n",
            "Loss S01:  0.024729934567124537\n",
            "Loss S2:  0.02826190096243183\n",
            "Loss S01:  0.024733033618545747\n",
            "Loss S2:  0.028249278194732493\n",
            "Loss S01:  0.024726029577826665\n",
            "Loss S2:  0.028238717882224353\n",
            "Loss S01:  0.02470206685421121\n",
            "Loss S2:  0.028278398078702787\n",
            "Loss S01:  0.02473752476697695\n",
            "Loss S2:  0.02828905405146433\n",
            "Loss S01:  0.024741580820833612\n",
            "Loss S2:  0.02829211581473714\n",
            "Loss S01:  0.024710587937361705\n",
            "Loss S2:  0.02829900989019723\n",
            "Loss S01:  0.024746080599071688\n",
            "Loss S2:  0.02837800800974606\n",
            "Loss S01:  0.024736856280193145\n",
            "Loss S2:  0.02838995686387489\n",
            "Loss S01:  0.02471884475601593\n",
            "Loss S2:  0.02838391562953045\n",
            "Loss S01:  0.024726962578014947\n",
            "Loss S2:  0.028397485669424283\n",
            "Loss S01:  0.024735258221273173\n",
            "Loss S2:  0.028393238006044902\n",
            "Loss S01:  0.02475290930918439\n",
            "Loss S2:  0.028418674593058108\n",
            "Loss S01:  0.024783659669937508\n",
            "Loss S2:  0.02841293756966983\n",
            "Loss S01:  0.024818444825492458\n",
            "Loss S2:  0.0284554835919028\n",
            "Loss S01:  0.024841844122782647\n",
            "Loss S2:  0.028472334784696778\n",
            "Loss S01:  0.02484444967717275\n",
            "Loss S2:  0.028466032603862643\n",
            "Loss S01:  0.024820714212981536\n",
            "Loss S2:  0.02845124515440191\n",
            "Loss S01:  0.024827631336249066\n",
            "Loss S2:  0.028448684890075085\n",
            "Loss S01:  0.02482405598067336\n",
            "Loss S2:  0.028467722352325302\n",
            "Loss S01:  0.024817732079233046\n",
            "Loss S2:  0.028462697792043322\n",
            "Loss S01:  0.02479904981695379\n",
            "Loss S2:  0.0284509419424645\n",
            "Loss S01:  0.024786787821310705\n",
            "Loss S2:  0.028455760074645932\n",
            "Loss S01:  0.02480269532954585\n",
            "Loss S2:  0.02847319459852135\n",
            "Loss S01:  0.024825192042940522\n",
            "Loss S2:  0.028499004406488535\n",
            "Loss S01:  0.02482040892978679\n",
            "Loss S2:  0.02852911426768004\n",
            "Loss S01:  0.02481595032082205\n",
            "Loss S2:  0.028541991678053654\n",
            "Loss S01:  0.02480822600765691\n",
            "Loss S2:  0.02854612518875104\n",
            "Loss S01:  0.024813339089744986\n",
            "Loss S2:  0.02854320576556242\n",
            "Loss S01:  0.02479518844705561\n",
            "Loss S2:  0.02852066482900811\n",
            "Loss S01:  0.024810521122523375\n",
            "Loss S2:  0.02852427803370126\n",
            "Loss S01:  0.024824731056453827\n",
            "Loss S2:  0.028519576873149894\n",
            "Loss S01:  0.0248366994938134\n",
            "Loss S2:  0.028530242819684134\n",
            "Loss S01:  0.024815993673079528\n",
            "Loss S2:  0.028510739400187668\n",
            "Loss S01:  0.02480916760844979\n",
            "Loss S2:  0.02850960482697503\n",
            "Loss S01:  0.024808525766541318\n",
            "Loss S2:  0.0285213928165827\n",
            "Loss S01:  0.024794616568793185\n",
            "Loss S2:  0.028521678477137827\n",
            "Loss S01:  0.024809659326215712\n",
            "Loss S2:  0.028538886722643916\n",
            "Loss S01:  0.024819826168974324\n",
            "Loss S2:  0.028560791768737742\n",
            "Loss S01:  0.02481513307287591\n",
            "Loss S2:  0.028565436465070107\n",
            "Validation: \n",
            " Loss S01:  0.024246225133538246\n",
            " Loss S2:  0.04464615136384964\n",
            " Loss S01:  0.0253045561590365\n",
            " Loss S2:  0.047727644975696294\n",
            " Loss S01:  0.02532231557841708\n",
            " Loss S2:  0.0476859170125752\n",
            " Loss S01:  0.02524280355724155\n",
            " Loss S2:  0.04720326586336386\n",
            " Loss S01:  0.025241987434802233\n",
            " Loss S2:  0.04712342043165808\n",
            "\n",
            "Epoch: 58\n",
            "Loss S01:  0.027027888223528862\n",
            "Loss S2:  0.031024625524878502\n",
            "Loss S01:  0.024868218566883694\n",
            "Loss S2:  0.028818597678433765\n",
            "Loss S01:  0.024324789465892883\n",
            "Loss S2:  0.02843161645744528\n",
            "Loss S01:  0.024632004540293448\n",
            "Loss S2:  0.028464696159766566\n",
            "Loss S01:  0.024607734936403065\n",
            "Loss S2:  0.0284793648596217\n",
            "Loss S01:  0.02463291183698411\n",
            "Loss S2:  0.02845855473595507\n",
            "Loss S01:  0.02458752194022546\n",
            "Loss S2:  0.02842209323263559\n",
            "Loss S01:  0.024582740732691656\n",
            "Loss S2:  0.0284428740785995\n",
            "Loss S01:  0.024455094043119453\n",
            "Loss S2:  0.028258982884847086\n",
            "Loss S01:  0.024463688299714865\n",
            "Loss S2:  0.02819807454943657\n",
            "Loss S01:  0.024472785560358867\n",
            "Loss S2:  0.028110758797957165\n",
            "Loss S01:  0.024461181817559508\n",
            "Loss S2:  0.028087355196475983\n",
            "Loss S01:  0.024468800468632014\n",
            "Loss S2:  0.028093848377466202\n",
            "Loss S01:  0.024453017647371036\n",
            "Loss S2:  0.028105185815054952\n",
            "Loss S01:  0.024514698152635114\n",
            "Loss S2:  0.02816498041787046\n",
            "Loss S01:  0.024558931996194733\n",
            "Loss S2:  0.028235273250680884\n",
            "Loss S01:  0.024536972032478135\n",
            "Loss S2:  0.028213228602309404\n",
            "Loss S01:  0.024596184927817673\n",
            "Loss S2:  0.028276573441791953\n",
            "Loss S01:  0.024598444137948653\n",
            "Loss S2:  0.028306110324698257\n",
            "Loss S01:  0.024594399297892734\n",
            "Loss S2:  0.028282363075002325\n",
            "Loss S01:  0.02458685071240017\n",
            "Loss S2:  0.02827309764254449\n",
            "Loss S01:  0.024607651491795106\n",
            "Loss S2:  0.028273673207273982\n",
            "Loss S01:  0.0246272454906373\n",
            "Loss S2:  0.02826177532788855\n",
            "Loss S01:  0.02465508443613847\n",
            "Loss S2:  0.028267242506275447\n",
            "Loss S01:  0.02470991063352937\n",
            "Loss S2:  0.028334419988868643\n",
            "Loss S01:  0.024713146437211817\n",
            "Loss S2:  0.028367404191498736\n",
            "Loss S01:  0.02471918414321896\n",
            "Loss S2:  0.028390382555709488\n",
            "Loss S01:  0.02470295073471386\n",
            "Loss S2:  0.02837892104888754\n",
            "Loss S01:  0.024694361899618152\n",
            "Loss S2:  0.028377649523450385\n",
            "Loss S01:  0.02468527781799487\n",
            "Loss S2:  0.028389142570165834\n",
            "Loss S01:  0.024697449708054233\n",
            "Loss S2:  0.028404082595509944\n",
            "Loss S01:  0.024676152190546898\n",
            "Loss S2:  0.028388797350443445\n",
            "Loss S01:  0.024665046605133564\n",
            "Loss S2:  0.02838377237343157\n",
            "Loss S01:  0.02468259707993611\n",
            "Loss S2:  0.028384280523279643\n",
            "Loss S01:  0.02469203647368115\n",
            "Loss S2:  0.028397681541150965\n",
            "Loss S01:  0.02472677114194105\n",
            "Loss S2:  0.028414177594234123\n",
            "Loss S01:  0.02473260422554538\n",
            "Loss S2:  0.028421755322871778\n",
            "Loss S01:  0.024738499252100518\n",
            "Loss S2:  0.02842098338946178\n",
            "Loss S01:  0.024738416227260286\n",
            "Loss S2:  0.028403270701215337\n",
            "Loss S01:  0.024736578511955487\n",
            "Loss S2:  0.028394236198395414\n",
            "Loss S01:  0.024744652327791116\n",
            "Loss S2:  0.028400216330576714\n",
            "Loss S01:  0.024744330604000973\n",
            "Loss S2:  0.028392455443159798\n",
            "Loss S01:  0.024754436407901894\n",
            "Loss S2:  0.02840259553966358\n",
            "Loss S01:  0.02474502321946372\n",
            "Loss S2:  0.028388131211086104\n",
            "Loss S01:  0.024744094900634825\n",
            "Loss S2:  0.028393134457859594\n",
            "Loss S01:  0.02475305795339153\n",
            "Loss S2:  0.02838514566817992\n",
            "Loss S01:  0.024750128849015576\n",
            "Loss S2:  0.028397063326712803\n",
            "Loss S01:  0.024756554636764173\n",
            "Loss S2:  0.028405458881913224\n",
            "Loss S01:  0.02475019292654218\n",
            "Loss S2:  0.028398266309976827\n",
            "Loss S01:  0.02474201725921053\n",
            "Loss S2:  0.028393488126956763\n",
            "Validation: \n",
            " Loss S01:  0.024026889353990555\n",
            " Loss S2:  0.045524902641773224\n",
            " Loss S01:  0.025456215476705915\n",
            " Loss S2:  0.04923008825807344\n",
            " Loss S01:  0.02540201834607415\n",
            " Loss S2:  0.04905690116489806\n",
            " Loss S01:  0.025359688906884583\n",
            " Loss S2:  0.048618899505646504\n",
            " Loss S01:  0.0253955026237685\n",
            " Loss S2:  0.04843016253945268\n",
            "\n",
            "Epoch: 59\n",
            "Loss S01:  0.027124295011162758\n",
            "Loss S2:  0.029158685356378555\n",
            "Loss S01:  0.024491225453940304\n",
            "Loss S2:  0.02812786264853044\n",
            "Loss S01:  0.024754131035435768\n",
            "Loss S2:  0.0286093247788293\n",
            "Loss S01:  0.024663692340254784\n",
            "Loss S2:  0.028536180274621133\n",
            "Loss S01:  0.024720066522316234\n",
            "Loss S2:  0.028575973539817626\n",
            "Loss S01:  0.024726451502418985\n",
            "Loss S2:  0.02853203634274941\n",
            "Loss S01:  0.024648667175750264\n",
            "Loss S2:  0.028503721396698326\n",
            "Loss S01:  0.02466983862326179\n",
            "Loss S2:  0.028523194459332546\n",
            "Loss S01:  0.024593976889679462\n",
            "Loss S2:  0.02834947356655274\n",
            "Loss S01:  0.024525933561253023\n",
            "Loss S2:  0.028218951833608386\n",
            "Loss S01:  0.024556069404329403\n",
            "Loss S2:  0.028144733666783512\n",
            "Loss S01:  0.024568851651237893\n",
            "Loss S2:  0.028128782152995333\n",
            "Loss S01:  0.02458162391789196\n",
            "Loss S2:  0.02809539306447033\n",
            "Loss S01:  0.02453135255411381\n",
            "Loss S2:  0.028074534581476497\n",
            "Loss S01:  0.024547855984023276\n",
            "Loss S2:  0.028095669053653453\n",
            "Loss S01:  0.024568061831573777\n",
            "Loss S2:  0.028128155574992004\n",
            "Loss S01:  0.024542526081667182\n",
            "Loss S2:  0.028079364795862517\n",
            "Loss S01:  0.024576871323655224\n",
            "Loss S2:  0.028169729512685922\n",
            "Loss S01:  0.024593323623524847\n",
            "Loss S2:  0.02819803800660273\n",
            "Loss S01:  0.024563571642986767\n",
            "Loss S2:  0.028193926936044743\n",
            "Loss S01:  0.024541807041239382\n",
            "Loss S2:  0.02816480343503442\n",
            "Loss S01:  0.024561589354234283\n",
            "Loss S2:  0.028171872278737232\n",
            "Loss S01:  0.024575252347200163\n",
            "Loss S2:  0.02819043627152076\n",
            "Loss S01:  0.02458841379457738\n",
            "Loss S2:  0.02819368223865311\n",
            "Loss S01:  0.02458592547544305\n",
            "Loss S2:  0.028198683247959464\n",
            "Loss S01:  0.02457848289959222\n",
            "Loss S2:  0.02820071972075449\n",
            "Loss S01:  0.02458982590685859\n",
            "Loss S2:  0.028208738842343917\n",
            "Loss S01:  0.024560972030972204\n",
            "Loss S2:  0.02819939510298831\n",
            "Loss S01:  0.024537233291094415\n",
            "Loss S2:  0.028186565284945363\n",
            "Loss S01:  0.024547118296737933\n",
            "Loss S2:  0.028207675902704194\n",
            "Loss S01:  0.024560855775824023\n",
            "Loss S2:  0.028213205966987086\n",
            "Loss S01:  0.02455488091089143\n",
            "Loss S2:  0.028198552105300295\n",
            "Loss S01:  0.024558961124527864\n",
            "Loss S2:  0.028222970600672227\n",
            "Loss S01:  0.02457223733003586\n",
            "Loss S2:  0.028244488217004118\n",
            "Loss S01:  0.024594695938210332\n",
            "Loss S2:  0.028269995834208653\n",
            "Loss S01:  0.024610821189533953\n",
            "Loss S2:  0.02830138904947331\n",
            "Loss S01:  0.024606451129029993\n",
            "Loss S2:  0.028302373555475987\n",
            "Loss S01:  0.024603648522913937\n",
            "Loss S2:  0.028297031915814407\n",
            "Loss S01:  0.024610041819141292\n",
            "Loss S2:  0.028284144771146023\n",
            "Loss S01:  0.024591246505489436\n",
            "Loss S2:  0.028267591786773308\n",
            "Loss S01:  0.02460269558886786\n",
            "Loss S2:  0.028257601399745726\n",
            "Loss S01:  0.024590870057325583\n",
            "Loss S2:  0.028244172900443818\n",
            "Loss S01:  0.024590342312237147\n",
            "Loss S2:  0.02826125143348038\n",
            "Loss S01:  0.02457706875701515\n",
            "Loss S2:  0.028229316510692274\n",
            "Loss S01:  0.024578600036415384\n",
            "Loss S2:  0.02823069949302814\n",
            "Loss S01:  0.024598462296497002\n",
            "Loss S2:  0.02824771125076087\n",
            "Loss S01:  0.02458894000653331\n",
            "Loss S2:  0.02824120848174985\n",
            "Loss S01:  0.024589749381357443\n",
            "Loss S2:  0.0282418580563648\n",
            "Loss S01:  0.024589257279949227\n",
            "Loss S2:  0.02825817638880374\n",
            "Loss S01:  0.02457753527537747\n",
            "Loss S2:  0.028256542840308665\n",
            "Validation: \n",
            " Loss S01:  0.02458306774497032\n",
            " Loss S2:  0.04540998861193657\n",
            " Loss S01:  0.02612294309905597\n",
            " Loss S2:  0.04789013734885624\n",
            " Loss S01:  0.02609702363246825\n",
            " Loss S2:  0.04745581691584936\n",
            " Loss S01:  0.026024324788910445\n",
            " Loss S2:  0.046980610025710745\n",
            " Loss S01:  0.026062786418162745\n",
            " Loss S2:  0.04685826958329589\n",
            "\n",
            "Epoch: 60\n",
            "Loss S01:  0.028002046048641205\n",
            "Loss S2:  0.03101276606321335\n",
            "Loss S01:  0.02472885935143991\n",
            "Loss S2:  0.027964416044679554\n",
            "Loss S01:  0.024662225285456293\n",
            "Loss S2:  0.028083609150988714\n",
            "Loss S01:  0.024602996846360546\n",
            "Loss S2:  0.028132204507147113\n",
            "Loss S01:  0.024645865962999624\n",
            "Loss S2:  0.02822066243828797\n",
            "Loss S01:  0.02467915612985106\n",
            "Loss S2:  0.028203886367526708\n",
            "Loss S01:  0.024610416017106323\n",
            "Loss S2:  0.028198575081883885\n",
            "Loss S01:  0.02458809478811815\n",
            "Loss S2:  0.028163196230438392\n",
            "Loss S01:  0.024486912368440333\n",
            "Loss S2:  0.027937456275577897\n",
            "Loss S01:  0.024461808583729868\n",
            "Loss S2:  0.02792453950086793\n",
            "Loss S01:  0.024476708567673616\n",
            "Loss S2:  0.027859176794933802\n",
            "Loss S01:  0.02441592579959212\n",
            "Loss S2:  0.02779130268472809\n",
            "Loss S01:  0.024408351783910074\n",
            "Loss S2:  0.027809517458081245\n",
            "Loss S01:  0.024383427731635918\n",
            "Loss S2:  0.027818545628026242\n",
            "Loss S01:  0.024383984770335203\n",
            "Loss S2:  0.02785369447359802\n",
            "Loss S01:  0.024416687263064826\n",
            "Loss S2:  0.02791431879266998\n",
            "Loss S01:  0.024402715138489416\n",
            "Loss S2:  0.027894359822413936\n",
            "Loss S01:  0.024461799012551532\n",
            "Loss S2:  0.02796879905270554\n",
            "Loss S01:  0.02446336876498072\n",
            "Loss S2:  0.02799078848892154\n",
            "Loss S01:  0.024451110092679244\n",
            "Loss S2:  0.02798760402460061\n",
            "Loss S01:  0.024465025666711937\n",
            "Loss S2:  0.028003409968235006\n",
            "Loss S01:  0.024474424261476193\n",
            "Loss S2:  0.028009598827545678\n",
            "Loss S01:  0.024490317161199194\n",
            "Loss S2:  0.02800670836744535\n",
            "Loss S01:  0.024512626625823253\n",
            "Loss S2:  0.02801658691298136\n",
            "Loss S01:  0.024553779396711543\n",
            "Loss S2:  0.028032893631839157\n",
            "Loss S01:  0.024560041540113105\n",
            "Loss S2:  0.028066855213378532\n",
            "Loss S01:  0.02457993489001446\n",
            "Loss S2:  0.028085970330512387\n",
            "Loss S01:  0.024583480159349986\n",
            "Loss S2:  0.028074422211994545\n",
            "Loss S01:  0.02458228662716112\n",
            "Loss S2:  0.028063224336907958\n",
            "Loss S01:  0.02458205622624081\n",
            "Loss S2:  0.028055493300620633\n",
            "Loss S01:  0.024595079392788814\n",
            "Loss S2:  0.028062074683433354\n",
            "Loss S01:  0.024580250712788375\n",
            "Loss S2:  0.02805066669293922\n",
            "Loss S01:  0.02456840448592125\n",
            "Loss S2:  0.028054471461340276\n",
            "Loss S01:  0.02457403531720091\n",
            "Loss S2:  0.028064040365851178\n",
            "Loss S01:  0.024580883260958357\n",
            "Loss S2:  0.028088527204485233\n",
            "Loss S01:  0.024598837620512373\n",
            "Loss S2:  0.028122394215687047\n",
            "Loss S01:  0.02460081877388122\n",
            "Loss S2:  0.02813088137481021\n",
            "Loss S01:  0.024581782981713183\n",
            "Loss S2:  0.02812441294345091\n",
            "Loss S01:  0.024588650131593226\n",
            "Loss S2:  0.028115783468514604\n",
            "Loss S01:  0.024563823133478383\n",
            "Loss S2:  0.028095775129049635\n",
            "Loss S01:  0.024573698054599642\n",
            "Loss S2:  0.02810624935327474\n",
            "Loss S01:  0.024572552193110296\n",
            "Loss S2:  0.028099496397948905\n",
            "Loss S01:  0.024579472220425367\n",
            "Loss S2:  0.02811135295026376\n",
            "Loss S01:  0.02456423464790851\n",
            "Loss S2:  0.028092274252161074\n",
            "Loss S01:  0.02455612974845363\n",
            "Loss S2:  0.02809858182761945\n",
            "Loss S01:  0.02455852436368455\n",
            "Loss S2:  0.02809787008027544\n",
            "Loss S01:  0.024547451181809968\n",
            "Loss S2:  0.028092748600658263\n",
            "Loss S01:  0.02455010379006149\n",
            "Loss S2:  0.028086855612109926\n",
            "Loss S01:  0.024555653081955137\n",
            "Loss S2:  0.028091594371596147\n",
            "Loss S01:  0.024543834341853067\n",
            "Loss S2:  0.02808873089459059\n",
            "Validation: \n",
            " Loss S01:  0.024318894371390343\n",
            " Loss S2:  0.04320594668388367\n",
            " Loss S01:  0.025476477774126188\n",
            " Loss S2:  0.04619402225528445\n",
            " Loss S01:  0.02545142350945531\n",
            " Loss S2:  0.04606989362254375\n",
            " Loss S01:  0.025387687578064498\n",
            " Loss S2:  0.04564802061583175\n",
            " Loss S01:  0.025378459328670562\n",
            " Loss S2:  0.045539350191384186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create ensamble model\n",
        "\n",
        "1.8 In this step you will create a new network class that takes s1, and s2 as perimeters. This class should initiate a new network that ensembles both s1 and s2, and have a classifier for cross-entropy. In the forward method pass the input x from both s1 and s2 and then concatenate there outputs along axis 1. Then pass this concatinated output through classifier of appropriate shape. "
      ],
      "metadata": {
        "id": "25tcDBBIu2H2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, s1,s2):\n",
        "        super(Net, self).__init__()\n",
        "        self.s1 = s1\n",
        "        self.s2 = s2\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.s1(x)\n",
        "        out2 = self.s2(x)\n",
        "        out = torch.cat((out1,out2),1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "net = Net(s01,s2)\n",
        "net.to(device)\n",
        "summary(net, (3, 32, 32))"
      ],
      "metadata": {
        "id": "EpzuTHRovW1T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d38b8cd1-54df-47ad-af03-675d0dbd80e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 256, 1, 1]               0\n",
            "           Linear-31                  [-1, 256]          65,792\n",
            "              VGG-32                  [-1, 256]               0\n",
            "           Conv2d-33           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-34           [-1, 32, 32, 32]              64\n",
            "             ReLU-35           [-1, 32, 32, 32]               0\n",
            "           Conv2d-36           [-1, 32, 32, 32]           9,248\n",
            "      BatchNorm2d-37           [-1, 32, 32, 32]              64\n",
            "             ReLU-38           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-39           [-1, 32, 16, 16]               0\n",
            "           Conv2d-40           [-1, 64, 16, 16]          18,496\n",
            "      BatchNorm2d-41           [-1, 64, 16, 16]             128\n",
            "             ReLU-42           [-1, 64, 16, 16]               0\n",
            "           Conv2d-43           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-44           [-1, 64, 16, 16]             128\n",
            "             ReLU-45           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-46             [-1, 64, 8, 8]               0\n",
            "           Conv2d-47            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-48            [-1, 128, 8, 8]             256\n",
            "             ReLU-49            [-1, 128, 8, 8]               0\n",
            "           Conv2d-50            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-51            [-1, 128, 8, 8]             256\n",
            "             ReLU-52            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-53            [-1, 128, 4, 4]               0\n",
            "           Conv2d-54            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-55            [-1, 256, 4, 4]             512\n",
            "             ReLU-56            [-1, 256, 4, 4]               0\n",
            "           Conv2d-57            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-58            [-1, 256, 4, 4]             512\n",
            "             ReLU-59            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-60            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-61            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-62            [-1, 256, 1, 1]               0\n",
            "           Linear-63                  [-1, 256]          65,792\n",
            "              VGG-64                  [-1, 256]               0\n",
            "           Linear-65                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 2,485,066\n",
            "Trainable params: 2,485,066\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 5.88\n",
            "Params size (MB): 9.48\n",
            "Estimated Total Size (MB): 15.37\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train The Ensambled network\n",
        "1.9 In this step you will freez all the conv layers in the ensambled network and then finetune it on orignal dataset. "
      ],
      "metadata": {
        "id": "qNFSWlD5wvZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in net.modules():\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        m.weight.requires_grad = False\n",
        "        if m.bias is not None:\n",
        "            m.bias.requires_grad = False\n",
        "net = net.to(device)\n",
        "summary(net, (3, 32, 32))"
      ],
      "metadata": {
        "id": "tbZ9_90YxCwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83b8107f-3605-4e6a-f181-34a575738016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 256, 1, 1]               0\n",
            "           Linear-31                  [-1, 256]          65,792\n",
            "              VGG-32                  [-1, 256]               0\n",
            "           Conv2d-33           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-34           [-1, 32, 32, 32]              64\n",
            "             ReLU-35           [-1, 32, 32, 32]               0\n",
            "           Conv2d-36           [-1, 32, 32, 32]           9,248\n",
            "      BatchNorm2d-37           [-1, 32, 32, 32]              64\n",
            "             ReLU-38           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-39           [-1, 32, 16, 16]               0\n",
            "           Conv2d-40           [-1, 64, 16, 16]          18,496\n",
            "      BatchNorm2d-41           [-1, 64, 16, 16]             128\n",
            "             ReLU-42           [-1, 64, 16, 16]               0\n",
            "           Conv2d-43           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-44           [-1, 64, 16, 16]             128\n",
            "             ReLU-45           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-46             [-1, 64, 8, 8]               0\n",
            "           Conv2d-47            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-48            [-1, 128, 8, 8]             256\n",
            "             ReLU-49            [-1, 128, 8, 8]               0\n",
            "           Conv2d-50            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-51            [-1, 128, 8, 8]             256\n",
            "             ReLU-52            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-53            [-1, 128, 4, 4]               0\n",
            "           Conv2d-54            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-55            [-1, 256, 4, 4]             512\n",
            "             ReLU-56            [-1, 256, 4, 4]               0\n",
            "           Conv2d-57            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-58            [-1, 256, 4, 4]             512\n",
            "             ReLU-59            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-60            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-61            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-62            [-1, 256, 1, 1]               0\n",
            "           Linear-63                  [-1, 256]          65,792\n",
            "              VGG-64                  [-1, 256]               0\n",
            "           Linear-65                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 2,485,066\n",
            "Trainable params: 140,554\n",
            "Non-trainable params: 2,344,512\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 5.88\n",
            "Params size (MB): 9.48\n",
            "Estimated Total Size (MB): 15.37\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        net.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if(batch_idx % 200 == 0):\n",
        "          print(\"Accuracy : \",100.*correct/total,\" Loss : \", train_loss/(batch_idx+1))\n",
        "def test(epoch):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\"Accuracy : \",100.*correct/total,\" Loss : \", test_loss/(batch_idx+1))"
      ],
      "metadata": {
        "id": "2SzjCW-6xHIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "    train(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test(epoch)"
      ],
      "metadata": {
        "id": "gxz8dPNXxMKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "130baf3e-ae55-40a6-cb19-d79bdb959ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Accuracy :  4.0  Loss :  2.5721871852874756\n",
            "Accuracy :  75.68656716417911  Loss :  1.0726231840712515\n",
            "Accuracy :  80.09476309226933  Loss :  0.7963963118277286\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4607113301753998\n",
            "Accuracy :  84.85714285714286  Loss :  0.465903381506602\n",
            "Accuracy :  84.09756097560975  Loss :  0.4800547274147592\n",
            "Accuracy :  83.8688524590164  Loss :  0.47959929997803735\n",
            "Accuracy :  83.93827160493827  Loss :  0.4771355927726369\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy :  85.0  Loss :  0.4412436783313751\n",
            "Accuracy :  85.17910447761194  Loss :  0.4412178424608648\n",
            "Accuracy :  85.31920199501246  Loss :  0.43435492126870334\n",
            "Validation: \n",
            "Accuracy :  85.0  Loss :  0.4136185050010681\n",
            "Accuracy :  85.04761904761905  Loss :  0.4412853866815567\n",
            "Accuracy :  84.36585365853658  Loss :  0.4556760987857493\n",
            "Accuracy :  84.36065573770492  Loss :  0.4555130300463223\n",
            "Accuracy :  84.41975308641975  Loss :  0.4533260306458414\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy :  85.0  Loss :  0.3970857560634613\n",
            "Accuracy :  85.68656716417911  Loss :  0.4196414703605187\n",
            "Accuracy :  85.85785536159601  Loss :  0.41686121833294704\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.39347589015960693\n",
            "Accuracy :  85.38095238095238  Loss :  0.4357900420824687\n",
            "Accuracy :  84.58536585365853  Loss :  0.44972243687001673\n",
            "Accuracy :  84.47540983606558  Loss :  0.44949315708191667\n",
            "Accuracy :  84.61728395061728  Loss :  0.4468659944372413\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy :  88.0  Loss :  0.41201716661453247\n",
            "Accuracy :  85.75124378109453  Loss :  0.4113863756259282\n",
            "Accuracy :  85.86284289276809  Loss :  0.41167078109602084\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.3844156265258789\n",
            "Accuracy :  85.47619047619048  Loss :  0.4317566880158016\n",
            "Accuracy :  84.78048780487805  Loss :  0.44450699701541807\n",
            "Accuracy :  84.80327868852459  Loss :  0.44399428269902214\n",
            "Accuracy :  84.8641975308642  Loss :  0.4413225327009036\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy :  84.0  Loss :  0.3908263146877289\n",
            "Accuracy :  86.09452736318408  Loss :  0.4052262222440682\n",
            "Accuracy :  86.12967581047381  Loss :  0.4052075294410796\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.37669241428375244\n",
            "Accuracy :  85.71428571428571  Loss :  0.4290049246379307\n",
            "Accuracy :  84.95121951219512  Loss :  0.4423333878924207\n",
            "Accuracy :  84.98360655737704  Loss :  0.44188638788754825\n",
            "Accuracy :  85.0246913580247  Loss :  0.43925461761745405\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy :  85.0  Loss :  0.37235745787620544\n",
            "Accuracy :  86.00995024875621  Loss :  0.4043276545716755\n",
            "Accuracy :  86.07231920199501  Loss :  0.4028383334750249\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.3694181740283966\n",
            "Accuracy :  85.71428571428571  Loss :  0.4274602135022481\n",
            "Accuracy :  84.92682926829268  Loss :  0.44023559078937624\n",
            "Accuracy :  84.93442622950819  Loss :  0.4398103356361389\n",
            "Accuracy :  85.03703703703704  Loss :  0.4369704841095724\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy :  88.0  Loss :  0.3660813868045807\n",
            "Accuracy :  86.16417910447761  Loss :  0.3971857976409333\n",
            "Accuracy :  86.41147132169576  Loss :  0.3966730209880339\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.3658735752105713\n",
            "Accuracy :  85.66666666666667  Loss :  0.4255982999290739\n",
            "Accuracy :  84.97560975609755  Loss :  0.4380195725981782\n",
            "Accuracy :  85.04918032786885  Loss :  0.4375001253163228\n",
            "Accuracy :  85.19753086419753  Loss :  0.4345691161759106\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy :  88.0  Loss :  0.3301312327384949\n",
            "Accuracy :  86.59701492537313  Loss :  0.3936046207722147\n",
            "Accuracy :  86.57356608478803  Loss :  0.394090998016688\n",
            "Validation: \n",
            "Accuracy :  88.0  Loss :  0.3632335960865021\n",
            "Accuracy :  85.71428571428571  Loss :  0.42558123100371587\n",
            "Accuracy :  85.04878048780488  Loss :  0.43806803335503836\n",
            "Accuracy :  85.08196721311475  Loss :  0.4366484628837617\n",
            "Accuracy :  85.22222222222223  Loss :  0.43383698533346626\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy :  88.0  Loss :  0.40162062644958496\n",
            "Accuracy :  86.39800995024876  Loss :  0.39229771635722166\n",
            "Accuracy :  86.43142144638404  Loss :  0.39402289302123156\n",
            "Validation: \n",
            "Accuracy :  88.0  Loss :  0.3560199737548828\n",
            "Accuracy :  85.66666666666667  Loss :  0.4226802658467066\n",
            "Accuracy :  85.02439024390245  Loss :  0.4350594285784698\n",
            "Accuracy :  85.06557377049181  Loss :  0.43423403457539983\n",
            "Accuracy :  85.24691358024691  Loss :  0.4314447871328872\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy :  85.0  Loss :  0.3517827093601227\n",
            "Accuracy :  86.68159203980099  Loss :  0.3877210026949792\n",
            "Accuracy :  86.6359102244389  Loss :  0.3890945977552276\n",
            "Validation: \n",
            "Accuracy :  88.0  Loss :  0.35661420226097107\n",
            "Accuracy :  85.76190476190476  Loss :  0.4212583977551687\n",
            "Accuracy :  85.09756097560975  Loss :  0.43346429452663515\n",
            "Accuracy :  85.18032786885246  Loss :  0.43234133524972884\n",
            "Accuracy :  85.32098765432099  Loss :  0.4296468523917375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_name = 'ss1student.pt'\n",
        "path = F\"/content/gdrive/MyDrive/{model_save_name}\" \n",
        "torch.save(s01.state_dict(), path)\n",
        "# #s01.load_state_dict(torch.load(path))\n",
        "model_save_name = 'ss2student.pt'\n",
        "path = F\"/content/gdrive/MyDrive/{model_save_name}\" \n",
        "torch.save(s2.state_dict(), path)\n",
        "# #s2.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "_TF6UDYswobN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Create** **4** **More Students.**\n",
        "\n"
      ],
      "metadata": {
        "id": "42aMmjCt8EOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "    'VGGS1': [32, 32, 'M', 64, 64, 'M', 128, 128, 'M', 256, 256, 'M','M'],\n",
        "    'VGGS33': [32, 32, 'M', 64, 64, 'M', 128, 'M','M','M'],\n",
        "    'VGGS2':[32,'M', 32, 'M', 64, 64, 'M', 128,128,'M',128,128, 'M'],\n",
        "    \n",
        "\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(128, 128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "s11 = VGG('VGGS2')\n",
        "s11 = s11.to(device)\n",
        "summary(s11, (3, 32, 32))\n",
        "s22 = VGG('VGGS2')\n",
        "s22 = s22.to(device)\n",
        "summary(s22, (3,32,32))\n",
        "s33 = VGG('VGGS2')\n",
        "s33 = s33.to(device)\n",
        "summary(s33, (3, 32, 32))\n",
        "s44 = VGG('VGGS2')\n",
        "s44 = s44.to(device)\n",
        "summary(s44, (3, 32, 32))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrL4hr--x0Hg",
        "outputId": "614a7c4e-9ff8-4292-9e0e-79e6c0b18df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "================================================================\n",
            "Total params: 600,096\n",
            "Trainable params: 600,096\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.33\n",
            "Params size (MB): 2.29\n",
            "Estimated Total Size (MB): 3.64\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "================================================================\n",
            "Total params: 600,096\n",
            "Trainable params: 600,096\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.33\n",
            "Params size (MB): 2.29\n",
            "Estimated Total Size (MB): 3.64\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "================================================================\n",
            "Total params: 600,096\n",
            "Trainable params: 600,096\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.33\n",
            "Params size (MB): 2.29\n",
            "Estimated Total Size (MB): 3.64\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "================================================================\n",
            "Total params: 600,096\n",
            "Trainable params: 600,096\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.33\n",
            "Params size (MB): 2.29\n",
            "Estimated Total Size (MB): 3.64\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TA_WOH = nn.Sequential(*list(s01.children())[:-1],nn.Flatten())\n",
        "# #TA_WOH = nn.Sequential(*list(s1.children())[:],nn.Flatten())\n",
        "# summary(s01, (3, 32, 32))\n",
        "# summary(TA_WOH, (3, 32, 32))"
      ],
      "metadata": {
        "id": "Jq3XMEdH0kYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TA2_WOH = nn.Sequential(*list(s2.children())[:-1],nn.Flatten())\n",
        "# summary(s2, (3, 32, 32))\n",
        "# summary(TA2_WOH, (3, 32, 32))"
      ],
      "metadata": {
        "id": "buiyd5tGEY4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # TA_WOH.eval()\n",
        "# # TA2_WOH.eval()\n",
        "# # s1.eval()\n",
        "# # s2.eval()\n",
        "# TADenseTrain = None\n",
        "# TADenseTest = None\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "#         inputs, targets = inputs.to(device), targets.to(device)\n",
        "#         outputs1 = TA_WOH(inputs)\n",
        "#         outputs2 = TA2_WOH(inputs)\n",
        "#         if(TADenseTrain == None):\n",
        "#             TADenseTrain = torch.cat((outputs1,outputs2),1) \n",
        "#         else:\n",
        "#             totalOUTPUT = torch.cat((outputs1,outputs2),1)         \n",
        "#             TADenseTrain = torch.cat((TADenseTrain,totalOUTPUT))\n",
        "           \n",
        "# with torch.no_grad():\n",
        "#     for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "#         inputs, targets = inputs.to(device), targets.to(device)\n",
        "#         outputs1 = TA_WOH(inputs)\n",
        "#         outputs2 = TA2_WOH(inputs)\n",
        "#         if(TADenseTest == None):\n",
        "#             TADenseTest = torch.cat((outputs1,outputs2),1)\n",
        "#         else:\n",
        "#             totalOUTPUT = torch.cat((outputs1,outputs2),1)         \n",
        "#             TADenseTest = torch.cat((TADenseTest,totalOUTPUT))\n",
        "           "
      ],
      "metadata": {
        "id": "0CQM3bJZFE70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(TADenseTrain.shape)  "
      ],
      "metadata": {
        "id": "vFUqixLL0tYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer1 = optim.Adam(s11.parameters(), lr=0.0001)\n",
        "optimizer2 = optim.Adam(s22.parameters(), lr=0.0001)\n",
        "optimizer3 = optim.Adam(s33.parameters(), lr=0.0001)\n",
        "optimizer4 = optim.Adam(s44.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def train4(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    s11.train()\n",
        "    train_loss1 = 0\n",
        "    train_loss2 = 0\n",
        "    train_loss3 = 0\n",
        "    train_loss4= 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        #code here\n",
        "        targets = S1DenseTrain[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "        targets.to(device)\n",
        "        s11.zero_grad()\n",
        "        s22.zero_grad()\n",
        "        s33.zero_grad()\n",
        "        s44.zero_grad()\n",
        "        output1 = s11(inputs)\n",
        "        output2 = s22(inputs)\n",
        "        output3 = s33(inputs)\n",
        "        output4 = s44(inputs)\n",
        "        \n",
        "        loss1 = criterion(output1, targets[:,:128])\n",
        "        loss2 = criterion(output2, targets[:,128:256])\n",
        "        loss3 = criterion(output3, targets[:,256:384])\n",
        "        loss4 = criterion(output4, targets[:,384:512])\n",
        "        loss1.backward()\n",
        "        loss2.backward()\n",
        "        loss3.backward()\n",
        "        loss4.backward()\n",
        "        optimizer1.step()\n",
        "        optimizer2.step()\n",
        "        optimizer3.step()\n",
        "        optimizer4.step()\n",
        "      \n",
        "\n",
        "        train_loss1 += loss1.item()\n",
        "        train_loss2 += loss2.item()\n",
        "        train_loss3 += loss3.item()\n",
        "        train_loss4 += loss4.item()\n",
        "        if(batch_idx % 10 == 0):\n",
        "          print(\"Loss S11: \", train_loss1/(batch_idx+1))\n",
        "          print(\"Loss S22: \", train_loss2/(batch_idx+1))\n",
        "          print(\"Loss S33: \", train_loss3/(batch_idx+1))\n",
        "          print(\"Loss S44: \", train_loss4/(batch_idx+1))\n",
        "def test4(epoch):\n",
        "    s11.eval()\n",
        "    \n",
        "    test_loss1 = 0\n",
        "    test_loss2 = 0\n",
        "    test_loss3 = 0\n",
        "    test_loss4= 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            #code here\n",
        "            targets = s1DenseTest[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "            targets.to(device)\n",
        "            output1 = s11(inputs)\n",
        "            output2 = s22(inputs)\n",
        "            output3 = s33(inputs)\n",
        "            output4 = s44(inputs)\n",
        "            loss1 = criterion(output1, targets[:,:128])\n",
        "            loss2 = criterion(output2, targets[:,128:256])\n",
        "            loss3 = criterion(output3, targets[:,256:384])\n",
        "            loss4 = criterion(output4, targets[:,384:512])\n",
        "            test_loss1 += loss1.item()\n",
        "            test_loss2 += loss2.item()\n",
        "            test_loss3 += loss3.item()\n",
        "            test_loss4 += loss4.item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\" Loss S11: \", test_loss1/(batch_idx+1))\n",
        "              print(\" Loss S22: \", test_loss2/(batch_idx+1))\n",
        "              print(\" Loss S33: \", test_loss3/(batch_idx+1))\n",
        "              print(\" Loss S44: \", test_loss4/(batch_idx+1))"
      ],
      "metadata": {
        "id": "e_zhZ-fv0vse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+60):\n",
        "    train4(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test4(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5b7RheK0x0Y",
        "outputId": "732ee246-6405-4c48-a842-5e9773e70beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Loss S44:  0.041593300783557524\n",
            "Loss S11:  0.03358945997310019\n",
            "Loss S22:  0.0361513559100134\n",
            "Loss S33:  0.03852142209065295\n",
            "Loss S44:  0.04159874789347159\n",
            "Loss S11:  0.03355841392094095\n",
            "Loss S22:  0.03615825169148582\n",
            "Loss S33:  0.03850166783231983\n",
            "Loss S44:  0.041638054516380286\n",
            "Loss S11:  0.03358996336148981\n",
            "Loss S22:  0.03622488099043845\n",
            "Loss S33:  0.03852689967564474\n",
            "Loss S44:  0.041659492660652504\n",
            "Loss S11:  0.03360727827391054\n",
            "Loss S22:  0.03627605151990031\n",
            "Loss S33:  0.0385355099377639\n",
            "Loss S44:  0.041699970517827575\n",
            "Loss S11:  0.03362002729894877\n",
            "Loss S22:  0.03627966703986362\n",
            "Loss S33:  0.038523078594263904\n",
            "Loss S44:  0.04168877383265799\n",
            "Loss S11:  0.033613092183183146\n",
            "Loss S22:  0.036250844070652745\n",
            "Loss S33:  0.038469527840533986\n",
            "Loss S44:  0.04164751606046993\n",
            "Loss S11:  0.033590202295561165\n",
            "Loss S22:  0.03625302219621622\n",
            "Loss S33:  0.038446644345528184\n",
            "Loss S44:  0.041655483491855654\n",
            "Loss S11:  0.033559720644065184\n",
            "Loss S22:  0.03622858635985943\n",
            "Loss S33:  0.03842796857857034\n",
            "Loss S44:  0.041646519089903675\n",
            "Loss S11:  0.033583568339410265\n",
            "Loss S22:  0.03624550455674864\n",
            "Loss S33:  0.038447566228242885\n",
            "Loss S44:  0.04165204210285831\n",
            "Loss S11:  0.03357994928049635\n",
            "Loss S22:  0.036257498709534784\n",
            "Loss S33:  0.03845059434790391\n",
            "Loss S44:  0.04165242283339918\n",
            "Loss S11:  0.03359744424827733\n",
            "Loss S22:  0.03625120879139866\n",
            "Loss S33:  0.03845000133413883\n",
            "Loss S44:  0.04166403878258979\n",
            "Loss S11:  0.03357842754736854\n",
            "Loss S22:  0.036244676934594065\n",
            "Loss S33:  0.03843782059934742\n",
            "Loss S44:  0.041651592445138434\n",
            "Loss S11:  0.033556613458696406\n",
            "Loss S22:  0.03623581473248886\n",
            "Loss S33:  0.038421423991723935\n",
            "Loss S44:  0.0416370548155843\n",
            "Loss S11:  0.0335560978334521\n",
            "Loss S22:  0.03622100763270702\n",
            "Loss S33:  0.03840027853615268\n",
            "Loss S44:  0.04163192453775596\n",
            "Loss S11:  0.03355881509546861\n",
            "Loss S22:  0.03620091213192185\n",
            "Loss S33:  0.03839247934762671\n",
            "Loss S44:  0.04161047845787961\n",
            "Loss S11:  0.03355151112735145\n",
            "Loss S22:  0.03621748247185837\n",
            "Loss S33:  0.03837332278964626\n",
            "Loss S44:  0.04160803352381773\n",
            "Loss S11:  0.03353467620444521\n",
            "Loss S22:  0.03621031600292656\n",
            "Loss S33:  0.03836763822115385\n",
            "Loss S44:  0.04159832835631162\n",
            "Loss S11:  0.03349680246030362\n",
            "Loss S22:  0.03618232365659686\n",
            "Loss S33:  0.03833184882148706\n",
            "Loss S44:  0.04155963927871584\n",
            "Validation: \n",
            " Loss S11:  0.03083193674683571\n",
            " Loss S22:  0.045361727476119995\n",
            " Loss S33:  0.04165210574865341\n",
            " Loss S44:  0.04690602049231529\n",
            " Loss S11:  0.032110615589079405\n",
            " Loss S22:  0.050637592162404745\n",
            " Loss S33:  0.05115940404080209\n",
            " Loss S44:  0.05467312942658152\n",
            " Loss S11:  0.032193767452021925\n",
            " Loss S22:  0.05093409875180663\n",
            " Loss S33:  0.05153434478291651\n",
            " Loss S44:  0.05397019553475264\n",
            " Loss S11:  0.032175989791018066\n",
            " Loss S22:  0.05060508315924738\n",
            " Loss S33:  0.050921658023459014\n",
            " Loss S44:  0.05388926995582268\n",
            " Loss S11:  0.03205575529531932\n",
            " Loss S22:  0.05010533171855373\n",
            " Loss S33:  0.050835794780725314\n",
            " Loss S44:  0.05371314687309442\n",
            "\n",
            "Epoch: 39\n",
            "Loss S11:  0.03502529487013817\n",
            "Loss S22:  0.03954794630408287\n",
            "Loss S33:  0.041591446846723557\n",
            "Loss S44:  0.04204680770635605\n",
            "Loss S11:  0.031903153454715553\n",
            "Loss S22:  0.03493132662366737\n",
            "Loss S33:  0.03816352039575577\n",
            "Loss S44:  0.04202885553240776\n",
            "Loss S11:  0.03295304643965903\n",
            "Loss S22:  0.035251282066816374\n",
            "Loss S33:  0.038230015231030326\n",
            "Loss S44:  0.04173064533443678\n",
            "Loss S11:  0.032987218770769336\n",
            "Loss S22:  0.03554172555525457\n",
            "Loss S33:  0.03796937941543518\n",
            "Loss S44:  0.04141226350780456\n",
            "Loss S11:  0.03336413091094029\n",
            "Loss S22:  0.035638132246165743\n",
            "Loss S33:  0.037894405788037835\n",
            "Loss S44:  0.04128358504030763\n",
            "Loss S11:  0.033450235667474126\n",
            "Loss S22:  0.03588302276444202\n",
            "Loss S33:  0.038049266034481576\n",
            "Loss S44:  0.04147409574658263\n",
            "Loss S11:  0.03334141458522101\n",
            "Loss S22:  0.035803653574625\n",
            "Loss S33:  0.03821449055046332\n",
            "Loss S44:  0.041306066586345926\n",
            "Loss S11:  0.033406407532977384\n",
            "Loss S22:  0.03590448009191265\n",
            "Loss S33:  0.03828527096291663\n",
            "Loss S44:  0.041377288302485375\n",
            "Loss S11:  0.033254663540441314\n",
            "Loss S22:  0.03583410895073119\n",
            "Loss S33:  0.03821948005093469\n",
            "Loss S44:  0.04121575145809739\n",
            "Loss S11:  0.0331443479018552\n",
            "Loss S22:  0.0357529163401533\n",
            "Loss S33:  0.03816413854832178\n",
            "Loss S44:  0.04112856802377072\n",
            "Loss S11:  0.033021319325607604\n",
            "Loss S22:  0.03572860907873895\n",
            "Loss S33:  0.03801229293688689\n",
            "Loss S44:  0.04104112630875984\n",
            "Loss S11:  0.03304117416274977\n",
            "Loss S22:  0.03573889003412144\n",
            "Loss S33:  0.037890708459927154\n",
            "Loss S44:  0.04096616623369423\n",
            "Loss S11:  0.033099655385229215\n",
            "Loss S22:  0.03576894917271354\n",
            "Loss S33:  0.03793183584843785\n",
            "Loss S44:  0.04089427945658195\n",
            "Loss S11:  0.03317624064631589\n",
            "Loss S22:  0.03586042493240524\n",
            "Loss S33:  0.03798090444721339\n",
            "Loss S44:  0.04103864430812479\n",
            "Loss S11:  0.03320538021412724\n",
            "Loss S22:  0.035943345873491135\n",
            "Loss S33:  0.03806644173484322\n",
            "Loss S44:  0.04108439293419216\n",
            "Loss S11:  0.03326539864257866\n",
            "Loss S22:  0.03593473008136876\n",
            "Loss S33:  0.03810939663984128\n",
            "Loss S44:  0.04116756172961747\n",
            "Loss S11:  0.03329360319636993\n",
            "Loss S22:  0.03586894836022247\n",
            "Loss S33:  0.03813464333366903\n",
            "Loss S44:  0.04120048984700108\n",
            "Loss S11:  0.0333770172594235\n",
            "Loss S22:  0.03593272678771911\n",
            "Loss S33:  0.038204249800645816\n",
            "Loss S44:  0.04131684171264632\n",
            "Loss S11:  0.03341350226935761\n",
            "Loss S22:  0.0359936840214782\n",
            "Loss S33:  0.038223437985497946\n",
            "Loss S44:  0.041372526210645286\n",
            "Loss S11:  0.03339359683048038\n",
            "Loss S22:  0.03594590762522832\n",
            "Loss S33:  0.03821473693504383\n",
            "Loss S44:  0.041364125010230776\n",
            "Loss S11:  0.03338244450811426\n",
            "Loss S22:  0.03593156872252327\n",
            "Loss S33:  0.038216001451460285\n",
            "Loss S44:  0.041360973755815135\n",
            "Loss S11:  0.03342876097814167\n",
            "Loss S22:  0.03596671644192171\n",
            "Loss S33:  0.03823029961419332\n",
            "Loss S44:  0.041465598948617684\n",
            "Loss S11:  0.03344088102526525\n",
            "Loss S22:  0.03595628039869248\n",
            "Loss S33:  0.03823227890476382\n",
            "Loss S44:  0.04147642080533019\n",
            "Loss S11:  0.03346567282461243\n",
            "Loss S22:  0.03593343280223541\n",
            "Loss S33:  0.03821513603453512\n",
            "Loss S44:  0.041443018637823334\n",
            "Loss S11:  0.03352272077714754\n",
            "Loss S22:  0.035997582359927324\n",
            "Loss S33:  0.03828828886955111\n",
            "Loss S44:  0.04146604569188292\n",
            "Loss S11:  0.033524743314698874\n",
            "Loss S22:  0.03599060635523967\n",
            "Loss S33:  0.03829463175034143\n",
            "Loss S44:  0.04149220956571074\n",
            "Loss S11:  0.03350992983214243\n",
            "Loss S22:  0.03597143433731178\n",
            "Loss S33:  0.03827880021769882\n",
            "Loss S44:  0.04144258862825189\n",
            "Loss S11:  0.033500138195953245\n",
            "Loss S22:  0.03595605768895677\n",
            "Loss S33:  0.038266885365837174\n",
            "Loss S44:  0.04142694491411927\n",
            "Loss S11:  0.033478601560579926\n",
            "Loss S22:  0.03594369579809204\n",
            "Loss S33:  0.03821601371323935\n",
            "Loss S44:  0.041391000151634216\n",
            "Loss S11:  0.03347846586577261\n",
            "Loss S22:  0.03592902274091834\n",
            "Loss S33:  0.03821825929933397\n",
            "Loss S44:  0.04140174949599296\n",
            "Loss S11:  0.033484663486332195\n",
            "Loss S22:  0.035946001239937804\n",
            "Loss S33:  0.03823233467597898\n",
            "Loss S44:  0.041410589485469454\n",
            "Loss S11:  0.033453278620862115\n",
            "Loss S22:  0.035944795319840454\n",
            "Loss S33:  0.038196034358149555\n",
            "Loss S44:  0.04139319674330509\n",
            "Loss S11:  0.03342868069714846\n",
            "Loss S22:  0.03592340609296646\n",
            "Loss S33:  0.03818947537526535\n",
            "Loss S44:  0.04138903909383162\n",
            "Loss S11:  0.0334150254658882\n",
            "Loss S22:  0.035912675963112595\n",
            "Loss S33:  0.03815520301551257\n",
            "Loss S44:  0.04142601504426709\n",
            "Loss S11:  0.033456727731262834\n",
            "Loss S22:  0.03595596813539431\n",
            "Loss S33:  0.03821293084371474\n",
            "Loss S44:  0.04147178391295095\n",
            "Loss S11:  0.03346940955184294\n",
            "Loss S22:  0.03601620644012578\n",
            "Loss S33:  0.03825978667308123\n",
            "Loss S44:  0.041515860515527234\n",
            "Loss S11:  0.03345393322903529\n",
            "Loss S22:  0.036028966999037446\n",
            "Loss S33:  0.038249488981889554\n",
            "Loss S44:  0.04151487338080631\n",
            "Loss S11:  0.03342801393725159\n",
            "Loss S22:  0.03601770212426982\n",
            "Loss S33:  0.03823617901000372\n",
            "Loss S44:  0.04149801836341539\n",
            "Loss S11:  0.0334187662986711\n",
            "Loss S22:  0.03601724591859057\n",
            "Loss S33:  0.03820976516197672\n",
            "Loss S44:  0.0414756987527443\n",
            "Loss S11:  0.03339579009243747\n",
            "Loss S22:  0.035994941706928754\n",
            "Loss S33:  0.03817177984072729\n",
            "Loss S44:  0.04145703923976635\n",
            "Loss S11:  0.033410274277341334\n",
            "Loss S22:  0.036040678016488394\n",
            "Loss S33:  0.03819060851436601\n",
            "Loss S44:  0.04147351408688505\n",
            "Loss S11:  0.03340939487219582\n",
            "Loss S22:  0.03603696136977841\n",
            "Loss S33:  0.038167345770374124\n",
            "Loss S44:  0.04150416591451696\n",
            "Loss S11:  0.03342401879594049\n",
            "Loss S22:  0.03602853951856246\n",
            "Loss S33:  0.03818275539686731\n",
            "Loss S44:  0.04150366701734321\n",
            "Loss S11:  0.03340470238119848\n",
            "Loss S22:  0.03599515012757131\n",
            "Loss S33:  0.03815807183456808\n",
            "Loss S44:  0.041465825131141394\n",
            "Loss S11:  0.03338636774274092\n",
            "Loss S22:  0.03601619378239119\n",
            "Loss S33:  0.03816646545314464\n",
            "Loss S44:  0.04146780675940232\n",
            "Loss S11:  0.033389459704843964\n",
            "Loss S22:  0.0360220088860941\n",
            "Loss S33:  0.038179491351290444\n",
            "Loss S44:  0.04145740634222781\n",
            "Loss S11:  0.0333857610377275\n",
            "Loss S22:  0.036008764381587115\n",
            "Loss S33:  0.038167771008552544\n",
            "Loss S44:  0.04144473547629059\n",
            "Loss S11:  0.0333695609375613\n",
            "Loss S22:  0.0360092172308206\n",
            "Loss S33:  0.0381511038924605\n",
            "Loss S44:  0.041444222947024996\n",
            "Loss S11:  0.03337509811637059\n",
            "Loss S22:  0.03600837701540479\n",
            "Loss S33:  0.038164433409419225\n",
            "Loss S44:  0.041435650784111815\n",
            "Loss S11:  0.033357732938773275\n",
            "Loss S22:  0.03597253724143607\n",
            "Loss S33:  0.03813606067399386\n",
            "Loss S44:  0.04140810135850596\n",
            "Validation: \n",
            " Loss S11:  0.031090673059225082\n",
            " Loss S22:  0.0453016459941864\n",
            " Loss S33:  0.04110084846615791\n",
            " Loss S44:  0.04607205465435982\n",
            " Loss S11:  0.032573086431338674\n",
            " Loss S22:  0.05011327617934772\n",
            " Loss S33:  0.050359416930448445\n",
            " Loss S44:  0.05476983015735944\n",
            " Loss S11:  0.03236014527700296\n",
            " Loss S22:  0.05051139051594385\n",
            " Loss S33:  0.05074615485784484\n",
            " Loss S44:  0.05423851701907995\n",
            " Loss S11:  0.032325828356332464\n",
            " Loss S22:  0.050093260761655746\n",
            " Loss S33:  0.05006395939920769\n",
            " Loss S44:  0.054185674083037455\n",
            " Loss S11:  0.03229604003789984\n",
            " Loss S22:  0.04958119197392169\n",
            " Loss S33:  0.05005557692529243\n",
            " Loss S44:  0.053983685227087984\n",
            "\n",
            "Epoch: 40\n",
            "Loss S11:  0.03619907796382904\n",
            "Loss S22:  0.04176558554172516\n",
            "Loss S33:  0.04156356304883957\n",
            "Loss S44:  0.044820431619882584\n",
            "Loss S11:  0.0327112635766918\n",
            "Loss S22:  0.03459091924808242\n",
            "Loss S33:  0.0376562469384887\n",
            "Loss S44:  0.04073960740457882\n",
            "Loss S11:  0.032834061465802644\n",
            "Loss S22:  0.03475621183003698\n",
            "Loss S33:  0.0377738509504568\n",
            "Loss S44:  0.0406506730332261\n",
            "Loss S11:  0.03295340971840966\n",
            "Loss S22:  0.034920641130978064\n",
            "Loss S33:  0.037670199188493916\n",
            "Loss S44:  0.04046072714751767\n",
            "Loss S11:  0.03312891826215314\n",
            "Loss S22:  0.035081069553043784\n",
            "Loss S33:  0.03774771076150057\n",
            "Loss S44:  0.040742291728170905\n",
            "Loss S11:  0.03328419553444666\n",
            "Loss S22:  0.035243548818078695\n",
            "Loss S33:  0.037678368462651386\n",
            "Loss S44:  0.04095382959234948\n",
            "Loss S11:  0.033240728232948505\n",
            "Loss S22:  0.035289662722192826\n",
            "Loss S33:  0.037674997673660025\n",
            "Loss S44:  0.040983074634778696\n",
            "Loss S11:  0.03318107650208641\n",
            "Loss S22:  0.035440260832998116\n",
            "Loss S33:  0.03777113126617082\n",
            "Loss S44:  0.04093805826465848\n",
            "Loss S11:  0.032980328494751895\n",
            "Loss S22:  0.0353047445324468\n",
            "Loss S33:  0.03754902228621053\n",
            "Loss S44:  0.04078018605525111\n",
            "Loss S11:  0.032869900730285014\n",
            "Loss S22:  0.03516485926869151\n",
            "Loss S33:  0.037487915522613366\n",
            "Loss S44:  0.040604016405868006\n",
            "Loss S11:  0.03289890252422578\n",
            "Loss S22:  0.03504318799270262\n",
            "Loss S33:  0.03743514095866444\n",
            "Loss S44:  0.04056828660835134\n",
            "Loss S11:  0.0329024517354933\n",
            "Loss S22:  0.035039386753966145\n",
            "Loss S33:  0.03743467451417231\n",
            "Loss S44:  0.040491639326016106\n",
            "Loss S11:  0.03290006221264354\n",
            "Loss S22:  0.03502160644789865\n",
            "Loss S33:  0.037519443795577555\n",
            "Loss S44:  0.040442368708366205\n",
            "Loss S11:  0.032950348227874925\n",
            "Loss S22:  0.035101500617070056\n",
            "Loss S33:  0.03752732494462083\n",
            "Loss S44:  0.040457141592757394\n",
            "Loss S11:  0.03303096492292611\n",
            "Loss S22:  0.035155714654647716\n",
            "Loss S33:  0.03759060431509576\n",
            "Loss S44:  0.040512388004056106\n",
            "Loss S11:  0.033087462359508935\n",
            "Loss S22:  0.03525687437134468\n",
            "Loss S33:  0.0376434326541937\n",
            "Loss S44:  0.040599632243446954\n",
            "Loss S11:  0.033151300334782335\n",
            "Loss S22:  0.035251693591845705\n",
            "Loss S33:  0.03768523915537766\n",
            "Loss S44:  0.040634110801338406\n",
            "Loss S11:  0.03322102363651607\n",
            "Loss S22:  0.035314399520294706\n",
            "Loss S33:  0.03776468385599161\n",
            "Loss S44:  0.04081349203373954\n",
            "Loss S11:  0.03326156889783085\n",
            "Loss S22:  0.03535901270395155\n",
            "Loss S33:  0.037851825222702314\n",
            "Loss S44:  0.040931838034595575\n",
            "Loss S11:  0.033249836132242416\n",
            "Loss S22:  0.035329817233519405\n",
            "Loss S33:  0.03784369496891948\n",
            "Loss S44:  0.04096404476705646\n",
            "Loss S11:  0.03323014371504831\n",
            "Loss S22:  0.03535250421446651\n",
            "Loss S33:  0.037794468609328884\n",
            "Loss S44:  0.04098270876119979\n",
            "Loss S11:  0.033230817351507916\n",
            "Loss S22:  0.03544540434051746\n",
            "Loss S33:  0.03779590561526929\n",
            "Loss S44:  0.04103363368889732\n",
            "Loss S11:  0.03321364666103507\n",
            "Loss S22:  0.03545753786770197\n",
            "Loss S33:  0.0378080492931928\n",
            "Loss S44:  0.04108329038072495\n",
            "Loss S11:  0.03322766142296585\n",
            "Loss S22:  0.03545225781763529\n",
            "Loss S33:  0.037798162057866784\n",
            "Loss S44:  0.04109797804128556\n",
            "Loss S11:  0.03329068041286775\n",
            "Loss S22:  0.03547879344787588\n",
            "Loss S33:  0.037847875969105736\n",
            "Loss S44:  0.041128750380391405\n",
            "Loss S11:  0.03327708525872563\n",
            "Loss S22:  0.03546700680042168\n",
            "Loss S33:  0.03784734599173544\n",
            "Loss S44:  0.04116195383893541\n",
            "Loss S11:  0.03325503501484449\n",
            "Loss S22:  0.035440166862645825\n",
            "Loss S33:  0.0378612601822706\n",
            "Loss S44:  0.04112393551745177\n",
            "Loss S11:  0.03322698933337007\n",
            "Loss S22:  0.035441086534754376\n",
            "Loss S33:  0.03786464961335008\n",
            "Loss S44:  0.04109754150808957\n",
            "Loss S11:  0.033232240112385715\n",
            "Loss S22:  0.03542976573215685\n",
            "Loss S33:  0.037830653968283716\n",
            "Loss S44:  0.04105069050797364\n",
            "Loss S11:  0.0332402379741681\n",
            "Loss S22:  0.035421377988820224\n",
            "Loss S33:  0.037855669068767854\n",
            "Loss S44:  0.04104627932572283\n",
            "Loss S11:  0.0332197243118801\n",
            "Loss S22:  0.035445779964228805\n",
            "Loss S33:  0.037877286905020575\n",
            "Loss S44:  0.04104510941032159\n",
            "Loss S11:  0.03323598927144452\n",
            "Loss S22:  0.03541260870874311\n",
            "Loss S33:  0.03786958459992309\n",
            "Loss S44:  0.04099696461362854\n",
            "Loss S11:  0.03321583115283945\n",
            "Loss S22:  0.03543554141397974\n",
            "Loss S33:  0.03788140497962448\n",
            "Loss S44:  0.04100289908339302\n",
            "Loss S11:  0.03320766753007818\n",
            "Loss S22:  0.035441410966960324\n",
            "Loss S33:  0.037896326329457795\n",
            "Loss S44:  0.041047553675322375\n",
            "Loss S11:  0.0332210804950282\n",
            "Loss S22:  0.03550832803822682\n",
            "Loss S33:  0.03790863647598151\n",
            "Loss S44:  0.04109759715077115\n",
            "Loss S11:  0.03323772749691098\n",
            "Loss S22:  0.035530573961741564\n",
            "Loss S33:  0.03791372041268396\n",
            "Loss S44:  0.041132550920091805\n",
            "Loss S11:  0.033240130057666774\n",
            "Loss S22:  0.03554356527460579\n",
            "Loss S33:  0.03793313196883472\n",
            "Loss S44:  0.04119014288002104\n",
            "Loss S11:  0.03324911507936019\n",
            "Loss S22:  0.035543820702764545\n",
            "Loss S33:  0.03792985164713506\n",
            "Loss S44:  0.04117351011285242\n",
            "Loss S11:  0.03322745956910721\n",
            "Loss S22:  0.035548654641574765\n",
            "Loss S33:  0.037922404799365934\n",
            "Loss S44:  0.041160931013856976\n",
            "Loss S11:  0.03320076029338038\n",
            "Loss S22:  0.03555765497924574\n",
            "Loss S33:  0.03788548222530986\n",
            "Loss S44:  0.04113192086481987\n",
            "Loss S11:  0.03322603717037567\n",
            "Loss S22:  0.03556865489486596\n",
            "Loss S33:  0.03789479376985099\n",
            "Loss S44:  0.04114378249258769\n",
            "Loss S11:  0.0332339283876538\n",
            "Loss S22:  0.03559189131642055\n",
            "Loss S33:  0.03790367460399503\n",
            "Loss S44:  0.041148530305737126\n",
            "Loss S11:  0.033240311449468846\n",
            "Loss S22:  0.03558897577456228\n",
            "Loss S33:  0.0379143865234849\n",
            "Loss S44:  0.04113421445787661\n",
            "Loss S11:  0.03321401769697251\n",
            "Loss S22:  0.035588042157985215\n",
            "Loss S33:  0.03791332287657703\n",
            "Loss S44:  0.041123446215402225\n",
            "Loss S11:  0.03320134238946059\n",
            "Loss S22:  0.035586932957037236\n",
            "Loss S33:  0.03791293161002551\n",
            "Loss S44:  0.04108134774562994\n",
            "Loss S11:  0.03323598954818333\n",
            "Loss S22:  0.03561275680353266\n",
            "Loss S33:  0.037913579729172975\n",
            "Loss S44:  0.04107796105901048\n",
            "Loss S11:  0.033218290976000195\n",
            "Loss S22:  0.035585258762127925\n",
            "Loss S33:  0.03789606152623569\n",
            "Loss S44:  0.041052802209287816\n",
            "Loss S11:  0.03319742940287175\n",
            "Loss S22:  0.035574820825944785\n",
            "Loss S33:  0.03788959096144339\n",
            "Loss S44:  0.041043998297865474\n",
            "Loss S11:  0.03319821498219288\n",
            "Loss S22:  0.03558886364845873\n",
            "Loss S33:  0.03790375112852163\n",
            "Loss S44:  0.04102904915716693\n",
            "Loss S11:  0.03316584221123435\n",
            "Loss S22:  0.03554810418191607\n",
            "Loss S33:  0.037877474105376334\n",
            "Loss S44:  0.0409999724606511\n",
            "Validation: \n",
            " Loss S11:  0.031147751957178116\n",
            " Loss S22:  0.044350456446409225\n",
            " Loss S33:  0.04125801846385002\n",
            " Loss S44:  0.04521825537085533\n",
            " Loss S11:  0.03230436350263301\n",
            " Loss S22:  0.05014879770931743\n",
            " Loss S33:  0.04966271349361965\n",
            " Loss S44:  0.05419922708755448\n",
            " Loss S11:  0.032248034300964054\n",
            " Loss S22:  0.050528909102445695\n",
            " Loss S33:  0.05006812349325273\n",
            " Loss S44:  0.05334311478385111\n",
            " Loss S11:  0.03211223059257523\n",
            " Loss S22:  0.05016707909888909\n",
            " Loss S33:  0.04923695338065507\n",
            " Loss S44:  0.053227093498237794\n",
            " Loss S11:  0.03206903816281277\n",
            " Loss S22:  0.049766286839673546\n",
            " Loss S33:  0.04907684313294328\n",
            " Loss S44:  0.05299938663288399\n",
            "\n",
            "Epoch: 41\n",
            "Loss S11:  0.03592805936932564\n",
            "Loss S22:  0.04141350835561752\n",
            "Loss S33:  0.0401645191013813\n",
            "Loss S44:  0.0451086051762104\n",
            "Loss S11:  0.03279531289908019\n",
            "Loss S22:  0.03578450767831369\n",
            "Loss S33:  0.037734806537628174\n",
            "Loss S44:  0.04057714309204708\n",
            "Loss S11:  0.03254132221142451\n",
            "Loss S22:  0.0355946109408424\n",
            "Loss S33:  0.03781583958438465\n",
            "Loss S44:  0.040387836595376335\n",
            "Loss S11:  0.03243882142968716\n",
            "Loss S22:  0.035547591745853424\n",
            "Loss S33:  0.037779365936594624\n",
            "Loss S44:  0.04043433418677699\n",
            "Loss S11:  0.032499045179021066\n",
            "Loss S22:  0.03557120808740941\n",
            "Loss S33:  0.03784664238734943\n",
            "Loss S44:  0.040328068522418416\n",
            "Loss S11:  0.032665597932303655\n",
            "Loss S22:  0.03568065144559916\n",
            "Loss S33:  0.03791065704004437\n",
            "Loss S44:  0.04057811587756755\n",
            "Loss S11:  0.0325388711739759\n",
            "Loss S22:  0.03541827006418197\n",
            "Loss S33:  0.03768524148913681\n",
            "Loss S44:  0.04045495384784996\n",
            "Loss S11:  0.032676541338294326\n",
            "Loss S22:  0.035549777389412195\n",
            "Loss S33:  0.03775162517394818\n",
            "Loss S44:  0.04062447565752016\n",
            "Loss S11:  0.03261312072015839\n",
            "Loss S22:  0.03550465280810992\n",
            "Loss S33:  0.037678159336432995\n",
            "Loss S44:  0.040459263379927037\n",
            "Loss S11:  0.03261276262883957\n",
            "Loss S22:  0.035402942837758376\n",
            "Loss S33:  0.037627754405468374\n",
            "Loss S44:  0.040332794189453125\n",
            "Loss S11:  0.03255683268503387\n",
            "Loss S22:  0.03527155319357862\n",
            "Loss S33:  0.03751388573926864\n",
            "Loss S44:  0.040236784716938985\n",
            "Loss S11:  0.03256858962478938\n",
            "Loss S22:  0.03514906329413255\n",
            "Loss S33:  0.03741298006797159\n",
            "Loss S44:  0.040114404113443046\n",
            "Loss S11:  0.032588478193179636\n",
            "Loss S22:  0.035131681233275036\n",
            "Loss S33:  0.03738296255161447\n",
            "Loss S44:  0.04007913393915192\n",
            "Loss S11:  0.03262126445770264\n",
            "Loss S22:  0.035189785902168\n",
            "Loss S33:  0.03743027945454339\n",
            "Loss S44:  0.04011344437608282\n",
            "Loss S11:  0.032602169716083415\n",
            "Loss S22:  0.03524104340649243\n",
            "Loss S33:  0.037525834304326815\n",
            "Loss S44:  0.04008649931626117\n",
            "Loss S11:  0.03264925040502027\n",
            "Loss S22:  0.03527946741819776\n",
            "Loss S33:  0.037644859366365614\n",
            "Loss S44:  0.04021107030428008\n",
            "Loss S11:  0.03261164983051904\n",
            "Loss S22:  0.03526744388784311\n",
            "Loss S33:  0.03758862155333439\n",
            "Loss S44:  0.04019863584211895\n",
            "Loss S11:  0.03271988646415939\n",
            "Loss S22:  0.03530921485296815\n",
            "Loss S33:  0.03768631239688536\n",
            "Loss S44:  0.04034643587574624\n",
            "Loss S11:  0.032797170123754285\n",
            "Loss S22:  0.035368477225879935\n",
            "Loss S33:  0.03771100647088902\n",
            "Loss S44:  0.04042374694627293\n",
            "Loss S11:  0.0328211099474998\n",
            "Loss S22:  0.03534830674646101\n",
            "Loss S33:  0.037683838822376664\n",
            "Loss S44:  0.04045165657841098\n",
            "Loss S11:  0.03279287018697357\n",
            "Loss S22:  0.03529863838866279\n",
            "Loss S33:  0.03769404781213151\n",
            "Loss S44:  0.04044528188767718\n",
            "Loss S11:  0.03283805250944967\n",
            "Loss S22:  0.0353738908077727\n",
            "Loss S33:  0.037738265257805444\n",
            "Loss S44:  0.0405578953147782\n",
            "Loss S11:  0.032866751985852\n",
            "Loss S22:  0.035403104111282536\n",
            "Loss S33:  0.03774598120328258\n",
            "Loss S44:  0.04057396375215971\n",
            "Loss S11:  0.03288845841276955\n",
            "Loss S22:  0.0354066351388311\n",
            "Loss S33:  0.037694433542750615\n",
            "Loss S44:  0.04056589671479159\n",
            "Loss S11:  0.03294717617085613\n",
            "Loss S22:  0.035468473018019524\n",
            "Loss S33:  0.03771986791485325\n",
            "Loss S44:  0.040622018354933294\n",
            "Loss S11:  0.03298884887708373\n",
            "Loss S22:  0.035504673366468266\n",
            "Loss S33:  0.037744894660860895\n",
            "Loss S44:  0.04063888809892286\n",
            "Loss S11:  0.03299426168945557\n",
            "Loss S22:  0.035477495977076993\n",
            "Loss S33:  0.037732166198877995\n",
            "Loss S44:  0.040649242085401126\n",
            "Loss S11:  0.0329686982000446\n",
            "Loss S22:  0.035450461049086494\n",
            "Loss S33:  0.03770322464681419\n",
            "Loss S44:  0.040651702765389125\n",
            "Loss S11:  0.03296340387513417\n",
            "Loss S22:  0.035449712732773656\n",
            "Loss S33:  0.037683073000392454\n",
            "Loss S44:  0.04060584856893244\n",
            "Loss S11:  0.032996752116944375\n",
            "Loss S22:  0.03545071803596626\n",
            "Loss S33:  0.03769700976146251\n",
            "Loss S44:  0.04058584266204605\n",
            "Loss S11:  0.033007684538134706\n",
            "Loss S22:  0.0354921101001509\n",
            "Loss S33:  0.03773416570725037\n",
            "Loss S44:  0.0405856816228046\n",
            "Loss S11:  0.03298795991652073\n",
            "Loss S22:  0.035443634974659445\n",
            "Loss S33:  0.03771556956376102\n",
            "Loss S44:  0.040549157013655475\n",
            "Loss S11:  0.03300034371422273\n",
            "Loss S22:  0.03546518183130527\n",
            "Loss S33:  0.03777115107905642\n",
            "Loss S44:  0.040577104727148636\n",
            "Loss S11:  0.032965865869671554\n",
            "Loss S22:  0.0354783410583468\n",
            "Loss S33:  0.03778586102778998\n",
            "Loss S44:  0.040625340685113316\n",
            "Loss S11:  0.0329879742946006\n",
            "Loss S22:  0.03550825781486601\n",
            "Loss S33:  0.03780232032875686\n",
            "Loss S44:  0.04064074133541926\n",
            "Loss S11:  0.032998245957697556\n",
            "Loss S22:  0.03549740613674336\n",
            "Loss S33:  0.03780184526452821\n",
            "Loss S44:  0.04066696590040823\n",
            "Loss S11:  0.033016819362412526\n",
            "Loss S22:  0.03550955130514841\n",
            "Loss S33:  0.037805250089956124\n",
            "Loss S44:  0.040706418991253976\n",
            "Loss S11:  0.033008605507867675\n",
            "Loss S22:  0.03547773826013839\n",
            "Loss S33:  0.03776455275473408\n",
            "Loss S44:  0.040683167662742645\n",
            "Loss S11:  0.03299028468452726\n",
            "Loss S22:  0.03545936403351193\n",
            "Loss S33:  0.0377377038063809\n",
            "Loss S44:  0.04067133091331467\n",
            "Loss S11:  0.032963282064251274\n",
            "Loss S22:  0.0354261680761033\n",
            "Loss S33:  0.03767830409738414\n",
            "Loss S44:  0.0406223111178564\n",
            "Loss S11:  0.03297112294080252\n",
            "Loss S22:  0.03546196549777824\n",
            "Loss S33:  0.037704687807105126\n",
            "Loss S44:  0.040627688643269405\n",
            "Loss S11:  0.03295365619924091\n",
            "Loss S22:  0.035460419830505865\n",
            "Loss S33:  0.03768152492506081\n",
            "Loss S44:  0.04062800546269637\n",
            "Loss S11:  0.03298760359916579\n",
            "Loss S22:  0.03546004012368637\n",
            "Loss S33:  0.037678393643712206\n",
            "Loss S44:  0.040608589659110954\n",
            "Loss S11:  0.03296163603836312\n",
            "Loss S22:  0.03546247364231439\n",
            "Loss S33:  0.03767700142415106\n",
            "Loss S44:  0.0405854850212272\n",
            "Loss S11:  0.032960910350084305\n",
            "Loss S22:  0.03546206595676016\n",
            "Loss S33:  0.037660867558652854\n",
            "Loss S44:  0.04056843920123009\n",
            "Loss S11:  0.03297293825516944\n",
            "Loss S22:  0.03545666728870832\n",
            "Loss S33:  0.03767115843302659\n",
            "Loss S44:  0.04058526647989607\n",
            "Loss S11:  0.03295074335406504\n",
            "Loss S22:  0.03543403377017075\n",
            "Loss S33:  0.03768330227397786\n",
            "Loss S44:  0.04057062425349643\n",
            "Loss S11:  0.03291947656407984\n",
            "Loss S22:  0.03542969126705151\n",
            "Loss S33:  0.037668639805856025\n",
            "Loss S44:  0.040569891595536735\n",
            "Loss S11:  0.03292009603859481\n",
            "Loss S22:  0.03542503021394141\n",
            "Loss S33:  0.037676468445084946\n",
            "Loss S44:  0.04056024597086431\n",
            "Loss S11:  0.03290514639235441\n",
            "Loss S22:  0.035389485816393514\n",
            "Loss S33:  0.037642258945890454\n",
            "Loss S44:  0.040543830769550045\n",
            "Validation: \n",
            " Loss S11:  0.029311181977391243\n",
            " Loss S22:  0.04556436464190483\n",
            " Loss S33:  0.041639331728219986\n",
            " Loss S44:  0.0470140241086483\n",
            " Loss S11:  0.03132856034097217\n",
            " Loss S22:  0.04930087959482556\n",
            " Loss S33:  0.049750820866652896\n",
            " Loss S44:  0.053446873667694274\n",
            " Loss S11:  0.031312763054923316\n",
            " Loss S22:  0.04952335075997725\n",
            " Loss S33:  0.050378754644132245\n",
            " Loss S44:  0.05251354960406699\n",
            " Loss S11:  0.031213395205921816\n",
            " Loss S22:  0.04921165591136354\n",
            " Loss S33:  0.04969829601831124\n",
            " Loss S44:  0.052410805689506845\n",
            " Loss S11:  0.03116251059152462\n",
            " Loss S22:  0.04881845525017491\n",
            " Loss S33:  0.04957355370308146\n",
            " Loss S44:  0.052221164383270124\n",
            "\n",
            "Epoch: 42\n",
            "Loss S11:  0.0384470596909523\n",
            "Loss S22:  0.038817305117845535\n",
            "Loss S33:  0.03882097452878952\n",
            "Loss S44:  0.04236030578613281\n",
            "Loss S11:  0.03224566104737195\n",
            "Loss S22:  0.03452537665990266\n",
            "Loss S33:  0.036573795771056954\n",
            "Loss S44:  0.039537216452035034\n",
            "Loss S11:  0.032030957972719556\n",
            "Loss S22:  0.034851212320583205\n",
            "Loss S33:  0.03721746553977331\n",
            "Loss S44:  0.04002519253463972\n",
            "Loss S11:  0.03199014277948487\n",
            "Loss S22:  0.03479988506484416\n",
            "Loss S33:  0.03694044005486273\n",
            "Loss S44:  0.03984230840879102\n",
            "Loss S11:  0.03246985371338158\n",
            "Loss S22:  0.03469491436532358\n",
            "Loss S33:  0.037058639544539335\n",
            "Loss S44:  0.04005340532195277\n",
            "Loss S11:  0.03257943335555348\n",
            "Loss S22:  0.03490969184420857\n",
            "Loss S33:  0.03705542899814306\n",
            "Loss S44:  0.04022873353724386\n",
            "Loss S11:  0.032475148342916225\n",
            "Loss S22:  0.03473367282479513\n",
            "Loss S33:  0.037034528665855285\n",
            "Loss S44:  0.04018350788315789\n",
            "Loss S11:  0.03240360217300099\n",
            "Loss S22:  0.03472146158382087\n",
            "Loss S33:  0.037131225191791295\n",
            "Loss S44:  0.040415633050069004\n",
            "Loss S11:  0.03224904924907066\n",
            "Loss S22:  0.03475034814097999\n",
            "Loss S33:  0.037075174029594586\n",
            "Loss S44:  0.040253899347634965\n",
            "Loss S11:  0.032244772026008305\n",
            "Loss S22:  0.034701686270616865\n",
            "Loss S33:  0.03697260870383336\n",
            "Loss S44:  0.04011015962440889\n",
            "Loss S11:  0.03218398864685309\n",
            "Loss S22:  0.034588185314199715\n",
            "Loss S33:  0.03683309179573956\n",
            "Loss S44:  0.039891880042482133\n",
            "Loss S11:  0.032138895914629775\n",
            "Loss S22:  0.03452012921171682\n",
            "Loss S33:  0.03671520911492743\n",
            "Loss S44:  0.03973607588055972\n",
            "Loss S11:  0.03222175316561845\n",
            "Loss S22:  0.03454762050681863\n",
            "Loss S33:  0.036726084807194956\n",
            "Loss S44:  0.03973799309700974\n",
            "Loss S11:  0.03227966385431417\n",
            "Loss S22:  0.03461777489945179\n",
            "Loss S33:  0.03675593495482707\n",
            "Loss S44:  0.03976460569004976\n",
            "Loss S11:  0.032323691307018836\n",
            "Loss S22:  0.03461377022801139\n",
            "Loss S33:  0.03679224246359886\n",
            "Loss S44:  0.03976803877991988\n",
            "Loss S11:  0.03241333213714969\n",
            "Loss S22:  0.03470224197583877\n",
            "Loss S33:  0.03684909547993679\n",
            "Loss S44:  0.03991911343194002\n",
            "Loss S11:  0.03249651161224946\n",
            "Loss S22:  0.034725492449928515\n",
            "Loss S33:  0.03686583410046115\n",
            "Loss S44:  0.03997163666608911\n",
            "Loss S11:  0.03255469177724325\n",
            "Loss S22:  0.034809295905002376\n",
            "Loss S33:  0.036952459934162116\n",
            "Loss S44:  0.04009189650590657\n",
            "Loss S11:  0.03255479080983288\n",
            "Loss S22:  0.034872024767718265\n",
            "Loss S33:  0.036986748857722095\n",
            "Loss S44:  0.0401754220175809\n",
            "Loss S11:  0.03254903973865259\n",
            "Loss S22:  0.03487220066185085\n",
            "Loss S33:  0.03699929101382875\n",
            "Loss S44:  0.04016823056324614\n",
            "Loss S11:  0.032544584574168596\n",
            "Loss S22:  0.03486343985655118\n",
            "Loss S33:  0.03698368416867446\n",
            "Loss S44:  0.0401878236464007\n",
            "Loss S11:  0.03256050804497507\n",
            "Loss S22:  0.034913175734934083\n",
            "Loss S33:  0.03694931264101611\n",
            "Loss S44:  0.0402311214832898\n",
            "Loss S11:  0.032579847160575076\n",
            "Loss S22:  0.03495028727093703\n",
            "Loss S33:  0.03698303934810388\n",
            "Loss S44:  0.04024067945879509\n",
            "Loss S11:  0.032575291125869855\n",
            "Loss S22:  0.03492368579523646\n",
            "Loss S33:  0.03697475147518245\n",
            "Loss S44:  0.040283041953782496\n",
            "Loss S11:  0.032631839474825446\n",
            "Loss S22:  0.034971814053867366\n",
            "Loss S33:  0.03703699729316462\n",
            "Loss S44:  0.04028794807826335\n",
            "Loss S11:  0.03264165464120795\n",
            "Loss S22:  0.034983789993531675\n",
            "Loss S33:  0.037066304648065\n",
            "Loss S44:  0.04031767546240077\n",
            "Loss S11:  0.03263799152497587\n",
            "Loss S22:  0.03499276751930686\n",
            "Loss S33:  0.03712243351271783\n",
            "Loss S44:  0.04033273108788834\n",
            "Loss S11:  0.03263233219208093\n",
            "Loss S22:  0.03498640031007383\n",
            "Loss S33:  0.037130030758706405\n",
            "Loss S44:  0.040327411447393935\n",
            "Loss S11:  0.032601950968552736\n",
            "Loss S22:  0.03497289660721487\n",
            "Loss S33:  0.03711957214035597\n",
            "Loss S44:  0.040300053700115336\n",
            "Loss S11:  0.03262325571497896\n",
            "Loss S22:  0.034973281982940495\n",
            "Loss S33:  0.03712669582045365\n",
            "Loss S44:  0.04032526284302633\n",
            "Loss S11:  0.03263221101284621\n",
            "Loss S22:  0.03500560527525075\n",
            "Loss S33:  0.037166209699604595\n",
            "Loss S44:  0.04034495112359326\n",
            "Loss S11:  0.03264175992087728\n",
            "Loss S22:  0.034977368399548764\n",
            "Loss S33:  0.037147220882955474\n",
            "Loss S44:  0.04031768916743744\n",
            "Loss S11:  0.03263276394723546\n",
            "Loss S22:  0.0349903655792601\n",
            "Loss S33:  0.0371538578558748\n",
            "Loss S44:  0.04037415488068931\n",
            "Loss S11:  0.03264216210389245\n",
            "Loss S22:  0.035020170615158774\n",
            "Loss S33:  0.03717132483319213\n",
            "Loss S44:  0.04041931167064837\n",
            "Loss S11:  0.032629160669323636\n",
            "Loss S22:  0.03505947652799992\n",
            "Loss S33:  0.03719379215383809\n",
            "Loss S44:  0.04044072518993683\n",
            "Loss S11:  0.03265803961608654\n",
            "Loss S22:  0.035079078159780584\n",
            "Loss S33:  0.03722623420449404\n",
            "Loss S44:  0.040468568093756326\n",
            "Loss S11:  0.03264450915983344\n",
            "Loss S22:  0.03507101720513729\n",
            "Loss S33:  0.03723572194060295\n",
            "Loss S44:  0.04047684928717045\n",
            "Loss S11:  0.03264595630435609\n",
            "Loss S22:  0.03505879205794669\n",
            "Loss S33:  0.03722877723649345\n",
            "Loss S44:  0.04045113323713249\n",
            "Loss S11:  0.032639704278017594\n",
            "Loss S22:  0.03506194233033914\n",
            "Loss S33:  0.03722035892190426\n",
            "Loss S44:  0.04043699960344107\n",
            "Loss S11:  0.0326173699382321\n",
            "Loss S22:  0.035029060533627525\n",
            "Loss S33:  0.03719029500794685\n",
            "Loss S44:  0.0404118677062909\n",
            "Loss S11:  0.032636205050304644\n",
            "Loss S22:  0.03506504364794775\n",
            "Loss S33:  0.03718128466416624\n",
            "Loss S44:  0.040420494843599504\n",
            "Loss S11:  0.03264409524825948\n",
            "Loss S22:  0.03508909267357521\n",
            "Loss S33:  0.037176935981116155\n",
            "Loss S44:  0.04041199852007729\n",
            "Loss S11:  0.03266467130134621\n",
            "Loss S22:  0.03509902232050046\n",
            "Loss S33:  0.03718627695073849\n",
            "Loss S44:  0.04039337947929274\n",
            "Loss S11:  0.032666009498582914\n",
            "Loss S22:  0.03512720283112227\n",
            "Loss S33:  0.03718257950847912\n",
            "Loss S44:  0.0403959915525952\n",
            "Loss S11:  0.032657655222075324\n",
            "Loss S22:  0.03513087776580365\n",
            "Loss S33:  0.03718555042448363\n",
            "Loss S44:  0.04040747100398654\n",
            "Loss S11:  0.0326712519888603\n",
            "Loss S22:  0.03514498102162735\n",
            "Loss S33:  0.037185760644448305\n",
            "Loss S44:  0.0404146644482988\n",
            "Loss S11:  0.032660425885089564\n",
            "Loss S22:  0.03514536454821098\n",
            "Loss S33:  0.03717106387196937\n",
            "Loss S44:  0.04042170200812067\n",
            "Loss S11:  0.032648886342368816\n",
            "Loss S22:  0.03513753930632729\n",
            "Loss S33:  0.03717505758244647\n",
            "Loss S44:  0.040396289413522\n",
            "Loss S11:  0.03263288577185971\n",
            "Loss S22:  0.03514471530077859\n",
            "Loss S33:  0.03719871975636408\n",
            "Loss S44:  0.040392941952123465\n",
            "Loss S11:  0.032594784878027416\n",
            "Loss S22:  0.03510874642586514\n",
            "Loss S33:  0.03717268668418625\n",
            "Loss S44:  0.04038157971109246\n",
            "Validation: \n",
            " Loss S11:  0.030218886211514473\n",
            " Loss S22:  0.04655147343873978\n",
            " Loss S33:  0.042226094752550125\n",
            " Loss S44:  0.04477837309241295\n",
            " Loss S11:  0.03124869925280412\n",
            " Loss S22:  0.04974057986622765\n",
            " Loss S33:  0.05000020120115507\n",
            " Loss S44:  0.0541495888360909\n",
            " Loss S11:  0.031061851351362902\n",
            " Loss S22:  0.04997831891949584\n",
            " Loss S33:  0.05071073034551085\n",
            " Loss S44:  0.053356650762441685\n",
            " Loss S11:  0.0310558304801339\n",
            " Loss S22:  0.049513242711297804\n",
            " Loss S33:  0.04998135896491223\n",
            " Loss S44:  0.05330991946527215\n",
            " Loss S11:  0.030944231138737115\n",
            " Loss S22:  0.049039694124165874\n",
            " Loss S33:  0.04990072019490195\n",
            " Loss S44:  0.053196624335315496\n",
            "\n",
            "Epoch: 43\n",
            "Loss S11:  0.03730257228016853\n",
            "Loss S22:  0.03676382079720497\n",
            "Loss S33:  0.039285946637392044\n",
            "Loss S44:  0.04223479703068733\n",
            "Loss S11:  0.03260462866588072\n",
            "Loss S22:  0.03416894410144199\n",
            "Loss S33:  0.037703613327308136\n",
            "Loss S44:  0.04046886549754576\n",
            "Loss S11:  0.03260093998341333\n",
            "Loss S22:  0.03444238645689828\n",
            "Loss S33:  0.037611213468369986\n",
            "Loss S44:  0.040112378696600594\n",
            "Loss S11:  0.03248916740619367\n",
            "Loss S22:  0.034879247567826704\n",
            "Loss S33:  0.0375000273268069\n",
            "Loss S44:  0.040292467681630965\n",
            "Loss S11:  0.032799129942204894\n",
            "Loss S22:  0.034720218308815144\n",
            "Loss S33:  0.03758523404234793\n",
            "Loss S44:  0.040084549748316045\n",
            "Loss S11:  0.03254997686428182\n",
            "Loss S22:  0.03465769527589574\n",
            "Loss S33:  0.037539300600103305\n",
            "Loss S44:  0.040117732961388194\n",
            "Loss S11:  0.03248394579916704\n",
            "Loss S22:  0.034758909926062724\n",
            "Loss S33:  0.03758323455198866\n",
            "Loss S44:  0.04013236044127433\n",
            "Loss S11:  0.03232446034819308\n",
            "Loss S22:  0.034827171470707574\n",
            "Loss S33:  0.03743134575410628\n",
            "Loss S44:  0.04008506847099519\n",
            "Loss S11:  0.03220578680895729\n",
            "Loss S22:  0.03483847144669221\n",
            "Loss S33:  0.03729270398616791\n",
            "Loss S44:  0.03992740938692917\n",
            "Loss S11:  0.03215184019735226\n",
            "Loss S22:  0.03478439252537031\n",
            "Loss S33:  0.03714835021522019\n",
            "Loss S44:  0.039825512730813285\n",
            "Loss S11:  0.03208833691285978\n",
            "Loss S22:  0.03473045636374172\n",
            "Loss S33:  0.0369638329980397\n",
            "Loss S44:  0.03976174344373221\n",
            "Loss S11:  0.0319999697374868\n",
            "Loss S22:  0.03470239404018398\n",
            "Loss S33:  0.03692123582502743\n",
            "Loss S44:  0.03962493214655567\n",
            "Loss S11:  0.03202124640407149\n",
            "Loss S22:  0.034662621773102066\n",
            "Loss S33:  0.036880622225359455\n",
            "Loss S44:  0.03959538492043156\n",
            "Loss S11:  0.032097001851515004\n",
            "Loss S22:  0.034749749123708894\n",
            "Loss S33:  0.03695600349029512\n",
            "Loss S44:  0.03967127977436735\n",
            "Loss S11:  0.03215486805965292\n",
            "Loss S22:  0.03476393316601608\n",
            "Loss S33:  0.03702080117683884\n",
            "Loss S44:  0.03975000560072297\n",
            "Loss S11:  0.03225690464408982\n",
            "Loss S22:  0.03482139782844395\n",
            "Loss S33:  0.03704721552153297\n",
            "Loss S44:  0.03990100911318861\n",
            "Loss S11:  0.03222691185587311\n",
            "Loss S22:  0.034786336066097205\n",
            "Loss S33:  0.037029368953305004\n",
            "Loss S44:  0.039984728261735866\n",
            "Loss S11:  0.03235545050766733\n",
            "Loss S22:  0.03486703559538426\n",
            "Loss S33:  0.03710865225011145\n",
            "Loss S44:  0.040104901842903676\n",
            "Loss S11:  0.03240357108866971\n",
            "Loss S22:  0.03492026499251305\n",
            "Loss S33:  0.03713676314903886\n",
            "Loss S44:  0.04015876647105533\n",
            "Loss S11:  0.0324237094655711\n",
            "Loss S22:  0.03488365102422799\n",
            "Loss S33:  0.03710842737196628\n",
            "Loss S44:  0.0401199674770158\n",
            "Loss S11:  0.03243298178066069\n",
            "Loss S22:  0.03486405513179836\n",
            "Loss S33:  0.037140284363754945\n",
            "Loss S44:  0.04010977432947254\n",
            "Loss S11:  0.03245730525085711\n",
            "Loss S22:  0.03486809639427899\n",
            "Loss S33:  0.03716141036689564\n",
            "Loss S44:  0.040220694060291724\n",
            "Loss S11:  0.03250152079970049\n",
            "Loss S22:  0.03490033580571818\n",
            "Loss S33:  0.03718843693714336\n",
            "Loss S44:  0.040212818231787614\n",
            "Loss S11:  0.0324791254499903\n",
            "Loss S22:  0.03488356107241147\n",
            "Loss S33:  0.03715077605404895\n",
            "Loss S44:  0.0402243450380765\n",
            "Loss S11:  0.032493335268309505\n",
            "Loss S22:  0.034924425895777975\n",
            "Loss S33:  0.03719838090333701\n",
            "Loss S44:  0.04025212873190765\n",
            "Loss S11:  0.032491843167885365\n",
            "Loss S22:  0.034909334286038145\n",
            "Loss S33:  0.03714875075446657\n",
            "Loss S44:  0.04026631219869116\n",
            "Loss S11:  0.0324717646471842\n",
            "Loss S22:  0.03488374167475207\n",
            "Loss S33:  0.03711733109249923\n",
            "Loss S44:  0.040221644643965354\n",
            "Loss S11:  0.03243265808233476\n",
            "Loss S22:  0.03488026017742403\n",
            "Loss S33:  0.037080341654510074\n",
            "Loss S44:  0.04019573315758107\n",
            "Loss S11:  0.03242706191921574\n",
            "Loss S22:  0.0348855373433052\n",
            "Loss S33:  0.037059619585284136\n",
            "Loss S44:  0.0401934297801761\n",
            "Loss S11:  0.032440028406193165\n",
            "Loss S22:  0.034872143167707925\n",
            "Loss S33:  0.03707472610053737\n",
            "Loss S44:  0.04021771591399953\n",
            "Loss S11:  0.032448895120888055\n",
            "Loss S22:  0.0349289234789503\n",
            "Loss S33:  0.03709632347638029\n",
            "Loss S44:  0.040200830112363015\n",
            "Loss S11:  0.032455469736188555\n",
            "Loss S22:  0.03490785491380278\n",
            "Loss S33:  0.037079440759213406\n",
            "Loss S44:  0.040144617486613354\n",
            "Loss S11:  0.032471072607647596\n",
            "Loss S22:  0.0349449392907159\n",
            "Loss S33:  0.037093465190345995\n",
            "Loss S44:  0.04013128207294369\n",
            "Loss S11:  0.032473601125113195\n",
            "Loss S22:  0.03493808155873751\n",
            "Loss S33:  0.03706768043676892\n",
            "Loss S44:  0.04017883037242284\n",
            "Loss S11:  0.03251917966125298\n",
            "Loss S22:  0.034970356495586656\n",
            "Loss S33:  0.037101156354125295\n",
            "Loss S44:  0.040201716168709864\n",
            "Loss S11:  0.03251251620784444\n",
            "Loss S22:  0.03499873923907253\n",
            "Loss S33:  0.03711735420035161\n",
            "Loss S44:  0.04021224444281002\n",
            "Loss S11:  0.032534750947248905\n",
            "Loss S22:  0.035023877763517015\n",
            "Loss S33:  0.037152772444271974\n",
            "Loss S44:  0.04023448275685971\n",
            "Loss S11:  0.03251264911054601\n",
            "Loss S22:  0.034986819715472565\n",
            "Loss S33:  0.03713635452633919\n",
            "Loss S44:  0.0402278251785313\n",
            "Loss S11:  0.032498476665046586\n",
            "Loss S22:  0.034982734401355894\n",
            "Loss S33:  0.03712544356548567\n",
            "Loss S44:  0.04018582988364177\n",
            "Loss S11:  0.03246920391002579\n",
            "Loss S22:  0.03497095412248388\n",
            "Loss S33:  0.03709281622753729\n",
            "Loss S44:  0.040158490171594084\n",
            "Loss S11:  0.03248396474505749\n",
            "Loss S22:  0.035008333739534284\n",
            "Loss S33:  0.03710860264791812\n",
            "Loss S44:  0.04016913820123137\n",
            "Loss S11:  0.032481794546917984\n",
            "Loss S22:  0.03498841075276516\n",
            "Loss S33:  0.03707832012805916\n",
            "Loss S44:  0.04017462747266693\n",
            "Loss S11:  0.03251720435315668\n",
            "Loss S22:  0.034996989627773575\n",
            "Loss S33:  0.037084762918411125\n",
            "Loss S44:  0.040190771602607395\n",
            "Loss S11:  0.03250416930064801\n",
            "Loss S22:  0.034965109473349326\n",
            "Loss S33:  0.037057590491533836\n",
            "Loss S44:  0.04017315785078604\n",
            "Loss S11:  0.03248213642105764\n",
            "Loss S22:  0.03496531546200349\n",
            "Loss S33:  0.037042990487381446\n",
            "Loss S44:  0.04016781501184786\n",
            "Loss S11:  0.032477506644974265\n",
            "Loss S22:  0.03495461879475682\n",
            "Loss S33:  0.03705603245257538\n",
            "Loss S44:  0.04016319978693108\n",
            "Loss S11:  0.03247416897428398\n",
            "Loss S22:  0.03495853189823426\n",
            "Loss S33:  0.03706031012270042\n",
            "Loss S44:  0.04016582721969569\n",
            "Loss S11:  0.032454152720052976\n",
            "Loss S22:  0.03496738993849471\n",
            "Loss S33:  0.037056988227772356\n",
            "Loss S44:  0.04016021085456172\n",
            "Loss S11:  0.032444310848932255\n",
            "Loss S22:  0.0349718114117203\n",
            "Loss S33:  0.037065433016871714\n",
            "Loss S44:  0.04017336174009248\n",
            "Loss S11:  0.03242039738902006\n",
            "Loss S22:  0.03493355477869875\n",
            "Loss S33:  0.03704271492094955\n",
            "Loss S44:  0.04017278056614511\n",
            "Validation: \n",
            " Loss S11:  0.030320672318339348\n",
            " Loss S22:  0.04499170184135437\n",
            " Loss S33:  0.04108823835849762\n",
            " Loss S44:  0.04507213830947876\n",
            " Loss S11:  0.031355323713450206\n",
            " Loss S22:  0.04930263686747778\n",
            " Loss S33:  0.048704945792754493\n",
            " Loss S44:  0.05428065643424079\n",
            " Loss S11:  0.03137772030583242\n",
            " Loss S22:  0.04950527029066551\n",
            " Loss S33:  0.04933134520926127\n",
            " Loss S44:  0.05343293834750245\n",
            " Loss S11:  0.03138189765884251\n",
            " Loss S22:  0.04899311639734956\n",
            " Loss S33:  0.048657647410377126\n",
            " Loss S44:  0.05311100441412848\n",
            " Loss S11:  0.03128291080119433\n",
            " Loss S22:  0.048582444772308254\n",
            " Loss S33:  0.048604368795583275\n",
            " Loss S44:  0.053005660941571366\n",
            "\n",
            "Epoch: 44\n",
            "Loss S11:  0.03500140458345413\n",
            "Loss S22:  0.03860356658697128\n",
            "Loss S33:  0.03881821036338806\n",
            "Loss S44:  0.04166564717888832\n",
            "Loss S11:  0.031662015413696114\n",
            "Loss S22:  0.0334793132814494\n",
            "Loss S33:  0.03690096752887422\n",
            "Loss S44:  0.03963257033716549\n",
            "Loss S11:  0.03204135490315301\n",
            "Loss S22:  0.03413051189411254\n",
            "Loss S33:  0.036864918434903735\n",
            "Loss S44:  0.03903116143885113\n",
            "Loss S11:  0.03191508611123408\n",
            "Loss S22:  0.03442960490863169\n",
            "Loss S33:  0.03658189348155452\n",
            "Loss S44:  0.03939127753819189\n",
            "Loss S11:  0.03234581794680619\n",
            "Loss S22:  0.034591871654478516\n",
            "Loss S33:  0.0368022406246604\n",
            "Loss S44:  0.039801640754065865\n",
            "Loss S11:  0.03230044822774682\n",
            "Loss S22:  0.034513857471300105\n",
            "Loss S33:  0.03683010474139569\n",
            "Loss S44:  0.039949072926652195\n",
            "Loss S11:  0.03229562040479457\n",
            "Loss S22:  0.03455004047174923\n",
            "Loss S33:  0.0367569022308119\n",
            "Loss S44:  0.03989110282454334\n",
            "Loss S11:  0.03234880535640347\n",
            "Loss S22:  0.03458154983293842\n",
            "Loss S33:  0.03674714820800533\n",
            "Loss S44:  0.03988927697211924\n",
            "Loss S11:  0.032253994410972535\n",
            "Loss S22:  0.03455824940752836\n",
            "Loss S33:  0.03669760120963609\n",
            "Loss S44:  0.0397668278971572\n",
            "Loss S11:  0.03218026651622175\n",
            "Loss S22:  0.034576342963091616\n",
            "Loss S33:  0.036672637050787174\n",
            "Loss S44:  0.03963966774088996\n",
            "Loss S11:  0.03207989110805021\n",
            "Loss S22:  0.034523208450415344\n",
            "Loss S33:  0.03657306703084176\n",
            "Loss S44:  0.03943202935970656\n",
            "Loss S11:  0.03211841477198644\n",
            "Loss S22:  0.03453399199019144\n",
            "Loss S33:  0.0365387907604108\n",
            "Loss S44:  0.03932075319929166\n",
            "Loss S11:  0.032050355837857425\n",
            "Loss S22:  0.034453058630720644\n",
            "Loss S33:  0.03650999809653798\n",
            "Loss S44:  0.03935159240995557\n",
            "Loss S11:  0.032073174852110045\n",
            "Loss S22:  0.03445096376515527\n",
            "Loss S33:  0.03658192981574827\n",
            "Loss S44:  0.03937879449311103\n",
            "Loss S11:  0.03213864039127708\n",
            "Loss S22:  0.03446502463086277\n",
            "Loss S33:  0.036649028975384454\n",
            "Loss S44:  0.03941589054909158\n",
            "Loss S11:  0.032228975661641714\n",
            "Loss S22:  0.03453527168919709\n",
            "Loss S33:  0.036677007877964844\n",
            "Loss S44:  0.03947405591113678\n",
            "Loss S11:  0.032193402580407836\n",
            "Loss S22:  0.03447280717747552\n",
            "Loss S33:  0.036661682395923954\n",
            "Loss S44:  0.03951111216074932\n",
            "Loss S11:  0.03223779481666827\n",
            "Loss S22:  0.03450764864901004\n",
            "Loss S33:  0.036754256920420635\n",
            "Loss S44:  0.039606551024300315\n",
            "Loss S11:  0.032220606738444194\n",
            "Loss S22:  0.034512530291936676\n",
            "Loss S33:  0.03674110092058037\n",
            "Loss S44:  0.03963111542700404\n",
            "Loss S11:  0.032212350361478265\n",
            "Loss S22:  0.034475426623568485\n",
            "Loss S33:  0.03667030769219885\n",
            "Loss S44:  0.039634124400259936\n",
            "Loss S11:  0.032217790831380815\n",
            "Loss S22:  0.034503000477949776\n",
            "Loss S33:  0.03671221120922423\n",
            "Loss S44:  0.03965162128135932\n",
            "Loss S11:  0.032267428793330895\n",
            "Loss S22:  0.03454243706018439\n",
            "Loss S33:  0.03673716880804837\n",
            "Loss S44:  0.03973008727575365\n",
            "Loss S11:  0.032300772726940354\n",
            "Loss S22:  0.03458679192678421\n",
            "Loss S33:  0.036751686085470664\n",
            "Loss S44:  0.03976819266426078\n",
            "Loss S11:  0.032298276773391865\n",
            "Loss S22:  0.03456687053979991\n",
            "Loss S33:  0.03676096116922376\n",
            "Loss S44:  0.03977399577439089\n",
            "Loss S11:  0.03232711489424418\n",
            "Loss S22:  0.03459936736684865\n",
            "Loss S33:  0.03677576213796851\n",
            "Loss S44:  0.0397719706706239\n",
            "Loss S11:  0.03235356099309912\n",
            "Loss S22:  0.03460938636347117\n",
            "Loss S33:  0.036772308232952876\n",
            "Loss S44:  0.03979916125535965\n",
            "Loss S11:  0.03232271004453929\n",
            "Loss S22:  0.034583815933701176\n",
            "Loss S33:  0.03677189425062174\n",
            "Loss S44:  0.039798225022823874\n",
            "Loss S11:  0.03229457118661861\n",
            "Loss S22:  0.0345647899000957\n",
            "Loss S33:  0.03677254936544438\n",
            "Loss S44:  0.03979471329998266\n",
            "Loss S11:  0.032282355218277284\n",
            "Loss S22:  0.03454793308235148\n",
            "Loss S33:  0.036738419626902434\n",
            "Loss S44:  0.039778119436998806\n",
            "Loss S11:  0.03229114826755835\n",
            "Loss S22:  0.03452799587648144\n",
            "Loss S33:  0.03676080233273432\n",
            "Loss S44:  0.03972981734476548\n",
            "Loss S11:  0.03229516148690963\n",
            "Loss S22:  0.03453177141267002\n",
            "Loss S33:  0.036749121525705454\n",
            "Loss S44:  0.039726295926147126\n",
            "Loss S11:  0.03226799175286983\n",
            "Loss S22:  0.034483976763975584\n",
            "Loss S33:  0.03672036100358633\n",
            "Loss S44:  0.0396852482984687\n",
            "Loss S11:  0.032241461177666984\n",
            "Loss S22:  0.034502087532313444\n",
            "Loss S33:  0.036722903781080174\n",
            "Loss S44:  0.03968632690074659\n",
            "Loss S11:  0.03224806151608866\n",
            "Loss S22:  0.03450777004627663\n",
            "Loss S33:  0.0367190474053434\n",
            "Loss S44:  0.03971104455543429\n",
            "Loss S11:  0.03223890014801207\n",
            "Loss S22:  0.0345323164442068\n",
            "Loss S33:  0.03677373095975418\n",
            "Loss S44:  0.03974746397076464\n",
            "Loss S11:  0.03225413492835655\n",
            "Loss S22:  0.03456310019894713\n",
            "Loss S33:  0.03679973165639955\n",
            "Loss S44:  0.03979623474796273\n",
            "Loss S11:  0.03225415234048941\n",
            "Loss S22:  0.034549677352371966\n",
            "Loss S33:  0.036801574504070005\n",
            "Loss S44:  0.03982757489065384\n",
            "Loss S11:  0.032217159400370246\n",
            "Loss S22:  0.03451620910386994\n",
            "Loss S33:  0.036791856724497765\n",
            "Loss S44:  0.039809382026086276\n",
            "Loss S11:  0.03218403173087463\n",
            "Loss S22:  0.034489112238832345\n",
            "Loss S33:  0.03678862693330904\n",
            "Loss S44:  0.03976711528155747\n",
            "Loss S11:  0.032152890311101516\n",
            "Loss S22:  0.03447359952780292\n",
            "Loss S33:  0.036739250340158375\n",
            "Loss S44:  0.03974447571827323\n",
            "Loss S11:  0.03215902994045444\n",
            "Loss S22:  0.034493011875342844\n",
            "Loss S33:  0.03677551665266702\n",
            "Loss S44:  0.03973175744128941\n",
            "Loss S11:  0.03216505579994833\n",
            "Loss S22:  0.03449832378385856\n",
            "Loss S33:  0.03677336688078668\n",
            "Loss S44:  0.03972536563837035\n",
            "Loss S11:  0.03218600064551462\n",
            "Loss S22:  0.03449062027054841\n",
            "Loss S33:  0.03676231670461046\n",
            "Loss S44:  0.03972292651650175\n",
            "Loss S11:  0.0321823616453777\n",
            "Loss S22:  0.03448741448278217\n",
            "Loss S33:  0.036751040998682066\n",
            "Loss S44:  0.039718389848199635\n",
            "Loss S11:  0.03216403420347205\n",
            "Loss S22:  0.034480784417820626\n",
            "Loss S33:  0.03674488275178841\n",
            "Loss S44:  0.039700987260966075\n",
            "Loss S11:  0.0321743907386276\n",
            "Loss S22:  0.034487944439119615\n",
            "Loss S33:  0.03674558454715624\n",
            "Loss S44:  0.03970789213865136\n",
            "Loss S11:  0.03216581741133765\n",
            "Loss S22:  0.03448818217954899\n",
            "Loss S33:  0.03672978685615249\n",
            "Loss S44:  0.0397128484529037\n",
            "Loss S11:  0.032149743648629533\n",
            "Loss S22:  0.03447656140459065\n",
            "Loss S33:  0.03671442736296138\n",
            "Loss S44:  0.03970271844076756\n",
            "Loss S11:  0.03213434741520584\n",
            "Loss S22:  0.03448002217744096\n",
            "Loss S33:  0.03672312777063455\n",
            "Loss S44:  0.039695584174563134\n",
            "Loss S11:  0.03209933751621217\n",
            "Loss S22:  0.03445077173472543\n",
            "Loss S33:  0.03669908759610959\n",
            "Loss S44:  0.039671996650105824\n",
            "Validation: \n",
            " Loss S11:  0.030232777819037437\n",
            " Loss S22:  0.04490260034799576\n",
            " Loss S33:  0.04084257408976555\n",
            " Loss S44:  0.04561098292469978\n",
            " Loss S11:  0.03139410460633891\n",
            " Loss S22:  0.048840505736214776\n",
            " Loss S33:  0.048650531186943964\n",
            " Loss S44:  0.052743722462937945\n",
            " Loss S11:  0.03149702463571618\n",
            " Loss S22:  0.04907068428469867\n",
            " Loss S33:  0.04944006335444567\n",
            " Loss S44:  0.05200442217472123\n",
            " Loss S11:  0.031393317719463444\n",
            " Loss S22:  0.04866973616060664\n",
            " Loss S33:  0.048645567087853545\n",
            " Loss S44:  0.05185804106905812\n",
            " Loss S11:  0.0313473088827766\n",
            " Loss S22:  0.04830100115986518\n",
            " Loss S33:  0.04862636267954921\n",
            " Loss S44:  0.05178505899729552\n",
            "\n",
            "Epoch: 45\n",
            "Loss S11:  0.03662281855940819\n",
            "Loss S22:  0.03866884484887123\n",
            "Loss S33:  0.03866105154156685\n",
            "Loss S44:  0.04334814101457596\n",
            "Loss S11:  0.03185084208168767\n",
            "Loss S22:  0.03331709297543222\n",
            "Loss S33:  0.036236785013567314\n",
            "Loss S44:  0.03909114274111661\n",
            "Loss S11:  0.0317748264365253\n",
            "Loss S22:  0.03400216828144732\n",
            "Loss S33:  0.036473702816736134\n",
            "Loss S44:  0.03921491191500709\n",
            "Loss S11:  0.031818312863188404\n",
            "Loss S22:  0.034176060028614536\n",
            "Loss S33:  0.03644701886561609\n",
            "Loss S44:  0.03902663855302718\n",
            "Loss S11:  0.031734430753603216\n",
            "Loss S22:  0.034096156315105715\n",
            "Loss S33:  0.036578583008632426\n",
            "Loss S44:  0.03894954429167073\n",
            "Loss S11:  0.031736855815146486\n",
            "Loss S22:  0.034213337974221096\n",
            "Loss S33:  0.03652348325533025\n",
            "Loss S44:  0.039139065307145025\n",
            "Loss S11:  0.0316629024741591\n",
            "Loss S22:  0.03406668249823031\n",
            "Loss S33:  0.03648446732368626\n",
            "Loss S44:  0.03906636611848581\n",
            "Loss S11:  0.03163797641828866\n",
            "Loss S22:  0.034055919845549155\n",
            "Loss S33:  0.0364805169823304\n",
            "Loss S44:  0.03902320899593998\n",
            "Loss S11:  0.03153802964974333\n",
            "Loss S22:  0.03389028411496569\n",
            "Loss S33:  0.03639172785627989\n",
            "Loss S44:  0.03891825901321423\n",
            "Loss S11:  0.03153698750167758\n",
            "Loss S22:  0.033863919838280465\n",
            "Loss S33:  0.03644569402376374\n",
            "Loss S44:  0.03889853924840361\n",
            "Loss S11:  0.03163620126940826\n",
            "Loss S22:  0.033879460084556355\n",
            "Loss S33:  0.03631520297120113\n",
            "Loss S44:  0.038800667233691355\n",
            "Loss S11:  0.03168567702979655\n",
            "Loss S22:  0.03390441267742767\n",
            "Loss S33:  0.036298157462665626\n",
            "Loss S44:  0.038802555485351664\n",
            "Loss S11:  0.03167784069318417\n",
            "Loss S22:  0.03391198561457563\n",
            "Loss S33:  0.03633066716272969\n",
            "Loss S44:  0.03884974891735502\n",
            "Loss S11:  0.03172636865318276\n",
            "Loss S22:  0.03400173166212235\n",
            "Loss S33:  0.036348929102639205\n",
            "Loss S44:  0.038931279180159095\n",
            "Loss S11:  0.031743071841221326\n",
            "Loss S22:  0.03407048011291112\n",
            "Loss S33:  0.03641871909828896\n",
            "Loss S44:  0.03898954822133619\n",
            "Loss S11:  0.031845400603303056\n",
            "Loss S22:  0.03414725902064747\n",
            "Loss S33:  0.036517198512096276\n",
            "Loss S44:  0.03914993177383941\n",
            "Loss S11:  0.03184882757363852\n",
            "Loss S22:  0.034230217615269724\n",
            "Loss S33:  0.0364771161159003\n",
            "Loss S44:  0.03918576367801021\n",
            "Loss S11:  0.03186986185828147\n",
            "Loss S22:  0.03432090909421792\n",
            "Loss S33:  0.03657577263071523\n",
            "Loss S44:  0.03927177288814595\n",
            "Loss S11:  0.031898302092983576\n",
            "Loss S22:  0.03438697866478019\n",
            "Loss S33:  0.03661712495743899\n",
            "Loss S44:  0.03934186948103141\n",
            "Loss S11:  0.031861682310310335\n",
            "Loss S22:  0.034328102139044185\n",
            "Loss S33:  0.03659859522006899\n",
            "Loss S44:  0.03934324117309136\n",
            "Loss S11:  0.03188009258934218\n",
            "Loss S22:  0.03436602968073899\n",
            "Loss S33:  0.036621387881129536\n",
            "Loss S44:  0.039361255975505015\n",
            "Loss S11:  0.031904738537649406\n",
            "Loss S22:  0.03444077004788894\n",
            "Loss S33:  0.03665960223471384\n",
            "Loss S44:  0.03945635432174421\n",
            "Loss S11:  0.031952293850987204\n",
            "Loss S22:  0.03447324321584194\n",
            "Loss S33:  0.036667209927717485\n",
            "Loss S44:  0.03949166276403682\n",
            "Loss S11:  0.031942328430099406\n",
            "Loss S22:  0.03445543037542017\n",
            "Loss S33:  0.03663420248212236\n",
            "Loss S44:  0.0395201946007999\n",
            "Loss S11:  0.03199308402681746\n",
            "Loss S22:  0.03451691339124782\n",
            "Loss S33:  0.03668865731519288\n",
            "Loss S44:  0.039554004677227424\n",
            "Loss S11:  0.031986762654496376\n",
            "Loss S22:  0.03451991668972836\n",
            "Loss S33:  0.036639705804952113\n",
            "Loss S44:  0.03956147128961001\n",
            "Loss S11:  0.03196848825448089\n",
            "Loss S22:  0.034493602127179335\n",
            "Loss S33:  0.03664720101260591\n",
            "Loss S44:  0.03957146170666848\n",
            "Loss S11:  0.031957795231764605\n",
            "Loss S22:  0.034465222134911266\n",
            "Loss S33:  0.036613068842360014\n",
            "Loss S44:  0.03958077611903423\n",
            "Loss S11:  0.031948829580138165\n",
            "Loss S22:  0.034465477559914366\n",
            "Loss S33:  0.03663736085980812\n",
            "Loss S44:  0.03954060411485064\n",
            "Loss S11:  0.031959788105690605\n",
            "Loss S22:  0.03446034184589829\n",
            "Loss S33:  0.03664835525798224\n",
            "Loss S44:  0.039554920994846275\n",
            "Loss S11:  0.03198999795827557\n",
            "Loss S22:  0.03445002456440086\n",
            "Loss S33:  0.03667731559603317\n",
            "Loss S44:  0.03957505671437397\n",
            "Loss S11:  0.031985154052016435\n",
            "Loss S22:  0.03441358252856701\n",
            "Loss S33:  0.036684072777101846\n",
            "Loss S44:  0.03954967637201981\n",
            "Loss S11:  0.03197140147984956\n",
            "Loss S22:  0.03442264391788255\n",
            "Loss S33:  0.036692264040125495\n",
            "Loss S44:  0.03954551193712285\n",
            "Loss S11:  0.03197557655278469\n",
            "Loss S22:  0.03440949315165825\n",
            "Loss S33:  0.03670319559170761\n",
            "Loss S44:  0.0395394137911386\n",
            "Loss S11:  0.03198588655884665\n",
            "Loss S22:  0.034436059596776265\n",
            "Loss S33:  0.03670808745007361\n",
            "Loss S44:  0.0395546959074012\n",
            "Loss S11:  0.031999375490721134\n",
            "Loss S22:  0.03446385683624493\n",
            "Loss S33:  0.036686655144575996\n",
            "Loss S44:  0.03956808491564884\n",
            "Loss S11:  0.03200368933028792\n",
            "Loss S22:  0.03447029936718148\n",
            "Loss S33:  0.03667346655889066\n",
            "Loss S44:  0.03957941080766041\n",
            "Loss S11:  0.031994753268690446\n",
            "Loss S22:  0.03445644691245093\n",
            "Loss S33:  0.03665946844232211\n",
            "Loss S44:  0.039562078177768266\n",
            "Loss S11:  0.03196436264051071\n",
            "Loss S22:  0.03444031529187217\n",
            "Loss S33:  0.03665449511836676\n",
            "Loss S44:  0.039562066865405386\n",
            "Loss S11:  0.03193134309538185\n",
            "Loss S22:  0.034412577400541364\n",
            "Loss S33:  0.036621115882607067\n",
            "Loss S44:  0.039532465398159175\n",
            "Loss S11:  0.031935554332195075\n",
            "Loss S22:  0.03443175585397015\n",
            "Loss S33:  0.03664825104195578\n",
            "Loss S44:  0.03954756546979236\n",
            "Loss S11:  0.03192612344349243\n",
            "Loss S22:  0.03442874973653442\n",
            "Loss S33:  0.03665373094579309\n",
            "Loss S44:  0.03953305367208165\n",
            "Loss S11:  0.03195916559998468\n",
            "Loss S22:  0.03442734085253469\n",
            "Loss S33:  0.03666002300707292\n",
            "Loss S44:  0.03953276106516992\n",
            "Loss S11:  0.031945558956320765\n",
            "Loss S22:  0.03443347660094292\n",
            "Loss S33:  0.036641230301071484\n",
            "Loss S44:  0.03949835077965343\n",
            "Loss S11:  0.031940296169818125\n",
            "Loss S22:  0.03442974023672999\n",
            "Loss S33:  0.03661486283451521\n",
            "Loss S44:  0.03947956015328431\n",
            "Loss S11:  0.03194760321794354\n",
            "Loss S22:  0.03442969211189287\n",
            "Loss S33:  0.036628894317903435\n",
            "Loss S44:  0.039469219132960504\n",
            "Loss S11:  0.031948597580517706\n",
            "Loss S22:  0.03440630315134298\n",
            "Loss S33:  0.03662785327712523\n",
            "Loss S44:  0.03946749974292685\n",
            "Loss S11:  0.03192658494612214\n",
            "Loss S22:  0.03440355032982973\n",
            "Loss S33:  0.0366156857357916\n",
            "Loss S44:  0.039460406609021934\n",
            "Loss S11:  0.03192777050322158\n",
            "Loss S22:  0.034400433332905204\n",
            "Loss S33:  0.03662481311546046\n",
            "Loss S44:  0.039456395227113535\n",
            "Loss S11:  0.03192300475041274\n",
            "Loss S22:  0.034378940819965606\n",
            "Loss S33:  0.03659714583133\n",
            "Loss S44:  0.03943033046762472\n",
            "Validation: \n",
            " Loss S11:  0.029842855408787727\n",
            " Loss S22:  0.04394953325390816\n",
            " Loss S33:  0.0404483899474144\n",
            " Loss S44:  0.04420659318566322\n",
            " Loss S11:  0.03109045575062434\n",
            " Loss S22:  0.048071752524092085\n",
            " Loss S33:  0.04903733925450416\n",
            " Loss S44:  0.05285046072233291\n",
            " Loss S11:  0.03104650297361176\n",
            " Loss S22:  0.04811643391120725\n",
            " Loss S33:  0.04978133574491594\n",
            " Loss S44:  0.05201887948120513\n",
            " Loss S11:  0.031039389888526964\n",
            " Loss S22:  0.04771329230460964\n",
            " Loss S33:  0.048941418467486494\n",
            " Loss S44:  0.05181272901961061\n",
            " Loss S11:  0.031020990676350065\n",
            " Loss S22:  0.047334918315395894\n",
            " Loss S33:  0.04887397164179955\n",
            " Loss S44:  0.05173227844046958\n",
            "\n",
            "Epoch: 46\n",
            "Loss S11:  0.03777526319026947\n",
            "Loss S22:  0.037607695907354355\n",
            "Loss S33:  0.03663431853055954\n",
            "Loss S44:  0.040727995336055756\n",
            "Loss S11:  0.03256849792193283\n",
            "Loss S22:  0.03411773829297586\n",
            "Loss S33:  0.03720729611814022\n",
            "Loss S44:  0.03919013996015896\n",
            "Loss S11:  0.032122109085321426\n",
            "Loss S22:  0.03390508224921567\n",
            "Loss S33:  0.037321980066952254\n",
            "Loss S44:  0.038936859085446314\n",
            "Loss S11:  0.03182161791670707\n",
            "Loss S22:  0.034063989356640845\n",
            "Loss S33:  0.03678626933645818\n",
            "Loss S44:  0.03895145678712476\n",
            "Loss S11:  0.03209051289936391\n",
            "Loss S22:  0.03413501772575262\n",
            "Loss S33:  0.03684246035792479\n",
            "Loss S44:  0.038960417778026765\n",
            "Loss S11:  0.03202317918048186\n",
            "Loss S22:  0.03415024788210205\n",
            "Loss S33:  0.036568177352641146\n",
            "Loss S44:  0.03893107789404252\n",
            "Loss S11:  0.03184636578452392\n",
            "Loss S22:  0.033995131305495245\n",
            "Loss S33:  0.03643655175434762\n",
            "Loss S44:  0.03898094545622341\n",
            "Loss S11:  0.03176333894297271\n",
            "Loss S22:  0.03401708676361702\n",
            "Loss S33:  0.03644757269238922\n",
            "Loss S44:  0.03894395769481927\n",
            "Loss S11:  0.031604562115706045\n",
            "Loss S22:  0.034005952148157874\n",
            "Loss S33:  0.036320060606539986\n",
            "Loss S44:  0.0388631260505429\n",
            "Loss S11:  0.031531329543053446\n",
            "Loss S22:  0.03400968616971603\n",
            "Loss S33:  0.036164967670709224\n",
            "Loss S44:  0.03878126706887078\n",
            "Loss S11:  0.0315304371242476\n",
            "Loss S22:  0.03389666214732841\n",
            "Loss S33:  0.03607160507673674\n",
            "Loss S44:  0.03869124450305901\n",
            "Loss S11:  0.03156641726424028\n",
            "Loss S22:  0.03382099134562252\n",
            "Loss S33:  0.036063370655651565\n",
            "Loss S44:  0.03874494467635412\n",
            "Loss S11:  0.031633993256683195\n",
            "Loss S22:  0.03382251696460996\n",
            "Loss S33:  0.036094528815347304\n",
            "Loss S44:  0.03879582989683821\n",
            "Loss S11:  0.03169474093854882\n",
            "Loss S22:  0.03385408804218733\n",
            "Loss S33:  0.03614420055603708\n",
            "Loss S44:  0.03887220705283507\n",
            "Loss S11:  0.03169300413776374\n",
            "Loss S22:  0.03393789038643347\n",
            "Loss S33:  0.036256110876904314\n",
            "Loss S44:  0.03900949361053764\n",
            "Loss S11:  0.03174210035100283\n",
            "Loss S22:  0.034002797468411214\n",
            "Loss S33:  0.036277704273924134\n",
            "Loss S44:  0.039121838101488074\n",
            "Loss S11:  0.03174016257944685\n",
            "Loss S22:  0.03399690286898465\n",
            "Loss S33:  0.03624964430234077\n",
            "Loss S44:  0.039094415996570764\n",
            "Loss S11:  0.031799051692785574\n",
            "Loss S22:  0.03406092389459498\n",
            "Loss S33:  0.0362950651334566\n",
            "Loss S44:  0.039198269708114755\n",
            "Loss S11:  0.03183429188079597\n",
            "Loss S22:  0.03413255431416614\n",
            "Loss S33:  0.03624513736046151\n",
            "Loss S44:  0.03919701179417458\n",
            "Loss S11:  0.03181510752174243\n",
            "Loss S22:  0.03408752443239639\n",
            "Loss S33:  0.03617988494639309\n",
            "Loss S44:  0.03912950533378811\n",
            "Loss S11:  0.03180319795486939\n",
            "Loss S22:  0.034053972343084826\n",
            "Loss S33:  0.03617842872603912\n",
            "Loss S44:  0.03914480222694909\n",
            "Loss S11:  0.03184523975467795\n",
            "Loss S22:  0.034122758602361546\n",
            "Loss S33:  0.03621730602995197\n",
            "Loss S44:  0.03923887459305225\n",
            "Loss S11:  0.031854339407512505\n",
            "Loss S22:  0.034136657152173205\n",
            "Loss S33:  0.03622755788401511\n",
            "Loss S44:  0.039272692256788325\n",
            "Loss S11:  0.03185651311046117\n",
            "Loss S22:  0.03411920167892784\n",
            "Loss S33:  0.03619989592307832\n",
            "Loss S44:  0.03926174523252429\n",
            "Loss S11:  0.031902901611882126\n",
            "Loss S22:  0.034178144851104354\n",
            "Loss S33:  0.03626340698002532\n",
            "Loss S44:  0.03932230049334621\n",
            "Loss S11:  0.031919415594928294\n",
            "Loss S22:  0.03417093320137476\n",
            "Loss S33:  0.03623749742440256\n",
            "Loss S44:  0.039333513088197825\n",
            "Loss S11:  0.03189365788437854\n",
            "Loss S22:  0.03414279101880346\n",
            "Loss S33:  0.036250610197127095\n",
            "Loss S44:  0.03932831554862731\n",
            "Loss S11:  0.03187433736763097\n",
            "Loss S22:  0.03413570078426621\n",
            "Loss S33:  0.03621318748708801\n",
            "Loss S44:  0.03934480526185564\n",
            "Loss S11:  0.03185354193069333\n",
            "Loss S22:  0.03412595358793209\n",
            "Loss S33:  0.0361799578053141\n",
            "Loss S44:  0.03932553605305767\n",
            "Loss S11:  0.03184498455439441\n",
            "Loss S22:  0.034115854279487\n",
            "Loss S33:  0.03616321393483898\n",
            "Loss S44:  0.03933645800216911\n",
            "Loss S11:  0.03184425309795478\n",
            "Loss S22:  0.034127745282313354\n",
            "Loss S33:  0.03619682939221891\n",
            "Loss S44:  0.03935229734972466\n",
            "Loss S11:  0.03182213680992364\n",
            "Loss S22:  0.03406944754159144\n",
            "Loss S33:  0.03615022654917654\n",
            "Loss S44:  0.03931561678983391\n",
            "Loss S11:  0.031828621628685536\n",
            "Loss S22:  0.03409595485208005\n",
            "Loss S33:  0.036165355469718156\n",
            "Loss S44:  0.039315351630297035\n",
            "Loss S11:  0.0318303498864354\n",
            "Loss S22:  0.034101490314003204\n",
            "Loss S33:  0.03616516736091264\n",
            "Loss S44:  0.03934528083284456\n",
            "Loss S11:  0.03185597463590134\n",
            "Loss S22:  0.03416238174039073\n",
            "Loss S33:  0.036197040727105306\n",
            "Loss S44:  0.0394135760662318\n",
            "Loss S11:  0.03187635205579959\n",
            "Loss S22:  0.0341819869858502\n",
            "Loss S33:  0.03624477175374826\n",
            "Loss S44:  0.039437769835236405\n",
            "Loss S11:  0.03185937550548371\n",
            "Loss S22:  0.03417643528545167\n",
            "Loss S33:  0.0362318200871885\n",
            "Loss S44:  0.03944408711964404\n",
            "Loss S11:  0.03184497303196362\n",
            "Loss S22:  0.034167747191502396\n",
            "Loss S33:  0.03620498359323833\n",
            "Loss S44:  0.03940779053056336\n",
            "Loss S11:  0.03183888905043677\n",
            "Loss S22:  0.03417042914650884\n",
            "Loss S33:  0.036172634455084486\n",
            "Loss S44:  0.039387330520419\n",
            "Loss S11:  0.031825085677911556\n",
            "Loss S22:  0.03415544834130865\n",
            "Loss S33:  0.03614064265052071\n",
            "Loss S44:  0.039376110876993754\n",
            "Loss S11:  0.03183219314290401\n",
            "Loss S22:  0.0341751878556245\n",
            "Loss S33:  0.03616427808089595\n",
            "Loss S44:  0.03937500309282705\n",
            "Loss S11:  0.031808576780006544\n",
            "Loss S22:  0.03416834778431123\n",
            "Loss S33:  0.03615660883413759\n",
            "Loss S44:  0.039364093067147145\n",
            "Loss S11:  0.031837209930860234\n",
            "Loss S22:  0.0341880674918587\n",
            "Loss S33:  0.03618419384075032\n",
            "Loss S44:  0.039369218582353795\n",
            "Loss S11:  0.031814867751608036\n",
            "Loss S22:  0.03417978864465678\n",
            "Loss S33:  0.03614806740386995\n",
            "Loss S44:  0.039360599052117765\n",
            "Loss S11:  0.031804750282782006\n",
            "Loss S22:  0.034190578817003436\n",
            "Loss S33:  0.0361621722181976\n",
            "Loss S44:  0.03936088592016778\n",
            "Loss S11:  0.031817588876693316\n",
            "Loss S22:  0.034213239591395245\n",
            "Loss S33:  0.03618440402469323\n",
            "Loss S44:  0.03937281623581825\n",
            "Loss S11:  0.031801465539152605\n",
            "Loss S22:  0.03419169069645591\n",
            "Loss S33:  0.03616489685360754\n",
            "Loss S44:  0.03937211972125958\n",
            "Loss S11:  0.03179215972021127\n",
            "Loss S22:  0.03418981147862797\n",
            "Loss S33:  0.03617063528144309\n",
            "Loss S44:  0.03935868245591024\n",
            "Loss S11:  0.031784248098793\n",
            "Loss S22:  0.034205286719075356\n",
            "Loss S33:  0.036181908035011895\n",
            "Loss S44:  0.039357904553909064\n",
            "Loss S11:  0.03175513169779433\n",
            "Loss S22:  0.03418616738830468\n",
            "Loss S33:  0.036159255282001194\n",
            "Loss S44:  0.03935289948177192\n",
            "Validation: \n",
            " Loss S11:  0.029178373515605927\n",
            " Loss S22:  0.04488155245780945\n",
            " Loss S33:  0.04092806577682495\n",
            " Loss S44:  0.04425065964460373\n",
            " Loss S11:  0.03083214784661929\n",
            " Loss S22:  0.049042473236719765\n",
            " Loss S33:  0.04875693417021206\n",
            " Loss S44:  0.05254214860144116\n",
            " Loss S11:  0.030901506679450592\n",
            " Loss S22:  0.04925573617219925\n",
            " Loss S33:  0.04943111075497255\n",
            " Loss S44:  0.05185970554991466\n",
            " Loss S11:  0.030839562141260164\n",
            " Loss S22:  0.04892528771621282\n",
            " Loss S33:  0.04879252191205494\n",
            " Loss S44:  0.05173074222001873\n",
            " Loss S11:  0.03086122537008774\n",
            " Loss S22:  0.04855292851542249\n",
            " Loss S33:  0.048827399764163996\n",
            " Loss S44:  0.05165826823608375\n",
            "\n",
            "Epoch: 47\n",
            "Loss S11:  0.03545435890555382\n",
            "Loss S22:  0.03628047555685043\n",
            "Loss S33:  0.039983175694942474\n",
            "Loss S44:  0.044455088675022125\n",
            "Loss S11:  0.031040158461440693\n",
            "Loss S22:  0.03272262554277073\n",
            "Loss S33:  0.03549820878966288\n",
            "Loss S44:  0.03918238661505959\n",
            "Loss S11:  0.03130166401110944\n",
            "Loss S22:  0.033216160855122974\n",
            "Loss S33:  0.03586073512477534\n",
            "Loss S44:  0.038874510675668716\n",
            "Loss S11:  0.031213362490938555\n",
            "Loss S22:  0.03329151904871387\n",
            "Loss S33:  0.03570489975954256\n",
            "Loss S44:  0.038584345531079076\n",
            "Loss S11:  0.03148812537149685\n",
            "Loss S22:  0.033546532526975724\n",
            "Loss S33:  0.036009891503831236\n",
            "Loss S44:  0.03855828622855791\n",
            "Loss S11:  0.031467209332713894\n",
            "Loss S22:  0.033643862926492504\n",
            "Loss S33:  0.035920314892542125\n",
            "Loss S44:  0.03876032775231436\n",
            "Loss S11:  0.03140744469204887\n",
            "Loss S22:  0.03356774523854256\n",
            "Loss S33:  0.03591385199764713\n",
            "Loss S44:  0.038733202780856464\n",
            "Loss S11:  0.031519717891031585\n",
            "Loss S22:  0.03363633890387038\n",
            "Loss S33:  0.035855403327396215\n",
            "Loss S44:  0.03894603672161908\n",
            "Loss S11:  0.03142727019242299\n",
            "Loss S22:  0.0336233419538648\n",
            "Loss S33:  0.035671940395309604\n",
            "Loss S44:  0.03867994984726847\n",
            "Loss S11:  0.03136130839913756\n",
            "Loss S22:  0.03362607214968283\n",
            "Loss S33:  0.035621146395147504\n",
            "Loss S44:  0.038637345390660424\n",
            "Loss S11:  0.031243142607335996\n",
            "Loss S22:  0.03352256377439688\n",
            "Loss S33:  0.03551700561869853\n",
            "Loss S44:  0.038503211102273206\n",
            "Loss S11:  0.03126582557016665\n",
            "Loss S22:  0.03350643011498022\n",
            "Loss S33:  0.03551161874857572\n",
            "Loss S44:  0.03842198456058631\n",
            "Loss S11:  0.03128455853289809\n",
            "Loss S22:  0.03356031379239126\n",
            "Loss S33:  0.035526354804881345\n",
            "Loss S44:  0.038407528141805945\n",
            "Loss S11:  0.03134999247907682\n",
            "Loss S22:  0.03361850045621395\n",
            "Loss S33:  0.035642796530182125\n",
            "Loss S44:  0.03841572439738812\n",
            "Loss S11:  0.031415111431521724\n",
            "Loss S22:  0.033697993448334386\n",
            "Loss S33:  0.03574592983416209\n",
            "Loss S44:  0.03850275497381569\n",
            "Loss S11:  0.031436183014946266\n",
            "Loss S22:  0.03374551472235594\n",
            "Loss S33:  0.03576210576621507\n",
            "Loss S44:  0.03865493202446312\n",
            "Loss S11:  0.0314265361738316\n",
            "Loss S22:  0.03370952097155293\n",
            "Loss S33:  0.035744175835518366\n",
            "Loss S44:  0.03866450939767109\n",
            "Loss S11:  0.03151942530309248\n",
            "Loss S22:  0.03375443349979077\n",
            "Loss S33:  0.03580939300271154\n",
            "Loss S44:  0.03877260420493215\n",
            "Loss S11:  0.031565890971632954\n",
            "Loss S22:  0.03381794384418271\n",
            "Loss S33:  0.03585592106766793\n",
            "Loss S44:  0.038824473811282637\n",
            "Loss S11:  0.03155718510726672\n",
            "Loss S22:  0.03377305630457963\n",
            "Loss S33:  0.03587397062146539\n",
            "Loss S44:  0.03889097314308451\n",
            "Loss S11:  0.03153464528013818\n",
            "Loss S22:  0.0337343566786887\n",
            "Loss S33:  0.03582463090989127\n",
            "Loss S44:  0.038836949852420324\n",
            "Loss S11:  0.03156070834934994\n",
            "Loss S22:  0.033785376132805765\n",
            "Loss S33:  0.03585865122588325\n",
            "Loss S44:  0.03889374839256725\n",
            "Loss S11:  0.03157364973183131\n",
            "Loss S22:  0.033799580926269425\n",
            "Loss S33:  0.03587334965486332\n",
            "Loss S44:  0.03884818039126526\n",
            "Loss S11:  0.03153483869580479\n",
            "Loss S22:  0.03378000586128338\n",
            "Loss S33:  0.03585280047415139\n",
            "Loss S44:  0.038825687198411854\n",
            "Loss S11:  0.03155836402689768\n",
            "Loss S22:  0.03382777111968064\n",
            "Loss S33:  0.03587129327209659\n",
            "Loss S44:  0.03886538538386218\n",
            "Loss S11:  0.0315504465400579\n",
            "Loss S22:  0.03380000387945498\n",
            "Loss S33:  0.03583019474647434\n",
            "Loss S44:  0.03886955676681967\n",
            "Loss S11:  0.03157779518728969\n",
            "Loss S22:  0.03379806448701926\n",
            "Loss S33:  0.03583575792296636\n",
            "Loss S44:  0.03887394447436278\n",
            "Loss S11:  0.0315683717079928\n",
            "Loss S22:  0.03378568971151576\n",
            "Loss S33:  0.03581710448454227\n",
            "Loss S44:  0.03887477954265376\n",
            "Loss S11:  0.03152944888004841\n",
            "Loss S22:  0.03379331977898752\n",
            "Loss S33:  0.035804411456041914\n",
            "Loss S44:  0.0388693706367788\n",
            "Loss S11:  0.0315251945425145\n",
            "Loss S22:  0.03376391026290627\n",
            "Loss S33:  0.03583068305888946\n",
            "Loss S44:  0.038880762135245134\n",
            "Loss S11:  0.03153483565909126\n",
            "Loss S22:  0.03379385151043287\n",
            "Loss S33:  0.035824966873738456\n",
            "Loss S44:  0.03889711378173179\n",
            "Loss S11:  0.0315156722339499\n",
            "Loss S22:  0.03375392178989875\n",
            "Loss S33:  0.0357997759314212\n",
            "Loss S44:  0.038834054106876396\n",
            "Loss S11:  0.03151264964861105\n",
            "Loss S22:  0.03377030922732435\n",
            "Loss S33:  0.03579043530832941\n",
            "Loss S44:  0.038846039720970525\n",
            "Loss S11:  0.03150769071006343\n",
            "Loss S22:  0.03378287601506962\n",
            "Loss S33:  0.035798775735578506\n",
            "Loss S44:  0.03886460291159837\n",
            "Loss S11:  0.0315244412841685\n",
            "Loss S22:  0.03381525106247394\n",
            "Loss S33:  0.035838031600559914\n",
            "Loss S44:  0.03891342721027363\n",
            "Loss S11:  0.03153415981075193\n",
            "Loss S22:  0.0338201732228794\n",
            "Loss S33:  0.03586100755084274\n",
            "Loss S44:  0.03896650825852682\n",
            "Loss S11:  0.03153906185747514\n",
            "Loss S22:  0.03383611474402889\n",
            "Loss S33:  0.035863365792254\n",
            "Loss S44:  0.03897357323410769\n",
            "Loss S11:  0.0315246518470367\n",
            "Loss S22:  0.033816581054758835\n",
            "Loss S33:  0.035865364541020996\n",
            "Loss S44:  0.038949327960088244\n",
            "Loss S11:  0.03150752716724641\n",
            "Loss S22:  0.03382140852686927\n",
            "Loss S33:  0.035859239695891935\n",
            "Loss S44:  0.03893803170542392\n",
            "Loss S11:  0.031487291364375586\n",
            "Loss S22:  0.03380875089837005\n",
            "Loss S33:  0.035839562892647045\n",
            "Loss S44:  0.03892121462108534\n",
            "Loss S11:  0.03150955703750513\n",
            "Loss S22:  0.03385449928740165\n",
            "Loss S33:  0.0358644150559816\n",
            "Loss S44:  0.03893699702293498\n",
            "Loss S11:  0.03151272599859539\n",
            "Loss S22:  0.033869372490439975\n",
            "Loss S33:  0.0358536854848592\n",
            "Loss S44:  0.03895456956613383\n",
            "Loss S11:  0.03153076662179835\n",
            "Loss S22:  0.033867722213480646\n",
            "Loss S33:  0.0358667312114723\n",
            "Loss S44:  0.03894626481940797\n",
            "Loss S11:  0.031517865239986134\n",
            "Loss S22:  0.03384956759843638\n",
            "Loss S33:  0.03584519015266419\n",
            "Loss S44:  0.03893372406988575\n",
            "Loss S11:  0.03151757650651764\n",
            "Loss S22:  0.03385278916244064\n",
            "Loss S33:  0.035839961089886505\n",
            "Loss S44:  0.03892392214902945\n",
            "Loss S11:  0.03151342172811671\n",
            "Loss S22:  0.03384782172772546\n",
            "Loss S33:  0.03583717088064307\n",
            "Loss S44:  0.038940043273297226\n",
            "Loss S11:  0.03150827171777955\n",
            "Loss S22:  0.03381197138764207\n",
            "Loss S33:  0.03583407492948587\n",
            "Loss S44:  0.03894331357128708\n",
            "Loss S11:  0.03148648690930597\n",
            "Loss S22:  0.0337994493575739\n",
            "Loss S33:  0.03582989954391609\n",
            "Loss S44:  0.03894330399810888\n",
            "Loss S11:  0.03149322706532206\n",
            "Loss S22:  0.033809829851817685\n",
            "Loss S33:  0.035844713604375876\n",
            "Loss S44:  0.0389435154283245\n",
            "Loss S11:  0.03146653807135925\n",
            "Loss S22:  0.0337641091178913\n",
            "Loss S33:  0.03580716247015961\n",
            "Loss S44:  0.03892232004985547\n",
            "Validation: \n",
            " Loss S11:  0.029694344848394394\n",
            " Loss S22:  0.04326765984296799\n",
            " Loss S33:  0.04066864028573036\n",
            " Loss S44:  0.0442948155105114\n",
            " Loss S11:  0.031094167204130264\n",
            " Loss S22:  0.048091821372509\n",
            " Loss S33:  0.04809658051956268\n",
            " Loss S44:  0.05348252008358637\n",
            " Loss S11:  0.031190626186932007\n",
            " Loss S22:  0.04833776930846819\n",
            " Loss S33:  0.04882556672503308\n",
            " Loss S44:  0.05269121978341079\n",
            " Loss S11:  0.031214843366722592\n",
            " Loss S22:  0.048015871986013946\n",
            " Loss S33:  0.048147014968219354\n",
            " Loss S44:  0.05250211902817742\n",
            " Loss S11:  0.0311965866037357\n",
            " Loss S22:  0.04764033950589321\n",
            " Loss S33:  0.04811319805405758\n",
            " Loss S44:  0.052418472084366244\n",
            "\n",
            "Epoch: 48\n",
            "Loss S11:  0.034783490002155304\n",
            "Loss S22:  0.03393813222646713\n",
            "Loss S33:  0.04066573083400726\n",
            "Loss S44:  0.04058524966239929\n",
            "Loss S11:  0.03091183982112191\n",
            "Loss S22:  0.03329343568872322\n",
            "Loss S33:  0.036360475488684395\n",
            "Loss S44:  0.0384057458828796\n",
            "Loss S11:  0.031458379878174694\n",
            "Loss S22:  0.03346806755732922\n",
            "Loss S33:  0.03605137694449652\n",
            "Loss S44:  0.03882802260063943\n",
            "Loss S11:  0.031233830137118217\n",
            "Loss S22:  0.03330506696816413\n",
            "Loss S33:  0.03566168152516888\n",
            "Loss S44:  0.03840021140152408\n",
            "Loss S11:  0.03154242365825467\n",
            "Loss S22:  0.03333168090661851\n",
            "Loss S33:  0.035876079666905286\n",
            "Loss S44:  0.0386974478095043\n",
            "Loss S11:  0.03148082610877121\n",
            "Loss S22:  0.03341640247141614\n",
            "Loss S33:  0.03586428999608638\n",
            "Loss S44:  0.038657587313768914\n",
            "Loss S11:  0.0313912392822934\n",
            "Loss S22:  0.03323074207320565\n",
            "Loss S33:  0.03580960268002065\n",
            "Loss S44:  0.0384575578032947\n",
            "Loss S11:  0.031352134762515485\n",
            "Loss S22:  0.03329615974405282\n",
            "Loss S33:  0.035716344956570945\n",
            "Loss S44:  0.038493527650413376\n",
            "Loss S11:  0.031197862469671683\n",
            "Loss S22:  0.03320840415027407\n",
            "Loss S33:  0.03552370647221436\n",
            "Loss S44:  0.03826947225096785\n",
            "Loss S11:  0.031142927133120023\n",
            "Loss S22:  0.033286437371766175\n",
            "Loss S33:  0.035605522916539685\n",
            "Loss S44:  0.03815454973296805\n",
            "Loss S11:  0.031077175982075163\n",
            "Loss S22:  0.03316470860771024\n",
            "Loss S33:  0.035499768589835355\n",
            "Loss S44:  0.03801618239814692\n",
            "Loss S11:  0.03104248183133366\n",
            "Loss S22:  0.033167983715732895\n",
            "Loss S33:  0.0354567607277417\n",
            "Loss S44:  0.03789595242690395\n",
            "Loss S11:  0.031015896076752134\n",
            "Loss S22:  0.03317799012769352\n",
            "Loss S33:  0.03545766651691977\n",
            "Loss S44:  0.037943151987288606\n",
            "Loss S11:  0.031014185242411743\n",
            "Loss S22:  0.03321306279483642\n",
            "Loss S33:  0.035569010700547056\n",
            "Loss S44:  0.037996128315234\n",
            "Loss S11:  0.0310671468338011\n",
            "Loss S22:  0.03325267453142937\n",
            "Loss S33:  0.03559568359232541\n",
            "Loss S44:  0.03804374248423475\n",
            "Loss S11:  0.03113672582568317\n",
            "Loss S22:  0.033296893073233545\n",
            "Loss S33:  0.035726483380853734\n",
            "Loss S44:  0.03822436376123239\n",
            "Loss S11:  0.031123210091768584\n",
            "Loss S22:  0.03327264617087308\n",
            "Loss S33:  0.03571376453228989\n",
            "Loss S44:  0.03823350851210008\n",
            "Loss S11:  0.031217880438120044\n",
            "Loss S22:  0.03334976923352445\n",
            "Loss S33:  0.03574053698072308\n",
            "Loss S44:  0.03836577629659608\n",
            "Loss S11:  0.03131285672029738\n",
            "Loss S22:  0.03338319405872189\n",
            "Loss S33:  0.035811885748861245\n",
            "Loss S44:  0.03842338024335013\n",
            "Loss S11:  0.03129378611621744\n",
            "Loss S22:  0.03330335412076942\n",
            "Loss S33:  0.03577368607968872\n",
            "Loss S44:  0.038441912646537055\n",
            "Loss S11:  0.03128455999997718\n",
            "Loss S22:  0.03328613786777453\n",
            "Loss S33:  0.035802368213658904\n",
            "Loss S44:  0.038478802945186845\n",
            "Loss S11:  0.03132422512035234\n",
            "Loss S22:  0.033360069626456754\n",
            "Loss S33:  0.035807104604715985\n",
            "Loss S44:  0.03856601915653283\n",
            "Loss S11:  0.03131468069836565\n",
            "Loss S22:  0.033363784768732425\n",
            "Loss S33:  0.03578309509987475\n",
            "Loss S44:  0.03853787950395998\n",
            "Loss S11:  0.0312924405955004\n",
            "Loss S22:  0.033353907269832894\n",
            "Loss S33:  0.03572598069873981\n",
            "Loss S44:  0.038498821196618016\n",
            "Loss S11:  0.03130315411913197\n",
            "Loss S22:  0.03342875593276192\n",
            "Loss S33:  0.03576475704142909\n",
            "Loss S44:  0.038547154716930945\n",
            "Loss S11:  0.03129903259742783\n",
            "Loss S22:  0.03342304039612947\n",
            "Loss S33:  0.035767841839279786\n",
            "Loss S44:  0.03858681860198063\n",
            "Loss S11:  0.031278491583991784\n",
            "Loss S22:  0.03342617433732269\n",
            "Loss S33:  0.03574869277176958\n",
            "Loss S44:  0.03856886682097026\n",
            "Loss S11:  0.031286513508465896\n",
            "Loss S22:  0.03341661441067708\n",
            "Loss S33:  0.035749931458892416\n",
            "Loss S44:  0.038613687637107395\n",
            "Loss S11:  0.03129374919943114\n",
            "Loss S22:  0.033426143321808544\n",
            "Loss S33:  0.035751060281094704\n",
            "Loss S44:  0.03862092069936817\n",
            "Loss S11:  0.031292892045171807\n",
            "Loss S22:  0.033402754064278097\n",
            "Loss S33:  0.03574084363160879\n",
            "Loss S44:  0.038646305319165034\n",
            "Loss S11:  0.03131371790363741\n",
            "Loss S22:  0.03343268850218022\n",
            "Loss S33:  0.03577368782579503\n",
            "Loss S44:  0.03866906522268869\n",
            "Loss S11:  0.03129220779062851\n",
            "Loss S22:  0.03341576233027066\n",
            "Loss S33:  0.03578461493422364\n",
            "Loss S44:  0.03864702437420366\n",
            "Loss S11:  0.0312942869183709\n",
            "Loss S22:  0.0334307916485632\n",
            "Loss S33:  0.03577418124587551\n",
            "Loss S44:  0.03867103907967282\n",
            "Loss S11:  0.031289398861525645\n",
            "Loss S22:  0.03342949029542168\n",
            "Loss S33:  0.03578445844892469\n",
            "Loss S44:  0.03870101701862142\n",
            "Loss S11:  0.03136344659319721\n",
            "Loss S22:  0.033484126225419755\n",
            "Loss S33:  0.03582782518457283\n",
            "Loss S44:  0.038731835367392935\n",
            "Loss S11:  0.031372844127465854\n",
            "Loss S22:  0.033529646158685374\n",
            "Loss S33:  0.03584934756732904\n",
            "Loss S44:  0.0387702886959766\n",
            "Loss S11:  0.031373467492429835\n",
            "Loss S22:  0.03352604099030805\n",
            "Loss S33:  0.03584289990902112\n",
            "Loss S44:  0.03880152578911953\n",
            "Loss S11:  0.03135494241676723\n",
            "Loss S22:  0.033523193668483726\n",
            "Loss S33:  0.03584707501743521\n",
            "Loss S44:  0.03878706594241276\n",
            "Loss S11:  0.03132796623888291\n",
            "Loss S22:  0.03350734437407628\n",
            "Loss S33:  0.03583424695305468\n",
            "Loss S44:  0.03876305572942799\n",
            "Loss S11:  0.03129061708784164\n",
            "Loss S22:  0.03349237617991312\n",
            "Loss S33:  0.03581490202347183\n",
            "Loss S44:  0.038744667711693916\n",
            "Loss S11:  0.03131451689988895\n",
            "Loss S22:  0.033526161824935986\n",
            "Loss S33:  0.03583493704751989\n",
            "Loss S44:  0.03873812827163206\n",
            "Loss S11:  0.031331105443247916\n",
            "Loss S22:  0.033551712916969095\n",
            "Loss S33:  0.03582128300048284\n",
            "Loss S44:  0.03873325674058167\n",
            "Loss S11:  0.03135969589360395\n",
            "Loss S22:  0.03356373728631228\n",
            "Loss S33:  0.035818693752349695\n",
            "Loss S44:  0.038723889065822345\n",
            "Loss S11:  0.03135419602823506\n",
            "Loss S22:  0.03355398372662731\n",
            "Loss S33:  0.03579649692206814\n",
            "Loss S44:  0.03871724075203429\n",
            "Loss S11:  0.031336297752027335\n",
            "Loss S22:  0.033537866095769434\n",
            "Loss S33:  0.03576274621648853\n",
            "Loss S44:  0.03869940030500462\n",
            "Loss S11:  0.03134606639852545\n",
            "Loss S22:  0.03353521497114799\n",
            "Loss S33:  0.03578324475534211\n",
            "Loss S44:  0.038702239614896924\n",
            "Loss S11:  0.03135156896118875\n",
            "Loss S22:  0.033516884714752886\n",
            "Loss S33:  0.03577538266971934\n",
            "Loss S44:  0.03869354762056127\n",
            "Loss S11:  0.03135544848242763\n",
            "Loss S22:  0.03351575579897613\n",
            "Loss S33:  0.035780512757456986\n",
            "Loss S44:  0.03869922629175925\n",
            "Loss S11:  0.03135887226341544\n",
            "Loss S22:  0.03354282152844814\n",
            "Loss S33:  0.03578523887619531\n",
            "Loss S44:  0.038708194346784804\n",
            "Loss S11:  0.03134514772297166\n",
            "Loss S22:  0.033529603353069905\n",
            "Loss S33:  0.03576600919961322\n",
            "Loss S44:  0.038684065960150386\n",
            "Validation: \n",
            " Loss S11:  0.02947002276778221\n",
            " Loss S22:  0.04536205157637596\n",
            " Loss S33:  0.040149230509996414\n",
            " Loss S44:  0.0452500581741333\n",
            " Loss S11:  0.030419916978904178\n",
            " Loss S22:  0.048821347987367994\n",
            " Loss S33:  0.0493673002790837\n",
            " Loss S44:  0.053304754020202724\n",
            " Loss S11:  0.030353036459262777\n",
            " Loss S22:  0.04910926929697758\n",
            " Loss S33:  0.04989600917551576\n",
            " Loss S44:  0.052479769943690885\n",
            " Loss S11:  0.03026523801391242\n",
            " Loss S22:  0.04864615684405702\n",
            " Loss S33:  0.04913896914632594\n",
            " Loss S44:  0.0522033412803392\n",
            " Loss S11:  0.030231118386174427\n",
            " Loss S22:  0.04825919884958385\n",
            " Loss S33:  0.049125738671900315\n",
            " Loss S44:  0.05208070801548016\n",
            "\n",
            "Epoch: 49\n",
            "Loss S11:  0.03398453816771507\n",
            "Loss S22:  0.0388057641685009\n",
            "Loss S33:  0.037795763462781906\n",
            "Loss S44:  0.037495292723178864\n",
            "Loss S11:  0.0312756152654236\n",
            "Loss S22:  0.033141653815453705\n",
            "Loss S33:  0.0354479236358946\n",
            "Loss S44:  0.03882115672935139\n",
            "Loss S11:  0.030898667073675563\n",
            "Loss S22:  0.03320279469092687\n",
            "Loss S33:  0.03543768557054656\n",
            "Loss S44:  0.038564209427152364\n",
            "Loss S11:  0.03071206597791564\n",
            "Loss S22:  0.03314069604441043\n",
            "Loss S33:  0.0350666934203717\n",
            "Loss S44:  0.03842000975724189\n",
            "Loss S11:  0.03098504754101358\n",
            "Loss S22:  0.03321495611311459\n",
            "Loss S33:  0.035398383783858\n",
            "Loss S44:  0.038560736470106174\n",
            "Loss S11:  0.03110474337111501\n",
            "Loss S22:  0.033281449374614976\n",
            "Loss S33:  0.03541574809773296\n",
            "Loss S44:  0.038551820697737674\n",
            "Loss S11:  0.03100231225739737\n",
            "Loss S22:  0.03322115216831692\n",
            "Loss S33:  0.035345594871972426\n",
            "Loss S44:  0.0385207580860521\n",
            "Loss S11:  0.031011765271844998\n",
            "Loss S22:  0.03334160581965682\n",
            "Loss S33:  0.03531145784531681\n",
            "Loss S44:  0.03848282851173844\n",
            "Loss S11:  0.030918723708133637\n",
            "Loss S22:  0.033311175734356595\n",
            "Loss S33:  0.035238705322514344\n",
            "Loss S44:  0.03836730012187251\n",
            "Loss S11:  0.03087431978393387\n",
            "Loss S22:  0.033206242228766066\n",
            "Loss S33:  0.035239693657546256\n",
            "Loss S44:  0.038306683056302124\n",
            "Loss S11:  0.030776150875014834\n",
            "Loss S22:  0.03316947413921947\n",
            "Loss S33:  0.03519777291555806\n",
            "Loss S44:  0.038161305950419735\n",
            "Loss S11:  0.030742749286530254\n",
            "Loss S22:  0.033121723648127134\n",
            "Loss S33:  0.03513006386053455\n",
            "Loss S44:  0.03810702150200938\n",
            "Loss S11:  0.030694990826786058\n",
            "Loss S22:  0.03310417034470838\n",
            "Loss S33:  0.03517397794841735\n",
            "Loss S44:  0.038117179015943824\n",
            "Loss S11:  0.03073168176276083\n",
            "Loss S22:  0.033138254785355724\n",
            "Loss S33:  0.035256374883287735\n",
            "Loss S44:  0.038189719537742264\n",
            "Loss S11:  0.030768913421647767\n",
            "Loss S22:  0.033164344675152016\n",
            "Loss S33:  0.035370325973482\n",
            "Loss S44:  0.03821003894116862\n",
            "Loss S11:  0.030821128742189598\n",
            "Loss S22:  0.03319909329840679\n",
            "Loss S33:  0.03539179240828318\n",
            "Loss S44:  0.03835171345152602\n",
            "Loss S11:  0.030833536215265345\n",
            "Loss S22:  0.03321021020134784\n",
            "Loss S33:  0.035452987212016715\n",
            "Loss S44:  0.03838026653165403\n",
            "Loss S11:  0.030905311284532323\n",
            "Loss S22:  0.03326870989032656\n",
            "Loss S33:  0.035493310159671373\n",
            "Loss S44:  0.03839150674597562\n",
            "Loss S11:  0.030983948853710738\n",
            "Loss S22:  0.03332103930866521\n",
            "Loss S33:  0.03554194569793525\n",
            "Loss S44:  0.038400562099330335\n",
            "Loss S11:  0.031019880763055143\n",
            "Loss S22:  0.03331678097394748\n",
            "Loss S33:  0.03550112186029007\n",
            "Loss S44:  0.03840542689980012\n",
            "Loss S11:  0.031002110945273987\n",
            "Loss S22:  0.03327092027931071\n",
            "Loss S33:  0.03547389134393995\n",
            "Loss S44:  0.038364175744169386\n",
            "Loss S11:  0.031008618385023414\n",
            "Loss S22:  0.033306265408747\n",
            "Loss S33:  0.03547607123922398\n",
            "Loss S44:  0.03838973068654255\n",
            "Loss S11:  0.031033814947934172\n",
            "Loss S22:  0.033320640100006065\n",
            "Loss S33:  0.035506894938411755\n",
            "Loss S44:  0.03839645192566501\n",
            "Loss S11:  0.031044084546498922\n",
            "Loss S22:  0.03333628171998443\n",
            "Loss S33:  0.03552782825835339\n",
            "Loss S44:  0.03841286152601242\n",
            "Loss S11:  0.031085214171852314\n",
            "Loss S22:  0.03336963471009771\n",
            "Loss S33:  0.03552301370256669\n",
            "Loss S44:  0.038462910770256986\n",
            "Loss S11:  0.03109372862098939\n",
            "Loss S22:  0.03335722126957193\n",
            "Loss S33:  0.03549820034805522\n",
            "Loss S44:  0.03845994780025634\n",
            "Loss S11:  0.031084044747254402\n",
            "Loss S22:  0.033329141315543785\n",
            "Loss S33:  0.03548026581605276\n",
            "Loss S44:  0.03843854503327859\n",
            "Loss S11:  0.03106663847596443\n",
            "Loss S22:  0.03329135149002515\n",
            "Loss S33:  0.03548200174176385\n",
            "Loss S44:  0.03843099651411451\n",
            "Loss S11:  0.031063606432271174\n",
            "Loss S22:  0.03329445702086776\n",
            "Loss S33:  0.03544580367695394\n",
            "Loss S44:  0.038419085809980845\n",
            "Loss S11:  0.03105692831716177\n",
            "Loss S22:  0.033289029176548586\n",
            "Loss S33:  0.03544861741389606\n",
            "Loss S44:  0.03841713611100547\n",
            "Loss S11:  0.03106900205544855\n",
            "Loss S22:  0.033331035911739866\n",
            "Loss S33:  0.03548409741532763\n",
            "Loss S44:  0.038447938983226536\n",
            "Loss S11:  0.031039960981852754\n",
            "Loss S22:  0.03332041663947213\n",
            "Loss S33:  0.03548865013088061\n",
            "Loss S44:  0.03841357976437765\n",
            "Loss S11:  0.03104489193968127\n",
            "Loss S22:  0.03334406294462466\n",
            "Loss S33:  0.035500799511730484\n",
            "Loss S44:  0.03842788573783878\n",
            "Loss S11:  0.031040936505866555\n",
            "Loss S22:  0.03334625336001107\n",
            "Loss S33:  0.03551106133534829\n",
            "Loss S44:  0.03844729180692546\n",
            "Loss S11:  0.03106778921269951\n",
            "Loss S22:  0.03339996219467907\n",
            "Loss S33:  0.035525365560547695\n",
            "Loss S44:  0.038495188289461246\n",
            "Loss S11:  0.03107734485675297\n",
            "Loss S22:  0.03342823436393527\n",
            "Loss S33:  0.03555634238908433\n",
            "Loss S44:  0.03851855923010413\n",
            "Loss S11:  0.031070577997167354\n",
            "Loss S22:  0.033442353881338295\n",
            "Loss S33:  0.035556182853485406\n",
            "Loss S44:  0.03852925901597887\n",
            "Loss S11:  0.031058601312078233\n",
            "Loss S22:  0.03341491782982555\n",
            "Loss S33:  0.03553414979392306\n",
            "Loss S44:  0.03850129792953758\n",
            "Loss S11:  0.03103894418169038\n",
            "Loss S22:  0.0334002919925323\n",
            "Loss S33:  0.03551929007013013\n",
            "Loss S44:  0.03850084773981039\n",
            "Loss S11:  0.03101645663494954\n",
            "Loss S22:  0.033404357421695424\n",
            "Loss S33:  0.03550949124404994\n",
            "Loss S44:  0.038478859426344145\n",
            "Loss S11:  0.031012272838493834\n",
            "Loss S22:  0.03343748978508678\n",
            "Loss S33:  0.03553623489180854\n",
            "Loss S44:  0.038497943839899025\n",
            "Loss S11:  0.031011554537173316\n",
            "Loss S22:  0.0334405243133904\n",
            "Loss S33:  0.035532512740569684\n",
            "Loss S44:  0.038491793656653735\n",
            "Loss S11:  0.031046601304355256\n",
            "Loss S22:  0.03343470411423833\n",
            "Loss S33:  0.03554071207263124\n",
            "Loss S44:  0.03851015409211365\n",
            "Loss S11:  0.031041951357384846\n",
            "Loss S22:  0.033427177048925734\n",
            "Loss S33:  0.03551901538106516\n",
            "Loss S44:  0.038499138609715794\n",
            "Loss S11:  0.031026111776325978\n",
            "Loss S22:  0.033422322512990765\n",
            "Loss S33:  0.035509742399394646\n",
            "Loss S44:  0.038471512183918706\n",
            "Loss S11:  0.031036390933288704\n",
            "Loss S22:  0.03341393478363422\n",
            "Loss S33:  0.03551599009173144\n",
            "Loss S44:  0.03848006820797656\n",
            "Loss S11:  0.031050058780743087\n",
            "Loss S22:  0.033384813921053344\n",
            "Loss S33:  0.03549705534656239\n",
            "Loss S44:  0.03846691800948079\n",
            "Loss S11:  0.03104271403575146\n",
            "Loss S22:  0.033399160318630265\n",
            "Loss S33:  0.0354824339112785\n",
            "Loss S44:  0.03846670078180904\n",
            "Loss S11:  0.03104610425921587\n",
            "Loss S22:  0.033408576142422375\n",
            "Loss S33:  0.03550142994665554\n",
            "Loss S44:  0.03845723197948412\n",
            "Loss S11:  0.03104562327538877\n",
            "Loss S22:  0.033390072544243816\n",
            "Loss S33:  0.03547227090934022\n",
            "Loss S44:  0.038454458367860975\n",
            "Validation: \n",
            " Loss S11:  0.030108574777841568\n",
            " Loss S22:  0.044506900012493134\n",
            " Loss S33:  0.041327908635139465\n",
            " Loss S44:  0.04670232534408569\n",
            " Loss S11:  0.03096344747713634\n",
            " Loss S22:  0.04795850192507108\n",
            " Loss S33:  0.04876626886072613\n",
            " Loss S44:  0.05490576032371748\n",
            " Loss S11:  0.030947887179691616\n",
            " Loss S22:  0.0482084997361753\n",
            " Loss S33:  0.049410626019646485\n",
            " Loss S44:  0.05443818194837105\n",
            " Loss S11:  0.030895176603168737\n",
            " Loss S22:  0.04781515865785177\n",
            " Loss S33:  0.048698585419381255\n",
            " Loss S44:  0.05433533462833186\n",
            " Loss S11:  0.03084531624192073\n",
            " Loss S22:  0.04740376926866578\n",
            " Loss S33:  0.04869429017474622\n",
            " Loss S44:  0.05414741049394196\n",
            "\n",
            "Epoch: 50\n",
            "Loss S11:  0.03565467894077301\n",
            "Loss S22:  0.03639507666230202\n",
            "Loss S33:  0.0370914451777935\n",
            "Loss S44:  0.040991902351379395\n",
            "Loss S11:  0.031056654385545036\n",
            "Loss S22:  0.03248240764845501\n",
            "Loss S33:  0.03536486253142357\n",
            "Loss S44:  0.038540409369902176\n",
            "Loss S11:  0.030792236860309328\n",
            "Loss S22:  0.032535407780891375\n",
            "Loss S33:  0.03548744941751162\n",
            "Loss S44:  0.03825906486738296\n",
            "Loss S11:  0.030714676625305606\n",
            "Loss S22:  0.03274845542205918\n",
            "Loss S33:  0.03528730643372382\n",
            "Loss S44:  0.03808665443812647\n",
            "Loss S11:  0.03075376216594766\n",
            "Loss S22:  0.03294865964207708\n",
            "Loss S33:  0.03545676644255475\n",
            "Loss S44:  0.03811301881583726\n",
            "Loss S11:  0.0307675348777397\n",
            "Loss S22:  0.032999235442748256\n",
            "Loss S33:  0.03543889913342747\n",
            "Loss S44:  0.038106847919669805\n",
            "Loss S11:  0.0306110252305621\n",
            "Loss S22:  0.03295446691088012\n",
            "Loss S33:  0.03544024717001641\n",
            "Loss S44:  0.03796419977653222\n",
            "Loss S11:  0.03063853974388519\n",
            "Loss S22:  0.03300246740625778\n",
            "Loss S33:  0.03549821614484552\n",
            "Loss S44:  0.03807127827280004\n",
            "Loss S11:  0.030589554688812776\n",
            "Loss S22:  0.03295120652075167\n",
            "Loss S33:  0.035426967696827134\n",
            "Loss S44:  0.038054165050939275\n",
            "Loss S11:  0.030570837617902965\n",
            "Loss S22:  0.032917827859029664\n",
            "Loss S33:  0.03535428649367212\n",
            "Loss S44:  0.03797294158529449\n",
            "Loss S11:  0.03051962685024384\n",
            "Loss S22:  0.03290714871248986\n",
            "Loss S33:  0.035213233976818546\n",
            "Loss S44:  0.03774920762470453\n",
            "Loss S11:  0.030454321940605704\n",
            "Loss S22:  0.03284846056621891\n",
            "Loss S33:  0.035095395923063565\n",
            "Loss S44:  0.037620906760026745\n",
            "Loss S11:  0.030446052012487877\n",
            "Loss S22:  0.03291456932432888\n",
            "Loss S33:  0.03513623101344286\n",
            "Loss S44:  0.03771306598974653\n",
            "Loss S11:  0.030508324134213324\n",
            "Loss S22:  0.03300277519089575\n",
            "Loss S33:  0.0352441196592937\n",
            "Loss S44:  0.03782660067650198\n",
            "Loss S11:  0.030541575441123747\n",
            "Loss S22:  0.0330291867705313\n",
            "Loss S33:  0.035363744534816305\n",
            "Loss S44:  0.03788935168520779\n",
            "Loss S11:  0.03062264182117601\n",
            "Loss S22:  0.033065611052493386\n",
            "Loss S33:  0.035397300052623086\n",
            "Loss S44:  0.03803623356665207\n",
            "Loss S11:  0.030599500474907598\n",
            "Loss S22:  0.033048636065969555\n",
            "Loss S33:  0.035370398937545205\n",
            "Loss S44:  0.03804338225944442\n",
            "Loss S11:  0.030647394405296673\n",
            "Loss S22:  0.03307864873817092\n",
            "Loss S33:  0.03542816745694618\n",
            "Loss S44:  0.038107926951862915\n",
            "Loss S11:  0.0306527898621164\n",
            "Loss S22:  0.03308258410440295\n",
            "Loss S33:  0.035378282200236345\n",
            "Loss S44:  0.03810448102776517\n",
            "Loss S11:  0.030654366094014406\n",
            "Loss S22:  0.03303839716845782\n",
            "Loss S33:  0.03533224535238056\n",
            "Loss S44:  0.03808483222002134\n",
            "Loss S11:  0.03061800468620376\n",
            "Loss S22:  0.03299902475888456\n",
            "Loss S33:  0.035346111588513675\n",
            "Loss S44:  0.0380647702337201\n",
            "Loss S11:  0.030649992306297426\n",
            "Loss S22:  0.03305616957175223\n",
            "Loss S33:  0.035355716272835486\n",
            "Loss S44:  0.038145287430258155\n",
            "Loss S11:  0.030713225365089615\n",
            "Loss S22:  0.0330924854626483\n",
            "Loss S33:  0.03536079813856884\n",
            "Loss S44:  0.03819741131695687\n",
            "Loss S11:  0.03072032814611604\n",
            "Loss S22:  0.03306823209315151\n",
            "Loss S33:  0.035333564674312416\n",
            "Loss S44:  0.03816631208960112\n",
            "Loss S11:  0.03078370482595135\n",
            "Loss S22:  0.03312013908849712\n",
            "Loss S33:  0.03538473315258738\n",
            "Loss S44:  0.03821319786839465\n",
            "Loss S11:  0.030813775986314296\n",
            "Loss S22:  0.033104925345911446\n",
            "Loss S33:  0.035327362861350714\n",
            "Loss S44:  0.03820987266907654\n",
            "Loss S11:  0.03078882219234189\n",
            "Loss S22:  0.03310497048029041\n",
            "Loss S33:  0.03532363105408305\n",
            "Loss S44:  0.03821238377256412\n",
            "Loss S11:  0.03080440617108037\n",
            "Loss S22:  0.03311541215327613\n",
            "Loss S33:  0.035307758608042535\n",
            "Loss S44:  0.038193168221694516\n",
            "Loss S11:  0.030814045065768673\n",
            "Loss S22:  0.03309320785052199\n",
            "Loss S33:  0.03528816598298499\n",
            "Loss S44:  0.03817128986650514\n",
            "Loss S11:  0.030828726205885205\n",
            "Loss S22:  0.033070310444612686\n",
            "Loss S33:  0.0352832762223646\n",
            "Loss S44:  0.03818380517750671\n",
            "Loss S11:  0.030856253362672274\n",
            "Loss S22:  0.033108087318432684\n",
            "Loss S33:  0.0352910758488103\n",
            "Loss S44:  0.03819549749558946\n",
            "Loss S11:  0.03082227358935922\n",
            "Loss S22:  0.03309356242154764\n",
            "Loss S33:  0.035278322163166724\n",
            "Loss S44:  0.038179110256517816\n",
            "Loss S11:  0.030834842159926334\n",
            "Loss S22:  0.03310109089444917\n",
            "Loss S33:  0.03528458391232097\n",
            "Loss S44:  0.03819463153680166\n",
            "Loss S11:  0.03083587198706732\n",
            "Loss S22:  0.033088314445447346\n",
            "Loss S33:  0.03529732692930453\n",
            "Loss S44:  0.03824566496012794\n",
            "Loss S11:  0.030846683936646958\n",
            "Loss S22:  0.03312256748179012\n",
            "Loss S33:  0.035337069817433606\n",
            "Loss S44:  0.038305135088756985\n",
            "Loss S11:  0.03087632091148117\n",
            "Loss S22:  0.033162169471552906\n",
            "Loss S33:  0.03536338889254974\n",
            "Loss S44:  0.038340356529947696\n",
            "Loss S11:  0.030881410469804114\n",
            "Loss S22:  0.033189295071313914\n",
            "Loss S33:  0.03538657794513035\n",
            "Loss S44:  0.038327745216127244\n",
            "Loss S11:  0.030888207417453396\n",
            "Loss S22:  0.03316823588910128\n",
            "Loss S33:  0.03535178948364489\n",
            "Loss S44:  0.03829841594892049\n",
            "Loss S11:  0.03086280913686189\n",
            "Loss S22:  0.033173287746904714\n",
            "Loss S33:  0.035323003311873734\n",
            "Loss S44:  0.03827908812115199\n",
            "Loss S11:  0.03083930217930118\n",
            "Loss S22:  0.033145684472587714\n",
            "Loss S33:  0.035264757230801656\n",
            "Loss S44:  0.03824265303133089\n",
            "Loss S11:  0.03083954106765495\n",
            "Loss S22:  0.03315520163467549\n",
            "Loss S33:  0.03528201774673123\n",
            "Loss S44:  0.038258905812837835\n",
            "Loss S11:  0.03084178491429836\n",
            "Loss S22:  0.033182131149182933\n",
            "Loss S33:  0.03527777617992131\n",
            "Loss S44:  0.03827224733475641\n",
            "Loss S11:  0.03087198539170262\n",
            "Loss S22:  0.0331775126160253\n",
            "Loss S33:  0.0352876131730309\n",
            "Loss S44:  0.03825377279144851\n",
            "Loss S11:  0.030857152986208412\n",
            "Loss S22:  0.03315197640118079\n",
            "Loss S33:  0.03525834480892616\n",
            "Loss S44:  0.03824465122860435\n",
            "Loss S11:  0.03084424520735027\n",
            "Loss S22:  0.03313724943299413\n",
            "Loss S33:  0.03525737350777028\n",
            "Loss S44:  0.038224763188367525\n",
            "Loss S11:  0.03085845494911306\n",
            "Loss S22:  0.033141140022754936\n",
            "Loss S33:  0.035273396127140706\n",
            "Loss S44:  0.0382279037289371\n",
            "Loss S11:  0.030849794228914762\n",
            "Loss S22:  0.03313112174061788\n",
            "Loss S33:  0.03526750372201527\n",
            "Loss S44:  0.03821754689104128\n",
            "Loss S11:  0.0308382645083271\n",
            "Loss S22:  0.033136099244758584\n",
            "Loss S33:  0.03527187445693335\n",
            "Loss S44:  0.03821176784053729\n",
            "Loss S11:  0.03084616221704379\n",
            "Loss S22:  0.03314480393846169\n",
            "Loss S33:  0.035288879651723434\n",
            "Loss S44:  0.0382077978331433\n",
            "Loss S11:  0.030827550255445015\n",
            "Loss S22:  0.03312616883312369\n",
            "Loss S33:  0.0352689243792395\n",
            "Loss S44:  0.038180088362601526\n",
            "Validation: \n",
            " Loss S11:  0.029500093311071396\n",
            " Loss S22:  0.04436499625444412\n",
            " Loss S33:  0.03960976377129555\n",
            " Loss S44:  0.044548243284225464\n",
            " Loss S11:  0.030299894689094452\n",
            " Loss S22:  0.048508807307197935\n",
            " Loss S33:  0.047224869685513635\n",
            " Loss S44:  0.0522545443049499\n",
            " Loss S11:  0.030282977832163254\n",
            " Loss S22:  0.04882948827452776\n",
            " Loss S33:  0.04780417871547908\n",
            " Loss S44:  0.05177001964028289\n",
            " Loss S11:  0.030215769700828145\n",
            " Loss S22:  0.04841359216170233\n",
            " Loss S33:  0.04724977061641021\n",
            " Loss S44:  0.05177799911528337\n",
            " Loss S11:  0.030221883920423777\n",
            " Loss S22:  0.04798897493768622\n",
            " Loss S33:  0.04727681174322411\n",
            " Loss S44:  0.05159181324236187\n",
            "\n",
            "Epoch: 51\n",
            "Loss S11:  0.03243689239025116\n",
            "Loss S22:  0.03859173506498337\n",
            "Loss S33:  0.03967992588877678\n",
            "Loss S44:  0.04054233804345131\n",
            "Loss S11:  0.031287208538163795\n",
            "Loss S22:  0.033028255132111634\n",
            "Loss S33:  0.03563152710822495\n",
            "Loss S44:  0.039051986214789475\n",
            "Loss S11:  0.03090648858674935\n",
            "Loss S22:  0.03322860598564148\n",
            "Loss S33:  0.035537706865441235\n",
            "Loss S44:  0.03885090901028542\n",
            "Loss S11:  0.03087949506457775\n",
            "Loss S22:  0.03302302109378\n",
            "Loss S33:  0.035435173960943374\n",
            "Loss S44:  0.03842087198169001\n",
            "Loss S11:  0.030978954601578595\n",
            "Loss S22:  0.0330502831990399\n",
            "Loss S33:  0.03532082464818547\n",
            "Loss S44:  0.038231851669346416\n",
            "Loss S11:  0.03097213056011527\n",
            "Loss S22:  0.033108236158595365\n",
            "Loss S33:  0.035184521191552576\n",
            "Loss S44:  0.038258676654567905\n",
            "Loss S11:  0.030901970189125813\n",
            "Loss S22:  0.033041428560849094\n",
            "Loss S33:  0.03517893988822327\n",
            "Loss S44:  0.03804590266014709\n",
            "Loss S11:  0.030818570824995846\n",
            "Loss S22:  0.032944353874510446\n",
            "Loss S33:  0.03517872640784358\n",
            "Loss S44:  0.03794226407165259\n",
            "Loss S11:  0.03071735125540951\n",
            "Loss S22:  0.03285652077124442\n",
            "Loss S33:  0.035022476619040524\n",
            "Loss S44:  0.03778217282192207\n",
            "Loss S11:  0.030667168610207327\n",
            "Loss S22:  0.03276978316460992\n",
            "Loss S33:  0.03504721009796792\n",
            "Loss S44:  0.03766140878036782\n",
            "Loss S11:  0.030564352833103426\n",
            "Loss S22:  0.032740780327579766\n",
            "Loss S33:  0.03490947378743993\n",
            "Loss S44:  0.037568939388683525\n",
            "Loss S11:  0.03059242041529836\n",
            "Loss S22:  0.0327181370912103\n",
            "Loss S33:  0.03487537432092804\n",
            "Loss S44:  0.03744332126534737\n",
            "Loss S11:  0.030629511199091092\n",
            "Loss S22:  0.032716625223844505\n",
            "Loss S33:  0.034852261631941994\n",
            "Loss S44:  0.03752509809353135\n",
            "Loss S11:  0.03061600534231153\n",
            "Loss S22:  0.032749731093645096\n",
            "Loss S33:  0.034849666144329175\n",
            "Loss S44:  0.03760379697871572\n",
            "Loss S11:  0.0306376835450213\n",
            "Loss S22:  0.03277723821448096\n",
            "Loss S33:  0.03491895371083672\n",
            "Loss S44:  0.037631700885422684\n",
            "Loss S11:  0.030628665529240835\n",
            "Loss S22:  0.0328634986592247\n",
            "Loss S33:  0.03493045423401902\n",
            "Loss S44:  0.0377256293466549\n",
            "Loss S11:  0.030601109823454983\n",
            "Loss S22:  0.032847445111097016\n",
            "Loss S33:  0.034908674911867756\n",
            "Loss S44:  0.037746956282705996\n",
            "Loss S11:  0.03064931589260436\n",
            "Loss S22:  0.032923406556422945\n",
            "Loss S33:  0.03500112026319866\n",
            "Loss S44:  0.0378753305875767\n",
            "Loss S11:  0.030671086841525294\n",
            "Loss S22:  0.03294274434241471\n",
            "Loss S33:  0.03505483434345182\n",
            "Loss S44:  0.03788153923924457\n",
            "Loss S11:  0.03070712976001632\n",
            "Loss S22:  0.0329171396226315\n",
            "Loss S33:  0.03507981609299545\n",
            "Loss S44:  0.0379078746700162\n",
            "Loss S11:  0.03070374121379793\n",
            "Loss S22:  0.03287051224256333\n",
            "Loss S33:  0.035106107814988095\n",
            "Loss S44:  0.037884772083356014\n",
            "Loss S11:  0.030729373943466712\n",
            "Loss S22:  0.032918829691565434\n",
            "Loss S33:  0.03511184041710544\n",
            "Loss S44:  0.03797354499734409\n",
            "Loss S11:  0.03077166510653172\n",
            "Loss S22:  0.03292120229537131\n",
            "Loss S33:  0.03510703103473553\n",
            "Loss S44:  0.03798049641140985\n",
            "Loss S11:  0.03076132938459322\n",
            "Loss S22:  0.032894479755908895\n",
            "Loss S33:  0.0350626315921545\n",
            "Loss S44:  0.03795725694208434\n",
            "Loss S11:  0.03078486533889632\n",
            "Loss S22:  0.03289092985209835\n",
            "Loss S33:  0.035070910964825834\n",
            "Loss S44:  0.03795418243747035\n",
            "Loss S11:  0.03078494684778124\n",
            "Loss S22:  0.032908272176268565\n",
            "Loss S33:  0.03505889288487425\n",
            "Loss S44:  0.03796439145903188\n",
            "Loss S11:  0.030767082518601784\n",
            "Loss S22:  0.03288379178821355\n",
            "Loss S33:  0.03506352526217813\n",
            "Loss S44:  0.03799997061719383\n",
            "Loss S11:  0.03074301357941214\n",
            "Loss S22:  0.03290345659307668\n",
            "Loss S33:  0.0350552014250487\n",
            "Loss S44:  0.03800407032795058\n",
            "Loss S11:  0.030742481569643546\n",
            "Loss S22:  0.03287900324075672\n",
            "Loss S33:  0.035042357342682275\n",
            "Loss S44:  0.03797961510320151\n",
            "Loss S11:  0.03076964815201628\n",
            "Loss S22:  0.032890677253661287\n",
            "Loss S33:  0.035053462615793514\n",
            "Loss S44:  0.03797668762684278\n",
            "Loss S11:  0.0307858198806891\n",
            "Loss S22:  0.03289695503654274\n",
            "Loss S33:  0.035096717738481456\n",
            "Loss S44:  0.03801085854388551\n",
            "Loss S11:  0.030790068981779734\n",
            "Loss S22:  0.03285255802382035\n",
            "Loss S33:  0.0350683707851689\n",
            "Loss S44:  0.03799093505098122\n",
            "Loss S11:  0.030784826690905563\n",
            "Loss S22:  0.032882962404892455\n",
            "Loss S33:  0.0350852486477277\n",
            "Loss S44:  0.03799657254389885\n",
            "Loss S11:  0.03077692368554565\n",
            "Loss S22:  0.03288572417780352\n",
            "Loss S33:  0.03509128211893344\n",
            "Loss S44:  0.03799693356197766\n",
            "Loss S11:  0.030796278431967906\n",
            "Loss S22:  0.03289967430673561\n",
            "Loss S33:  0.03511471945868909\n",
            "Loss S44:  0.038014187687827695\n",
            "Loss S11:  0.030818694210120415\n",
            "Loss S22:  0.03292481379949639\n",
            "Loss S33:  0.03513817000932503\n",
            "Loss S44:  0.03804347299018137\n",
            "Loss S11:  0.030822049263978267\n",
            "Loss S22:  0.03293914364913989\n",
            "Loss S33:  0.03517044617322343\n",
            "Loss S44:  0.03810273928160152\n",
            "Loss S11:  0.030806041184581835\n",
            "Loss S22:  0.03294413078405626\n",
            "Loss S33:  0.03517344226493347\n",
            "Loss S44:  0.03810385006136007\n",
            "Loss S11:  0.03078995093585938\n",
            "Loss S22:  0.032935460931633714\n",
            "Loss S33:  0.035167415711901635\n",
            "Loss S44:  0.038087898000018806\n",
            "Loss S11:  0.030778374289498305\n",
            "Loss S22:  0.03291802458426989\n",
            "Loss S33:  0.035137469868373385\n",
            "Loss S44:  0.03805131395645154\n",
            "Loss S11:  0.03079146760378841\n",
            "Loss S22:  0.032949218415002576\n",
            "Loss S33:  0.035162171845944445\n",
            "Loss S44:  0.038048178617124846\n",
            "Loss S11:  0.030816423713508314\n",
            "Loss S22:  0.03296527465909647\n",
            "Loss S33:  0.03515384230227946\n",
            "Loss S44:  0.03806199415285512\n",
            "Loss S11:  0.030841147664253048\n",
            "Loss S22:  0.032956920674783036\n",
            "Loss S33:  0.03515204518697183\n",
            "Loss S44:  0.03804659512504546\n",
            "Loss S11:  0.030820007814124677\n",
            "Loss S22:  0.03294485166513311\n",
            "Loss S33:  0.0351244523352025\n",
            "Loss S44:  0.03803346348438075\n",
            "Loss S11:  0.03082202725923386\n",
            "Loss S22:  0.03294476885306322\n",
            "Loss S33:  0.0351176239558561\n",
            "Loss S44:  0.03803611773144361\n",
            "Loss S11:  0.030824644635627646\n",
            "Loss S22:  0.03297017065771007\n",
            "Loss S33:  0.035136504631547334\n",
            "Loss S44:  0.03804956829204263\n",
            "Loss S11:  0.03081854870805513\n",
            "Loss S22:  0.03295651560653316\n",
            "Loss S33:  0.035120851830973285\n",
            "Loss S44:  0.03805354435538263\n",
            "Loss S11:  0.030814648489220633\n",
            "Loss S22:  0.032951991609643724\n",
            "Loss S33:  0.035132660085608246\n",
            "Loss S44:  0.03806949443245137\n",
            "Loss S11:  0.030806198716163635\n",
            "Loss S22:  0.032952626590111646\n",
            "Loss S33:  0.035139719198560515\n",
            "Loss S44:  0.038068463657972965\n",
            "Loss S11:  0.030794747431749238\n",
            "Loss S22:  0.03292509432419254\n",
            "Loss S33:  0.03510463560975011\n",
            "Loss S44:  0.03806014815743732\n",
            "Validation: \n",
            " Loss S11:  0.028672313317656517\n",
            " Loss S22:  0.04519277811050415\n",
            " Loss S33:  0.040943555533885956\n",
            " Loss S44:  0.046654652804136276\n",
            " Loss S11:  0.02981658173458917\n",
            " Loss S22:  0.04845402815512249\n",
            " Loss S33:  0.04820472037508374\n",
            " Loss S44:  0.052289720092500956\n",
            " Loss S11:  0.02971587907068613\n",
            " Loss S22:  0.04887804570721417\n",
            " Loss S33:  0.048957099729194875\n",
            " Loss S44:  0.051748944855317836\n",
            " Loss S11:  0.029643600989804893\n",
            " Loss S22:  0.048287800345264496\n",
            " Loss S33:  0.04830017893529329\n",
            " Loss S44:  0.0516978169440246\n",
            " Loss S11:  0.02965190967567909\n",
            " Loss S22:  0.047885262625820844\n",
            " Loss S33:  0.0482434019630338\n",
            " Loss S44:  0.05163573041374301\n",
            "\n",
            "Epoch: 52\n",
            "Loss S11:  0.03429993614554405\n",
            "Loss S22:  0.03733830526471138\n",
            "Loss S33:  0.036505021154880524\n",
            "Loss S44:  0.03944850713014603\n",
            "Loss S11:  0.030252555893226105\n",
            "Loss S22:  0.03253694285045971\n",
            "Loss S33:  0.034650263291868294\n",
            "Loss S44:  0.03755164823748849\n",
            "Loss S11:  0.03032141959383374\n",
            "Loss S22:  0.03247570539159434\n",
            "Loss S33:  0.03483648704630988\n",
            "Loss S44:  0.037890678005559106\n",
            "Loss S11:  0.03044182638968191\n",
            "Loss S22:  0.032708309951328465\n",
            "Loss S33:  0.03468015002867868\n",
            "Loss S44:  0.0380719471121988\n",
            "Loss S11:  0.03055977612370398\n",
            "Loss S22:  0.03281241328251071\n",
            "Loss S33:  0.034769184387675144\n",
            "Loss S44:  0.038014085963368416\n",
            "Loss S11:  0.03054660580614034\n",
            "Loss S22:  0.03284528946467474\n",
            "Loss S33:  0.034754762827765705\n",
            "Loss S44:  0.03789477828232681\n",
            "Loss S11:  0.03045876107377107\n",
            "Loss S22:  0.032649083032471236\n",
            "Loss S33:  0.03483845024812417\n",
            "Loss S44:  0.037794235696802375\n",
            "Loss S11:  0.030452814661491086\n",
            "Loss S22:  0.0327870158977072\n",
            "Loss S33:  0.03486379152032691\n",
            "Loss S44:  0.03779708166462435\n",
            "Loss S11:  0.030288038959289776\n",
            "Loss S22:  0.032651875053107\n",
            "Loss S33:  0.034717226699914464\n",
            "Loss S44:  0.03773384226233135\n",
            "Loss S11:  0.03022178405752549\n",
            "Loss S22:  0.03263243805658031\n",
            "Loss S33:  0.03463371349805659\n",
            "Loss S44:  0.0375641529335753\n",
            "Loss S11:  0.03013639037709425\n",
            "Loss S22:  0.032560563932108405\n",
            "Loss S33:  0.034537613262782\n",
            "Loss S44:  0.03743669595384952\n",
            "Loss S11:  0.03013027758077458\n",
            "Loss S22:  0.0324665797488378\n",
            "Loss S33:  0.03447584781023833\n",
            "Loss S44:  0.037394506252697995\n",
            "Loss S11:  0.030210986671861537\n",
            "Loss S22:  0.032487164305384494\n",
            "Loss S33:  0.03456857828192474\n",
            "Loss S44:  0.037443415331076986\n",
            "Loss S11:  0.030180102654995808\n",
            "Loss S22:  0.03251132228276657\n",
            "Loss S33:  0.03463888509582927\n",
            "Loss S44:  0.03750080392049014\n",
            "Loss S11:  0.030247452794343023\n",
            "Loss S22:  0.03258838668359932\n",
            "Loss S33:  0.03475628217272725\n",
            "Loss S44:  0.0375310175569979\n",
            "Loss S11:  0.03036043435236476\n",
            "Loss S22:  0.032644504878595965\n",
            "Loss S33:  0.03482191168413257\n",
            "Loss S44:  0.037648412380490874\n",
            "Loss S11:  0.03033997864688035\n",
            "Loss S22:  0.03256934935585683\n",
            "Loss S33:  0.034812442021247766\n",
            "Loss S44:  0.037638930788969405\n",
            "Loss S11:  0.030380325544385883\n",
            "Loss S22:  0.03263054870896869\n",
            "Loss S33:  0.03485176739506206\n",
            "Loss S44:  0.03767851418788321\n",
            "Loss S11:  0.030362868167007166\n",
            "Loss S22:  0.03261957779650201\n",
            "Loss S33:  0.034877617992666546\n",
            "Loss S44:  0.03772358375071491\n",
            "Loss S11:  0.030359977193137738\n",
            "Loss S22:  0.03260528378149602\n",
            "Loss S33:  0.03490599713907504\n",
            "Loss S44:  0.03774954886687676\n",
            "Loss S11:  0.03030868764243909\n",
            "Loss S22:  0.032610021846656186\n",
            "Loss S33:  0.03489430025405255\n",
            "Loss S44:  0.03771730031428942\n",
            "Loss S11:  0.030350689224510397\n",
            "Loss S22:  0.03263869418175582\n",
            "Loss S33:  0.034917546806954095\n",
            "Loss S44:  0.037749413127229674\n",
            "Loss S11:  0.03034836835147838\n",
            "Loss S22:  0.032671960264579206\n",
            "Loss S33:  0.034929653081823796\n",
            "Loss S44:  0.03772422522137877\n",
            "Loss S11:  0.03032274211220669\n",
            "Loss S22:  0.03265890446834234\n",
            "Loss S33:  0.03492880538428501\n",
            "Loss S44:  0.037721820828783045\n",
            "Loss S11:  0.030376277454351983\n",
            "Loss S22:  0.03271428430723451\n",
            "Loss S33:  0.03496748890681386\n",
            "Loss S44:  0.03777397447941956\n",
            "Loss S11:  0.030354726465930976\n",
            "Loss S22:  0.03270831825100331\n",
            "Loss S33:  0.03495411090998061\n",
            "Loss S44:  0.037778300392734575\n",
            "Loss S11:  0.030378469034058837\n",
            "Loss S22:  0.0327357062463331\n",
            "Loss S33:  0.03497128478354878\n",
            "Loss S44:  0.03776999900538574\n",
            "Loss S11:  0.030385332532584446\n",
            "Loss S22:  0.0327323747874406\n",
            "Loss S33:  0.03496093048655679\n",
            "Loss S44:  0.037752080440686196\n",
            "Loss S11:  0.030371585310672102\n",
            "Loss S22:  0.03277455196049714\n",
            "Loss S33:  0.03495684484835197\n",
            "Loss S44:  0.03773316207384936\n",
            "Loss S11:  0.030381033241851225\n",
            "Loss S22:  0.03279049782229658\n",
            "Loss S33:  0.03498064517923647\n",
            "Loss S44:  0.03773447695185024\n",
            "Loss S11:  0.03038364995976421\n",
            "Loss S22:  0.0328320835181744\n",
            "Loss S33:  0.03501141109686357\n",
            "Loss S44:  0.03771805300986846\n",
            "Loss S11:  0.03038329166327259\n",
            "Loss S22:  0.03282952887811653\n",
            "Loss S33:  0.03498987415116699\n",
            "Loss S44:  0.037688521296983746\n",
            "Loss S11:  0.03038172674513309\n",
            "Loss S22:  0.03285817366174634\n",
            "Loss S33:  0.0350044181866345\n",
            "Loss S44:  0.037685426114550634\n",
            "Loss S11:  0.030374010822248963\n",
            "Loss S22:  0.0328459201768896\n",
            "Loss S33:  0.03500062324601898\n",
            "Loss S44:  0.03769451004433308\n",
            "Loss S11:  0.030407121252060985\n",
            "Loss S22:  0.032885397830841476\n",
            "Loss S33:  0.03503081898404356\n",
            "Loss S44:  0.03775674592474164\n",
            "Loss S11:  0.030415009956626472\n",
            "Loss S22:  0.032903533280320314\n",
            "Loss S33:  0.03505685661210973\n",
            "Loss S44:  0.03774708790168633\n",
            "Loss S11:  0.030429498635624583\n",
            "Loss S22:  0.03290923724564984\n",
            "Loss S33:  0.03507247013026988\n",
            "Loss S44:  0.03777739661036271\n",
            "Loss S11:  0.03044137705589401\n",
            "Loss S22:  0.032887469967099216\n",
            "Loss S33:  0.035071158866998\n",
            "Loss S44:  0.03775990989832383\n",
            "Loss S11:  0.030432160533084646\n",
            "Loss S22:  0.0328773733404443\n",
            "Loss S33:  0.03507085904244363\n",
            "Loss S44:  0.03775203893366918\n",
            "Loss S11:  0.030413046655485696\n",
            "Loss S22:  0.032854344062221326\n",
            "Loss S33:  0.03504834266479515\n",
            "Loss S44:  0.03773039185425357\n",
            "Loss S11:  0.030431144580505137\n",
            "Loss S22:  0.03288084503142465\n",
            "Loss S33:  0.035053069951788446\n",
            "Loss S44:  0.03774596063740682\n",
            "Loss S11:  0.030456950221836133\n",
            "Loss S22:  0.03288265270092627\n",
            "Loss S33:  0.03502533954189787\n",
            "Loss S44:  0.0377373727233378\n",
            "Loss S11:  0.03048139102226057\n",
            "Loss S22:  0.03286945861443488\n",
            "Loss S33:  0.03504200127957977\n",
            "Loss S44:  0.037742700980812526\n",
            "Loss S11:  0.030479476300960626\n",
            "Loss S22:  0.03286684141116186\n",
            "Loss S33:  0.03502553302509464\n",
            "Loss S44:  0.037733256795255604\n",
            "Loss S11:  0.030496279943455645\n",
            "Loss S22:  0.03286026231944561\n",
            "Loss S33:  0.03503354867738661\n",
            "Loss S44:  0.03773280982004137\n",
            "Loss S11:  0.030506563321284604\n",
            "Loss S22:  0.03285489821116835\n",
            "Loss S33:  0.03503903707782604\n",
            "Loss S44:  0.03775010483046196\n",
            "Loss S11:  0.030500773409151204\n",
            "Loss S22:  0.03283898285923449\n",
            "Loss S33:  0.035020930825888204\n",
            "Loss S44:  0.03774252549670394\n",
            "Loss S11:  0.030498990682279986\n",
            "Loss S22:  0.032833580378514186\n",
            "Loss S33:  0.034995528444204865\n",
            "Loss S44:  0.03773058468827903\n",
            "Loss S11:  0.030509336772152142\n",
            "Loss S22:  0.03284214837418525\n",
            "Loss S33:  0.03500285333026348\n",
            "Loss S44:  0.03772835563984458\n",
            "Loss S11:  0.03048990008681102\n",
            "Loss S22:  0.03282741634383828\n",
            "Loss S33:  0.034970904285061624\n",
            "Loss S44:  0.037701510518880335\n",
            "Validation: \n",
            " Loss S11:  0.028609080240130424\n",
            " Loss S22:  0.043823741376399994\n",
            " Loss S33:  0.03915446996688843\n",
            " Loss S44:  0.04388110712170601\n",
            " Loss S11:  0.030359319189474696\n",
            " Loss S22:  0.04761281112829844\n",
            " Loss S33:  0.04715489897699583\n",
            " Loss S44:  0.051232079310076575\n",
            " Loss S11:  0.03027173785901651\n",
            " Loss S22:  0.0476660677572576\n",
            " Loss S33:  0.047781207485169896\n",
            " Loss S44:  0.050621114852951794\n",
            " Loss S11:  0.030174632113976557\n",
            " Loss S22:  0.04723344645539268\n",
            " Loss S33:  0.04708267186508804\n",
            " Loss S44:  0.05052534961065308\n",
            " Loss S11:  0.030218154789856923\n",
            " Loss S22:  0.04683126798934407\n",
            " Loss S33:  0.04703404096725546\n",
            " Loss S44:  0.05045438180735082\n",
            "\n",
            "Epoch: 53\n",
            "Loss S11:  0.03175047039985657\n",
            "Loss S22:  0.03522145003080368\n",
            "Loss S33:  0.035726312547922134\n",
            "Loss S44:  0.037452250719070435\n",
            "Loss S11:  0.03007497469132597\n",
            "Loss S22:  0.03228183429349552\n",
            "Loss S33:  0.03439170257611708\n",
            "Loss S44:  0.036738073283975776\n",
            "Loss S11:  0.03004511357063339\n",
            "Loss S22:  0.03239560171606995\n",
            "Loss S33:  0.03517155366994086\n",
            "Loss S44:  0.0371656950031008\n",
            "Loss S11:  0.029775919452790293\n",
            "Loss S22:  0.032402637865274184\n",
            "Loss S33:  0.03486489035910176\n",
            "Loss S44:  0.03716083688120688\n",
            "Loss S11:  0.03004021224815671\n",
            "Loss S22:  0.03253112224543967\n",
            "Loss S33:  0.0348518649797614\n",
            "Loss S44:  0.03707194164758775\n",
            "Loss S11:  0.030118828091551277\n",
            "Loss S22:  0.03264671290183768\n",
            "Loss S33:  0.034549255435373266\n",
            "Loss S44:  0.03717866350038379\n",
            "Loss S11:  0.030057595401513773\n",
            "Loss S22:  0.03254373014339658\n",
            "Loss S33:  0.03439760522642096\n",
            "Loss S44:  0.03713540250404936\n",
            "Loss S11:  0.029989874856153006\n",
            "Loss S22:  0.032487789195188334\n",
            "Loss S33:  0.03433062715954344\n",
            "Loss S44:  0.03720963898469025\n",
            "Loss S11:  0.02992925825126377\n",
            "Loss S22:  0.03230507320000066\n",
            "Loss S33:  0.034256897015888015\n",
            "Loss S44:  0.03718554762042599\n",
            "Loss S11:  0.029854387495216433\n",
            "Loss S22:  0.03227199472814471\n",
            "Loss S33:  0.034304153358379566\n",
            "Loss S44:  0.037140874785708856\n",
            "Loss S11:  0.029900718369696398\n",
            "Loss S22:  0.03215279791614797\n",
            "Loss S33:  0.034207892244552625\n",
            "Loss S44:  0.03699965900418782\n",
            "Loss S11:  0.029855806783244416\n",
            "Loss S22:  0.032103876188934385\n",
            "Loss S33:  0.03408754769679125\n",
            "Loss S44:  0.036922792358709884\n",
            "Loss S11:  0.02989484309719121\n",
            "Loss S22:  0.032124008937192354\n",
            "Loss S33:  0.034107172830907764\n",
            "Loss S44:  0.036973660765600595\n",
            "Loss S11:  0.029901509137667773\n",
            "Loss S22:  0.03223557684940236\n",
            "Loss S33:  0.03418119113807005\n",
            "Loss S44:  0.03701492407508479\n",
            "Loss S11:  0.02991620954522427\n",
            "Loss S22:  0.03225276421042199\n",
            "Loss S33:  0.034240444514451296\n",
            "Loss S44:  0.037085031681026975\n",
            "Loss S11:  0.02999402955174446\n",
            "Loss S22:  0.03228953658350256\n",
            "Loss S33:  0.03427210868342428\n",
            "Loss S44:  0.03724850692002978\n",
            "Loss S11:  0.029959147217573586\n",
            "Loss S22:  0.032242369195966986\n",
            "Loss S33:  0.03427089934717424\n",
            "Loss S44:  0.03731898778343793\n",
            "Loss S11:  0.029985850143153765\n",
            "Loss S22:  0.032275360273687465\n",
            "Loss S33:  0.03434635227143067\n",
            "Loss S44:  0.03741651098107734\n",
            "Loss S11:  0.030026569170105523\n",
            "Loss S22:  0.03232194741580697\n",
            "Loss S33:  0.03440897824314747\n",
            "Loss S44:  0.037463129904553374\n",
            "Loss S11:  0.030030986157621387\n",
            "Loss S22:  0.03228919949207006\n",
            "Loss S33:  0.03438360660449061\n",
            "Loss S44:  0.037489362948461974\n",
            "Loss S11:  0.029989599857238394\n",
            "Loss S22:  0.03225374928865563\n",
            "Loss S33:  0.03434964592814149\n",
            "Loss S44:  0.03746063012944822\n",
            "Loss S11:  0.030016852533930286\n",
            "Loss S22:  0.03231555929681136\n",
            "Loss S33:  0.03438030099452107\n",
            "Loss S44:  0.03756363094029551\n",
            "Loss S11:  0.030018335992124823\n",
            "Loss S22:  0.03235369956129277\n",
            "Loss S33:  0.03441072227559748\n",
            "Loss S44:  0.037565305281211346\n",
            "Loss S11:  0.029980409906540082\n",
            "Loss S22:  0.03232886204375075\n",
            "Loss S33:  0.034404234928421644\n",
            "Loss S44:  0.037537986396517585\n",
            "Loss S11:  0.030048736374964358\n",
            "Loss S22:  0.03240355472099732\n",
            "Loss S33:  0.03446724991549842\n",
            "Loss S44:  0.03755666381250526\n",
            "Loss S11:  0.03006997107123711\n",
            "Loss S22:  0.0324247818573775\n",
            "Loss S33:  0.034486844460387155\n",
            "Loss S44:  0.03760370685611351\n",
            "Loss S11:  0.03009630300789729\n",
            "Loss S22:  0.032425614637661714\n",
            "Loss S33:  0.034490201358194555\n",
            "Loss S44:  0.037601245995396854\n",
            "Loss S11:  0.030075928622451217\n",
            "Loss S22:  0.03240763233129169\n",
            "Loss S33:  0.034482459114981316\n",
            "Loss S44:  0.03755879768503768\n",
            "Loss S11:  0.03007798817578796\n",
            "Loss S22:  0.03242505720263062\n",
            "Loss S33:  0.03444692129236099\n",
            "Loss S44:  0.0375219844613105\n",
            "Loss S11:  0.03010227273880821\n",
            "Loss S22:  0.03244731285292463\n",
            "Loss S33:  0.034467113284310937\n",
            "Loss S44:  0.03754815015673023\n",
            "Loss S11:  0.030113606846758297\n",
            "Loss S22:  0.032476841017257334\n",
            "Loss S33:  0.034498237308572696\n",
            "Loss S44:  0.03754617946463764\n",
            "Loss S11:  0.03010496000648503\n",
            "Loss S22:  0.0324774298553394\n",
            "Loss S33:  0.03449413376797434\n",
            "Loss S44:  0.037517501041293144\n",
            "Loss S11:  0.030131562418562602\n",
            "Loss S22:  0.032505204663191255\n",
            "Loss S33:  0.03449249909929583\n",
            "Loss S44:  0.03753004321172787\n",
            "Loss S11:  0.03014919747601283\n",
            "Loss S22:  0.03250901888932166\n",
            "Loss S33:  0.034490164374972036\n",
            "Loss S44:  0.0375490837148039\n",
            "Loss S11:  0.030188830290809054\n",
            "Loss S22:  0.03254495950038832\n",
            "Loss S33:  0.034518325133276476\n",
            "Loss S44:  0.03759438952545791\n",
            "Loss S11:  0.030202958812443618\n",
            "Loss S22:  0.03258196202947883\n",
            "Loss S33:  0.03455200070180954\n",
            "Loss S44:  0.03761249081764975\n",
            "Loss S11:  0.030218103575491838\n",
            "Loss S22:  0.03260253649978922\n",
            "Loss S33:  0.03454852380229991\n",
            "Loss S44:  0.03762036735847221\n",
            "Loss S11:  0.030229011369803844\n",
            "Loss S22:  0.03260363411971662\n",
            "Loss S33:  0.03455989895202721\n",
            "Loss S44:  0.037609820111059754\n",
            "Loss S11:  0.03021525377969886\n",
            "Loss S22:  0.032605632636060554\n",
            "Loss S33:  0.03455045170045588\n",
            "Loss S44:  0.037590459668691074\n",
            "Loss S11:  0.03019513338423141\n",
            "Loss S22:  0.03258199881181083\n",
            "Loss S33:  0.03450370812907701\n",
            "Loss S44:  0.037566619575061765\n",
            "Loss S11:  0.030213478929421254\n",
            "Loss S22:  0.032595579774859836\n",
            "Loss S33:  0.034528450311122096\n",
            "Loss S44:  0.03757168957847461\n",
            "Loss S11:  0.03020068276175037\n",
            "Loss S22:  0.03259730537097964\n",
            "Loss S33:  0.03452575972208576\n",
            "Loss S44:  0.03755786328365768\n",
            "Loss S11:  0.030211295923682685\n",
            "Loss S22:  0.03260556999992163\n",
            "Loss S33:  0.034547088313803535\n",
            "Loss S44:  0.037561050516657195\n",
            "Loss S11:  0.030201726755140275\n",
            "Loss S22:  0.032591132386619974\n",
            "Loss S33:  0.03453488554157265\n",
            "Loss S44:  0.0375532861971869\n",
            "Loss S11:  0.030183653725214015\n",
            "Loss S22:  0.03257992691751105\n",
            "Loss S33:  0.03452244874030825\n",
            "Loss S44:  0.037563400030507794\n",
            "Loss S11:  0.03020401351451213\n",
            "Loss S22:  0.032583177184681934\n",
            "Loss S33:  0.03453532048808762\n",
            "Loss S44:  0.03757243758344201\n",
            "Loss S11:  0.030189747398884848\n",
            "Loss S22:  0.032557739381256175\n",
            "Loss S33:  0.034527521043078505\n",
            "Loss S44:  0.03753860838258732\n",
            "Loss S11:  0.030180364851137887\n",
            "Loss S22:  0.03254752209501029\n",
            "Loss S33:  0.034532784209477925\n",
            "Loss S44:  0.03753815167791145\n",
            "Loss S11:  0.030186409858711793\n",
            "Loss S22:  0.0325585918156372\n",
            "Loss S33:  0.034542868521815775\n",
            "Loss S44:  0.03752605874563825\n",
            "Loss S11:  0.03016518420196478\n",
            "Loss S22:  0.03254004778784307\n",
            "Loss S33:  0.034515944136925\n",
            "Loss S44:  0.03749605345628897\n",
            "Validation: \n",
            " Loss S11:  0.02929343655705452\n",
            " Loss S22:  0.04386305809020996\n",
            " Loss S33:  0.04115874320268631\n",
            " Loss S44:  0.04386123642325401\n",
            " Loss S11:  0.030787851395351545\n",
            " Loss S22:  0.0485866041410537\n",
            " Loss S33:  0.04867846678410258\n",
            " Loss S44:  0.0520600743946575\n",
            " Loss S11:  0.03069857939532617\n",
            " Loss S22:  0.04877599692199289\n",
            " Loss S33:  0.04937794368441512\n",
            " Loss S44:  0.051439355877114505\n",
            " Loss S11:  0.03064699348856191\n",
            " Loss S22:  0.04826833326064172\n",
            " Loss S33:  0.048499526486533585\n",
            " Loss S44:  0.05127164434458389\n",
            " Loss S11:  0.03066564553681715\n",
            " Loss S22:  0.0478604713247882\n",
            " Loss S33:  0.04841985562701284\n",
            " Loss S44:  0.05111306620600783\n",
            "\n",
            "Epoch: 54\n",
            "Loss S11:  0.034666307270526886\n",
            "Loss S22:  0.03562029451131821\n",
            "Loss S33:  0.038406819105148315\n",
            "Loss S44:  0.04302019998431206\n",
            "Loss S11:  0.031507529826326805\n",
            "Loss S22:  0.03268667733804746\n",
            "Loss S33:  0.034899865023114464\n",
            "Loss S44:  0.03770244629545645\n",
            "Loss S11:  0.030677397691068194\n",
            "Loss S22:  0.03229634694400288\n",
            "Loss S33:  0.03463800198265484\n",
            "Loss S44:  0.03751349697510401\n",
            "Loss S11:  0.030334981759229013\n",
            "Loss S22:  0.03182950459661022\n",
            "Loss S33:  0.03422984757250355\n",
            "Loss S44:  0.03721121975010441\n",
            "Loss S11:  0.0303357910455727\n",
            "Loss S22:  0.032019638297397915\n",
            "Loss S33:  0.03427510713113517\n",
            "Loss S44:  0.037414202602898204\n",
            "Loss S11:  0.03027301044294647\n",
            "Loss S22:  0.03195455714183695\n",
            "Loss S33:  0.0341783309099721\n",
            "Loss S44:  0.037370185714726355\n",
            "Loss S11:  0.030237257572226835\n",
            "Loss S22:  0.03209279221100885\n",
            "Loss S33:  0.03430125166158207\n",
            "Loss S44:  0.03730227327981933\n",
            "Loss S11:  0.030201970061785738\n",
            "Loss S22:  0.03217515401857\n",
            "Loss S33:  0.03435800951236571\n",
            "Loss S44:  0.037332522984541636\n",
            "Loss S11:  0.030179573108016708\n",
            "Loss S22:  0.03217043295318698\n",
            "Loss S33:  0.0343909911857343\n",
            "Loss S44:  0.037237524434372234\n",
            "Loss S11:  0.030118655085891156\n",
            "Loss S22:  0.0321276742801234\n",
            "Loss S33:  0.034401850938633245\n",
            "Loss S44:  0.03717146682870257\n",
            "Loss S11:  0.030095514149801567\n",
            "Loss S22:  0.03204593576933488\n",
            "Loss S33:  0.03431154195551235\n",
            "Loss S44:  0.03711398189315702\n",
            "Loss S11:  0.030012611215850256\n",
            "Loss S22:  0.03199589739108945\n",
            "Loss S33:  0.034192859958689495\n",
            "Loss S44:  0.037022549038132034\n",
            "Loss S11:  0.030023702982166584\n",
            "Loss S22:  0.03198185761666988\n",
            "Loss S33:  0.03420376508258098\n",
            "Loss S44:  0.03699255409873714\n",
            "Loss S11:  0.030066626686745018\n",
            "Loss S22:  0.032072200560729014\n",
            "Loss S33:  0.03431396264554435\n",
            "Loss S44:  0.03711675543992119\n",
            "Loss S11:  0.030097308839466555\n",
            "Loss S22:  0.0321409754745715\n",
            "Loss S33:  0.0343328197982083\n",
            "Loss S44:  0.037188333941372574\n",
            "Loss S11:  0.030178277833473603\n",
            "Loss S22:  0.032229668960369974\n",
            "Loss S33:  0.03439943427034956\n",
            "Loss S44:  0.037260556815574504\n",
            "Loss S11:  0.03020754071264904\n",
            "Loss S22:  0.03224545507836416\n",
            "Loss S33:  0.03439224874658614\n",
            "Loss S44:  0.03726003391911154\n",
            "Loss S11:  0.030270147077434244\n",
            "Loss S22:  0.03227446128053275\n",
            "Loss S33:  0.03446712301314225\n",
            "Loss S44:  0.03727525833188093\n",
            "Loss S11:  0.03036329707964349\n",
            "Loss S22:  0.032307403553174345\n",
            "Loss S33:  0.034514147541305634\n",
            "Loss S44:  0.03728363013605057\n",
            "Loss S11:  0.03037192069813219\n",
            "Loss S22:  0.03227696272286133\n",
            "Loss S33:  0.03449064904716626\n",
            "Loss S44:  0.037299470858970236\n",
            "Loss S11:  0.030347208080081203\n",
            "Loss S22:  0.03226381594046431\n",
            "Loss S33:  0.034463009004717444\n",
            "Loss S44:  0.037294079961987275\n",
            "Loss S11:  0.030303653453198656\n",
            "Loss S22:  0.03227758404944463\n",
            "Loss S33:  0.034467167975778264\n",
            "Loss S44:  0.03727887236288091\n",
            "Loss S11:  0.03033325319679884\n",
            "Loss S22:  0.032259811820015645\n",
            "Loss S33:  0.034447576252246336\n",
            "Loss S44:  0.037292898147961134\n",
            "Loss S11:  0.030323684384877033\n",
            "Loss S22:  0.032269874458640685\n",
            "Loss S33:  0.034450453315010836\n",
            "Loss S44:  0.03733869605733977\n",
            "Loss S11:  0.03036472324235558\n",
            "Loss S22:  0.03230012255369133\n",
            "Loss S33:  0.034493503458334206\n",
            "Loss S44:  0.03736597349813123\n",
            "Loss S11:  0.03034209016250424\n",
            "Loss S22:  0.03226704215742202\n",
            "Loss S33:  0.03444122706365538\n",
            "Loss S44:  0.03736909191714102\n",
            "Loss S11:  0.030336385664183976\n",
            "Loss S22:  0.03224863483788182\n",
            "Loss S33:  0.03442349333416\n",
            "Loss S44:  0.03734471578279446\n",
            "Loss S11:  0.030309830060249326\n",
            "Loss S22:  0.0322394942816663\n",
            "Loss S33:  0.034399055006374295\n",
            "Loss S44:  0.03728072915047517\n",
            "Loss S11:  0.03026581026640227\n",
            "Loss S22:  0.03223820275285702\n",
            "Loss S33:  0.03438818255220656\n",
            "Loss S44:  0.037252775920402534\n",
            "Loss S11:  0.030265558613432234\n",
            "Loss S22:  0.032258368094045274\n",
            "Loss S33:  0.034424643995579575\n",
            "Loss S44:  0.037267353276872556\n",
            "Loss S11:  0.03026603610932431\n",
            "Loss S22:  0.032288513062800284\n",
            "Loss S33:  0.03445972028835073\n",
            "Loss S44:  0.03728373411213639\n",
            "Loss S11:  0.030276104706037085\n",
            "Loss S22:  0.03230032864419019\n",
            "Loss S33:  0.034473188215465404\n",
            "Loss S44:  0.037293023899964195\n",
            "Loss S11:  0.030288859662190776\n",
            "Loss S22:  0.03235364811512354\n",
            "Loss S33:  0.034473579556817575\n",
            "Loss S44:  0.03731799365320124\n",
            "Loss S11:  0.03029109734557907\n",
            "Loss S22:  0.03238385222469572\n",
            "Loss S33:  0.034495353704357794\n",
            "Loss S44:  0.037312855223739616\n",
            "Loss S11:  0.030313557844139263\n",
            "Loss S22:  0.03241709980272477\n",
            "Loss S33:  0.03453103938519605\n",
            "Loss S44:  0.03736914701410228\n",
            "Loss S11:  0.0303086954516563\n",
            "Loss S22:  0.03243158212690442\n",
            "Loss S33:  0.03455338161704038\n",
            "Loss S44:  0.0373885366917081\n",
            "Loss S11:  0.030332847072683543\n",
            "Loss S22:  0.03244630609564174\n",
            "Loss S33:  0.034564960216509996\n",
            "Loss S44:  0.03742264086829493\n",
            "Loss S11:  0.030323901914080197\n",
            "Loss S22:  0.032421315857020995\n",
            "Loss S33:  0.03455917435294052\n",
            "Loss S44:  0.037403945045450304\n",
            "Loss S11:  0.030303919560757522\n",
            "Loss S22:  0.032420418017453406\n",
            "Loss S33:  0.03455444482543806\n",
            "Loss S44:  0.037401286288704774\n",
            "Loss S11:  0.03026520807172179\n",
            "Loss S22:  0.032406132549161804\n",
            "Loss S33:  0.03451605889555591\n",
            "Loss S44:  0.03735082765655292\n",
            "Loss S11:  0.030274085268526898\n",
            "Loss S22:  0.032430337035930956\n",
            "Loss S33:  0.03452463861917171\n",
            "Loss S44:  0.03735806508235949\n",
            "Loss S11:  0.030260987801889724\n",
            "Loss S22:  0.03244555901980748\n",
            "Loss S33:  0.03452433132251539\n",
            "Loss S44:  0.0373321539720117\n",
            "Loss S11:  0.0302754526475617\n",
            "Loss S22:  0.03245059625553122\n",
            "Loss S33:  0.034526836122618434\n",
            "Loss S44:  0.03733637751546863\n",
            "Loss S11:  0.030255042491891267\n",
            "Loss S22:  0.032443569320537095\n",
            "Loss S33:  0.03450555859993616\n",
            "Loss S44:  0.0373202226926376\n",
            "Loss S11:  0.030247530234711512\n",
            "Loss S22:  0.03243185256242482\n",
            "Loss S33:  0.03448368920237164\n",
            "Loss S44:  0.037308534184922706\n",
            "Loss S11:  0.030261115705854612\n",
            "Loss S22:  0.03243370841106528\n",
            "Loss S33:  0.03449531265032661\n",
            "Loss S44:  0.0373127530153403\n",
            "Loss S11:  0.03025303110722735\n",
            "Loss S22:  0.03241514812128663\n",
            "Loss S33:  0.03447306908085263\n",
            "Loss S44:  0.03729920042779725\n",
            "Loss S11:  0.030244267780942774\n",
            "Loss S22:  0.032406699410669364\n",
            "Loss S33:  0.034473949956318126\n",
            "Loss S44:  0.03729911220918415\n",
            "Loss S11:  0.030241756454079165\n",
            "Loss S22:  0.032403194490750464\n",
            "Loss S33:  0.03447716427180846\n",
            "Loss S44:  0.03731081369497548\n",
            "Loss S11:  0.03021300113295222\n",
            "Loss S22:  0.03238389425950842\n",
            "Loss S33:  0.034448923664489976\n",
            "Loss S44:  0.03728170203170199\n",
            "Validation: \n",
            " Loss S11:  0.027147740125656128\n",
            " Loss S22:  0.043779075145721436\n",
            " Loss S33:  0.03968288004398346\n",
            " Loss S44:  0.04299766570329666\n",
            " Loss S11:  0.029048489761494455\n",
            " Loss S22:  0.04738009525906472\n",
            " Loss S33:  0.047650939651897976\n",
            " Loss S44:  0.051169044914699736\n",
            " Loss S11:  0.028964799351808502\n",
            " Loss S22:  0.047341184826885783\n",
            " Loss S33:  0.04827531717899369\n",
            " Loss S44:  0.05074746225301812\n",
            " Loss S11:  0.028954011861418116\n",
            " Loss S22:  0.046968731845988605\n",
            " Loss S33:  0.04755023335579966\n",
            " Loss S44:  0.050602853053905925\n",
            " Loss S11:  0.02892085298159976\n",
            " Loss S22:  0.04651709282655775\n",
            " Loss S33:  0.047576699083969914\n",
            " Loss S44:  0.05051479336840135\n",
            "\n",
            "Epoch: 55\n",
            "Loss S11:  0.03219124674797058\n",
            "Loss S22:  0.036382101476192474\n",
            "Loss S33:  0.03664303570985794\n",
            "Loss S44:  0.04158617928624153\n",
            "Loss S11:  0.029290279881520706\n",
            "Loss S22:  0.0318249257450754\n",
            "Loss S33:  0.03409952771934596\n",
            "Loss S44:  0.03732600130818107\n",
            "Loss S11:  0.02943234784262521\n",
            "Loss S22:  0.03190640031936623\n",
            "Loss S33:  0.0343624134326265\n",
            "Loss S44:  0.037253218569925854\n",
            "Loss S11:  0.029430516124252352\n",
            "Loss S22:  0.03186742877287249\n",
            "Loss S33:  0.03412575466978934\n",
            "Loss S44:  0.03699929002792605\n",
            "Loss S11:  0.029623243412593515\n",
            "Loss S22:  0.03213548664821357\n",
            "Loss S33:  0.03436480403491637\n",
            "Loss S44:  0.03697789187838391\n",
            "Loss S11:  0.029840757741647607\n",
            "Loss S22:  0.032255760923612355\n",
            "Loss S33:  0.034348865642267114\n",
            "Loss S44:  0.03713432852836216\n",
            "Loss S11:  0.029759358400936988\n",
            "Loss S22:  0.03211246409499254\n",
            "Loss S33:  0.03447825467733086\n",
            "Loss S44:  0.0369441564820829\n",
            "Loss S11:  0.029868614899230674\n",
            "Loss S22:  0.032287873590076475\n",
            "Loss S33:  0.034470946855948\n",
            "Loss S44:  0.037006240023273816\n",
            "Loss S11:  0.02978461587594615\n",
            "Loss S22:  0.03213923863698671\n",
            "Loss S33:  0.034328045001552426\n",
            "Loss S44:  0.03679303293702779\n",
            "Loss S11:  0.02981074528952876\n",
            "Loss S22:  0.03208668922984993\n",
            "Loss S33:  0.03424237228438749\n",
            "Loss S44:  0.036745899428541846\n",
            "Loss S11:  0.02973374901431622\n",
            "Loss S22:  0.03202981946270655\n",
            "Loss S33:  0.03419415090269971\n",
            "Loss S44:  0.03669744326115245\n",
            "Loss S11:  0.029851724603423127\n",
            "Loss S22:  0.0319811997280733\n",
            "Loss S33:  0.03415739427211585\n",
            "Loss S44:  0.0366317108441312\n",
            "Loss S11:  0.029844403405450592\n",
            "Loss S22:  0.03201359084014557\n",
            "Loss S33:  0.034112636600274686\n",
            "Loss S44:  0.03661770972400165\n",
            "Loss S11:  0.029847115164494696\n",
            "Loss S22:  0.03204172413142128\n",
            "Loss S33:  0.03416901708843599\n",
            "Loss S44:  0.036619418094517624\n",
            "Loss S11:  0.029876242266267748\n",
            "Loss S22:  0.032094896130316644\n",
            "Loss S33:  0.03425005565773934\n",
            "Loss S44:  0.03667908007318669\n",
            "Loss S11:  0.029981532423997558\n",
            "Loss S22:  0.03213342572778266\n",
            "Loss S33:  0.0343304674776382\n",
            "Loss S44:  0.03675204805339014\n",
            "Loss S11:  0.030002940330446138\n",
            "Loss S22:  0.032111065890292945\n",
            "Loss S33:  0.03435770388669479\n",
            "Loss S44:  0.03674579834021767\n",
            "Loss S11:  0.030103792997392995\n",
            "Loss S22:  0.032214108941673536\n",
            "Loss S33:  0.0344549116531485\n",
            "Loss S44:  0.03683930810824124\n",
            "Loss S11:  0.030156466245486593\n",
            "Loss S22:  0.032289373909653224\n",
            "Loss S33:  0.03451794170831119\n",
            "Loss S44:  0.0368833870663004\n",
            "Loss S11:  0.030152439125155278\n",
            "Loss S22:  0.032267188906903665\n",
            "Loss S33:  0.03452085083379795\n",
            "Loss S44:  0.03690335613855829\n",
            "Loss S11:  0.030158395401725723\n",
            "Loss S22:  0.032232926287387144\n",
            "Loss S33:  0.03453594814077835\n",
            "Loss S44:  0.03689902936530054\n",
            "Loss S11:  0.03019085438189348\n",
            "Loss S22:  0.032251037357119025\n",
            "Loss S33:  0.034552299669144845\n",
            "Loss S44:  0.0369569807273658\n",
            "Loss S11:  0.030218117444763357\n",
            "Loss S22:  0.032262585905477474\n",
            "Loss S33:  0.03456810322417393\n",
            "Loss S44:  0.03699512546249914\n",
            "Loss S11:  0.03021037301201841\n",
            "Loss S22:  0.03225708989934488\n",
            "Loss S33:  0.0345261971188056\n",
            "Loss S44:  0.036963550841589\n",
            "Loss S11:  0.03023275411475249\n",
            "Loss S22:  0.032284156747811564\n",
            "Loss S33:  0.034559210167262565\n",
            "Loss S44:  0.03700889010925263\n",
            "Loss S11:  0.030227797172696465\n",
            "Loss S22:  0.03228158328637659\n",
            "Loss S33:  0.034536244926521506\n",
            "Loss S44:  0.03701422670002715\n",
            "Loss S11:  0.03020075287569985\n",
            "Loss S22:  0.032268145817449724\n",
            "Loss S33:  0.03454619087278843\n",
            "Loss S44:  0.03699136180696131\n",
            "Loss S11:  0.03019001886138617\n",
            "Loss S22:  0.032239836252318535\n",
            "Loss S33:  0.034516514797877124\n",
            "Loss S44:  0.03699067773166838\n",
            "Loss S11:  0.030194855106205704\n",
            "Loss S22:  0.032234484051417205\n",
            "Loss S33:  0.034482245900240655\n",
            "Loss S44:  0.036974119526264504\n",
            "Loss S11:  0.030199563924440814\n",
            "Loss S22:  0.03222765482936528\n",
            "Loss S33:  0.034514972831273\n",
            "Loss S44:  0.0370003146493865\n",
            "Loss S11:  0.030202689299056697\n",
            "Loss S22:  0.03227222595747523\n",
            "Loss S33:  0.03451246721824736\n",
            "Loss S44:  0.03701334546298482\n",
            "Loss S11:  0.030192793158281273\n",
            "Loss S22:  0.032254414914409446\n",
            "Loss S33:  0.03448210612921661\n",
            "Loss S44:  0.03697688099583821\n",
            "Loss S11:  0.03019231711035577\n",
            "Loss S22:  0.03226189405840134\n",
            "Loss S33:  0.034460292374744225\n",
            "Loss S44:  0.03700190488353511\n",
            "Loss S11:  0.030204233980746068\n",
            "Loss S22:  0.032289296473036\n",
            "Loss S33:  0.034496733924413374\n",
            "Loss S44:  0.037077051359496085\n",
            "Loss S11:  0.03022251485183267\n",
            "Loss S22:  0.032327346126835715\n",
            "Loss S33:  0.03451788901758334\n",
            "Loss S44:  0.037099249447938985\n",
            "Loss S11:  0.030231588209668796\n",
            "Loss S22:  0.032388460867510224\n",
            "Loss S33:  0.03454884673413049\n",
            "Loss S44:  0.037141635117155535\n",
            "Loss S11:  0.030264388238417806\n",
            "Loss S22:  0.03240883314254542\n",
            "Loss S33:  0.034559087750406475\n",
            "Loss S44:  0.037157925475295894\n",
            "Loss S11:  0.030238549995695484\n",
            "Loss S22:  0.032383254390320367\n",
            "Loss S33:  0.03455326843735664\n",
            "Loss S44:  0.03715842543044341\n",
            "Loss S11:  0.030217654338189622\n",
            "Loss S22:  0.03237459530902347\n",
            "Loss S33:  0.03452563065830178\n",
            "Loss S44:  0.03714422144856196\n",
            "Loss S11:  0.030179442613935837\n",
            "Loss S22:  0.032354370140663495\n",
            "Loss S33:  0.0344879267179905\n",
            "Loss S44:  0.03712263827681389\n",
            "Loss S11:  0.030200093376108832\n",
            "Loss S22:  0.03237587246031238\n",
            "Loss S33:  0.03450704467229712\n",
            "Loss S44:  0.0371582930178342\n",
            "Loss S11:  0.030190820246028493\n",
            "Loss S22:  0.03238019417889797\n",
            "Loss S33:  0.03449294349029117\n",
            "Loss S44:  0.03714208761252771\n",
            "Loss S11:  0.03020074949854202\n",
            "Loss S22:  0.03237420886383346\n",
            "Loss S33:  0.03449561898187334\n",
            "Loss S44:  0.03713723171210346\n",
            "Loss S11:  0.030193881034470765\n",
            "Loss S22:  0.032362445998772665\n",
            "Loss S33:  0.03448117197666257\n",
            "Loss S44:  0.037142431687589586\n",
            "Loss S11:  0.030182339255265096\n",
            "Loss S22:  0.03235393726264809\n",
            "Loss S33:  0.034476163778073936\n",
            "Loss S44:  0.037129035181536964\n",
            "Loss S11:  0.030187179125894994\n",
            "Loss S22:  0.03236062398919642\n",
            "Loss S33:  0.034476504039572774\n",
            "Loss S44:  0.037138126104937955\n",
            "Loss S11:  0.03017458510373006\n",
            "Loss S22:  0.03233196731422444\n",
            "Loss S33:  0.03445614692138876\n",
            "Loss S44:  0.0371314304155538\n",
            "Loss S11:  0.030180544898926326\n",
            "Loss S22:  0.03235286243671191\n",
            "Loss S33:  0.03443759799225062\n",
            "Loss S44:  0.037126992918123866\n",
            "Loss S11:  0.03018444271568449\n",
            "Loss S22:  0.03236702872970744\n",
            "Loss S33:  0.03444247882573124\n",
            "Loss S44:  0.03711749773604211\n",
            "Loss S11:  0.030161463665015344\n",
            "Loss S22:  0.03233783487647832\n",
            "Loss S33:  0.03441194817803178\n",
            "Loss S44:  0.03709164294442674\n",
            "Validation: \n",
            " Loss S11:  0.028133437037467957\n",
            " Loss S22:  0.04228135943412781\n",
            " Loss S33:  0.041436873376369476\n",
            " Loss S44:  0.04461672529578209\n",
            " Loss S11:  0.029506372199172064\n",
            " Loss S22:  0.0471293025073551\n",
            " Loss S33:  0.048587066609235036\n",
            " Loss S44:  0.05124276663575854\n",
            " Loss S11:  0.02961759214721075\n",
            " Loss S22:  0.04720234489295541\n",
            " Loss S33:  0.04927551700938039\n",
            " Loss S44:  0.05078746078581345\n",
            " Loss S11:  0.029576409852407018\n",
            " Loss S22:  0.046796237408626276\n",
            " Loss S33:  0.04850840324261149\n",
            " Loss S44:  0.05058894056032916\n",
            " Loss S11:  0.029609692455441865\n",
            " Loss S22:  0.04650594832168685\n",
            " Loss S33:  0.04846198557887548\n",
            " Loss S44:  0.050494005236728694\n",
            "\n",
            "Epoch: 56\n",
            "Loss S11:  0.03214387968182564\n",
            "Loss S22:  0.034965768456459045\n",
            "Loss S33:  0.033678825944662094\n",
            "Loss S44:  0.044765133410692215\n",
            "Loss S11:  0.029420979659665714\n",
            "Loss S22:  0.03112343258478425\n",
            "Loss S33:  0.03425606475635008\n",
            "Loss S44:  0.037009774622592057\n",
            "Loss S11:  0.02985133079900628\n",
            "Loss S22:  0.03133525353457246\n",
            "Loss S33:  0.03444817875112806\n",
            "Loss S44:  0.03689390012905711\n",
            "Loss S11:  0.029855655506253242\n",
            "Loss S22:  0.031625699131719524\n",
            "Loss S33:  0.03450519579552835\n",
            "Loss S44:  0.036624276109280124\n",
            "Loss S11:  0.029856280355555254\n",
            "Loss S22:  0.03191266077139029\n",
            "Loss S33:  0.03459468865539969\n",
            "Loss S44:  0.03684453920620244\n",
            "Loss S11:  0.02986634876944271\n",
            "Loss S22:  0.031980778875888564\n",
            "Loss S33:  0.03468437792331565\n",
            "Loss S44:  0.03686622698225227\n",
            "Loss S11:  0.02988640096832494\n",
            "Loss S22:  0.03200988144781746\n",
            "Loss S33:  0.03464219456569093\n",
            "Loss S44:  0.03676032317710704\n",
            "Loss S11:  0.02984909301387592\n",
            "Loss S22:  0.03196063732177439\n",
            "Loss S33:  0.03444959540707125\n",
            "Loss S44:  0.03674527971257626\n",
            "Loss S11:  0.02970475059600524\n",
            "Loss S22:  0.03186785553892454\n",
            "Loss S33:  0.03422507862158028\n",
            "Loss S44:  0.036650477368154646\n",
            "Loss S11:  0.029681816205873596\n",
            "Loss S22:  0.03188071553441849\n",
            "Loss S33:  0.03429491482265703\n",
            "Loss S44:  0.036597564980223936\n",
            "Loss S11:  0.029619101379619966\n",
            "Loss S22:  0.03182739771828793\n",
            "Loss S33:  0.03413778510276634\n",
            "Loss S44:  0.03639938687850343\n",
            "Loss S11:  0.02963135102847675\n",
            "Loss S22:  0.03183471607799466\n",
            "Loss S33:  0.03418904749324193\n",
            "Loss S44:  0.03640696901391755\n",
            "Loss S11:  0.029635420869574075\n",
            "Loss S22:  0.031891717306099646\n",
            "Loss S33:  0.03418358159828777\n",
            "Loss S44:  0.03649462299221311\n",
            "Loss S11:  0.02971489663513107\n",
            "Loss S22:  0.03196183685924261\n",
            "Loss S33:  0.03423497541487672\n",
            "Loss S44:  0.036601512188106096\n",
            "Loss S11:  0.029736722522276514\n",
            "Loss S22:  0.032053517349434235\n",
            "Loss S33:  0.03431778232045207\n",
            "Loss S44:  0.03667933302304001\n",
            "Loss S11:  0.02980183253669186\n",
            "Loss S22:  0.03213668929178588\n",
            "Loss S33:  0.03437885378074172\n",
            "Loss S44:  0.03678817300755062\n",
            "Loss S11:  0.029825192534868022\n",
            "Loss S22:  0.03213326790317986\n",
            "Loss S33:  0.034323445781602624\n",
            "Loss S44:  0.03676589005713507\n",
            "Loss S11:  0.029873937313929635\n",
            "Loss S22:  0.03218607804928607\n",
            "Loss S33:  0.0343671865891992\n",
            "Loss S44:  0.03681197197160177\n",
            "Loss S11:  0.029894094492272778\n",
            "Loss S22:  0.03222010156026532\n",
            "Loss S33:  0.03438166491893115\n",
            "Loss S44:  0.036836961140207826\n",
            "Loss S11:  0.02992434033158562\n",
            "Loss S22:  0.03216026729236098\n",
            "Loss S33:  0.03436521890314774\n",
            "Loss S44:  0.036836023388810805\n",
            "Loss S11:  0.02990920905635428\n",
            "Loss S22:  0.03211359624097596\n",
            "Loss S33:  0.034333257940574666\n",
            "Loss S44:  0.03685837269607765\n",
            "Loss S11:  0.029905475402400957\n",
            "Loss S22:  0.03212673314162905\n",
            "Loss S33:  0.03428126728577072\n",
            "Loss S44:  0.036904936267936964\n",
            "Loss S11:  0.02989612347804583\n",
            "Loss S22:  0.03210962810701105\n",
            "Loss S33:  0.034255267925796466\n",
            "Loss S44:  0.036894188165597246\n",
            "Loss S11:  0.029899514090769734\n",
            "Loss S22:  0.03209584888506245\n",
            "Loss S33:  0.03422407456774474\n",
            "Loss S44:  0.03686078572524833\n",
            "Loss S11:  0.029951900413793153\n",
            "Loss S22:  0.032123625023421906\n",
            "Loss S33:  0.03422420605615214\n",
            "Loss S44:  0.036891569371849176\n",
            "Loss S11:  0.02996536626283866\n",
            "Loss S22:  0.03213124207232103\n",
            "Loss S33:  0.03421974657066315\n",
            "Loss S44:  0.03693974428472528\n",
            "Loss S11:  0.029964151902845078\n",
            "Loss S22:  0.03214334092777351\n",
            "Loss S33:  0.03422280191889896\n",
            "Loss S44:  0.03696675767758112\n",
            "Loss S11:  0.02997711851395584\n",
            "Loss S22:  0.03212923322876441\n",
            "Loss S33:  0.03421383806782675\n",
            "Loss S44:  0.03698162117979843\n",
            "Loss S11:  0.02998088476549688\n",
            "Loss S22:  0.03210659523981746\n",
            "Loss S33:  0.03417324122319866\n",
            "Loss S44:  0.03697080953291939\n",
            "Loss S11:  0.02997671607945793\n",
            "Loss S22:  0.032088540251209975\n",
            "Loss S33:  0.034199082526247114\n",
            "Loss S44:  0.03698157019196302\n",
            "Loss S11:  0.02998501364995673\n",
            "Loss S22:  0.03208431799546429\n",
            "Loss S33:  0.034206951538292674\n",
            "Loss S44:  0.03698019112191129\n",
            "Loss S11:  0.029980269377494165\n",
            "Loss S22:  0.032068746709459466\n",
            "Loss S33:  0.03422123724984011\n",
            "Loss S44:  0.03697113914960259\n",
            "Loss S11:  0.029993489367243286\n",
            "Loss S22:  0.03207147697050623\n",
            "Loss S33:  0.034222228162431645\n",
            "Loss S44:  0.03698726851968936\n",
            "Loss S11:  0.02999769664985716\n",
            "Loss S22:  0.032077596880202924\n",
            "Loss S33:  0.034226957207765106\n",
            "Loss S44:  0.036995973893837984\n",
            "Loss S11:  0.030046242521392985\n",
            "Loss S22:  0.03210469318930704\n",
            "Loss S33:  0.03428421419146648\n",
            "Loss S44:  0.03705577441433833\n",
            "Loss S11:  0.030049762680617154\n",
            "Loss S22:  0.03211373996063854\n",
            "Loss S33:  0.03430755564842129\n",
            "Loss S44:  0.03706423069570649\n",
            "Loss S11:  0.030053391975255223\n",
            "Loss S22:  0.03210343747629353\n",
            "Loss S33:  0.03431152290254418\n",
            "Loss S44:  0.03708741575004817\n",
            "Loss S11:  0.030031550349291124\n",
            "Loss S22:  0.03208663775472628\n",
            "Loss S33:  0.03430586547221778\n",
            "Loss S44:  0.037077905941563793\n",
            "Loss S11:  0.030016313856980933\n",
            "Loss S22:  0.032089225791414266\n",
            "Loss S33:  0.0342980008719005\n",
            "Loss S44:  0.03707522680381621\n",
            "Loss S11:  0.029980993065077937\n",
            "Loss S22:  0.03207271824331235\n",
            "Loss S33:  0.034248871757360674\n",
            "Loss S44:  0.03703370354021602\n",
            "Loss S11:  0.029984735166119814\n",
            "Loss S22:  0.032115720704829605\n",
            "Loss S33:  0.034272201703002034\n",
            "Loss S44:  0.03703216333267397\n",
            "Loss S11:  0.029983555503787784\n",
            "Loss S22:  0.03211844412025309\n",
            "Loss S33:  0.034251004792840524\n",
            "Loss S44:  0.03702923827725315\n",
            "Loss S11:  0.029992089971829197\n",
            "Loss S22:  0.03209469014792006\n",
            "Loss S33:  0.034249000388447574\n",
            "Loss S44:  0.037039736617630846\n",
            "Loss S11:  0.029967668550542777\n",
            "Loss S22:  0.03208572987959722\n",
            "Loss S33:  0.034224511567166126\n",
            "Loss S44:  0.03701803236854049\n",
            "Loss S11:  0.029972139201095316\n",
            "Loss S22:  0.03208146461807276\n",
            "Loss S33:  0.03420480271873155\n",
            "Loss S44:  0.03700189869386269\n",
            "Loss S11:  0.029965851352676057\n",
            "Loss S22:  0.03211226317197283\n",
            "Loss S33:  0.03422257388444275\n",
            "Loss S44:  0.036992424186277816\n",
            "Loss S11:  0.02993322948878732\n",
            "Loss S22:  0.03209966021527308\n",
            "Loss S33:  0.03420443400142638\n",
            "Loss S44:  0.03700510219321334\n",
            "Loss S11:  0.029922487835089367\n",
            "Loss S22:  0.03210235455107031\n",
            "Loss S33:  0.0341928754152133\n",
            "Loss S44:  0.03701759108407482\n",
            "Loss S11:  0.02992252151243652\n",
            "Loss S22:  0.03210045880175306\n",
            "Loss S33:  0.034192794767400086\n",
            "Loss S44:  0.037021604604755766\n",
            "Loss S11:  0.029910812617228374\n",
            "Loss S22:  0.03208007547405004\n",
            "Loss S33:  0.03417669210104981\n",
            "Loss S44:  0.036982755061018974\n",
            "Validation: \n",
            " Loss S11:  0.02903876081109047\n",
            " Loss S22:  0.04383840411901474\n",
            " Loss S33:  0.04031990095973015\n",
            " Loss S44:  0.04281488433480263\n",
            " Loss S11:  0.03022307583263942\n",
            " Loss S22:  0.04759925781261353\n",
            " Loss S33:  0.04769447871616909\n",
            " Loss S44:  0.0519427073498567\n",
            " Loss S11:  0.030453692167633918\n",
            " Loss S22:  0.04767897225371221\n",
            " Loss S33:  0.048391458646553316\n",
            " Loss S44:  0.0515730462786628\n",
            " Loss S11:  0.03031478431381163\n",
            " Loss S22:  0.047238015371267914\n",
            " Loss S33:  0.047661089139883636\n",
            " Loss S44:  0.05132538587099216\n",
            " Loss S11:  0.030299624819078563\n",
            " Loss S22:  0.046872843516829577\n",
            " Loss S33:  0.04768157258261869\n",
            " Loss S44:  0.05124560302054441\n",
            "\n",
            "Epoch: 57\n",
            "Loss S11:  0.0344388447701931\n",
            "Loss S22:  0.036885593086481094\n",
            "Loss S33:  0.03742258623242378\n",
            "Loss S44:  0.04054044932126999\n",
            "Loss S11:  0.029703569852493027\n",
            "Loss S22:  0.032438254661180756\n",
            "Loss S33:  0.03506986441260034\n",
            "Loss S44:  0.036351645196026024\n",
            "Loss S11:  0.02920880683121227\n",
            "Loss S22:  0.03184826033455985\n",
            "Loss S33:  0.03477878345265275\n",
            "Loss S44:  0.036342812258572804\n",
            "Loss S11:  0.029290420694216605\n",
            "Loss S22:  0.03175417736413017\n",
            "Loss S33:  0.0344835709660284\n",
            "Loss S44:  0.03621894157221241\n",
            "Loss S11:  0.029503171127743838\n",
            "Loss S22:  0.03183418271563402\n",
            "Loss S33:  0.03443588752572129\n",
            "Loss S44:  0.03621708983328284\n",
            "Loss S11:  0.029715880495952626\n",
            "Loss S22:  0.03194117834608929\n",
            "Loss S33:  0.0343866011164352\n",
            "Loss S44:  0.03637228548234584\n",
            "Loss S11:  0.02970151749790692\n",
            "Loss S22:  0.031809733447725655\n",
            "Loss S33:  0.03423051438370689\n",
            "Loss S44:  0.03633484665731915\n",
            "Loss S11:  0.029670236842103407\n",
            "Loss S22:  0.03184258567930107\n",
            "Loss S33:  0.034217303540085404\n",
            "Loss S44:  0.03639223096026501\n",
            "Loss S11:  0.02955928730008043\n",
            "Loss S22:  0.031731038066891974\n",
            "Loss S33:  0.03409159659511513\n",
            "Loss S44:  0.036275969013387775\n",
            "Loss S11:  0.029531168945870555\n",
            "Loss S22:  0.03162612851995688\n",
            "Loss S33:  0.03403818136551878\n",
            "Loss S44:  0.03617712102093539\n",
            "Loss S11:  0.02943287138817924\n",
            "Loss S22:  0.03160695259523864\n",
            "Loss S33:  0.03393642316655357\n",
            "Loss S44:  0.03605460375547409\n",
            "Loss S11:  0.029471835104731826\n",
            "Loss S22:  0.03159703255572298\n",
            "Loss S33:  0.03390508526013241\n",
            "Loss S44:  0.03605624519892641\n",
            "Loss S11:  0.029492493641893727\n",
            "Loss S22:  0.03159017629983012\n",
            "Loss S33:  0.033911901733968866\n",
            "Loss S44:  0.03607279148476183\n",
            "Loss S11:  0.029583524798846426\n",
            "Loss S22:  0.03169320614738319\n",
            "Loss S33:  0.03401008736029381\n",
            "Loss S44:  0.03619132654471252\n",
            "Loss S11:  0.029640368232173277\n",
            "Loss S22:  0.031737943979125494\n",
            "Loss S33:  0.034055170226604384\n",
            "Loss S44:  0.036263233777267714\n",
            "Loss S11:  0.029690440608472222\n",
            "Loss S22:  0.03181466292466549\n",
            "Loss S33:  0.03412490093846195\n",
            "Loss S44:  0.03636517972740906\n",
            "Loss S11:  0.029725127014684383\n",
            "Loss S22:  0.03181227564996814\n",
            "Loss S33:  0.034132267099729975\n",
            "Loss S44:  0.036400549206304256\n",
            "Loss S11:  0.029732907136455614\n",
            "Loss S22:  0.031907873010339094\n",
            "Loss S33:  0.03420391951126662\n",
            "Loss S44:  0.036518203889765934\n",
            "Loss S11:  0.029748835760420856\n",
            "Loss S22:  0.031952433349842525\n",
            "Loss S33:  0.03417382469575708\n",
            "Loss S44:  0.03654858799270504\n",
            "Loss S11:  0.029741624165423877\n",
            "Loss S22:  0.031927744843338796\n",
            "Loss S33:  0.03417002339481683\n",
            "Loss S44:  0.0365231000374593\n",
            "Loss S11:  0.029745812551002598\n",
            "Loss S22:  0.0318939516886105\n",
            "Loss S33:  0.034209336186597\n",
            "Loss S44:  0.036574979428554054\n",
            "Loss S11:  0.029772538056164557\n",
            "Loss S22:  0.031973230021330415\n",
            "Loss S33:  0.034214934254709575\n",
            "Loss S44:  0.03664955820764693\n",
            "Loss S11:  0.029829038838051023\n",
            "Loss S22:  0.032031500872174\n",
            "Loss S33:  0.03425235947703614\n",
            "Loss S44:  0.03667691696384644\n",
            "Loss S11:  0.029827984291818235\n",
            "Loss S22:  0.03201308736811469\n",
            "Loss S33:  0.03422639006950381\n",
            "Loss S44:  0.036674955166328\n",
            "Loss S11:  0.02985678868236878\n",
            "Loss S22:  0.03207664995519947\n",
            "Loss S33:  0.03426117127707143\n",
            "Loss S44:  0.036688940877798185\n",
            "Loss S11:  0.029859757013767363\n",
            "Loss S22:  0.03205713484273014\n",
            "Loss S33:  0.03426877118320579\n",
            "Loss S44:  0.0367275701086241\n",
            "Loss S11:  0.029838769450441175\n",
            "Loss S22:  0.03204891625863154\n",
            "Loss S33:  0.034243466639427386\n",
            "Loss S44:  0.03671833102759045\n",
            "Loss S11:  0.02982715257474417\n",
            "Loss S22:  0.032025373936029375\n",
            "Loss S33:  0.03423565339530746\n",
            "Loss S44:  0.036701299256999116\n",
            "Loss S11:  0.02980965693411666\n",
            "Loss S22:  0.03202622713938727\n",
            "Loss S33:  0.03420587882293501\n",
            "Loss S44:  0.036699127730684775\n",
            "Loss S11:  0.029835173853791457\n",
            "Loss S22:  0.03199903748903897\n",
            "Loss S33:  0.03421412586295318\n",
            "Loss S44:  0.036730892399383576\n",
            "Loss S11:  0.029867906630831303\n",
            "Loss S22:  0.032026446068603734\n",
            "Loss S33:  0.03424377453535102\n",
            "Loss S44:  0.03678696689836409\n",
            "Loss S11:  0.029862423556171046\n",
            "Loss S22:  0.03201105526477769\n",
            "Loss S33:  0.034239994523705394\n",
            "Loss S44:  0.03675595187609027\n",
            "Loss S11:  0.029866357197596276\n",
            "Loss S22:  0.03203001240633915\n",
            "Loss S33:  0.0342405596138718\n",
            "Loss S44:  0.03678278971428626\n",
            "Loss S11:  0.029862210880458536\n",
            "Loss S22:  0.03203775648173969\n",
            "Loss S33:  0.034244796683240875\n",
            "Loss S44:  0.03677735952069572\n",
            "Loss S11:  0.029903260062385865\n",
            "Loss S22:  0.032073219637236285\n",
            "Loss S33:  0.03428047790555311\n",
            "Loss S44:  0.036822990123783386\n",
            "Loss S11:  0.029896686447948812\n",
            "Loss S22:  0.03209664551090141\n",
            "Loss S33:  0.034293652175158854\n",
            "Loss S44:  0.03682742164897443\n",
            "Loss S11:  0.029903223418174026\n",
            "Loss S22:  0.032085179204219265\n",
            "Loss S33:  0.03428053357470729\n",
            "Loss S44:  0.036827514943901525\n",
            "Loss S11:  0.029904162752218647\n",
            "Loss S22:  0.03208261681093039\n",
            "Loss S33:  0.034283842941660765\n",
            "Loss S44:  0.036824995360645964\n",
            "Loss S11:  0.029895931120541466\n",
            "Loss S22:  0.032078513401386934\n",
            "Loss S33:  0.03428464621461908\n",
            "Loss S44:  0.03682140488265537\n",
            "Loss S11:  0.02988270110429248\n",
            "Loss S22:  0.03205391939472207\n",
            "Loss S33:  0.03426521371030594\n",
            "Loss S44:  0.03679601082583065\n",
            "Loss S11:  0.029873403620690183\n",
            "Loss S22:  0.03205432079063538\n",
            "Loss S33:  0.03429592593556776\n",
            "Loss S44:  0.03681776827280212\n",
            "Loss S11:  0.02987861879387476\n",
            "Loss S22:  0.03205909610809781\n",
            "Loss S33:  0.03427791663438735\n",
            "Loss S44:  0.03683830971247233\n",
            "Loss S11:  0.0298777108883702\n",
            "Loss S22:  0.03204495650041415\n",
            "Loss S33:  0.03425617135547969\n",
            "Loss S44:  0.036837258750037453\n",
            "Loss S11:  0.02987990303617619\n",
            "Loss S22:  0.032024539021770805\n",
            "Loss S33:  0.03424455854272483\n",
            "Loss S44:  0.03682266304965224\n",
            "Loss S11:  0.029863226360508373\n",
            "Loss S22:  0.03201708598421409\n",
            "Loss S33:  0.03422118189941053\n",
            "Loss S44:  0.036792864963885334\n",
            "Loss S11:  0.02986557809780548\n",
            "Loss S22:  0.03201340536221432\n",
            "Loss S33:  0.03421898958523099\n",
            "Loss S44:  0.03680565755426091\n",
            "Loss S11:  0.029873163510041226\n",
            "Loss S22:  0.032011417910424333\n",
            "Loss S33:  0.03420327465229722\n",
            "Loss S44:  0.036802635893807494\n",
            "Loss S11:  0.029869674097928525\n",
            "Loss S22:  0.032015490296223616\n",
            "Loss S33:  0.034190428805104485\n",
            "Loss S44:  0.03680600692179932\n",
            "Loss S11:  0.029876790960123783\n",
            "Loss S22:  0.03202117578728655\n",
            "Loss S33:  0.03421383060267091\n",
            "Loss S44:  0.036792281871804826\n",
            "Loss S11:  0.029876150501903353\n",
            "Loss S22:  0.032011054903875784\n",
            "Loss S33:  0.034180612798378085\n",
            "Loss S44:  0.03679329663317825\n",
            "Validation: \n",
            " Loss S11:  0.028617404401302338\n",
            " Loss S22:  0.0424058698117733\n",
            " Loss S33:  0.039083659648895264\n",
            " Loss S44:  0.0436447337269783\n",
            " Loss S11:  0.029597077518701553\n",
            " Loss S22:  0.04674265320811953\n",
            " Loss S33:  0.047239795504581364\n",
            " Loss S44:  0.05173981721912112\n",
            " Loss S11:  0.029869045971370325\n",
            " Loss S22:  0.04701961767746181\n",
            " Loss S33:  0.04812548972847985\n",
            " Loss S44:  0.051415768883577205\n",
            " Loss S11:  0.02983458544753614\n",
            " Loss S22:  0.04676639282556831\n",
            " Loss S33:  0.04757911852392994\n",
            " Loss S44:  0.05121767410977942\n",
            " Loss S11:  0.029759210913821502\n",
            " Loss S22:  0.04648912665836605\n",
            " Loss S33:  0.04752127881403322\n",
            " Loss S44:  0.051147264784868855\n",
            "\n",
            "Epoch: 58\n",
            "Loss S11:  0.0341629832983017\n",
            "Loss S22:  0.03727516531944275\n",
            "Loss S33:  0.03759456053376198\n",
            "Loss S44:  0.03824456036090851\n",
            "Loss S11:  0.030458702113140713\n",
            "Loss S22:  0.03194892897524617\n",
            "Loss S33:  0.0347762487151406\n",
            "Loss S44:  0.036986883729696274\n",
            "Loss S11:  0.029833389091349784\n",
            "Loss S22:  0.031641328086455665\n",
            "Loss S33:  0.03447891266218254\n",
            "Loss S44:  0.036649404714504875\n",
            "Loss S11:  0.029708104388367747\n",
            "Loss S22:  0.031686111503551086\n",
            "Loss S33:  0.034001621688085217\n",
            "Loss S44:  0.03660833018441354\n",
            "Loss S11:  0.029639639778108132\n",
            "Loss S22:  0.0317617852876826\n",
            "Loss S33:  0.03401642878789727\n",
            "Loss S44:  0.03654269610599774\n",
            "Loss S11:  0.029708388060623526\n",
            "Loss S22:  0.03176698220126769\n",
            "Loss S33:  0.034168820243840124\n",
            "Loss S44:  0.036725504506452414\n",
            "Loss S11:  0.029566732128379774\n",
            "Loss S22:  0.03191119917958486\n",
            "Loss S33:  0.03413839858086383\n",
            "Loss S44:  0.0367219976347978\n",
            "Loss S11:  0.029539856718669474\n",
            "Loss S22:  0.03194438235860475\n",
            "Loss S33:  0.03424342291455873\n",
            "Loss S44:  0.036764656993704785\n",
            "Loss S11:  0.029491296405961483\n",
            "Loss S22:  0.03193250746914634\n",
            "Loss S33:  0.034165020710156285\n",
            "Loss S44:  0.03667747762836056\n",
            "Loss S11:  0.02951954157797845\n",
            "Loss S22:  0.031788196885487535\n",
            "Loss S33:  0.03404718715247217\n",
            "Loss S44:  0.03658263446701752\n",
            "Loss S11:  0.029442469856821665\n",
            "Loss S22:  0.031635143821782404\n",
            "Loss S33:  0.03388789740603159\n",
            "Loss S44:  0.03641940221780598\n",
            "Loss S11:  0.0294071579402363\n",
            "Loss S22:  0.03157385280943132\n",
            "Loss S33:  0.03378108180723749\n",
            "Loss S44:  0.036276227114973844\n",
            "Loss S11:  0.029359739674024346\n",
            "Loss S22:  0.031583191934695914\n",
            "Loss S33:  0.03373346269931182\n",
            "Loss S44:  0.03624228582032456\n",
            "Loss S11:  0.029467998499064955\n",
            "Loss S22:  0.03159576603247009\n",
            "Loss S33:  0.033727566789807255\n",
            "Loss S44:  0.03626733955763679\n",
            "Loss S11:  0.02954412208117069\n",
            "Loss S22:  0.0316626656774088\n",
            "Loss S33:  0.033809770374222003\n",
            "Loss S44:  0.03635538665326775\n",
            "Loss S11:  0.029606032845200293\n",
            "Loss S22:  0.03168805096520493\n",
            "Loss S33:  0.033772827872377356\n",
            "Loss S44:  0.036366745508861854\n",
            "Loss S11:  0.029663907676379872\n",
            "Loss S22:  0.03172173060661888\n",
            "Loss S33:  0.03379610894675951\n",
            "Loss S44:  0.036434132304991255\n",
            "Loss S11:  0.029721222175230756\n",
            "Loss S22:  0.03177025323809936\n",
            "Loss S33:  0.03385463732768569\n",
            "Loss S44:  0.036494932611260494\n",
            "Loss S11:  0.02973954398082106\n",
            "Loss S22:  0.03176384310119718\n",
            "Loss S33:  0.03388819711807683\n",
            "Loss S44:  0.03653328289478523\n",
            "Loss S11:  0.029716271916843208\n",
            "Loss S22:  0.03177266065713935\n",
            "Loss S33:  0.03389400404668291\n",
            "Loss S44:  0.03649872270320099\n",
            "Loss S11:  0.02970260264009089\n",
            "Loss S22:  0.031782067394745886\n",
            "Loss S33:  0.0339112421636706\n",
            "Loss S44:  0.03646313197058232\n",
            "Loss S11:  0.02968527577040602\n",
            "Loss S22:  0.03180201382539566\n",
            "Loss S33:  0.03388635501689256\n",
            "Loss S44:  0.0365113653066034\n",
            "Loss S11:  0.029690624126212088\n",
            "Loss S22:  0.03178170256788644\n",
            "Loss S33:  0.03385202450955885\n",
            "Loss S44:  0.0365242245375301\n",
            "Loss S11:  0.029675566127676984\n",
            "Loss S22:  0.0317703378568222\n",
            "Loss S33:  0.03383939618897903\n",
            "Loss S44:  0.036499899445158066\n",
            "Loss S11:  0.02972709237601747\n",
            "Loss S22:  0.03183363718115937\n",
            "Loss S33:  0.0338883195690718\n",
            "Loss S44:  0.03653416295702032\n",
            "Loss S11:  0.029723177014653428\n",
            "Loss S22:  0.03180781927746368\n",
            "Loss S33:  0.03387307679184167\n",
            "Loss S44:  0.036514164442085176\n",
            "Loss S11:  0.029709991724212508\n",
            "Loss S22:  0.03175616297885148\n",
            "Loss S33:  0.033857952839056196\n",
            "Loss S44:  0.03649720312649263\n",
            "Loss S11:  0.029689797561107085\n",
            "Loss S22:  0.031742684900540705\n",
            "Loss S33:  0.03385001445031034\n",
            "Loss S44:  0.036499566450110224\n",
            "Loss S11:  0.029694778519911275\n",
            "Loss S22:  0.03173870065007558\n",
            "Loss S33:  0.03384271593519165\n",
            "Loss S44:  0.03648629002405655\n",
            "Loss S11:  0.029720137414244032\n",
            "Loss S22:  0.03173402675725136\n",
            "Loss S33:  0.033828594731455\n",
            "Loss S44:  0.03653696548078478\n",
            "Loss S11:  0.0297668495979717\n",
            "Loss S22:  0.03175989898583818\n",
            "Loss S33:  0.03385239977550467\n",
            "Loss S44:  0.03657962665258848\n",
            "Loss S11:  0.02976042466459742\n",
            "Loss S22:  0.031748181328752414\n",
            "Loss S33:  0.03382901314322588\n",
            "Loss S44:  0.03655178714363905\n",
            "Loss S11:  0.029750965959232916\n",
            "Loss S22:  0.03174236922226034\n",
            "Loss S33:  0.03381141075556895\n",
            "Loss S44:  0.03655734336775411\n",
            "Loss S11:  0.029735657034773842\n",
            "Loss S22:  0.031733968461163095\n",
            "Loss S33:  0.033799529373870874\n",
            "Loss S44:  0.036573035276728456\n",
            "Loss S11:  0.02977435950659587\n",
            "Loss S22:  0.03175851105114232\n",
            "Loss S33:  0.033821614044022\n",
            "Loss S44:  0.03663547814460444\n",
            "Loss S11:  0.029788227242409673\n",
            "Loss S22:  0.0317824979835426\n",
            "Loss S33:  0.03387539823701409\n",
            "Loss S44:  0.036665101846059166\n",
            "Loss S11:  0.02979920414428632\n",
            "Loss S22:  0.03177414652878558\n",
            "Loss S33:  0.033880890968805207\n",
            "Loss S44:  0.03667453988106958\n",
            "Loss S11:  0.02979792015391058\n",
            "Loss S22:  0.03176507750410597\n",
            "Loss S33:  0.03385566482245119\n",
            "Loss S44:  0.03668679521893555\n",
            "Loss S11:  0.029794373424855743\n",
            "Loss S22:  0.0317539696500996\n",
            "Loss S33:  0.033847832196731896\n",
            "Loss S44:  0.036684957262021034\n",
            "Loss S11:  0.02977352003421625\n",
            "Loss S22:  0.03174951740199952\n",
            "Loss S33:  0.03383649015784873\n",
            "Loss S44:  0.03666092691671513\n",
            "Loss S11:  0.029782070139622748\n",
            "Loss S22:  0.03178089878932942\n",
            "Loss S33:  0.033852522509948274\n",
            "Loss S44:  0.03668289733795156\n",
            "Loss S11:  0.02978157670828548\n",
            "Loss S22:  0.03180311311607378\n",
            "Loss S33:  0.033842940141793586\n",
            "Loss S44:  0.036707702498438875\n",
            "Loss S11:  0.029787027602241046\n",
            "Loss S22:  0.03179947750072156\n",
            "Loss S33:  0.03384288011954969\n",
            "Loss S44:  0.03670664559032175\n",
            "Loss S11:  0.029763724335108446\n",
            "Loss S22:  0.03177746060777568\n",
            "Loss S33:  0.03383124608485992\n",
            "Loss S44:  0.036700018675103266\n",
            "Loss S11:  0.02975135548737179\n",
            "Loss S22:  0.03176114740707572\n",
            "Loss S33:  0.03381374777799426\n",
            "Loss S44:  0.03670556240983291\n",
            "Loss S11:  0.029756159234602012\n",
            "Loss S22:  0.03177171193905786\n",
            "Loss S33:  0.03383838202383064\n",
            "Loss S44:  0.03671424803706072\n",
            "Loss S11:  0.02974777759624536\n",
            "Loss S22:  0.03176311045529527\n",
            "Loss S33:  0.03383780377886688\n",
            "Loss S44:  0.03671157645332322\n",
            "Loss S11:  0.02974404328979251\n",
            "Loss S22:  0.03176056050286172\n",
            "Loss S33:  0.033851968258967316\n",
            "Loss S44:  0.036712566861238194\n",
            "Loss S11:  0.02974367353159028\n",
            "Loss S22:  0.03176740043856622\n",
            "Loss S33:  0.033866499796540724\n",
            "Loss S44:  0.03672263272524871\n",
            "Loss S11:  0.02970822982854246\n",
            "Loss S22:  0.03173882518031932\n",
            "Loss S33:  0.03383550961746582\n",
            "Loss S44:  0.0366946673684596\n",
            "Validation: \n",
            " Loss S11:  0.02812417969107628\n",
            " Loss S22:  0.04250534996390343\n",
            " Loss S33:  0.03826884925365448\n",
            " Loss S44:  0.042161110788583755\n",
            " Loss S11:  0.028851155102962538\n",
            " Loss S22:  0.04622347333601543\n",
            " Loss S33:  0.04628177377439681\n",
            " Loss S44:  0.05073456022711027\n",
            " Loss S11:  0.029033993302685458\n",
            " Loss S22:  0.04642157074881763\n",
            " Loss S33:  0.04725936754811101\n",
            " Loss S44:  0.050318498073554624\n",
            " Loss S11:  0.02895575890042743\n",
            " Loss S22:  0.04597504473611957\n",
            " Loss S33:  0.046652834312837635\n",
            " Loss S44:  0.05013157976943938\n",
            " Loss S11:  0.028846962239455293\n",
            " Loss S22:  0.045634638932016164\n",
            " Loss S33:  0.04665779175213826\n",
            " Loss S44:  0.05005976930260658\n",
            "\n",
            "Epoch: 59\n",
            "Loss S11:  0.030967092141509056\n",
            "Loss S22:  0.03531982749700546\n",
            "Loss S33:  0.03494394198060036\n",
            "Loss S44:  0.04022974520921707\n",
            "Loss S11:  0.02923815782097253\n",
            "Loss S22:  0.031182768500664017\n",
            "Loss S33:  0.03267177973281254\n",
            "Loss S44:  0.036872773854569954\n",
            "Loss S11:  0.0292289922279971\n",
            "Loss S22:  0.03156567941464129\n",
            "Loss S33:  0.033111467780101864\n",
            "Loss S44:  0.03673316742337886\n",
            "Loss S11:  0.029228731328921932\n",
            "Loss S22:  0.031729457779757435\n",
            "Loss S33:  0.032997676862343665\n",
            "Loss S44:  0.03667485227267588\n",
            "Loss S11:  0.029321642637979692\n",
            "Loss S22:  0.03152802313973264\n",
            "Loss S33:  0.03329260829018384\n",
            "Loss S44:  0.036620940967667395\n",
            "Loss S11:  0.029319566254522287\n",
            "Loss S22:  0.031654016352167316\n",
            "Loss S33:  0.033406504740317665\n",
            "Loss S44:  0.03662100679003725\n",
            "Loss S11:  0.029329923088433314\n",
            "Loss S22:  0.03153432120920205\n",
            "Loss S33:  0.03355766096931012\n",
            "Loss S44:  0.03653003785331718\n",
            "Loss S11:  0.029368224671818842\n",
            "Loss S22:  0.03165603767503315\n",
            "Loss S33:  0.033722408824193646\n",
            "Loss S44:  0.03656144626438618\n",
            "Loss S11:  0.029339669732215964\n",
            "Loss S22:  0.03159787951980108\n",
            "Loss S33:  0.03366179927539678\n",
            "Loss S44:  0.03648224443105268\n",
            "Loss S11:  0.02921146934258414\n",
            "Loss S22:  0.031542056352718846\n",
            "Loss S33:  0.03368792643995731\n",
            "Loss S44:  0.03644862830393262\n",
            "Loss S11:  0.02924230869970109\n",
            "Loss S22:  0.03149005344672368\n",
            "Loss S33:  0.03360293814820228\n",
            "Loss S44:  0.036335666304325116\n",
            "Loss S11:  0.029219937233908755\n",
            "Loss S22:  0.03151030737806011\n",
            "Loss S33:  0.03354140033794416\n",
            "Loss S44:  0.03622737028510184\n",
            "Loss S11:  0.029283780696963475\n",
            "Loss S22:  0.031564648686484856\n",
            "Loss S33:  0.033583769790274054\n",
            "Loss S44:  0.03624518377290777\n",
            "Loss S11:  0.02933732590830053\n",
            "Loss S22:  0.031570848177066284\n",
            "Loss S33:  0.03357232511327922\n",
            "Loss S44:  0.03623530031217419\n",
            "Loss S11:  0.02936567465200069\n",
            "Loss S22:  0.0316529780467774\n",
            "Loss S33:  0.033623036063203576\n",
            "Loss S44:  0.036273564284680586\n",
            "Loss S11:  0.029415298771384536\n",
            "Loss S22:  0.03172317018966801\n",
            "Loss S33:  0.03373017076970331\n",
            "Loss S44:  0.03633845430977692\n",
            "Loss S11:  0.029406863582893187\n",
            "Loss S22:  0.031658791375826605\n",
            "Loss S33:  0.03373006966127002\n",
            "Loss S44:  0.036328250819944445\n",
            "Loss S11:  0.029491177721940286\n",
            "Loss S22:  0.03165692150897798\n",
            "Loss S33:  0.03380371260930572\n",
            "Loss S44:  0.03643863193952201\n",
            "Loss S11:  0.029521093719242684\n",
            "Loss S22:  0.03167453729316016\n",
            "Loss S33:  0.03386070053427588\n",
            "Loss S44:  0.036432081771736645\n",
            "Loss S11:  0.029487505769698406\n",
            "Loss S22:  0.031624877201246966\n",
            "Loss S33:  0.0338192556189929\n",
            "Loss S44:  0.036383419380528144\n",
            "Loss S11:  0.02946179878170514\n",
            "Loss S22:  0.03161026867319695\n",
            "Loss S33:  0.03382436843097803\n",
            "Loss S44:  0.036395279116076026\n",
            "Loss S11:  0.029480526303228043\n",
            "Loss S22:  0.03161638566420824\n",
            "Loss S33:  0.0337876704172783\n",
            "Loss S44:  0.0364301306111694\n",
            "Loss S11:  0.029501367680627298\n",
            "Loss S22:  0.03163199009567634\n",
            "Loss S33:  0.033757206111539546\n",
            "Loss S44:  0.03644796717928815\n",
            "Loss S11:  0.029559926609649803\n",
            "Loss S22:  0.03162164404762514\n",
            "Loss S33:  0.03374003053711348\n",
            "Loss S44:  0.03644770768458967\n",
            "Loss S11:  0.029613672651319582\n",
            "Loss S22:  0.03165220353304103\n",
            "Loss S33:  0.03377373123162762\n",
            "Loss S44:  0.03648084368850433\n",
            "Loss S11:  0.02961110125529576\n",
            "Loss S22:  0.031672023793318835\n",
            "Loss S33:  0.03374927961434501\n",
            "Loss S44:  0.03647959565528598\n",
            "Loss S11:  0.029625007154783983\n",
            "Loss S22:  0.031684003527707984\n",
            "Loss S33:  0.033746688030802885\n",
            "Loss S44:  0.03645894876984573\n",
            "Loss S11:  0.02960934084444908\n",
            "Loss S22:  0.03167355974029571\n",
            "Loss S33:  0.033739889432363404\n",
            "Loss S44:  0.03648573420666901\n",
            "Loss S11:  0.029612175338102828\n",
            "Loss S22:  0.031678538391651634\n",
            "Loss S33:  0.033727714533377376\n",
            "Loss S44:  0.03648667630749453\n",
            "Loss S11:  0.029632033504980945\n",
            "Loss S22:  0.03167870186934971\n",
            "Loss S33:  0.033752752320770546\n",
            "Loss S44:  0.036517629438300725\n",
            "Loss S11:  0.029643253191216443\n",
            "Loss S22:  0.031683316305774784\n",
            "Loss S33:  0.03379193323345676\n",
            "Loss S44:  0.03651237869604283\n",
            "Loss S11:  0.029635678557529327\n",
            "Loss S22:  0.031671894486646174\n",
            "Loss S33:  0.033769770498444415\n",
            "Loss S44:  0.03650800895149493\n",
            "Loss S11:  0.02962135744637975\n",
            "Loss S22:  0.031692981395023266\n",
            "Loss S33:  0.03379105755658907\n",
            "Loss S44:  0.03651614224311905\n",
            "Loss S11:  0.02961242346942785\n",
            "Loss S22:  0.03170157209308846\n",
            "Loss S33:  0.03378942343786224\n",
            "Loss S44:  0.03652372047462672\n",
            "Loss S11:  0.029640797492870487\n",
            "Loss S22:  0.031710670763883425\n",
            "Loss S33:  0.03381335565443438\n",
            "Loss S44:  0.036560493715304086\n",
            "Loss S11:  0.029644102834568403\n",
            "Loss S22:  0.03171413487348801\n",
            "Loss S33:  0.033822802577348175\n",
            "Loss S44:  0.036575315769581375\n",
            "Loss S11:  0.0296501421583739\n",
            "Loss S22:  0.0316893269573635\n",
            "Loss S33:  0.03382229741528589\n",
            "Loss S44:  0.036599959837597826\n",
            "Loss S11:  0.029641642845223214\n",
            "Loss S22:  0.0316655275404373\n",
            "Loss S33:  0.03380712502789144\n",
            "Loss S44:  0.03660118791734112\n",
            "Loss S11:  0.029631224170014302\n",
            "Loss S22:  0.031650909459770823\n",
            "Loss S33:  0.033800308555951265\n",
            "Loss S44:  0.03661824433313893\n",
            "Loss S11:  0.029612970431251905\n",
            "Loss S22:  0.031625808495313615\n",
            "Loss S33:  0.03377685213790221\n",
            "Loss S44:  0.03660019995916225\n",
            "Loss S11:  0.02963196639055177\n",
            "Loss S22:  0.03165399575805723\n",
            "Loss S33:  0.03379641882517837\n",
            "Loss S44:  0.03660284997221835\n",
            "Loss S11:  0.02962173681950917\n",
            "Loss S22:  0.03163688070618903\n",
            "Loss S33:  0.03379702742535558\n",
            "Loss S44:  0.03659522575110995\n",
            "Loss S11:  0.02963670037439062\n",
            "Loss S22:  0.0316477265439025\n",
            "Loss S33:  0.033805654253677916\n",
            "Loss S44:  0.036605678764205084\n",
            "Loss S11:  0.02960855392804002\n",
            "Loss S22:  0.03163317056791965\n",
            "Loss S33:  0.03378969087781873\n",
            "Loss S44:  0.03659389456280285\n",
            "Loss S11:  0.029604770357207377\n",
            "Loss S22:  0.03165676646238687\n",
            "Loss S33:  0.03379847703167911\n",
            "Loss S44:  0.03660192771005928\n",
            "Loss S11:  0.029605041668975433\n",
            "Loss S22:  0.0316669951727874\n",
            "Loss S33:  0.03380884595231841\n",
            "Loss S44:  0.03658623784004056\n",
            "Loss S11:  0.02958735121401944\n",
            "Loss S22:  0.03163111936745442\n",
            "Loss S33:  0.033795519766282624\n",
            "Loss S44:  0.036588979173863785\n",
            "Loss S11:  0.029572109729367965\n",
            "Loss S22:  0.031624663909877435\n",
            "Loss S33:  0.03379240934984193\n",
            "Loss S44:  0.0365761254662984\n",
            "Loss S11:  0.029569782332174496\n",
            "Loss S22:  0.03160903116459658\n",
            "Loss S33:  0.03378815156274055\n",
            "Loss S44:  0.0365486638097716\n",
            "Loss S11:  0.029551550302115818\n",
            "Loss S22:  0.031592551034603976\n",
            "Loss S33:  0.03376669019779338\n",
            "Loss S44:  0.03651222281249016\n",
            "Validation: \n",
            " Loss S11:  0.028092036023736\n",
            " Loss S22:  0.043266184628009796\n",
            " Loss S33:  0.03870870918035507\n",
            " Loss S44:  0.04218534380197525\n",
            " Loss S11:  0.029113723231213435\n",
            " Loss S22:  0.047029180718319755\n",
            " Loss S33:  0.04637157118746212\n",
            " Loss S44:  0.05152582749724388\n",
            " Loss S11:  0.029131179675459862\n",
            " Loss S22:  0.04732458820430244\n",
            " Loss S33:  0.04722755674908801\n",
            " Loss S44:  0.05105285946188903\n",
            " Loss S11:  0.02907808002878408\n",
            " Loss S22:  0.04694642975437836\n",
            " Loss S33:  0.04646147995210085\n",
            " Loss S44:  0.05087524426520848\n",
            " Loss S11:  0.02904921381469862\n",
            " Loss S22:  0.04660678765288106\n",
            " Loss S33:  0.046333709018833845\n",
            " Loss S44:  0.05079733552756133\n",
            "\n",
            "Epoch: 60\n",
            "Loss S11:  0.031525228172540665\n",
            "Loss S22:  0.032861411571502686\n",
            "Loss S33:  0.03471216931939125\n",
            "Loss S44:  0.03976031392812729\n",
            "Loss S11:  0.028637942773374645\n",
            "Loss S22:  0.031115415252067825\n",
            "Loss S33:  0.03268888118592175\n",
            "Loss S44:  0.03590581261298873\n",
            "Loss S11:  0.028802707968723206\n",
            "Loss S22:  0.03132825371410165\n",
            "Loss S33:  0.03317382550310521\n",
            "Loss S44:  0.035915475161302654\n",
            "Loss S11:  0.02881414833809099\n",
            "Loss S22:  0.03130763246407432\n",
            "Loss S33:  0.03272431109461092\n",
            "Loss S44:  0.03568088275290305\n",
            "Loss S11:  0.029010978232069714\n",
            "Loss S22:  0.031422656029462814\n",
            "Loss S33:  0.03301592239337724\n",
            "Loss S44:  0.035807161432940784\n",
            "Loss S11:  0.029337279242919942\n",
            "Loss S22:  0.031523029447770586\n",
            "Loss S33:  0.03326072934649739\n",
            "Loss S44:  0.035854068456911574\n",
            "Loss S11:  0.029298713980395286\n",
            "Loss S22:  0.031503495286966936\n",
            "Loss S33:  0.03330563708040558\n",
            "Loss S44:  0.03605204277106973\n",
            "Loss S11:  0.029440830527266985\n",
            "Loss S22:  0.031494047807555804\n",
            "Loss S33:  0.03327315662738303\n",
            "Loss S44:  0.035964799126688866\n",
            "Loss S11:  0.029334603383401294\n",
            "Loss S22:  0.03148907061988189\n",
            "Loss S33:  0.03308932897117403\n",
            "Loss S44:  0.03579933709108535\n",
            "Loss S11:  0.02928893691808968\n",
            "Loss S22:  0.03146441698401839\n",
            "Loss S33:  0.03301908929351267\n",
            "Loss S44:  0.03572954616130709\n",
            "Loss S11:  0.029228628400971395\n",
            "Loss S22:  0.03135001226005578\n",
            "Loss S33:  0.03290049748329243\n",
            "Loss S44:  0.035598823210538026\n",
            "Loss S11:  0.02921650187859127\n",
            "Loss S22:  0.031307968503988544\n",
            "Loss S33:  0.03289103070022287\n",
            "Loss S44:  0.035515934816217634\n",
            "Loss S11:  0.02916692694541344\n",
            "Loss S22:  0.03128438450642361\n",
            "Loss S33:  0.03290789010973016\n",
            "Loss S44:  0.035600024606447574\n",
            "Loss S11:  0.02918723517076205\n",
            "Loss S22:  0.03134113435014969\n",
            "Loss S33:  0.032967751931029424\n",
            "Loss S44:  0.035681274456491\n",
            "Loss S11:  0.02926279563483194\n",
            "Loss S22:  0.03135765498464412\n",
            "Loss S33:  0.03305620591796882\n",
            "Loss S44:  0.03580488051875686\n",
            "Loss S11:  0.0293431794825966\n",
            "Loss S22:  0.03141170251645789\n",
            "Loss S33:  0.033099862642043475\n",
            "Loss S44:  0.03588809984557281\n",
            "Loss S11:  0.02936147237832872\n",
            "Loss S22:  0.03140212681075061\n",
            "Loss S33:  0.0330754043013783\n",
            "Loss S44:  0.03588838545570833\n",
            "Loss S11:  0.02943515734017244\n",
            "Loss S22:  0.03137423415063766\n",
            "Loss S33:  0.033119098572006005\n",
            "Loss S44:  0.035935174052913986\n",
            "Loss S11:  0.029439441098198706\n",
            "Loss S22:  0.03140242479082959\n",
            "Loss S33:  0.03318706941044792\n",
            "Loss S44:  0.036027845365731094\n",
            "Loss S11:  0.029455891444654988\n",
            "Loss S22:  0.03137972783199779\n",
            "Loss S33:  0.03312321645037042\n",
            "Loss S44:  0.03602491674154841\n",
            "Loss S11:  0.029445775863320672\n",
            "Loss S22:  0.0313574247599686\n",
            "Loss S33:  0.0331239338352609\n",
            "Loss S44:  0.036024833181455954\n",
            "Loss S11:  0.02945805348067487\n",
            "Loss S22:  0.03138493316609995\n",
            "Loss S33:  0.033164238592530315\n",
            "Loss S44:  0.03605417705083628\n",
            "Loss S11:  0.029471864173719785\n",
            "Loss S22:  0.0314056214063146\n",
            "Loss S33:  0.03318577657592782\n",
            "Loss S44:  0.03600907464810896\n",
            "Loss S11:  0.029469443952817937\n",
            "Loss S22:  0.03134371649909329\n",
            "Loss S33:  0.0331581419503147\n",
            "Loss S44:  0.03601969353596628\n",
            "Loss S11:  0.029470172567733592\n",
            "Loss S22:  0.03136375384150204\n",
            "Loss S33:  0.0331902649964658\n",
            "Loss S44:  0.036030053590034056\n",
            "Loss S11:  0.029459586508008114\n",
            "Loss S22:  0.031339776787565526\n",
            "Loss S33:  0.03319051549343236\n",
            "Loss S44:  0.03605214708978199\n",
            "Loss S11:  0.02946209532861737\n",
            "Loss S22:  0.03134769243384458\n",
            "Loss S33:  0.0332130270999396\n",
            "Loss S44:  0.036058575492994537\n",
            "Loss S11:  0.029443838423963403\n",
            "Loss S22:  0.03130957251242826\n",
            "Loss S33:  0.03320112335066074\n",
            "Loss S44:  0.036056683876472644\n",
            "Loss S11:  0.029406440719378802\n",
            "Loss S22:  0.031294219514017\n",
            "Loss S33:  0.0331818294774383\n",
            "Loss S44:  0.03604053872309333\n",
            "Loss S11:  0.029386817180004316\n",
            "Loss S22:  0.03129050054961873\n",
            "Loss S33:  0.03321212267701568\n",
            "Loss S44:  0.036045799607929495\n",
            "Loss S11:  0.02941555467332122\n",
            "Loss S22:  0.03132898917427887\n",
            "Loss S33:  0.03325836413054173\n",
            "Loss S44:  0.03607780129286736\n",
            "Loss S11:  0.029408600971917248\n",
            "Loss S22:  0.031321136730085235\n",
            "Loss S33:  0.03325691323883089\n",
            "Loss S44:  0.03607186861336231\n",
            "Loss S11:  0.029400144503495405\n",
            "Loss S22:  0.03134088419563488\n",
            "Loss S33:  0.03326523512880379\n",
            "Loss S44:  0.036092408192909764\n",
            "Loss S11:  0.029395823645492695\n",
            "Loss S22:  0.03133965940904041\n",
            "Loss S33:  0.03326683948407361\n",
            "Loss S44:  0.03612757312679759\n",
            "Loss S11:  0.02940634707988532\n",
            "Loss S22:  0.031425469601390296\n",
            "Loss S33:  0.03331518537007119\n",
            "Loss S44:  0.036193622265262335\n",
            "Loss S11:  0.029400797279556933\n",
            "Loss S22:  0.031443016348868355\n",
            "Loss S33:  0.0333346772535659\n",
            "Loss S44:  0.03620837698358926\n",
            "Loss S11:  0.029405837073674494\n",
            "Loss S22:  0.03143313163349668\n",
            "Loss S33:  0.03335606747944104\n",
            "Loss S44:  0.036226946339423964\n",
            "Loss S11:  0.029379302866736835\n",
            "Loss S22:  0.03143501712506351\n",
            "Loss S33:  0.0333497140988346\n",
            "Loss S44:  0.03620916498558862\n",
            "Loss S11:  0.029366448746500365\n",
            "Loss S22:  0.03142953063226904\n",
            "Loss S33:  0.03333931677825651\n",
            "Loss S44:  0.03620490698715833\n",
            "Loss S11:  0.029340435767455784\n",
            "Loss S22:  0.03142142175313305\n",
            "Loss S33:  0.033323857637927355\n",
            "Loss S44:  0.036168718200815304\n",
            "Loss S11:  0.02935237844853181\n",
            "Loss S22:  0.03142305084055944\n",
            "Loss S33:  0.033357227871431375\n",
            "Loss S44:  0.0361514063807497\n",
            "Loss S11:  0.0293587575782172\n",
            "Loss S22:  0.03140900199757005\n",
            "Loss S33:  0.03335208183385595\n",
            "Loss S44:  0.036133937573926\n",
            "Loss S11:  0.029369507144225183\n",
            "Loss S22:  0.03139494181192544\n",
            "Loss S33:  0.03335617070756587\n",
            "Loss S44:  0.03613520595477341\n",
            "Loss S11:  0.029360730481915297\n",
            "Loss S22:  0.03139013302644278\n",
            "Loss S33:  0.033337976022897904\n",
            "Loss S44:  0.03613813469519195\n",
            "Loss S11:  0.02935277587837643\n",
            "Loss S22:  0.03137741894239471\n",
            "Loss S33:  0.03332680796617283\n",
            "Loss S44:  0.036131915127406586\n",
            "Loss S11:  0.02936237154243526\n",
            "Loss S22:  0.03139462658594427\n",
            "Loss S33:  0.033351845824533446\n",
            "Loss S44:  0.036140334967961596\n",
            "Loss S11:  0.029354677307308365\n",
            "Loss S22:  0.03137359676434135\n",
            "Loss S33:  0.0333553822610099\n",
            "Loss S44:  0.036135804013161237\n",
            "Loss S11:  0.02933777319445352\n",
            "Loss S22:  0.031349873494538785\n",
            "Loss S33:  0.03335311440572997\n",
            "Loss S44:  0.03613906313374544\n",
            "Loss S11:  0.02933839162164939\n",
            "Loss S22:  0.03135712439360837\n",
            "Loss S33:  0.03337907278426224\n",
            "Loss S44:  0.03613359945789444\n",
            "Loss S11:  0.029332731905452103\n",
            "Loss S22:  0.03134496223234596\n",
            "Loss S33:  0.03337014446038821\n",
            "Loss S44:  0.03612038488604143\n",
            "Validation: \n",
            " Loss S11:  0.029192231595516205\n",
            " Loss S22:  0.04272709786891937\n",
            " Loss S33:  0.040672071278095245\n",
            " Loss S44:  0.04366857558488846\n",
            " Loss S11:  0.029564810916781425\n",
            " Loss S22:  0.04650982398362387\n",
            " Loss S33:  0.047730867351804464\n",
            " Loss S44:  0.051230122290906455\n",
            " Loss S11:  0.029495483673200373\n",
            " Loss S22:  0.04660326433254451\n",
            " Loss S33:  0.04837654331108419\n",
            " Loss S44:  0.050984218444039185\n",
            " Loss S11:  0.02937109176008428\n",
            " Loss S22:  0.04632604696222993\n",
            " Loss S33:  0.047627246709632094\n",
            " Loss S44:  0.0506929411384903\n",
            " Loss S11:  0.029322690111987383\n",
            " Loss S22:  0.045931647773142216\n",
            " Loss S33:  0.047523758431643616\n",
            " Loss S44:  0.05068424394653167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net2(nn.Module):\n",
        "    def __init__(self, s11,s22,s33,s44):\n",
        "        super(Net2, self).__init__()\n",
        "        self.s11 = s11\n",
        "        self.s22 = s22\n",
        "        self.s33 = s33\n",
        "        self.s44 = s44\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.s11(x)\n",
        "        out2 = self.s22(x)\n",
        "        out3 = self.s33(x)\n",
        "        out4 = self.s44(x)\n",
        "\n",
        "        out = torch.cat((out1,out2,out3,out4),1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "net4students = Net2(s11,s22,s33,s44)\n",
        "net4students.to(device)\n",
        "summary(net4students, (3, 32, 32))"
      ],
      "metadata": {
        "id": "HuuyhtB_NQoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6804c1be-d503-4b22-beb3-156a7d3bb52e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "              VGG-32                  [-1, 128]               0\n",
            "           Conv2d-33           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-34           [-1, 32, 32, 32]              64\n",
            "             ReLU-35           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-36           [-1, 32, 16, 16]               0\n",
            "           Conv2d-37           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-38           [-1, 32, 16, 16]              64\n",
            "             ReLU-39           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-40             [-1, 32, 8, 8]               0\n",
            "           Conv2d-41             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-42             [-1, 64, 8, 8]             128\n",
            "             ReLU-43             [-1, 64, 8, 8]               0\n",
            "           Conv2d-44             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-45             [-1, 64, 8, 8]             128\n",
            "             ReLU-46             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-47             [-1, 64, 4, 4]               0\n",
            "           Conv2d-48            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-49            [-1, 128, 4, 4]             256\n",
            "             ReLU-50            [-1, 128, 4, 4]               0\n",
            "           Conv2d-51            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-52            [-1, 128, 4, 4]             256\n",
            "             ReLU-53            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-54            [-1, 128, 2, 2]               0\n",
            "           Conv2d-55            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-56            [-1, 128, 2, 2]             256\n",
            "             ReLU-57            [-1, 128, 2, 2]               0\n",
            "           Conv2d-58            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-59            [-1, 128, 2, 2]             256\n",
            "             ReLU-60            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-61            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-62            [-1, 128, 1, 1]               0\n",
            "           Linear-63                  [-1, 128]          16,512\n",
            "              VGG-64                  [-1, 128]               0\n",
            "           Conv2d-65           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-66           [-1, 32, 32, 32]              64\n",
            "             ReLU-67           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-68           [-1, 32, 16, 16]               0\n",
            "           Conv2d-69           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-70           [-1, 32, 16, 16]              64\n",
            "             ReLU-71           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-72             [-1, 32, 8, 8]               0\n",
            "           Conv2d-73             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-74             [-1, 64, 8, 8]             128\n",
            "             ReLU-75             [-1, 64, 8, 8]               0\n",
            "           Conv2d-76             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-77             [-1, 64, 8, 8]             128\n",
            "             ReLU-78             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-79             [-1, 64, 4, 4]               0\n",
            "           Conv2d-80            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-81            [-1, 128, 4, 4]             256\n",
            "             ReLU-82            [-1, 128, 4, 4]               0\n",
            "           Conv2d-83            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-84            [-1, 128, 4, 4]             256\n",
            "             ReLU-85            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-86            [-1, 128, 2, 2]               0\n",
            "           Conv2d-87            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-88            [-1, 128, 2, 2]             256\n",
            "             ReLU-89            [-1, 128, 2, 2]               0\n",
            "           Conv2d-90            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-91            [-1, 128, 2, 2]             256\n",
            "             ReLU-92            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-93            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-94            [-1, 128, 1, 1]               0\n",
            "           Linear-95                  [-1, 128]          16,512\n",
            "              VGG-96                  [-1, 128]               0\n",
            "           Conv2d-97           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-98           [-1, 32, 32, 32]              64\n",
            "             ReLU-99           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-100           [-1, 32, 16, 16]               0\n",
            "          Conv2d-101           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-102           [-1, 32, 16, 16]              64\n",
            "            ReLU-103           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-104             [-1, 32, 8, 8]               0\n",
            "          Conv2d-105             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-106             [-1, 64, 8, 8]             128\n",
            "            ReLU-107             [-1, 64, 8, 8]               0\n",
            "          Conv2d-108             [-1, 64, 8, 8]          36,928\n",
            "     BatchNorm2d-109             [-1, 64, 8, 8]             128\n",
            "            ReLU-110             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-111             [-1, 64, 4, 4]               0\n",
            "          Conv2d-112            [-1, 128, 4, 4]          73,856\n",
            "     BatchNorm2d-113            [-1, 128, 4, 4]             256\n",
            "            ReLU-114            [-1, 128, 4, 4]               0\n",
            "          Conv2d-115            [-1, 128, 4, 4]         147,584\n",
            "     BatchNorm2d-116            [-1, 128, 4, 4]             256\n",
            "            ReLU-117            [-1, 128, 4, 4]               0\n",
            "       MaxPool2d-118            [-1, 128, 2, 2]               0\n",
            "          Conv2d-119            [-1, 128, 2, 2]         147,584\n",
            "     BatchNorm2d-120            [-1, 128, 2, 2]             256\n",
            "            ReLU-121            [-1, 128, 2, 2]               0\n",
            "          Conv2d-122            [-1, 128, 2, 2]         147,584\n",
            "     BatchNorm2d-123            [-1, 128, 2, 2]             256\n",
            "            ReLU-124            [-1, 128, 2, 2]               0\n",
            "       MaxPool2d-125            [-1, 128, 1, 1]               0\n",
            "       AvgPool2d-126            [-1, 128, 1, 1]               0\n",
            "          Linear-127                  [-1, 128]          16,512\n",
            "             VGG-128                  [-1, 128]               0\n",
            "          Linear-129                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 2,405,514\n",
            "Trainable params: 2,405,514\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 5.34\n",
            "Params size (MB): 9.18\n",
            "Estimated Total Size (MB): 14.53\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for m in net4students.modules():\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        m.weight.requires_grad = False\n",
        "        if m.bias is not None:\n",
        "            m.bias.requires_grad = False\n",
        "net4students = net4students.to(device)\n",
        "summary(net4students, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMUeIq5X02oI",
        "outputId": "8c640d79-b20b-4696-dd9f-dc5fa5c4749a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "              VGG-32                  [-1, 128]               0\n",
            "           Conv2d-33           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-34           [-1, 32, 32, 32]              64\n",
            "             ReLU-35           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-36           [-1, 32, 16, 16]               0\n",
            "           Conv2d-37           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-38           [-1, 32, 16, 16]              64\n",
            "             ReLU-39           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-40             [-1, 32, 8, 8]               0\n",
            "           Conv2d-41             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-42             [-1, 64, 8, 8]             128\n",
            "             ReLU-43             [-1, 64, 8, 8]               0\n",
            "           Conv2d-44             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-45             [-1, 64, 8, 8]             128\n",
            "             ReLU-46             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-47             [-1, 64, 4, 4]               0\n",
            "           Conv2d-48            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-49            [-1, 128, 4, 4]             256\n",
            "             ReLU-50            [-1, 128, 4, 4]               0\n",
            "           Conv2d-51            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-52            [-1, 128, 4, 4]             256\n",
            "             ReLU-53            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-54            [-1, 128, 2, 2]               0\n",
            "           Conv2d-55            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-56            [-1, 128, 2, 2]             256\n",
            "             ReLU-57            [-1, 128, 2, 2]               0\n",
            "           Conv2d-58            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-59            [-1, 128, 2, 2]             256\n",
            "             ReLU-60            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-61            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-62            [-1, 128, 1, 1]               0\n",
            "           Linear-63                  [-1, 128]          16,512\n",
            "              VGG-64                  [-1, 128]               0\n",
            "           Conv2d-65           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-66           [-1, 32, 32, 32]              64\n",
            "             ReLU-67           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-68           [-1, 32, 16, 16]               0\n",
            "           Conv2d-69           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-70           [-1, 32, 16, 16]              64\n",
            "             ReLU-71           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-72             [-1, 32, 8, 8]               0\n",
            "           Conv2d-73             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-74             [-1, 64, 8, 8]             128\n",
            "             ReLU-75             [-1, 64, 8, 8]               0\n",
            "           Conv2d-76             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-77             [-1, 64, 8, 8]             128\n",
            "             ReLU-78             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-79             [-1, 64, 4, 4]               0\n",
            "           Conv2d-80            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-81            [-1, 128, 4, 4]             256\n",
            "             ReLU-82            [-1, 128, 4, 4]               0\n",
            "           Conv2d-83            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-84            [-1, 128, 4, 4]             256\n",
            "             ReLU-85            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-86            [-1, 128, 2, 2]               0\n",
            "           Conv2d-87            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-88            [-1, 128, 2, 2]             256\n",
            "             ReLU-89            [-1, 128, 2, 2]               0\n",
            "           Conv2d-90            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-91            [-1, 128, 2, 2]             256\n",
            "             ReLU-92            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-93            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-94            [-1, 128, 1, 1]               0\n",
            "           Linear-95                  [-1, 128]          16,512\n",
            "              VGG-96                  [-1, 128]               0\n",
            "           Conv2d-97           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-98           [-1, 32, 32, 32]              64\n",
            "             ReLU-99           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-100           [-1, 32, 16, 16]               0\n",
            "          Conv2d-101           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-102           [-1, 32, 16, 16]              64\n",
            "            ReLU-103           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-104             [-1, 32, 8, 8]               0\n",
            "          Conv2d-105             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-106             [-1, 64, 8, 8]             128\n",
            "            ReLU-107             [-1, 64, 8, 8]               0\n",
            "          Conv2d-108             [-1, 64, 8, 8]          36,928\n",
            "     BatchNorm2d-109             [-1, 64, 8, 8]             128\n",
            "            ReLU-110             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-111             [-1, 64, 4, 4]               0\n",
            "          Conv2d-112            [-1, 128, 4, 4]          73,856\n",
            "     BatchNorm2d-113            [-1, 128, 4, 4]             256\n",
            "            ReLU-114            [-1, 128, 4, 4]               0\n",
            "          Conv2d-115            [-1, 128, 4, 4]         147,584\n",
            "     BatchNorm2d-116            [-1, 128, 4, 4]             256\n",
            "            ReLU-117            [-1, 128, 4, 4]               0\n",
            "       MaxPool2d-118            [-1, 128, 2, 2]               0\n",
            "          Conv2d-119            [-1, 128, 2, 2]         147,584\n",
            "     BatchNorm2d-120            [-1, 128, 2, 2]             256\n",
            "            ReLU-121            [-1, 128, 2, 2]               0\n",
            "          Conv2d-122            [-1, 128, 2, 2]         147,584\n",
            "     BatchNorm2d-123            [-1, 128, 2, 2]             256\n",
            "            ReLU-124            [-1, 128, 2, 2]               0\n",
            "       MaxPool2d-125            [-1, 128, 1, 1]               0\n",
            "       AvgPool2d-126            [-1, 128, 1, 1]               0\n",
            "          Linear-127                  [-1, 128]          16,512\n",
            "             VGG-128                  [-1, 128]               0\n",
            "          Linear-129                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 2,405,514\n",
            "Trainable params: 76,810\n",
            "Non-trainable params: 2,328,704\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 5.34\n",
            "Params size (MB): 9.18\n",
            "Estimated Total Size (MB): 14.53\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net4students.parameters(), lr=0.0001)\n",
        "\n",
        "def train41(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    net4students.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        net4students.zero_grad()\n",
        "        outputs = net4students(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if(batch_idx % 200 == 0):\n",
        "          print(\"Accuracy : \",100.*correct/total,\" Loss : \", train_loss/(batch_idx+1))\n",
        "def test42(epoch):\n",
        "    net4students.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net4students(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\"Accuracy : \",100.*correct/total,\" Loss : \", test_loss/(batch_idx+1))"
      ],
      "metadata": {
        "id": "Nl6WfyLk04H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "    train41(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test42(epoch)"
      ],
      "metadata": {
        "id": "wi8hV35xQ6Fg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19145b0c-1673-4f66-e647-501e4eb32763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Accuracy :  26.0  Loss :  2.5862815380096436\n",
            "Accuracy :  73.69651741293532  Loss :  1.1676214880018092\n",
            "Accuracy :  78.35910224438902  Loss :  0.8690274864658156\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.474447101354599\n",
            "Accuracy :  83.19047619047619  Loss :  0.5040500305947804\n",
            "Accuracy :  82.70731707317073  Loss :  0.5196995618866711\n",
            "Accuracy :  82.72131147540983  Loss :  0.5172153574521424\n",
            "Accuracy :  82.81481481481481  Loss :  0.5134969408865329\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy :  84.0  Loss :  0.5228055715560913\n",
            "Accuracy :  83.56218905472637  Loss :  0.4840512572236322\n",
            "Accuracy :  83.84788029925187  Loss :  0.47575937690877557\n",
            "Validation: \n",
            "Accuracy :  88.0  Loss :  0.4355241060256958\n",
            "Accuracy :  84.19047619047619  Loss :  0.47791553678966703\n",
            "Accuracy :  83.46341463414635  Loss :  0.49652906089294246\n",
            "Accuracy :  83.29508196721312  Loss :  0.4926418574129949\n",
            "Accuracy :  83.39506172839506  Loss :  0.4885882275339998\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy :  88.0  Loss :  0.4041728675365448\n",
            "Accuracy :  84.19900497512438  Loss :  0.4600710245359003\n",
            "Accuracy :  84.30174563591022  Loss :  0.4567312380099237\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.42599499225616455\n",
            "Accuracy :  84.04761904761905  Loss :  0.4730636924505234\n",
            "Accuracy :  83.41463414634147  Loss :  0.4924383028978255\n",
            "Accuracy :  83.37704918032787  Loss :  0.48780415072792865\n",
            "Accuracy :  83.55555555555556  Loss :  0.4830875825366856\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy :  84.0  Loss :  0.4090234041213989\n",
            "Accuracy :  84.2089552238806  Loss :  0.45430617464419026\n",
            "Accuracy :  84.39152119700748  Loss :  0.45120455621929834\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.42002102732658386\n",
            "Accuracy :  83.95238095238095  Loss :  0.4685655143998918\n",
            "Accuracy :  83.26829268292683  Loss :  0.48802277264071675\n",
            "Accuracy :  83.37704918032787  Loss :  0.483756494570951\n",
            "Accuracy :  83.5925925925926  Loss :  0.4790976306906453\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy :  85.0  Loss :  0.378398060798645\n",
            "Accuracy :  84.56716417910448  Loss :  0.44792481426575886\n",
            "Accuracy :  84.58852867830424  Loss :  0.4453470792036104\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.41661080718040466\n",
            "Accuracy :  84.0  Loss :  0.4661815017461777\n",
            "Accuracy :  83.36585365853658  Loss :  0.4858153066257151\n",
            "Accuracy :  83.49180327868852  Loss :  0.48135970239756537\n",
            "Accuracy :  83.76543209876543  Loss :  0.4761512318143138\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy :  85.0  Loss :  0.3994355797767639\n",
            "Accuracy :  84.69651741293532  Loss :  0.4452524020600675\n",
            "Accuracy :  84.82793017456359  Loss :  0.4422592007162565\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.412678062915802\n",
            "Accuracy :  84.0  Loss :  0.4633831892694746\n",
            "Accuracy :  83.26829268292683  Loss :  0.48339767136224887\n",
            "Accuracy :  83.27868852459017  Loss :  0.4791467580638948\n",
            "Accuracy :  83.65432098765432  Loss :  0.47412911903711014\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy :  84.0  Loss :  0.4322812259197235\n",
            "Accuracy :  84.7412935323383  Loss :  0.4406311116853164\n",
            "Accuracy :  84.9426433915212  Loss :  0.4389527514762712\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.41388416290283203\n",
            "Accuracy :  84.04761904761905  Loss :  0.4613403429587682\n",
            "Accuracy :  83.39024390243902  Loss :  0.4816697737792643\n",
            "Accuracy :  83.49180327868852  Loss :  0.47726727338110814\n",
            "Accuracy :  83.82716049382717  Loss :  0.47185537347823014\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy :  83.0  Loss :  0.43914371728897095\n",
            "Accuracy :  84.94029850746269  Loss :  0.4369504290137125\n",
            "Accuracy :  85.01496259351622  Loss :  0.4357557975294584\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.41027987003326416\n",
            "Accuracy :  84.0952380952381  Loss :  0.4601770923251197\n",
            "Accuracy :  83.4390243902439  Loss :  0.48058278168120033\n",
            "Accuracy :  83.57377049180327  Loss :  0.47594345055642673\n",
            "Accuracy :  83.88888888888889  Loss :  0.47041408993579725\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy :  86.0  Loss :  0.40428778529167175\n",
            "Accuracy :  84.88557213930348  Loss :  0.43610697473163035\n",
            "Accuracy :  84.90523690773067  Loss :  0.4367369452022258\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.4076771140098572\n",
            "Accuracy :  83.95238095238095  Loss :  0.4599179611319587\n",
            "Accuracy :  83.39024390243902  Loss :  0.48005152984363275\n",
            "Accuracy :  83.47540983606558  Loss :  0.4752483626858133\n",
            "Accuracy :  83.77777777777777  Loss :  0.46968314713901943\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy :  85.0  Loss :  0.42572659254074097\n",
            "Accuracy :  85.0  Loss :  0.4344660381772625\n",
            "Accuracy :  85.06982543640898  Loss :  0.4337739026605934\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.4046124517917633\n",
            "Accuracy :  84.0  Loss :  0.4577252247503826\n",
            "Accuracy :  83.39024390243902  Loss :  0.47842632815605257\n",
            "Accuracy :  83.49180327868852  Loss :  0.4737120209658732\n",
            "Accuracy :  83.71604938271605  Loss :  0.4682464377011782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiple Students PART 2-- 8 students now**\n"
      ],
      "metadata": {
        "id": "J3x1rV4fS6Mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "    'VGGS1': [32, 32, 'M', 64, 64, 'M', 128, 128, 'M', 256, 256, 'M','M'],\n",
        "    'VGGS33': [32,32,'M', 32,32,'M',32,64,'M', 64,64,'M',64,64,64,'M'],\n",
        "    'VGGS2': [32,'M', 32, 'M', 64, 64, 'M', 128,128,'M', 256,'M'],\n",
        "\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(64, 64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        #out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "ss11 = VGG('VGGS33')\n",
        "ss11 = ss11.to(device)\n",
        "summary(ss11, (3, 32, 32))\n",
        "ss22 = VGG('VGGS33')\n",
        "ss22 = ss22.to(device)\n",
        "summary(ss22, (3,32,32))\n",
        "ss33 = VGG('VGGS33')\n",
        "ss33 = ss33.to(device)\n",
        "summary(ss33, (3, 32, 32))\n",
        "ss44 = VGG('VGGS33')\n",
        "ss44 = ss44.to(device)\n",
        "summary(ss44, (3, 32, 32))\n",
        "ss55 = VGG('VGGS33')\n",
        "ss55 = ss55.to(device)\n",
        "summary(ss55, (3, 32, 32))\n",
        "ss66 = VGG('VGGS33')\n",
        "ss66 = ss66.to(device)\n",
        "summary(ss66, (3,32,32))\n",
        "ss77 = VGG('VGGS33')\n",
        "ss77 = ss77.to(device)\n",
        "summary(ss77, (3, 32, 32))\n",
        "ss88 = VGG('VGGS33')\n",
        "ss88 = ss88.to(device)\n",
        "summary(ss88, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf6ZMLwaDqm0",
        "outputId": "81c685b8-6877-41ad-af1e-d65c37b45b58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TAA1_WOH = nn.Sequential(*list(s11.children())[:-1],nn.Flatten())\n",
        "# summary(s11, (3, 32, 32))\n",
        "# summary(TAA1_WOH, (3, 32, 32))\n",
        "\n",
        "# TAA2_WOH = nn.Sequential(*list(s22.children())[:-1],nn.Flatten())\n",
        "# summary(s22, (3, 32, 32))\n",
        "# summary(TAA2_WOH, (3, 32, 32))\n",
        "\n",
        "# TAA3_WOH = nn.Sequential(*list(s33.children())[:-1],nn.Flatten())\n",
        "# summary(s33, (3, 32, 32))\n",
        "# summary(TAA3_WOH, (3, 32, 32))\n",
        "\n",
        "# TAA4_WOH = nn.Sequential(*list(s44.children())[:-1],nn.Flatten())\n",
        "# summary(s44, (3, 32, 32))\n",
        "# summary(TAA4_WOH, (3, 32, 32))"
      ],
      "metadata": {
        "id": "5oDdCLJkFvsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TAA1_WOH.eval()\n",
        "# TAA2_WOH.eval()\n",
        "# TAA3_WOH.eval()\n",
        "# TAA4_WOH.eval()\n",
        "# TA2DenseTrain = None\n",
        "# TA2DenseTest = None\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "#         inputs, targets = inputs.to(device), targets.to(device)\n",
        "#         outputs1 = TAA1_WOH(inputs)\n",
        "#         outputs2 = TAA2_WOH(inputs)\n",
        "#         outputs3 = TAA3_WOH(inputs)\n",
        "#         outputs4 = TAA4_WOH(inputs)\n",
        "#         if(TA2DenseTrain == None):\n",
        "#             TA2DenseTrain = torch.cat((outputs1,outputs2,outputs3,outputs4),1) \n",
        "#         else:\n",
        "#             totalOUTPUT = torch.cat((outputs1,outputs2,outputs3,outputs4),1)         \n",
        "#             TA2DenseTrain = torch.cat((TA2DenseTrain,totalOUTPUT))\n",
        "           \n",
        "# with torch.no_grad():\n",
        "#     for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "#         inputs, targets = inputs.to(device), targets.to(device)\n",
        "#         outputs1 = TAA1_WOH(inputs)\n",
        "#         outputs2 = TAA2_WOH(inputs)\n",
        "#         outputs3 = TAA3_WOH(inputs)\n",
        "#         outputs4 = TAA4_WOH(inputs)\n",
        "#         if(TA2DenseTest == None):\n",
        "#             TA2DenseTest = torch.cat((outputs1,outputs2,outputs3,outputs4),1) \n",
        "#         else:\n",
        "#             totalOUTPUT = torch.cat((outputs1,outputs2,outputs3,outputs4),1)      \n",
        "#             TA2DenseTest = torch.cat((TA2DenseTest,totalOUTPUT))\n",
        "           "
      ],
      "metadata": {
        "id": "-X97wNRHGSkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(TA2DenseTrain.shape)"
      ],
      "metadata": {
        "id": "XgpdmBzrHEZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer1 = optim.Adam(ss11.parameters(), lr=0.0001)\n",
        "optimizer2 = optim.Adam(ss22.parameters(), lr=0.0001)\n",
        "optimizer3 = optim.Adam(ss33.parameters(), lr=0.0001)\n",
        "optimizer4 = optim.Adam(ss44.parameters(), lr=0.0001)\n",
        "optimizer5 = optim.Adam(ss55.parameters(), lr=0.0001)\n",
        "optimizer6 = optim.Adam(ss66.parameters(), lr=0.0001)\n",
        "optimizer7 = optim.Adam(ss77.parameters(), lr=0.0001)\n",
        "optimizer8 = optim.Adam(ss88.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def train8(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    s11.train()\n",
        "    train_loss1 = 0\n",
        "    train_loss2 = 0\n",
        "    train_loss3 = 0\n",
        "    train_loss4= 0\n",
        "    train_loss5 = 0\n",
        "    train_loss6 = 0\n",
        "    train_loss7 = 0\n",
        "    train_loss8= 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        #code here\n",
        "        targets = S1DenseTrain[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "        targets.to(device)\n",
        "        ss11.zero_grad()\n",
        "        ss22.zero_grad()\n",
        "        ss33.zero_grad()\n",
        "        ss44.zero_grad()\n",
        "        ss55.zero_grad()\n",
        "        ss66.zero_grad()\n",
        "        ss77.zero_grad()\n",
        "        ss88.zero_grad()\n",
        "        output1 = ss11(inputs)\n",
        "        output2 = ss22(inputs)\n",
        "        output3 = ss33(inputs)\n",
        "        output4 = ss44(inputs)\n",
        "        output5 = ss55(inputs)\n",
        "        output6 = ss66(inputs)\n",
        "        output7 = ss77(inputs)\n",
        "        output8 = ss88(inputs)\n",
        "        \n",
        "        loss1 = criterion(output1, targets[:,:64])\n",
        "        loss2 = criterion(output2, targets[:,64:128])\n",
        "        loss3 = criterion(output3, targets[:,128:192])\n",
        "        loss4 = criterion(output4, targets[:,192:256])\n",
        "        loss5 = criterion(output5, targets[:,256:320])\n",
        "        loss6 = criterion(output6, targets[:,320:384])\n",
        "        loss7 = criterion(output7, targets[:,384:448])\n",
        "        loss8 = criterion(output8, targets[:,448:512])\n",
        "\n",
        "        loss1.backward()\n",
        "        loss2.backward()\n",
        "        loss3.backward()\n",
        "        loss4.backward()\n",
        "        loss5.backward()\n",
        "        loss6.backward()\n",
        "        loss7.backward()\n",
        "        loss8.backward()\n",
        "        optimizer1.step()\n",
        "        optimizer2.step()\n",
        "        optimizer3.step()\n",
        "        optimizer4.step()\n",
        "        optimizer5.step()\n",
        "        optimizer6.step()\n",
        "        optimizer7.step()\n",
        "        optimizer8.step()\n",
        "      \n",
        "\n",
        "        train_loss1 += loss1.item()\n",
        "        train_loss2 += loss2.item()\n",
        "        train_loss3 += loss3.item()\n",
        "        train_loss4 += loss4.item()\n",
        "        train_loss5 += loss5.item()\n",
        "        train_loss6 += loss6.item()\n",
        "        train_loss7 += loss7.item()\n",
        "        train_loss8 += loss8.item()\n",
        "        if(batch_idx % 10 == 0):\n",
        "          print(\"Loss SS11: \", train_loss1/(batch_idx+1))\n",
        "          print(\"Loss SS22: \", train_loss2/(batch_idx+1))\n",
        "          print(\"Loss SS33: \", train_loss3/(batch_idx+1))\n",
        "          print(\"Loss SS44: \", train_loss4/(batch_idx+1))\n",
        "          print(\"Loss SS55: \", train_loss5/(batch_idx+1))\n",
        "          print(\"Loss SS66: \", train_loss6/(batch_idx+1))\n",
        "          print(\"Loss SS77: \", train_loss7/(batch_idx+1))\n",
        "          print(\"Loss SS88: \", train_loss8/(batch_idx+1))\n",
        "def test8(epoch):\n",
        "    ss11.eval()\n",
        "    \n",
        "    test_loss1 = 0\n",
        "    test_loss2 = 0\n",
        "    test_loss3 = 0\n",
        "    test_loss4= 0\n",
        "    test_loss5 = 0\n",
        "    test_loss6 = 0\n",
        "    test_loss7 = 0\n",
        "    test_loss8= 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            #code here\n",
        "            targets =   s1DenseTest[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "            targets.to(device)\n",
        "            output1 = ss11(inputs)\n",
        "            output2 = ss22(inputs)\n",
        "            output3 = ss33(inputs)\n",
        "            output4 = ss44(inputs)\n",
        "            output5 = ss55(inputs)\n",
        "            output6 = ss66(inputs)\n",
        "            output7 = ss77(inputs)\n",
        "            output8 = ss88(inputs)\n",
        "            loss1 = criterion(output1, targets[:,:64])\n",
        "            loss2 = criterion(output2, targets[:,64:128])\n",
        "            loss3 = criterion(output3, targets[:,128:192])\n",
        "            loss4 = criterion(output4, targets[:,192:256])\n",
        "            loss5 = criterion(output5, targets[:,256:320])\n",
        "            loss6 = criterion(output6, targets[:,320:384])\n",
        "            loss7 = criterion(output7, targets[:,384:448])\n",
        "            loss8 = criterion(output8, targets[:,448:512])\n",
        "            test_loss1 += loss1.item()\n",
        "            test_loss2 += loss2.item()\n",
        "            test_loss3 += loss3.item()\n",
        "            test_loss4 += loss4.item()\n",
        "\n",
        "            test_loss5 += loss5.item()\n",
        "            test_loss6 += loss6.item()\n",
        "            test_loss7 += loss7.item()\n",
        "            test_loss8 += loss8.item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\" Loss SS11: \", test_loss1/(batch_idx+1))\n",
        "              print(\" Loss SS22: \", test_loss2/(batch_idx+1))\n",
        "              print(\" Loss SS33: \", test_loss3/(batch_idx+1))\n",
        "              print(\" Loss SS44: \", test_loss4/(batch_idx+1))\n",
        "              print(\" Loss SS55: \", test_loss5/(batch_idx+1))\n",
        "              print(\" Loss SS66: \", test_loss6/(batch_idx+1))\n",
        "              print(\" Loss SS77: \", test_loss7/(batch_idx+1))\n",
        "              print(\" Loss SS88: \", test_loss8/(batch_idx+1))"
      ],
      "metadata": {
        "id": "EsiP7r2XHExk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+60):\n",
        "    train8(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test8(epoch)"
      ],
      "metadata": {
        "id": "lpbuGedJI_2y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f81ba267-6ef1-4bc0-cb5c-9d1ba24e16ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Loss SS33:  0.030099085844157603\n",
            "Loss SS44:  0.03940099960817096\n",
            "Loss SS55:  0.041487391828499795\n",
            "Loss SS66:  0.03372475795943261\n",
            "Loss SS77:  0.04204665821836428\n",
            "Loss SS88:  0.037911063174495614\n",
            "Loss SS11:  0.03513001892576342\n",
            "Loss SS22:  0.033769242251826044\n",
            "Loss SS33:  0.03011040861934349\n",
            "Loss SS44:  0.03943429851992766\n",
            "Loss SS55:  0.0414892865479587\n",
            "Loss SS66:  0.033750974361847465\n",
            "Loss SS77:  0.04206380065056749\n",
            "Loss SS88:  0.037917338334897216\n",
            "Loss SS11:  0.03512453000512146\n",
            "Loss SS22:  0.03378360563494863\n",
            "Loss SS33:  0.030101229991863532\n",
            "Loss SS44:  0.03944364233608663\n",
            "Loss SS55:  0.04150627100264649\n",
            "Loss SS66:  0.03372454839937588\n",
            "Loss SS77:  0.04205544401694388\n",
            "Loss SS88:  0.03789606344616471\n",
            "Loss SS11:  0.03513397019790074\n",
            "Loss SS22:  0.033797359417000176\n",
            "Loss SS33:  0.03011147310236146\n",
            "Loss SS44:  0.039445145759120975\n",
            "Loss SS55:  0.041515296789642754\n",
            "Loss SS66:  0.033730129748936785\n",
            "Loss SS77:  0.0420497296081586\n",
            "Loss SS88:  0.037889250574053894\n",
            "Loss SS11:  0.0351302006871822\n",
            "Loss SS22:  0.03378905278572212\n",
            "Loss SS33:  0.030122107975271352\n",
            "Loss SS44:  0.03944271355175916\n",
            "Loss SS55:  0.04149991744079059\n",
            "Loss SS66:  0.03373532499383331\n",
            "Loss SS77:  0.0420557683000985\n",
            "Loss SS88:  0.03787174517950021\n",
            "Loss SS11:  0.0351184862760865\n",
            "Loss SS22:  0.03379283019694206\n",
            "Loss SS33:  0.03012553243443809\n",
            "Loss SS44:  0.03943675301985946\n",
            "Loss SS55:  0.0414882825683304\n",
            "Loss SS66:  0.033725922441536604\n",
            "Loss SS77:  0.042054362451476035\n",
            "Loss SS88:  0.03786888990187996\n",
            "Loss SS11:  0.03513986652019019\n",
            "Loss SS22:  0.033803516519803165\n",
            "Loss SS33:  0.030131133760587868\n",
            "Loss SS44:  0.03943767733128531\n",
            "Loss SS55:  0.04149511634081007\n",
            "Loss SS66:  0.0337298342136216\n",
            "Loss SS77:  0.04203106231673594\n",
            "Loss SS88:  0.03787774405547494\n",
            "Loss SS11:  0.03512624572808985\n",
            "Loss SS22:  0.033784230872019\n",
            "Loss SS33:  0.030126354053067024\n",
            "Loss SS44:  0.03939462055547118\n",
            "Loss SS55:  0.04149766425539511\n",
            "Loss SS66:  0.033737955816946684\n",
            "Loss SS77:  0.04204911132567358\n",
            "Loss SS88:  0.037859966800651165\n",
            "Loss SS11:  0.03510800440599964\n",
            "Loss SS22:  0.033785173815779876\n",
            "Loss SS33:  0.030118347143639677\n",
            "Loss SS44:  0.039400613682854704\n",
            "Loss SS55:  0.04148534000426088\n",
            "Loss SS66:  0.033739304818576815\n",
            "Loss SS77:  0.0420692914468088\n",
            "Loss SS88:  0.03784147918524889\n",
            "Loss SS11:  0.03508221015349619\n",
            "Loss SS22:  0.03378197843917566\n",
            "Loss SS33:  0.030117623333716592\n",
            "Loss SS44:  0.039392894060349015\n",
            "Loss SS55:  0.04146973337211381\n",
            "Loss SS66:  0.033760544288276136\n",
            "Loss SS77:  0.04208237624372861\n",
            "Loss SS88:  0.03782507882931748\n",
            "Loss SS11:  0.035081267174780974\n",
            "Loss SS22:  0.033740099504316896\n",
            "Loss SS33:  0.030094592462155338\n",
            "Loss SS44:  0.03935940067479178\n",
            "Loss SS55:  0.041432328931911414\n",
            "Loss SS66:  0.03372556456723184\n",
            "Loss SS77:  0.04206327363363108\n",
            "Loss SS88:  0.03777949354571505\n",
            "Validation: \n",
            " Loss SS11:  0.03160441666841507\n",
            " Loss SS22:  0.043229009956121445\n",
            " Loss SS33:  0.039195552468299866\n",
            " Loss SS44:  0.052116069942712784\n",
            " Loss SS55:  0.05018807202577591\n",
            " Loss SS66:  0.041481759399175644\n",
            " Loss SS77:  0.05828822776675224\n",
            " Loss SS88:  0.03935638442635536\n",
            " Loss SS11:  0.035361492385466896\n",
            " Loss SS22:  0.04942494346981957\n",
            " Loss SS33:  0.045645645509163536\n",
            " Loss SS44:  0.059756760618516376\n",
            " Loss SS55:  0.062034119630143755\n",
            " Loss SS66:  0.05026543158150855\n",
            " Loss SS77:  0.06523850134440831\n",
            " Loss SS88:  0.053851885632390066\n",
            " Loss SS11:  0.03538464564012318\n",
            " Loss SS22:  0.04891458717061252\n",
            " Loss SS33:  0.04547924475698936\n",
            " Loss SS44:  0.059259052047642265\n",
            " Loss SS55:  0.061794140171713946\n",
            " Loss SS66:  0.0500851534307003\n",
            " Loss SS77:  0.065112515920546\n",
            " Loss SS88:  0.054141037075257886\n",
            " Loss SS11:  0.035150181410498305\n",
            " Loss SS22:  0.048615230766476177\n",
            " Loss SS33:  0.04494240965510978\n",
            " Loss SS44:  0.05868210216037563\n",
            " Loss SS55:  0.0610109843680116\n",
            " Loss SS66:  0.04991944641119144\n",
            " Loss SS77:  0.06454025935687002\n",
            " Loss SS88:  0.053576311493506196\n",
            " Loss SS11:  0.03513255094488462\n",
            " Loss SS22:  0.04849940032502751\n",
            " Loss SS33:  0.04499345930454172\n",
            " Loss SS44:  0.05869815843524756\n",
            " Loss SS55:  0.060761339916491214\n",
            " Loss SS66:  0.04974164229668217\n",
            " Loss SS77:  0.0642706402749927\n",
            " Loss SS88:  0.05357100317875544\n",
            "\n",
            "Epoch: 50\n",
            "Loss SS11:  0.03811764717102051\n",
            "Loss SS22:  0.0343073345720768\n",
            "Loss SS33:  0.03291599825024605\n",
            "Loss SS44:  0.04252532869577408\n",
            "Loss SS55:  0.04174168035387993\n",
            "Loss SS66:  0.03724786639213562\n",
            "Loss SS77:  0.04740063101053238\n",
            "Loss SS88:  0.04203953966498375\n",
            "Loss SS11:  0.03522411310537295\n",
            "Loss SS22:  0.03300340507518162\n",
            "Loss SS33:  0.02948742698539387\n",
            "Loss SS44:  0.03892807594754479\n",
            "Loss SS55:  0.04160055721347982\n",
            "Loss SS66:  0.032397875054316086\n",
            "Loss SS77:  0.041355027732524\n",
            "Loss SS88:  0.03790166906335137\n",
            "Loss SS11:  0.035270863256993745\n",
            "Loss SS22:  0.033526668058974404\n",
            "Loss SS33:  0.02952014087211518\n",
            "Loss SS44:  0.039086240920282546\n",
            "Loss SS55:  0.04169007869703429\n",
            "Loss SS66:  0.03316486786518778\n",
            "Loss SS77:  0.041660720393771215\n",
            "Loss SS88:  0.03820732200429553\n",
            "Loss SS11:  0.034911261030262514\n",
            "Loss SS22:  0.033545170580187154\n",
            "Loss SS33:  0.029498744515642043\n",
            "Loss SS44:  0.03918777706642305\n",
            "Loss SS55:  0.041290494943818735\n",
            "Loss SS66:  0.03314249451843\n",
            "Loss SS77:  0.04148243523893818\n",
            "Loss SS88:  0.038022595427690015\n",
            "Loss SS11:  0.03462311421043989\n",
            "Loss SS22:  0.033708027768425824\n",
            "Loss SS33:  0.029651170732771476\n",
            "Loss SS44:  0.03941585614186961\n",
            "Loss SS55:  0.04128336306752228\n",
            "Loss SS66:  0.033192516690710695\n",
            "Loss SS77:  0.041708869995867336\n",
            "Loss SS88:  0.037873189623762925\n",
            "Loss SS11:  0.034594006325099985\n",
            "Loss SS22:  0.03370077860559903\n",
            "Loss SS33:  0.029913728289744434\n",
            "Loss SS44:  0.03967590337874843\n",
            "Loss SS55:  0.04130782866302658\n",
            "Loss SS66:  0.033225104687552826\n",
            "Loss SS77:  0.04178101328365943\n",
            "Loss SS88:  0.037836301925719955\n",
            "Loss SS11:  0.034636033790521936\n",
            "Loss SS22:  0.033678403246353884\n",
            "Loss SS33:  0.0297865452641835\n",
            "Loss SS44:  0.03956118378727162\n",
            "Loss SS55:  0.04111107414374586\n",
            "Loss SS66:  0.03305633992078851\n",
            "Loss SS77:  0.041615427395359415\n",
            "Loss SS88:  0.0376665430967925\n",
            "Loss SS11:  0.034485122425035694\n",
            "Loss SS22:  0.033694630803566585\n",
            "Loss SS33:  0.029892591943203563\n",
            "Loss SS44:  0.03959518167334543\n",
            "Loss SS55:  0.04125911327944675\n",
            "Loss SS66:  0.033045634627342224\n",
            "Loss SS77:  0.041769816999284316\n",
            "Loss SS88:  0.037775054574012756\n",
            "Loss SS11:  0.03450074497564339\n",
            "Loss SS22:  0.03347342279682189\n",
            "Loss SS33:  0.029821941973986448\n",
            "Loss SS44:  0.03933940239158677\n",
            "Loss SS55:  0.041135155538349975\n",
            "Loss SS66:  0.03296876259516051\n",
            "Loss SS77:  0.041630538571395995\n",
            "Loss SS88:  0.03763933631557005\n",
            "Loss SS11:  0.03440961378861915\n",
            "Loss SS22:  0.03334576945629094\n",
            "Loss SS33:  0.029711771510787063\n",
            "Loss SS44:  0.03933170744842225\n",
            "Loss SS55:  0.04101229999419097\n",
            "Loss SS66:  0.03288929637234945\n",
            "Loss SS77:  0.04151992602171479\n",
            "Loss SS88:  0.03756447607180574\n",
            "Loss SS11:  0.03450393470207063\n",
            "Loss SS22:  0.033257464518641484\n",
            "Loss SS33:  0.0296188880606453\n",
            "Loss SS44:  0.039223183610356684\n",
            "Loss SS55:  0.04085779665867881\n",
            "Loss SS66:  0.032824063839593735\n",
            "Loss SS77:  0.041407389990469015\n",
            "Loss SS88:  0.03749626026590272\n",
            "Loss SS11:  0.034611500128432435\n",
            "Loss SS22:  0.03324457147234195\n",
            "Loss SS33:  0.02965764348974099\n",
            "Loss SS44:  0.03912718234969689\n",
            "Loss SS55:  0.04079331877129572\n",
            "Loss SS66:  0.032799040697313646\n",
            "Loss SS77:  0.041311908278379356\n",
            "Loss SS88:  0.03743520097152607\n",
            "Loss SS11:  0.03459094914276738\n",
            "Loss SS22:  0.033296538332153944\n",
            "Loss SS33:  0.029685443190257413\n",
            "Loss SS44:  0.039128932824804764\n",
            "Loss SS55:  0.04075337093600557\n",
            "Loss SS66:  0.03287917335540795\n",
            "Loss SS77:  0.041433845392682335\n",
            "Loss SS88:  0.037454669109799645\n",
            "Loss SS11:  0.034742517354151674\n",
            "Loss SS22:  0.033407329446600596\n",
            "Loss SS33:  0.029736141411175254\n",
            "Loss SS44:  0.03919385120500135\n",
            "Loss SS55:  0.04087887090126067\n",
            "Loss SS66:  0.03302233346261596\n",
            "Loss SS77:  0.04145139691825132\n",
            "Loss SS88:  0.03751421443947399\n",
            "Loss SS11:  0.03474218544966363\n",
            "Loss SS22:  0.0333985760807991\n",
            "Loss SS33:  0.029759735106787782\n",
            "Loss SS44:  0.0392694280396962\n",
            "Loss SS55:  0.040897467933225295\n",
            "Loss SS66:  0.033011559766869175\n",
            "Loss SS77:  0.041549118803748\n",
            "Loss SS88:  0.03750526725400424\n",
            "Loss SS11:  0.03467737241888678\n",
            "Loss SS22:  0.03348906450466999\n",
            "Loss SS33:  0.029810625282640488\n",
            "Loss SS44:  0.039257238316812264\n",
            "Loss SS55:  0.04094471483929268\n",
            "Loss SS66:  0.033023164404050405\n",
            "Loss SS77:  0.041574337254494234\n",
            "Loss SS88:  0.037558547659030814\n",
            "Loss SS11:  0.03464246516966302\n",
            "Loss SS22:  0.03343768700124314\n",
            "Loss SS33:  0.029777207498594842\n",
            "Loss SS44:  0.039117701467334856\n",
            "Loss SS55:  0.04088922712651099\n",
            "Loss SS66:  0.03302306375286964\n",
            "Loss SS77:  0.04154067847484387\n",
            "Loss SS88:  0.03755823684775311\n",
            "Loss SS11:  0.03464517081210836\n",
            "Loss SS22:  0.033530147953165904\n",
            "Loss SS33:  0.02983460697339989\n",
            "Loss SS44:  0.03916202165317117\n",
            "Loss SS55:  0.04093311912222215\n",
            "Loss SS66:  0.033106427955610016\n",
            "Loss SS77:  0.041636708971352604\n",
            "Loss SS88:  0.037638996188577856\n",
            "Loss SS11:  0.03469346324753696\n",
            "Loss SS22:  0.03353713078228808\n",
            "Loss SS33:  0.029824823316042595\n",
            "Loss SS44:  0.039185739580274284\n",
            "Loss SS55:  0.04102494176580102\n",
            "Loss SS66:  0.0331561756912022\n",
            "Loss SS77:  0.04179115060366978\n",
            "Loss SS88:  0.03774463537648238\n",
            "Loss SS11:  0.03472470892934587\n",
            "Loss SS22:  0.03358430142333994\n",
            "Loss SS33:  0.029768213064102603\n",
            "Loss SS44:  0.039117642078568174\n",
            "Loss SS55:  0.04100709761546544\n",
            "Loss SS66:  0.03314536381805443\n",
            "Loss SS77:  0.041772491449773\n",
            "Loss SS88:  0.03773607502823101\n",
            "Loss SS11:  0.03481126466386057\n",
            "Loss SS22:  0.03357391146507429\n",
            "Loss SS33:  0.029798061739820154\n",
            "Loss SS44:  0.039102428013560786\n",
            "Loss SS55:  0.041007906402373194\n",
            "Loss SS66:  0.033158338021727934\n",
            "Loss SS77:  0.04176590385944096\n",
            "Loss SS88:  0.037734438234300754\n",
            "Loss SS11:  0.03485212452109391\n",
            "Loss SS22:  0.03359380880838604\n",
            "Loss SS33:  0.029859835776319437\n",
            "Loss SS44:  0.039101768443934724\n",
            "Loss SS55:  0.04100018464276011\n",
            "Loss SS66:  0.0331912450404105\n",
            "Loss SS77:  0.04182894450228361\n",
            "Loss SS88:  0.03776810258203208\n",
            "Loss SS11:  0.03479851482275924\n",
            "Loss SS22:  0.033557771557596476\n",
            "Loss SS33:  0.029836291421997063\n",
            "Loss SS44:  0.039068532290097276\n",
            "Loss SS55:  0.04097057980110203\n",
            "Loss SS66:  0.03316528461367836\n",
            "Loss SS77:  0.041839696450066244\n",
            "Loss SS88:  0.037773017008784666\n",
            "Loss SS11:  0.03479195720802396\n",
            "Loss SS22:  0.033515394895107714\n",
            "Loss SS33:  0.02985366949916402\n",
            "Loss SS44:  0.03903277824709426\n",
            "Loss SS55:  0.0409193148118851\n",
            "Loss SS66:  0.033155848582585655\n",
            "Loss SS77:  0.041829936932046695\n",
            "Loss SS88:  0.03782080505768974\n",
            "Loss SS11:  0.034773157538529255\n",
            "Loss SS22:  0.03357741754517515\n",
            "Loss SS33:  0.02988504649321568\n",
            "Loss SS44:  0.03903121978848307\n",
            "Loss SS55:  0.04095780572765101\n",
            "Loss SS66:  0.033195851919438336\n",
            "Loss SS77:  0.04188355752479486\n",
            "Loss SS88:  0.03784628710187817\n",
            "Loss SS11:  0.034813887918316036\n",
            "Loss SS22:  0.033567794862080856\n",
            "Loss SS33:  0.029881699611822448\n",
            "Loss SS44:  0.03904462981212187\n",
            "Loss SS55:  0.040943601736390736\n",
            "Loss SS66:  0.03323769248814222\n",
            "Loss SS77:  0.04192512420069174\n",
            "Loss SS88:  0.037860706834441635\n",
            "Loss SS11:  0.034768449884778695\n",
            "Loss SS22:  0.033563652457633694\n",
            "Loss SS33:  0.02987703511141726\n",
            "Loss SS44:  0.03900282507426894\n",
            "Loss SS55:  0.040934750389207825\n",
            "Loss SS66:  0.03324294302227168\n",
            "Loss SS77:  0.04198435538402005\n",
            "Loss SS88:  0.03785714768301481\n",
            "Loss SS11:  0.03472574298507173\n",
            "Loss SS22:  0.03347526033618573\n",
            "Loss SS33:  0.029847076760666835\n",
            "Loss SS44:  0.03897013823154668\n",
            "Loss SS55:  0.040905316446210184\n",
            "Loss SS66:  0.033193279978753897\n",
            "Loss SS77:  0.041972317090223636\n",
            "Loss SS88:  0.03778974750385073\n",
            "Loss SS11:  0.03472376508088086\n",
            "Loss SS22:  0.03347195468816469\n",
            "Loss SS33:  0.029854293090698984\n",
            "Loss SS44:  0.038933950188002976\n",
            "Loss SS55:  0.04092357338164201\n",
            "Loss SS66:  0.033201449732975605\n",
            "Loss SS77:  0.041929400834218464\n",
            "Loss SS88:  0.03775046982560506\n",
            "Loss SS11:  0.03474441310870893\n",
            "Loss SS22:  0.03349307132084755\n",
            "Loss SS33:  0.029837663621478473\n",
            "Loss SS44:  0.03895762954483327\n",
            "Loss SS55:  0.040921534805260985\n",
            "Loss SS66:  0.03325442519179734\n",
            "Loss SS77:  0.04195907289527126\n",
            "Loss SS88:  0.037750396077421935\n",
            "Loss SS11:  0.03475879870998701\n",
            "Loss SS22:  0.033527562629889415\n",
            "Loss SS33:  0.029856750939623065\n",
            "Loss SS44:  0.03898745415070128\n",
            "Loss SS55:  0.040918412349152805\n",
            "Loss SS66:  0.03327461164382803\n",
            "Loss SS77:  0.041958012763546944\n",
            "Loss SS88:  0.03774056415943014\n",
            "Loss SS11:  0.03475388935452681\n",
            "Loss SS22:  0.0335121896549149\n",
            "Loss SS33:  0.02984091591006115\n",
            "Loss SS44:  0.03894759411549262\n",
            "Loss SS55:  0.04090307353489652\n",
            "Loss SS66:  0.03325551841108554\n",
            "Loss SS77:  0.04189464193544204\n",
            "Loss SS88:  0.03769526725868512\n",
            "Loss SS11:  0.0347498546403795\n",
            "Loss SS22:  0.03351784499240256\n",
            "Loss SS33:  0.029834396887651857\n",
            "Loss SS44:  0.038943916164640324\n",
            "Loss SS55:  0.040889468705542734\n",
            "Loss SS66:  0.03325215997493527\n",
            "Loss SS77:  0.04188400864415451\n",
            "Loss SS88:  0.03770449020061352\n",
            "Loss SS11:  0.03475359142434381\n",
            "Loss SS22:  0.033496094878841746\n",
            "Loss SS33:  0.02981183680725422\n",
            "Loss SS44:  0.038967253139462\n",
            "Loss SS55:  0.040893574315615654\n",
            "Loss SS66:  0.033250190300748786\n",
            "Loss SS77:  0.04189574537275421\n",
            "Loss SS88:  0.03770339576206719\n",
            "Loss SS11:  0.03479669628735861\n",
            "Loss SS22:  0.033519255663773534\n",
            "Loss SS33:  0.029821015370031956\n",
            "Loss SS44:  0.03899365757480045\n",
            "Loss SS55:  0.040912857262794575\n",
            "Loss SS66:  0.03328118883510314\n",
            "Loss SS77:  0.04194808892272085\n",
            "Loss SS88:  0.037737255107578645\n",
            "Loss SS11:  0.03485903947314306\n",
            "Loss SS22:  0.03351726523597865\n",
            "Loss SS33:  0.029857661145237777\n",
            "Loss SS44:  0.03900471654457924\n",
            "Loss SS55:  0.04093165004737357\n",
            "Loss SS66:  0.033333413138391284\n",
            "Loss SS77:  0.041964712571299655\n",
            "Loss SS88:  0.03772380121816427\n",
            "Loss SS11:  0.03488585252799816\n",
            "Loss SS22:  0.03350367641638851\n",
            "Loss SS33:  0.029878334841404595\n",
            "Loss SS44:  0.039032841240600205\n",
            "Loss SS55:  0.04094366622970045\n",
            "Loss SS66:  0.03334750687584818\n",
            "Loss SS77:  0.04197338672349658\n",
            "Loss SS88:  0.03772499180628487\n",
            "Loss SS11:  0.03488638131525317\n",
            "Loss SS22:  0.033469792816918496\n",
            "Loss SS33:  0.029864085160416734\n",
            "Loss SS44:  0.03900212253320892\n",
            "Loss SS55:  0.040939787237069036\n",
            "Loss SS66:  0.03333800050588149\n",
            "Loss SS77:  0.041960987189629005\n",
            "Loss SS88:  0.037712376228360475\n",
            "Loss SS11:  0.03487991335804225\n",
            "Loss SS22:  0.033450028631748174\n",
            "Loss SS33:  0.02986252365091185\n",
            "Loss SS44:  0.03898343223873086\n",
            "Loss SS55:  0.04095110912218181\n",
            "Loss SS66:  0.03333380276684373\n",
            "Loss SS77:  0.04192443527379061\n",
            "Loss SS88:  0.03770098917851141\n",
            "Loss SS11:  0.0348592458526268\n",
            "Loss SS22:  0.033431171561064926\n",
            "Loss SS33:  0.02985220911252834\n",
            "Loss SS44:  0.03895408185699102\n",
            "Loss SS55:  0.040941558797341175\n",
            "Loss SS66:  0.03330705956558285\n",
            "Loss SS77:  0.04189543705195417\n",
            "Loss SS88:  0.03766470609704399\n",
            "Loss SS11:  0.03486482141321139\n",
            "Loss SS22:  0.033437140481058794\n",
            "Loss SS33:  0.029870453399798817\n",
            "Loss SS44:  0.03899322644321996\n",
            "Loss SS55:  0.04099101454316827\n",
            "Loss SS66:  0.033346580452938326\n",
            "Loss SS77:  0.04189415139487557\n",
            "Loss SS88:  0.03768707676662917\n",
            "Loss SS11:  0.03485580967000511\n",
            "Loss SS22:  0.033425803230554225\n",
            "Loss SS33:  0.029864308843502455\n",
            "Loss SS44:  0.03900079492597394\n",
            "Loss SS55:  0.04103127915022437\n",
            "Loss SS66:  0.033324332998435575\n",
            "Loss SS77:  0.04188766951362292\n",
            "Loss SS88:  0.03767253331151177\n",
            "Loss SS11:  0.034895815919739614\n",
            "Loss SS22:  0.03344745549092384\n",
            "Loss SS33:  0.02987431828153813\n",
            "Loss SS44:  0.03900174119657972\n",
            "Loss SS55:  0.04104089906832951\n",
            "Loss SS66:  0.03332891873130464\n",
            "Loss SS77:  0.04188533065367198\n",
            "Loss SS88:  0.037673616436732636\n",
            "Loss SS11:  0.034899319545852056\n",
            "Loss SS22:  0.033422713645511325\n",
            "Loss SS33:  0.029865005306398233\n",
            "Loss SS44:  0.03898203220832099\n",
            "Loss SS55:  0.04103887483062711\n",
            "Loss SS66:  0.03331924094591644\n",
            "Loss SS77:  0.041878112351949694\n",
            "Loss SS88:  0.037658647588711326\n",
            "Loss SS11:  0.034920104688493846\n",
            "Loss SS22:  0.033412068533181065\n",
            "Loss SS33:  0.029857723202868926\n",
            "Loss SS44:  0.03899756519128136\n",
            "Loss SS55:  0.04103092030704427\n",
            "Loss SS66:  0.033303784827391304\n",
            "Loss SS77:  0.04186851238143417\n",
            "Loss SS88:  0.03766820759273846\n",
            "Loss SS11:  0.0349633462867491\n",
            "Loss SS22:  0.03343814755160106\n",
            "Loss SS33:  0.029878481103317964\n",
            "Loss SS44:  0.03900350609633188\n",
            "Loss SS55:  0.04102533536151613\n",
            "Loss SS66:  0.03330330723306028\n",
            "Loss SS77:  0.041860859006841535\n",
            "Loss SS88:  0.03769179260535806\n",
            "Loss SS11:  0.034976602222875505\n",
            "Loss SS22:  0.03342115947746403\n",
            "Loss SS33:  0.029887434846667043\n",
            "Loss SS44:  0.03899759518835136\n",
            "Loss SS55:  0.041020233095209406\n",
            "Loss SS66:  0.03331256908346412\n",
            "Loss SS77:  0.041862520342793745\n",
            "Loss SS88:  0.03769051380164582\n",
            "Loss SS11:  0.03496746527082475\n",
            "Loss SS22:  0.033415005038118666\n",
            "Loss SS33:  0.029888360237113484\n",
            "Loss SS44:  0.0390019996166862\n",
            "Loss SS55:  0.04102207034541543\n",
            "Loss SS66:  0.03331661625763287\n",
            "Loss SS77:  0.041881635650644386\n",
            "Loss SS88:  0.03768229091860306\n",
            "Loss SS11:  0.03495286861260319\n",
            "Loss SS22:  0.033409328651062664\n",
            "Loss SS33:  0.02988702689225857\n",
            "Loss SS44:  0.03900713715198878\n",
            "Loss SS55:  0.04104466682876966\n",
            "Loss SS66:  0.0333300697348098\n",
            "Loss SS77:  0.041887157331385386\n",
            "Loss SS88:  0.03768803690813807\n",
            "Loss SS11:  0.034929345003395604\n",
            "Loss SS22:  0.033383480616505666\n",
            "Loss SS33:  0.029863759507320807\n",
            "Loss SS44:  0.038975320301206436\n",
            "Loss SS55:  0.04099394602591054\n",
            "Loss SS66:  0.033292530779774225\n",
            "Loss SS77:  0.041858800719141234\n",
            "Loss SS88:  0.03766114270417001\n",
            "Validation: \n",
            " Loss SS11:  0.031947314739227295\n",
            " Loss SS22:  0.04016350954771042\n",
            " Loss SS33:  0.038258615881204605\n",
            " Loss SS44:  0.05053482577204704\n",
            " Loss SS55:  0.05032612383365631\n",
            " Loss SS66:  0.04099859297275543\n",
            " Loss SS77:  0.058277152478694916\n",
            " Loss SS88:  0.03998688980937004\n",
            " Loss SS11:  0.034502332764012475\n",
            " Loss SS22:  0.047327338762226556\n",
            " Loss SS33:  0.043990629592112133\n",
            " Loss SS44:  0.05820587791857265\n",
            " Loss SS55:  0.0619934071742353\n",
            " Loss SS66:  0.049592504189127966\n",
            " Loss SS77:  0.06383264313141505\n",
            " Loss SS88:  0.05367455542797134\n",
            " Loss SS11:  0.034592204068492095\n",
            " Loss SS22:  0.047171672306409694\n",
            " Loss SS33:  0.043958573715715876\n",
            " Loss SS44:  0.057982020170950305\n",
            " Loss SS55:  0.061654740989935106\n",
            " Loss SS66:  0.04943472792099162\n",
            " Loss SS77:  0.06372262473876883\n",
            " Loss SS88:  0.053880867616432467\n",
            " Loss SS11:  0.034343739024928356\n",
            " Loss SS22:  0.04683292278500854\n",
            " Loss SS33:  0.04340684896365541\n",
            " Loss SS44:  0.05759458335452392\n",
            " Loss SS55:  0.060823214164034266\n",
            " Loss SS66:  0.04927380016592682\n",
            " Loss SS77:  0.06316533996189228\n",
            " Loss SS88:  0.053267369077342454\n",
            " Loss SS11:  0.034361223264792816\n",
            " Loss SS22:  0.046696001640808435\n",
            " Loss SS33:  0.04353125439381894\n",
            " Loss SS44:  0.057666135790907305\n",
            " Loss SS55:  0.06062741821378837\n",
            " Loss SS66:  0.04910852093203568\n",
            " Loss SS77:  0.06293885704175925\n",
            " Loss SS88:  0.05329270752859704\n",
            "\n",
            "Epoch: 51\n",
            "Loss SS11:  0.03659970313310623\n",
            "Loss SS22:  0.03472156077623367\n",
            "Loss SS33:  0.03312492370605469\n",
            "Loss SS44:  0.04332812875509262\n",
            "Loss SS55:  0.04135710746049881\n",
            "Loss SS66:  0.03427407518029213\n",
            "Loss SS77:  0.0441875196993351\n",
            "Loss SS88:  0.03768943250179291\n",
            "Loss SS11:  0.035585805773735046\n",
            "Loss SS22:  0.03330832584337755\n",
            "Loss SS33:  0.029834923588416794\n",
            "Loss SS44:  0.03860363939946348\n",
            "Loss SS55:  0.04123172130097042\n",
            "Loss SS66:  0.033102931793440475\n",
            "Loss SS77:  0.04140986840833317\n",
            "Loss SS88:  0.03754294866865331\n",
            "Loss SS11:  0.03516801269281478\n",
            "Loss SS22:  0.033423633891202155\n",
            "Loss SS33:  0.029808217216105687\n",
            "Loss SS44:  0.03870265895412082\n",
            "Loss SS55:  0.040915856581358684\n",
            "Loss SS66:  0.033049394332227255\n",
            "Loss SS77:  0.041513488051437196\n",
            "Loss SS88:  0.037425377539225986\n",
            "Loss SS11:  0.034895308435924595\n",
            "Loss SS22:  0.03303899910421141\n",
            "Loss SS33:  0.02992292908170531\n",
            "Loss SS44:  0.03878073442366815\n",
            "Loss SS55:  0.04046433887654735\n",
            "Loss SS66:  0.03269110169381865\n",
            "Loss SS77:  0.04109767283643446\n",
            "Loss SS88:  0.03710655868053436\n",
            "Loss SS11:  0.03506486045151222\n",
            "Loss SS22:  0.033177436352139565\n",
            "Loss SS33:  0.02968481273912802\n",
            "Loss SS44:  0.03870502523169285\n",
            "Loss SS55:  0.04059524316249824\n",
            "Loss SS66:  0.03293923361272347\n",
            "Loss SS77:  0.041284699628992776\n",
            "Loss SS88:  0.037038748402421065\n",
            "Loss SS11:  0.035086924867594946\n",
            "Loss SS22:  0.03337840031028962\n",
            "Loss SS33:  0.029827405205544305\n",
            "Loss SS44:  0.038895700419066\n",
            "Loss SS55:  0.0406836946951408\n",
            "Loss SS66:  0.03317700432357835\n",
            "Loss SS77:  0.04146286545723092\n",
            "Loss SS88:  0.03723103567665698\n",
            "Loss SS11:  0.03506796865067521\n",
            "Loss SS22:  0.03337870580983943\n",
            "Loss SS33:  0.02966938210559673\n",
            "Loss SS44:  0.0386967552734203\n",
            "Loss SS55:  0.04044632748013637\n",
            "Loss SS66:  0.033085652336966795\n",
            "Loss SS77:  0.0414049076374437\n",
            "Loss SS88:  0.037165495704432006\n",
            "Loss SS11:  0.035066554427776536\n",
            "Loss SS22:  0.0333558675688757\n",
            "Loss SS33:  0.02988141460317961\n",
            "Loss SS44:  0.038959947480282316\n",
            "Loss SS55:  0.040689103147933184\n",
            "Loss SS66:  0.0331387524033936\n",
            "Loss SS77:  0.041637477950311044\n",
            "Loss SS88:  0.03727142402613667\n",
            "Loss SS11:  0.0349110824826323\n",
            "Loss SS22:  0.03323361178699099\n",
            "Loss SS33:  0.02987311729862366\n",
            "Loss SS44:  0.03891399291194515\n",
            "Loss SS55:  0.04067925520149278\n",
            "Loss SS66:  0.0330997834465018\n",
            "Loss SS77:  0.04150660368211476\n",
            "Loss SS88:  0.03723797551643701\n",
            "Loss SS11:  0.03481024925361623\n",
            "Loss SS22:  0.03310236107598949\n",
            "Loss SS33:  0.02979482683752264\n",
            "Loss SS44:  0.03885261493397283\n",
            "Loss SS55:  0.04057917727546378\n",
            "Loss SS66:  0.032918732858948654\n",
            "Loss SS77:  0.041434983671694015\n",
            "Loss SS88:  0.037050380811586486\n",
            "Loss SS11:  0.034608569457241806\n",
            "Loss SS22:  0.03297216917323594\n",
            "Loss SS33:  0.0297397713702504\n",
            "Loss SS44:  0.03863227843205527\n",
            "Loss SS55:  0.04049593365133399\n",
            "Loss SS66:  0.03274432632445109\n",
            "Loss SS77:  0.04117271097579805\n",
            "Loss SS88:  0.036955505701722485\n",
            "Loss SS11:  0.03456636873988418\n",
            "Loss SS22:  0.032886825720066425\n",
            "Loss SS33:  0.029679204177883296\n",
            "Loss SS44:  0.038499932098496066\n",
            "Loss SS55:  0.04048953557739387\n",
            "Loss SS66:  0.03268551059604228\n",
            "Loss SS77:  0.041005816277082975\n",
            "Loss SS88:  0.03693971693985634\n",
            "Loss SS11:  0.03452722321857105\n",
            "Loss SS22:  0.03289123566066923\n",
            "Loss SS33:  0.029639955671611897\n",
            "Loss SS44:  0.038520585068247536\n",
            "Loss SS55:  0.04050492193580659\n",
            "Loss SS66:  0.03273099414573228\n",
            "Loss SS77:  0.041082274722905196\n",
            "Loss SS88:  0.036873151463541115\n",
            "Loss SS11:  0.034564392015565444\n",
            "Loss SS22:  0.032927065376789515\n",
            "Loss SS33:  0.029695826632375934\n",
            "Loss SS44:  0.03858539439339674\n",
            "Loss SS55:  0.04058462864797534\n",
            "Loss SS66:  0.03285856647345856\n",
            "Loss SS77:  0.04109931014876329\n",
            "Loss SS88:  0.036932866067718007\n",
            "Loss SS11:  0.03458233501313003\n",
            "Loss SS22:  0.032949198737529155\n",
            "Loss SS33:  0.029700267359825738\n",
            "Loss SS44:  0.03863508346761372\n",
            "Loss SS55:  0.040640226862532026\n",
            "Loss SS66:  0.032918281681465766\n",
            "Loss SS77:  0.04122716502834719\n",
            "Loss SS88:  0.03706303285104586\n",
            "Loss SS11:  0.03465993096743593\n",
            "Loss SS22:  0.03311453434015742\n",
            "Loss SS33:  0.029728421312294258\n",
            "Loss SS44:  0.03868874737265094\n",
            "Loss SS55:  0.040712197095353085\n",
            "Loss SS66:  0.033014398438251574\n",
            "Loss SS77:  0.0412908409851671\n",
            "Loss SS88:  0.037157197526057825\n",
            "Loss SS11:  0.03457161021760161\n",
            "Loss SS22:  0.033068190554376715\n",
            "Loss SS33:  0.029675758264449813\n",
            "Loss SS44:  0.03865308890709226\n",
            "Loss SS55:  0.04064824598731462\n",
            "Loss SS66:  0.03302038695825183\n",
            "Loss SS77:  0.04130044753044288\n",
            "Loss SS88:  0.03719351208126693\n",
            "Loss SS11:  0.03456106141471026\n",
            "Loss SS22:  0.03315227489635261\n",
            "Loss SS33:  0.029715668562560055\n",
            "Loss SS44:  0.03869141701456399\n",
            "Loss SS55:  0.040705069202428674\n",
            "Loss SS66:  0.033070009953358716\n",
            "Loss SS77:  0.04138951286760687\n",
            "Loss SS88:  0.03725779730804831\n",
            "Loss SS11:  0.034601507543776576\n",
            "Loss SS22:  0.033235719931718394\n",
            "Loss SS33:  0.029718305329015243\n",
            "Loss SS44:  0.038759028364116974\n",
            "Loss SS55:  0.04079944996461684\n",
            "Loss SS66:  0.03311966948252357\n",
            "Loss SS77:  0.04149250918742043\n",
            "Loss SS88:  0.03737378522794879\n",
            "Loss SS11:  0.03465498083744062\n",
            "Loss SS22:  0.033318200777650506\n",
            "Loss SS33:  0.029693438715413602\n",
            "Loss SS44:  0.03871440762624691\n",
            "Loss SS55:  0.04075195669581753\n",
            "Loss SS66:  0.033112326987984914\n",
            "Loss SS77:  0.04150466788657673\n",
            "Loss SS88:  0.03731292922108273\n",
            "Loss SS11:  0.03475479847422583\n",
            "Loss SS22:  0.03329244338494925\n",
            "Loss SS33:  0.02968775428164361\n",
            "Loss SS44:  0.038659622994673196\n",
            "Loss SS55:  0.04066784896734935\n",
            "Loss SS66:  0.033108035352692675\n",
            "Loss SS77:  0.04153162756221211\n",
            "Loss SS88:  0.03732274349463816\n",
            "Loss SS11:  0.03480441962761619\n",
            "Loss SS22:  0.03336590938093538\n",
            "Loss SS33:  0.029720523911064833\n",
            "Loss SS44:  0.03870048490463275\n",
            "Loss SS55:  0.040660568835187295\n",
            "Loss SS66:  0.033164913947053995\n",
            "Loss SS77:  0.041580394598968784\n",
            "Loss SS88:  0.03735912343160518\n",
            "Loss SS11:  0.03480930306961364\n",
            "Loss SS22:  0.033406195732263416\n",
            "Loss SS33:  0.02976248528791499\n",
            "Loss SS44:  0.03879039709317199\n",
            "Loss SS55:  0.04062998623532407\n",
            "Loss SS66:  0.03319211346329068\n",
            "Loss SS77:  0.04164501512212451\n",
            "Loss SS88:  0.037386197133234184\n",
            "Loss SS11:  0.03481415908348251\n",
            "Loss SS22:  0.03342905551098384\n",
            "Loss SS33:  0.029760681455592058\n",
            "Loss SS44:  0.03879983673170531\n",
            "Loss SS55:  0.04056054129551499\n",
            "Loss SS66:  0.03317024400739959\n",
            "Loss SS77:  0.04164065517388381\n",
            "Loss SS88:  0.037383266381951635\n",
            "Loss SS11:  0.03483215394078201\n",
            "Loss SS22:  0.033455252400077726\n",
            "Loss SS33:  0.02980022436139247\n",
            "Loss SS44:  0.03884228083973604\n",
            "Loss SS55:  0.04058284482520646\n",
            "Loss SS66:  0.0331718814617247\n",
            "Loss SS77:  0.0417131847118441\n",
            "Loss SS88:  0.03739026690450694\n",
            "Loss SS11:  0.03479399219659932\n",
            "Loss SS22:  0.03348172867885861\n",
            "Loss SS33:  0.029831958214837716\n",
            "Loss SS44:  0.0388367915770922\n",
            "Loss SS55:  0.0405864824366522\n",
            "Loss SS66:  0.033220957697506444\n",
            "Loss SS77:  0.0417495366942835\n",
            "Loss SS88:  0.037456071761203004\n",
            "Loss SS11:  0.034735811446076154\n",
            "Loss SS22:  0.03348301618691833\n",
            "Loss SS33:  0.029844890487776405\n",
            "Loss SS44:  0.03884840608214053\n",
            "Loss SS55:  0.040597079742799774\n",
            "Loss SS66:  0.03321775352275463\n",
            "Loss SS77:  0.04178154821083007\n",
            "Loss SS88:  0.03748029728073955\n",
            "Loss SS11:  0.0347021975214301\n",
            "Loss SS22:  0.0334351338047493\n",
            "Loss SS33:  0.029809933238253823\n",
            "Loss SS44:  0.03882469168118445\n",
            "Loss SS55:  0.040628907810278045\n",
            "Loss SS66:  0.03320244825496665\n",
            "Loss SS77:  0.04177153968283171\n",
            "Loss SS88:  0.037449016266698325\n",
            "Loss SS11:  0.03464049566868573\n",
            "Loss SS22:  0.03340968103860621\n",
            "Loss SS33:  0.029804863884143558\n",
            "Loss SS44:  0.0387962537807714\n",
            "Loss SS55:  0.04061501485119935\n",
            "Loss SS66:  0.033198992413505116\n",
            "Loss SS77:  0.04175956455785185\n",
            "Loss SS88:  0.03741515266752116\n",
            "Loss SS11:  0.034634331522887106\n",
            "Loss SS22:  0.03337312321405845\n",
            "Loss SS33:  0.029808744256899937\n",
            "Loss SS44:  0.038758611037712734\n",
            "Loss SS55:  0.04059571555185154\n",
            "Loss SS66:  0.03321686661658213\n",
            "Loss SS77:  0.04173402635451035\n",
            "Loss SS88:  0.03743229836507147\n",
            "Loss SS11:  0.03466514414454813\n",
            "Loss SS22:  0.03339358712500116\n",
            "Loss SS33:  0.02983762864085329\n",
            "Loss SS44:  0.038776855105131965\n",
            "Loss SS55:  0.04061783753657262\n",
            "Loss SS66:  0.033239993467877475\n",
            "Loss SS77:  0.041747897813882544\n",
            "Loss SS88:  0.037433939524910774\n",
            "Loss SS11:  0.034706177896673275\n",
            "Loss SS22:  0.03337596632971449\n",
            "Loss SS33:  0.02981694360709842\n",
            "Loss SS44:  0.038756511201093816\n",
            "Loss SS55:  0.04060141248718335\n",
            "Loss SS66:  0.03321600793426635\n",
            "Loss SS77:  0.04171677087089257\n",
            "Loss SS88:  0.037377564691701885\n",
            "Loss SS11:  0.034697132359180496\n",
            "Loss SS22:  0.033391797741022064\n",
            "Loss SS33:  0.029813956092126274\n",
            "Loss SS44:  0.03878648055142889\n",
            "Loss SS55:  0.04062062073673043\n",
            "Loss SS66:  0.033249707173010644\n",
            "Loss SS77:  0.041766044901353175\n",
            "Loss SS88:  0.0373668767790371\n",
            "Loss SS11:  0.034663183293854004\n",
            "Loss SS22:  0.033397089436740315\n",
            "Loss SS33:  0.02978769322896652\n",
            "Loss SS44:  0.03878298539702085\n",
            "Loss SS55:  0.04061503784894223\n",
            "Loss SS66:  0.03328148506334181\n",
            "Loss SS77:  0.041774957460489155\n",
            "Loss SS88:  0.03739093705516568\n",
            "Loss SS11:  0.03468436287863513\n",
            "Loss SS22:  0.03342531910411948\n",
            "Loss SS33:  0.029824848262227175\n",
            "Loss SS44:  0.038819350438776955\n",
            "Loss SS55:  0.040659689472829844\n",
            "Loss SS66:  0.03330727865990481\n",
            "Loss SS77:  0.04179999696587887\n",
            "Loss SS88:  0.03744686981581173\n",
            "Loss SS11:  0.034728119017137085\n",
            "Loss SS22:  0.033391840517138824\n",
            "Loss SS33:  0.029830505116245687\n",
            "Loss SS44:  0.03884704052828825\n",
            "Loss SS55:  0.04067968430682125\n",
            "Loss SS66:  0.033322486623145235\n",
            "Loss SS77:  0.04181083494484255\n",
            "Loss SS88:  0.03747243237843541\n",
            "Loss SS11:  0.03472851641434877\n",
            "Loss SS22:  0.033396770754555584\n",
            "Loss SS33:  0.029845223150672676\n",
            "Loss SS44:  0.03885234099325216\n",
            "Loss SS55:  0.04072411823759779\n",
            "Loss SS66:  0.033313538693407564\n",
            "Loss SS77:  0.041818795978527654\n",
            "Loss SS88:  0.03749891970089928\n",
            "Loss SS11:  0.034765769845512356\n",
            "Loss SS22:  0.03338297218024088\n",
            "Loss SS33:  0.029827832417547544\n",
            "Loss SS44:  0.03886257529238646\n",
            "Loss SS55:  0.04072373404576772\n",
            "Loss SS66:  0.03330226422279993\n",
            "Loss SS77:  0.04182688545465791\n",
            "Loss SS88:  0.03749809426197787\n",
            "Loss SS11:  0.034785858450794786\n",
            "Loss SS22:  0.03339042845536561\n",
            "Loss SS33:  0.02983327735498978\n",
            "Loss SS44:  0.0388461522846006\n",
            "Loss SS55:  0.04075163143278733\n",
            "Loss SS66:  0.03329088302265628\n",
            "Loss SS77:  0.04180280083981086\n",
            "Loss SS88:  0.037469539175352715\n",
            "Loss SS11:  0.03477629271271589\n",
            "Loss SS22:  0.03338566014681326\n",
            "Loss SS33:  0.029811469947590548\n",
            "Loss SS44:  0.038816511769162114\n",
            "Loss SS55:  0.04072166000828718\n",
            "Loss SS66:  0.03324515826981086\n",
            "Loss SS77:  0.041768508322556\n",
            "Loss SS88:  0.0374684020438615\n",
            "Loss SS11:  0.03477232578092085\n",
            "Loss SS22:  0.03338252725613831\n",
            "Loss SS33:  0.029818847250574248\n",
            "Loss SS44:  0.03884400471468965\n",
            "Loss SS55:  0.040710805889748576\n",
            "Loss SS66:  0.033264017531513576\n",
            "Loss SS77:  0.04176665298139068\n",
            "Loss SS88:  0.03746579304114542\n",
            "Loss SS11:  0.03478375706286906\n",
            "Loss SS22:  0.03338092852411044\n",
            "Loss SS33:  0.029823675153464298\n",
            "Loss SS44:  0.03884024747223604\n",
            "Loss SS55:  0.04070333412275117\n",
            "Loss SS66:  0.03325297033137794\n",
            "Loss SS77:  0.04174242886531092\n",
            "Loss SS88:  0.03746920820425317\n",
            "Loss SS11:  0.034788465976821274\n",
            "Loss SS22:  0.03340190113245591\n",
            "Loss SS33:  0.02983566712048303\n",
            "Loss SS44:  0.03884210203488904\n",
            "Loss SS55:  0.04072542393894207\n",
            "Loss SS66:  0.03325833662937344\n",
            "Loss SS77:  0.04175054838531374\n",
            "Loss SS88:  0.037476049289284295\n",
            "Loss SS11:  0.03478599927904573\n",
            "Loss SS22:  0.033398781066842534\n",
            "Loss SS33:  0.029822149147601527\n",
            "Loss SS44:  0.03882929578579081\n",
            "Loss SS55:  0.040717476836282525\n",
            "Loss SS66:  0.03324213365131769\n",
            "Loss SS77:  0.041718988547538104\n",
            "Loss SS88:  0.03743951187281072\n",
            "Loss SS11:  0.03478305325199282\n",
            "Loss SS22:  0.03339746870235656\n",
            "Loss SS33:  0.029820485108124427\n",
            "Loss SS44:  0.038853958819289595\n",
            "Loss SS55:  0.040707135340726834\n",
            "Loss SS66:  0.033226077721502774\n",
            "Loss SS77:  0.041705339935834175\n",
            "Loss SS88:  0.03744958603412521\n",
            "Loss SS11:  0.034800887120239224\n",
            "Loss SS22:  0.03340894331589771\n",
            "Loss SS33:  0.029830772553605674\n",
            "Loss SS44:  0.03886126561251421\n",
            "Loss SS55:  0.04072091464952725\n",
            "Loss SS66:  0.03324169979044577\n",
            "Loss SS77:  0.04169323727356357\n",
            "Loss SS88:  0.03746247211855293\n",
            "Loss SS11:  0.03476985801746685\n",
            "Loss SS22:  0.03340971740588966\n",
            "Loss SS33:  0.029819679998039163\n",
            "Loss SS44:  0.038863884866916436\n",
            "Loss SS55:  0.040725518284676646\n",
            "Loss SS66:  0.03321974427154421\n",
            "Loss SS77:  0.04168404835940959\n",
            "Loss SS88:  0.03746103837585113\n",
            "Loss SS11:  0.034744430681386065\n",
            "Loss SS22:  0.033399785910347465\n",
            "Loss SS33:  0.029806489152576766\n",
            "Loss SS44:  0.038851502780543515\n",
            "Loss SS55:  0.040731571915296486\n",
            "Loss SS66:  0.03320271167469126\n",
            "Loss SS77:  0.041669849757176296\n",
            "Loss SS88:  0.037445052663327025\n",
            "Loss SS11:  0.03473386634722073\n",
            "Loss SS22:  0.03340879851359711\n",
            "Loss SS33:  0.029807595175975574\n",
            "Loss SS44:  0.038845682958152584\n",
            "Loss SS55:  0.04075145822119068\n",
            "Loss SS66:  0.03320040264923954\n",
            "Loss SS77:  0.0416587142355105\n",
            "Loss SS88:  0.03743524105518499\n",
            "Loss SS11:  0.03469860402757184\n",
            "Loss SS22:  0.03336849984335438\n",
            "Loss SS33:  0.0297702251916444\n",
            "Loss SS44:  0.038804856286424\n",
            "Loss SS55:  0.04072786820578235\n",
            "Loss SS66:  0.03315589799642441\n",
            "Loss SS77:  0.04160876914312543\n",
            "Loss SS88:  0.03739461001168079\n",
            "Validation: \n",
            " Loss SS11:  0.030942628160119057\n",
            " Loss SS22:  0.040896542370319366\n",
            " Loss SS33:  0.03909879922866821\n",
            " Loss SS44:  0.05066857114434242\n",
            " Loss SS55:  0.05057448893785477\n",
            " Loss SS66:  0.04277898743748665\n",
            " Loss SS77:  0.0567956306040287\n",
            " Loss SS88:  0.04109775274991989\n",
            " Loss SS11:  0.03452865664093267\n",
            " Loss SS22:  0.04692454352265313\n",
            " Loss SS33:  0.044401604150022776\n",
            " Loss SS44:  0.058215558174110595\n",
            " Loss SS55:  0.06166114729075205\n",
            " Loss SS66:  0.05096011058915229\n",
            " Loss SS77:  0.06405879628090631\n",
            " Loss SS88:  0.05375521860661961\n",
            " Loss SS11:  0.034680211280540725\n",
            " Loss SS22:  0.04677841785113986\n",
            " Loss SS33:  0.04442315703121627\n",
            " Loss SS44:  0.05800294349106347\n",
            " Loss SS55:  0.06139513886556393\n",
            " Loss SS66:  0.050807172477972215\n",
            " Loss SS77:  0.06417352297320598\n",
            " Loss SS88:  0.05413888131336468\n",
            " Loss SS11:  0.034475959684760846\n",
            " Loss SS22:  0.046434668610330486\n",
            " Loss SS33:  0.04381582861552473\n",
            " Loss SS44:  0.05774498804182303\n",
            " Loss SS55:  0.060418814970333065\n",
            " Loss SS66:  0.050631276835672194\n",
            " Loss SS77:  0.06370054209818606\n",
            " Loss SS88:  0.05345357172801846\n",
            " Loss SS11:  0.034493313228458534\n",
            " Loss SS22:  0.046317903255974804\n",
            " Loss SS33:  0.04396774426654533\n",
            " Loss SS44:  0.05781910986995992\n",
            " Loss SS55:  0.06027752561149774\n",
            " Loss SS66:  0.05048450077941388\n",
            " Loss SS77:  0.06343650265976235\n",
            " Loss SS88:  0.05343190465628365\n",
            "\n",
            "Epoch: 52\n",
            "Loss SS11:  0.034489765763282776\n",
            "Loss SS22:  0.03712070360779762\n",
            "Loss SS33:  0.036386873573064804\n",
            "Loss SS44:  0.04469110816717148\n",
            "Loss SS55:  0.04337555915117264\n",
            "Loss SS66:  0.03512843698263168\n",
            "Loss SS77:  0.04955556243658066\n",
            "Loss SS88:  0.04141736775636673\n",
            "Loss SS11:  0.03495278073982759\n",
            "Loss SS22:  0.03255360912192951\n",
            "Loss SS33:  0.029497880996628242\n",
            "Loss SS44:  0.03852760893377391\n",
            "Loss SS55:  0.04009018567475406\n",
            "Loss SS66:  0.03264704295857386\n",
            "Loss SS77:  0.041997761888937515\n",
            "Loss SS88:  0.0366745533590967\n",
            "Loss SS11:  0.03538181286837373\n",
            "Loss SS22:  0.03301616261402766\n",
            "Loss SS33:  0.029538430007440702\n",
            "Loss SS44:  0.03881181758784112\n",
            "Loss SS55:  0.041077876197440286\n",
            "Loss SS66:  0.03319910079950378\n",
            "Loss SS77:  0.04219484098610424\n",
            "Loss SS88:  0.037088705315476374\n",
            "Loss SS11:  0.03479462321246824\n",
            "Loss SS22:  0.032902872009623434\n",
            "Loss SS33:  0.029411900908716263\n",
            "Loss SS44:  0.0386549228381726\n",
            "Loss SS55:  0.04068940553453661\n",
            "Loss SS66:  0.03292235053114353\n",
            "Loss SS77:  0.04175984162476755\n",
            "Loss SS88:  0.03692069950122987\n",
            "Loss SS11:  0.0346770303823599\n",
            "Loss SS22:  0.033201685039008534\n",
            "Loss SS33:  0.0294601263069525\n",
            "Loss SS44:  0.038728434774206906\n",
            "Loss SS55:  0.04090228349697299\n",
            "Loss SS66:  0.033019422439903745\n",
            "Loss SS77:  0.041738505861381205\n",
            "Loss SS88:  0.03724402698074899\n",
            "Loss SS11:  0.034663132738833334\n",
            "Loss SS22:  0.03363885606328646\n",
            "Loss SS33:  0.029574011142055195\n",
            "Loss SS44:  0.03887257644650983\n",
            "Loss SS55:  0.04116034734190679\n",
            "Loss SS66:  0.033245315483095596\n",
            "Loss SS77:  0.04180504593486879\n",
            "Loss SS88:  0.03741998576066073\n",
            "Loss SS11:  0.03469491490452993\n",
            "Loss SS22:  0.03340040425174549\n",
            "Loss SS33:  0.02930992908897947\n",
            "Loss SS44:  0.038643867265982706\n",
            "Loss SS55:  0.04096179492160922\n",
            "Loss SS66:  0.03298930890980314\n",
            "Loss SS77:  0.04172332725319706\n",
            "Loss SS88:  0.03720969474706493\n",
            "Loss SS11:  0.03470969685471394\n",
            "Loss SS22:  0.03338910020153287\n",
            "Loss SS33:  0.029431762121302982\n",
            "Loss SS44:  0.038896423935050696\n",
            "Loss SS55:  0.041119296261122526\n",
            "Loss SS66:  0.03306653888397653\n",
            "Loss SS77:  0.0418543047993116\n",
            "Loss SS88:  0.03722390894528846\n",
            "Loss SS11:  0.03452271968126297\n",
            "Loss SS22:  0.03324087173390536\n",
            "Loss SS33:  0.029361206808207946\n",
            "Loss SS44:  0.03860634156031373\n",
            "Loss SS55:  0.04080460344751676\n",
            "Loss SS66:  0.03297804305214941\n",
            "Loss SS77:  0.0417655186244735\n",
            "Loss SS88:  0.03708560376163618\n",
            "Loss SS11:  0.03444445698143362\n",
            "Loss SS22:  0.033194935747555325\n",
            "Loss SS33:  0.029320898128079843\n",
            "Loss SS44:  0.03865363743606504\n",
            "Loss SS55:  0.04084349771613603\n",
            "Loss SS66:  0.032994756659308636\n",
            "Loss SS77:  0.041633473811568796\n",
            "Loss SS88:  0.03700710785994818\n",
            "Loss SS11:  0.03441549544360968\n",
            "Loss SS22:  0.03310579474609677\n",
            "Loss SS33:  0.029208596260978444\n",
            "Loss SS44:  0.03852120790593695\n",
            "Loss SS55:  0.0406800829390488\n",
            "Loss SS66:  0.032848722568833\n",
            "Loss SS77:  0.04146545771324989\n",
            "Loss SS88:  0.03685561386813031\n",
            "Loss SS11:  0.034323808745489466\n",
            "Loss SS22:  0.03305763051517912\n",
            "Loss SS33:  0.02917584026786121\n",
            "Loss SS44:  0.03846831455289781\n",
            "Loss SS55:  0.04063296677158759\n",
            "Loss SS66:  0.032678720415443986\n",
            "Loss SS77:  0.04132840067550943\n",
            "Loss SS88:  0.03668904834770941\n",
            "Loss SS11:  0.034271844839754186\n",
            "Loss SS22:  0.033151889327636436\n",
            "Loss SS33:  0.02916211557905536\n",
            "Loss SS44:  0.03856443053434703\n",
            "Loss SS55:  0.04068086096201061\n",
            "Loss SS66:  0.03269416714194885\n",
            "Loss SS77:  0.04129161294703641\n",
            "Loss SS88:  0.03675799093340054\n",
            "Loss SS11:  0.03429562786153255\n",
            "Loss SS22:  0.03319837933562639\n",
            "Loss SS33:  0.029230602526368984\n",
            "Loss SS44:  0.038556572220480166\n",
            "Loss SS55:  0.040745390849259065\n",
            "Loss SS66:  0.032749237720179195\n",
            "Loss SS77:  0.04123935484476672\n",
            "Loss SS88:  0.0369182275394902\n",
            "Loss SS11:  0.03428561073985505\n",
            "Loss SS22:  0.03320047867002217\n",
            "Loss SS33:  0.029283542899375268\n",
            "Loss SS44:  0.03852967991896555\n",
            "Loss SS55:  0.040766792543602326\n",
            "Loss SS66:  0.03278998491611886\n",
            "Loss SS77:  0.04134904450558601\n",
            "Loss SS88:  0.03697052362857135\n",
            "Loss SS11:  0.03424359034031432\n",
            "Loss SS22:  0.033220350458625925\n",
            "Loss SS33:  0.02940569172878534\n",
            "Loss SS44:  0.03854252780411417\n",
            "Loss SS55:  0.04084176350606988\n",
            "Loss SS66:  0.03284442766464704\n",
            "Loss SS77:  0.041353444365278776\n",
            "Loss SS88:  0.03708928307851419\n",
            "Loss SS11:  0.034251467492547094\n",
            "Loss SS22:  0.033169273020965714\n",
            "Loss SS33:  0.029375393255989743\n",
            "Loss SS44:  0.03844954580161142\n",
            "Loss SS55:  0.04075341803594405\n",
            "Loss SS66:  0.03284379223257489\n",
            "Loss SS77:  0.04141107058117849\n",
            "Loss SS88:  0.03712509801576597\n",
            "Loss SS11:  0.03430090671437874\n",
            "Loss SS22:  0.033242749801853246\n",
            "Loss SS33:  0.02940137574329362\n",
            "Loss SS44:  0.03847395794259177\n",
            "Loss SS55:  0.04073460846703652\n",
            "Loss SS66:  0.032888863072322125\n",
            "Loss SS77:  0.04145806284937245\n",
            "Loss SS88:  0.03718952161439678\n",
            "Loss SS11:  0.03433392267900606\n",
            "Loss SS22:  0.03324649065156668\n",
            "Loss SS33:  0.029411169633433962\n",
            "Loss SS44:  0.03855469359795033\n",
            "Loss SS55:  0.040832785735143484\n",
            "Loss SS66:  0.032932856547239736\n",
            "Loss SS77:  0.041540810725142284\n",
            "Loss SS88:  0.037284258938296726\n",
            "Loss SS11:  0.03435293804483576\n",
            "Loss SS22:  0.03318355099849052\n",
            "Loss SS33:  0.029348232281145626\n",
            "Loss SS44:  0.03847131722104487\n",
            "Loss SS55:  0.0408155745314678\n",
            "Loss SS66:  0.03294947602986041\n",
            "Loss SS77:  0.04158044848298527\n",
            "Loss SS88:  0.037235335139704\n",
            "Loss SS11:  0.03439837719188697\n",
            "Loss SS22:  0.033210238469625585\n",
            "Loss SS33:  0.029342668381199907\n",
            "Loss SS44:  0.03845596902850849\n",
            "Loss SS55:  0.04078383721522431\n",
            "Loss SS66:  0.03296285893638336\n",
            "Loss SS77:  0.04158064754522262\n",
            "Loss SS88:  0.03725264568938248\n",
            "Loss SS11:  0.03443249935610882\n",
            "Loss SS22:  0.03323408378667741\n",
            "Loss SS33:  0.029352558132314004\n",
            "Loss SS44:  0.03848318621445606\n",
            "Loss SS55:  0.040781520501273504\n",
            "Loss SS66:  0.03297596060233941\n",
            "Loss SS77:  0.04158402502678017\n",
            "Loss SS88:  0.03729392495447708\n",
            "Loss SS11:  0.03447150611803273\n",
            "Loss SS22:  0.0332250014048626\n",
            "Loss SS33:  0.029355347173381174\n",
            "Loss SS44:  0.038508771012676246\n",
            "Loss SS55:  0.040811115007729555\n",
            "Loss SS66:  0.03300353328431893\n",
            "Loss SS77:  0.04164763863674656\n",
            "Loss SS88:  0.037345707694666955\n",
            "Loss SS11:  0.034442287664134784\n",
            "Loss SS22:  0.03318503153917594\n",
            "Loss SS33:  0.029341255850864178\n",
            "Loss SS44:  0.038493837854820925\n",
            "Loss SS55:  0.04076920746466814\n",
            "Loss SS66:  0.03298826257645826\n",
            "Loss SS77:  0.041643203949773465\n",
            "Loss SS88:  0.03736969114994848\n",
            "Loss SS11:  0.034466388098184496\n",
            "Loss SS22:  0.0332050097727553\n",
            "Loss SS33:  0.029349866249749276\n",
            "Loss SS44:  0.038533152386360646\n",
            "Loss SS55:  0.04079077273792746\n",
            "Loss SS66:  0.033016062708749315\n",
            "Loss SS77:  0.041657876254241\n",
            "Loss SS88:  0.03736816823513429\n",
            "Loss SS11:  0.03443286206528723\n",
            "Loss SS22:  0.03323434144110081\n",
            "Loss SS33:  0.02934964019730984\n",
            "Loss SS44:  0.03853111038823052\n",
            "Loss SS55:  0.040795623528886125\n",
            "Loss SS66:  0.03305064460106817\n",
            "Loss SS77:  0.04169824695682146\n",
            "Loss SS88:  0.03741575045088135\n",
            "Loss SS11:  0.03437200666787067\n",
            "Loss SS22:  0.03323383498722794\n",
            "Loss SS33:  0.029367292428324962\n",
            "Loss SS44:  0.038508115293479514\n",
            "Loss SS55:  0.04079993899868822\n",
            "Loss SS66:  0.033039979422572016\n",
            "Loss SS77:  0.041701255644532455\n",
            "Loss SS88:  0.03745122547207893\n",
            "Loss SS11:  0.03434455077201678\n",
            "Loss SS22:  0.03321133445384757\n",
            "Loss SS33:  0.02935436292383064\n",
            "Loss SS44:  0.03852168170260987\n",
            "Loss SS55:  0.0407984697318385\n",
            "Loss SS66:  0.03298460611750499\n",
            "Loss SS77:  0.04169521774972937\n",
            "Loss SS88:  0.03739015450804216\n",
            "Loss SS11:  0.034299498039473415\n",
            "Loss SS22:  0.03320617127328369\n",
            "Loss SS33:  0.029354430861055214\n",
            "Loss SS44:  0.038514444710574115\n",
            "Loss SS55:  0.04076583540312336\n",
            "Loss SS66:  0.032965204748076474\n",
            "Loss SS77:  0.04161899990246389\n",
            "Loss SS88:  0.037377332340514956\n",
            "Loss SS11:  0.03429882711638085\n",
            "Loss SS22:  0.033216579724423254\n",
            "Loss SS33:  0.029338923439741954\n",
            "Loss SS44:  0.03853965853420934\n",
            "Loss SS55:  0.040781816550853736\n",
            "Loss SS66:  0.032972106287620734\n",
            "Loss SS77:  0.041633951090455465\n",
            "Loss SS88:  0.037392916605900654\n",
            "Loss SS11:  0.03431755951621208\n",
            "Loss SS22:  0.033249276619020884\n",
            "Loss SS33:  0.029361847249722958\n",
            "Loss SS44:  0.038575123729450364\n",
            "Loss SS55:  0.04078398125116215\n",
            "Loss SS66:  0.03297045072571582\n",
            "Loss SS77:  0.04161928285940541\n",
            "Loss SS88:  0.03738549242161239\n",
            "Loss SS11:  0.03430867571251952\n",
            "Loss SS22:  0.03324621963730962\n",
            "Loss SS33:  0.029330973923685465\n",
            "Loss SS44:  0.03851950173999911\n",
            "Loss SS55:  0.040767245988849656\n",
            "Loss SS66:  0.032961158075899936\n",
            "Loss SS77:  0.04155822782223248\n",
            "Loss SS88:  0.03735430203857337\n",
            "Loss SS11:  0.034306644441739795\n",
            "Loss SS22:  0.03325489309625091\n",
            "Loss SS33:  0.029330497091154442\n",
            "Loss SS44:  0.038507117074551615\n",
            "Loss SS55:  0.040763221507039024\n",
            "Loss SS66:  0.0329680207165138\n",
            "Loss SS77:  0.04158988897906286\n",
            "Loss SS88:  0.03733359725791903\n",
            "Loss SS11:  0.0342907755441536\n",
            "Loss SS22:  0.03323374030228108\n",
            "Loss SS33:  0.029330258519357784\n",
            "Loss SS44:  0.038506771812041\n",
            "Loss SS55:  0.04075431347658627\n",
            "Loss SS66:  0.03298237998590008\n",
            "Loss SS77:  0.04163759582172348\n",
            "Loss SS88:  0.037333046263393316\n",
            "Loss SS11:  0.03431981993713512\n",
            "Loss SS22:  0.033266713799051176\n",
            "Loss SS33:  0.029362938948565565\n",
            "Loss SS44:  0.03853976011516586\n",
            "Loss SS55:  0.04078720853038547\n",
            "Loss SS66:  0.03300593193631368\n",
            "Loss SS77:  0.04169672079755764\n",
            "Loss SS88:  0.0373675689658374\n",
            "Loss SS11:  0.03432399318407574\n",
            "Loss SS22:  0.03323673968471353\n",
            "Loss SS33:  0.029368983093447494\n",
            "Loss SS44:  0.038534049852154195\n",
            "Loss SS55:  0.04078242392876209\n",
            "Loss SS66:  0.03301982640519611\n",
            "Loss SS77:  0.04172255719701449\n",
            "Loss SS88:  0.037396264429657884\n",
            "Loss SS11:  0.03434458296033979\n",
            "Loss SS22:  0.033245061289446834\n",
            "Loss SS33:  0.02939585759884928\n",
            "Loss SS44:  0.03855659096009018\n",
            "Loss SS55:  0.040788250761184\n",
            "Loss SS66:  0.033043875191309115\n",
            "Loss SS77:  0.041732703570348736\n",
            "Loss SS88:  0.037407378544850364\n",
            "Loss SS11:  0.03434205469436883\n",
            "Loss SS22:  0.033241420552495994\n",
            "Loss SS33:  0.02940928190946579\n",
            "Loss SS44:  0.03857287682713363\n",
            "Loss SS55:  0.04076738752966942\n",
            "Loss SS66:  0.03303911520866049\n",
            "Loss SS77:  0.04169100530023845\n",
            "Loss SS88:  0.037379824639250325\n",
            "Loss SS11:  0.03432134862040754\n",
            "Loss SS22:  0.033243437119199845\n",
            "Loss SS33:  0.02939416629474933\n",
            "Loss SS44:  0.038558134897135374\n",
            "Loss SS55:  0.04074222320724973\n",
            "Loss SS66:  0.033042918010724814\n",
            "Loss SS77:  0.04163812991632564\n",
            "Loss SS88:  0.03734767473158561\n",
            "Loss SS11:  0.034296893112151824\n",
            "Loss SS22:  0.03321353881560323\n",
            "Loss SS33:  0.029399680659708466\n",
            "Loss SS44:  0.03853935583987657\n",
            "Loss SS55:  0.04071141101057877\n",
            "Loss SS66:  0.032998387194464886\n",
            "Loss SS77:  0.0416196375951895\n",
            "Loss SS88:  0.03733512464805942\n",
            "Loss SS11:  0.03428637110477225\n",
            "Loss SS22:  0.03318621634396532\n",
            "Loss SS33:  0.02939329294492479\n",
            "Loss SS44:  0.03858239614774015\n",
            "Loss SS55:  0.04070878105976635\n",
            "Loss SS66:  0.033007007784342526\n",
            "Loss SS77:  0.04161199355065971\n",
            "Loss SS88:  0.03735434763746666\n",
            "Loss SS11:  0.03429451244464031\n",
            "Loss SS22:  0.033209066915069765\n",
            "Loss SS33:  0.029395543152144647\n",
            "Loss SS44:  0.0385818469888755\n",
            "Loss SS55:  0.040724002114902735\n",
            "Loss SS66:  0.03297870429889854\n",
            "Loss SS77:  0.041569737468250184\n",
            "Loss SS88:  0.03734143425477102\n",
            "Loss SS11:  0.034290610841362604\n",
            "Loss SS22:  0.03322233565841888\n",
            "Loss SS33:  0.02942401514943875\n",
            "Loss SS44:  0.03858065724178193\n",
            "Loss SS55:  0.04074258401529806\n",
            "Loss SS66:  0.03297342710427038\n",
            "Loss SS77:  0.041561035850835246\n",
            "Loss SS88:  0.03734604717893725\n",
            "Loss SS11:  0.03427941634334849\n",
            "Loss SS22:  0.033211304609536293\n",
            "Loss SS33:  0.02942653795112175\n",
            "Loss SS44:  0.03856403385386251\n",
            "Loss SS55:  0.04071685856774346\n",
            "Loss SS66:  0.0329817580599381\n",
            "Loss SS77:  0.04155852186672925\n",
            "Loss SS88:  0.03732703883728129\n",
            "Loss SS11:  0.03427273030403385\n",
            "Loss SS22:  0.0332134790389108\n",
            "Loss SS33:  0.029421157813937216\n",
            "Loss SS44:  0.038558507270587274\n",
            "Loss SS55:  0.040696351478497185\n",
            "Loss SS66:  0.032970598191360764\n",
            "Loss SS77:  0.04154681972736945\n",
            "Loss SS88:  0.03732565954293794\n",
            "Loss SS11:  0.03431349866786051\n",
            "Loss SS22:  0.03321909919348101\n",
            "Loss SS33:  0.029441299775982907\n",
            "Loss SS44:  0.03857238619214547\n",
            "Loss SS55:  0.04070372270738206\n",
            "Loss SS66:  0.03298318177511184\n",
            "Loss SS77:  0.04152795531169538\n",
            "Loss SS88:  0.0373427145381328\n",
            "Loss SS11:  0.03429751120782468\n",
            "Loss SS22:  0.033220938857394286\n",
            "Loss SS33:  0.02943125285143578\n",
            "Loss SS44:  0.03857171933475777\n",
            "Loss SS55:  0.040711802590048494\n",
            "Loss SS66:  0.03296413122314693\n",
            "Loss SS77:  0.041540853727665486\n",
            "Loss SS88:  0.03735397836908601\n",
            "Loss SS11:  0.034329648167211284\n",
            "Loss SS22:  0.033219232475048925\n",
            "Loss SS33:  0.029424290510879202\n",
            "Loss SS44:  0.03856674545778717\n",
            "Loss SS55:  0.04072674934113608\n",
            "Loss SS66:  0.03296503532211239\n",
            "Loss SS77:  0.041552201348441925\n",
            "Loss SS88:  0.03736532680816838\n",
            "Loss SS11:  0.03432273924381718\n",
            "Loss SS22:  0.03321595715742225\n",
            "Loss SS33:  0.02943204759993821\n",
            "Loss SS44:  0.03858516062851879\n",
            "Loss SS55:  0.040728327599421854\n",
            "Loss SS66:  0.032970144794847264\n",
            "Loss SS77:  0.04156518781755174\n",
            "Loss SS88:  0.0373593535150349\n",
            "Loss SS11:  0.03427982227516393\n",
            "Loss SS22:  0.03317645899647972\n",
            "Loss SS33:  0.029424328675641546\n",
            "Loss SS44:  0.03855350146605017\n",
            "Loss SS55:  0.04067602202812183\n",
            "Loss SS66:  0.032919157895803935\n",
            "Loss SS77:  0.04154335697562777\n",
            "Loss SS88:  0.037315156693947776\n",
            "Validation: \n",
            " Loss SS11:  0.03389767184853554\n",
            " Loss SS22:  0.04233021289110184\n",
            " Loss SS33:  0.03924280032515526\n",
            " Loss SS44:  0.0527668371796608\n",
            " Loss SS55:  0.051025133579969406\n",
            " Loss SS66:  0.041485078632831573\n",
            " Loss SS77:  0.05923993140459061\n",
            " Loss SS88:  0.0422561913728714\n",
            " Loss SS11:  0.034098291237439425\n",
            " Loss SS22:  0.04841517337730953\n",
            " Loss SS33:  0.044814342189402806\n",
            " Loss SS44:  0.06017845673930077\n",
            " Loss SS55:  0.062276724548566906\n",
            " Loss SS66:  0.05067287012934685\n",
            " Loss SS77:  0.06771712359927949\n",
            " Loss SS88:  0.05530040037064325\n",
            " Loss SS11:  0.03420206545511397\n",
            " Loss SS22:  0.048115807789854886\n",
            " Loss SS33:  0.04490184384148296\n",
            " Loss SS44:  0.05997651978963759\n",
            " Loss SS55:  0.06184782460331917\n",
            " Loss SS66:  0.050664568456207836\n",
            " Loss SS77:  0.06706051473937384\n",
            " Loss SS88:  0.055643887781515355\n",
            " Loss SS11:  0.033968301001386564\n",
            " Loss SS22:  0.04765551433455749\n",
            " Loss SS33:  0.044226538817413515\n",
            " Loss SS44:  0.059655295226906165\n",
            " Loss SS55:  0.06125648582323653\n",
            " Loss SS66:  0.05053922656129618\n",
            " Loss SS77:  0.06674076989293098\n",
            " Loss SS88:  0.055092017486935756\n",
            " Loss SS11:  0.033927908926098434\n",
            " Loss SS22:  0.04759926164959684\n",
            " Loss SS33:  0.04435452901654773\n",
            " Loss SS44:  0.05969376356145482\n",
            " Loss SS55:  0.06102358834978975\n",
            " Loss SS66:  0.05034705246856183\n",
            " Loss SS77:  0.06657313633664155\n",
            " Loss SS88:  0.05510248142628022\n",
            "\n",
            "Epoch: 53\n",
            "Loss SS11:  0.03729653358459473\n",
            "Loss SS22:  0.03433006629347801\n",
            "Loss SS33:  0.031285084784030914\n",
            "Loss SS44:  0.04380890354514122\n",
            "Loss SS55:  0.04339243471622467\n",
            "Loss SS66:  0.03690960630774498\n",
            "Loss SS77:  0.04762396588921547\n",
            "Loss SS88:  0.039992235600948334\n",
            "Loss SS11:  0.034346947107802735\n",
            "Loss SS22:  0.031059070574966343\n",
            "Loss SS33:  0.028951478614048523\n",
            "Loss SS44:  0.037864079868251625\n",
            "Loss SS55:  0.0410885868424719\n",
            "Loss SS66:  0.0326265005225485\n",
            "Loss SS77:  0.04122710160233758\n",
            "Loss SS88:  0.036633100021969185\n",
            "Loss SS11:  0.03424117997998283\n",
            "Loss SS22:  0.031649897229813394\n",
            "Loss SS33:  0.02906172163784504\n",
            "Loss SS44:  0.03866331119622503\n",
            "Loss SS55:  0.04055539128326234\n",
            "Loss SS66:  0.03274681578789439\n",
            "Loss SS77:  0.04174789839557239\n",
            "Loss SS88:  0.03731038368174008\n",
            "Loss SS11:  0.033907844715060725\n",
            "Loss SS22:  0.03188768383716383\n",
            "Loss SS33:  0.02899452557246531\n",
            "Loss SS44:  0.038368895409568664\n",
            "Loss SS55:  0.04006158896992283\n",
            "Loss SS66:  0.03254435133309134\n",
            "Loss SS77:  0.04144751016170748\n",
            "Loss SS88:  0.03707328775236683\n",
            "Loss SS11:  0.03422785118767401\n",
            "Loss SS22:  0.03216481599502447\n",
            "Loss SS33:  0.02912165274525561\n",
            "Loss SS44:  0.03842472057880425\n",
            "Loss SS55:  0.040096863013942066\n",
            "Loss SS66:  0.032772307397752276\n",
            "Loss SS77:  0.04156272113323212\n",
            "Loss SS88:  0.03731943594246376\n",
            "Loss SS11:  0.03444593965861143\n",
            "Loss SS22:  0.03246115491378541\n",
            "Loss SS33:  0.029352880064763276\n",
            "Loss SS44:  0.038546430933124876\n",
            "Loss SS55:  0.040255049718361274\n",
            "Loss SS66:  0.03289472454172723\n",
            "Loss SS77:  0.04148223777027691\n",
            "Loss SS88:  0.03721891160981328\n",
            "Loss SS11:  0.03457696015228991\n",
            "Loss SS22:  0.03254757998663871\n",
            "Loss SS33:  0.029227459436801613\n",
            "Loss SS44:  0.038549149989104664\n",
            "Loss SS55:  0.040191174164170125\n",
            "Loss SS66:  0.03277124873683101\n",
            "Loss SS77:  0.041422221259992634\n",
            "Loss SS88:  0.03696239244986753\n",
            "Loss SS11:  0.03453738541460373\n",
            "Loss SS22:  0.03257930137112107\n",
            "Loss SS33:  0.0293185428099733\n",
            "Loss SS44:  0.038565447265413444\n",
            "Loss SS55:  0.0403072435251424\n",
            "Loss SS66:  0.03279420974808679\n",
            "Loss SS77:  0.04149321923163575\n",
            "Loss SS88:  0.03705096585859715\n",
            "Loss SS11:  0.03451755688882169\n",
            "Loss SS22:  0.03241802374889821\n",
            "Loss SS33:  0.02925113418403967\n",
            "Loss SS44:  0.03837193332520532\n",
            "Loss SS55:  0.04008825730394434\n",
            "Loss SS66:  0.03272136332996098\n",
            "Loss SS77:  0.041356085535184835\n",
            "Loss SS88:  0.03691110056307581\n",
            "Loss SS11:  0.03449620142742828\n",
            "Loss SS22:  0.03223425137636426\n",
            "Loss SS33:  0.029199973547032902\n",
            "Loss SS44:  0.03847593605354592\n",
            "Loss SS55:  0.04009162524080539\n",
            "Loss SS66:  0.03263249068142294\n",
            "Loss SS77:  0.04107688117649529\n",
            "Loss SS88:  0.03682060597034601\n",
            "Loss SS11:  0.034376435369105625\n",
            "Loss SS22:  0.03226380774954168\n",
            "Loss SS33:  0.029139254094645527\n",
            "Loss SS44:  0.0382923978463848\n",
            "Loss SS55:  0.040122957291579484\n",
            "Loss SS66:  0.032489176728937884\n",
            "Loss SS77:  0.040913098666927605\n",
            "Loss SS88:  0.03675575716660755\n",
            "Loss SS11:  0.034333355926178595\n",
            "Loss SS22:  0.03228384489613073\n",
            "Loss SS33:  0.029117217186737706\n",
            "Loss SS44:  0.03821889092927581\n",
            "Loss SS55:  0.040212047999506596\n",
            "Loss SS66:  0.03244475981740801\n",
            "Loss SS77:  0.04090230123282553\n",
            "Loss SS88:  0.03665606331852105\n",
            "Loss SS11:  0.03424255768499099\n",
            "Loss SS22:  0.03232970675222637\n",
            "Loss SS33:  0.02916417588010307\n",
            "Loss SS44:  0.038238002957145044\n",
            "Loss SS55:  0.040210985478537145\n",
            "Loss SS66:  0.032539775656643975\n",
            "Loss SS77:  0.040860524785912726\n",
            "Loss SS88:  0.03662886352024295\n",
            "Loss SS11:  0.034270343605343624\n",
            "Loss SS22:  0.03248694964208221\n",
            "Loss SS33:  0.029315651330442827\n",
            "Loss SS44:  0.038300611419987134\n",
            "Loss SS55:  0.040289749426923635\n",
            "Loss SS66:  0.03262780959369572\n",
            "Loss SS77:  0.04085603529829105\n",
            "Loss SS88:  0.03675886208035109\n",
            "Loss SS11:  0.03426423442966126\n",
            "Loss SS22:  0.03250359607767974\n",
            "Loss SS33:  0.029349621781643402\n",
            "Loss SS44:  0.0384147399441993\n",
            "Loss SS55:  0.040324674483309404\n",
            "Loss SS66:  0.032687063850726644\n",
            "Loss SS77:  0.04093034866642445\n",
            "Loss SS88:  0.0368264099807604\n",
            "Loss SS11:  0.03424949535717633\n",
            "Loss SS22:  0.03260714186984577\n",
            "Loss SS33:  0.02942027748617905\n",
            "Loss SS44:  0.038479539066158384\n",
            "Loss SS55:  0.04046059463987287\n",
            "Loss SS66:  0.03281793954307275\n",
            "Loss SS77:  0.040997190363951865\n",
            "Loss SS88:  0.03695394244316398\n",
            "Loss SS11:  0.03426590785152794\n",
            "Loss SS22:  0.03256467767985341\n",
            "Loss SS33:  0.02940583067095798\n",
            "Loss SS44:  0.03835739632663519\n",
            "Loss SS55:  0.040385902784070615\n",
            "Loss SS66:  0.03278517959095677\n",
            "Loss SS77:  0.04097744274120894\n",
            "Loss SS88:  0.036970534101591346\n",
            "Loss SS11:  0.034232405052460425\n",
            "Loss SS22:  0.032564818946241636\n",
            "Loss SS33:  0.02945233801957111\n",
            "Loss SS44:  0.03838744158284706\n",
            "Loss SS55:  0.04041294559662105\n",
            "Loss SS66:  0.03284130724724273\n",
            "Loss SS77:  0.04104423963012751\n",
            "Loss SS88:  0.037008247746710195\n",
            "Loss SS11:  0.03425428397871184\n",
            "Loss SS22:  0.032610219383601986\n",
            "Loss SS33:  0.029468163158435846\n",
            "Loss SS44:  0.03846592200889113\n",
            "Loss SS55:  0.04048159529161717\n",
            "Loss SS66:  0.032895856737596554\n",
            "Loss SS77:  0.04114272953808637\n",
            "Loss SS88:  0.03710247154311581\n",
            "Loss SS11:  0.03420194787465777\n",
            "Loss SS22:  0.03265751236123252\n",
            "Loss SS33:  0.029451042677017408\n",
            "Loss SS44:  0.03846697476131754\n",
            "Loss SS55:  0.0405096785794378\n",
            "Loss SS66:  0.03288571175714438\n",
            "Loss SS77:  0.04113252837386431\n",
            "Loss SS88:  0.03708639610266186\n",
            "Loss SS11:  0.03416286756409638\n",
            "Loss SS22:  0.032646137229468096\n",
            "Loss SS33:  0.029429979603830262\n",
            "Loss SS44:  0.03843931690673923\n",
            "Loss SS55:  0.0404416270442863\n",
            "Loss SS66:  0.03281868818165058\n",
            "Loss SS77:  0.041129732365483666\n",
            "Loss SS88:  0.03702147117820545\n",
            "Loss SS11:  0.03417617685527881\n",
            "Loss SS22:  0.032691527396369886\n",
            "Loss SS33:  0.029418632750050716\n",
            "Loss SS44:  0.03841590842505767\n",
            "Loss SS55:  0.040403465453482355\n",
            "Loss SS66:  0.03280952210886783\n",
            "Loss SS77:  0.041122380891258685\n",
            "Loss SS88:  0.037035262574093036\n",
            "Loss SS11:  0.034253427404826044\n",
            "Loss SS22:  0.032728662436229614\n",
            "Loss SS33:  0.029462887294489336\n",
            "Loss SS44:  0.038479146391692746\n",
            "Loss SS55:  0.040448795026378936\n",
            "Loss SS66:  0.03284712659186637\n",
            "Loss SS77:  0.04119743357888714\n",
            "Loss SS88:  0.037077749392430706\n",
            "Loss SS11:  0.034269377331197004\n",
            "Loss SS22:  0.03273037194528363\n",
            "Loss SS33:  0.029459825051682337\n",
            "Loss SS44:  0.03848040286145169\n",
            "Loss SS55:  0.04041216464398743\n",
            "Loss SS66:  0.03283147208263606\n",
            "Loss SS77:  0.04116109361896267\n",
            "Loss SS88:  0.03710424816195583\n",
            "Loss SS11:  0.03430166786451557\n",
            "Loss SS22:  0.032797470449226526\n",
            "Loss SS33:  0.029504916449058106\n",
            "Loss SS44:  0.03857082865854022\n",
            "Loss SS55:  0.04047570159635603\n",
            "Loss SS66:  0.03286151718580129\n",
            "Loss SS77:  0.04120042734620977\n",
            "Loss SS88:  0.03715422217776666\n",
            "Loss SS11:  0.034308251616845566\n",
            "Loss SS22:  0.03281749756004943\n",
            "Loss SS33:  0.02953624513846232\n",
            "Loss SS44:  0.038571795159245865\n",
            "Loss SS55:  0.04050030606795117\n",
            "Loss SS66:  0.03288778993653586\n",
            "Loss SS77:  0.041223001863020824\n",
            "Loss SS88:  0.037213520952073705\n",
            "Loss SS11:  0.034319537081595124\n",
            "Loss SS22:  0.03280010616042833\n",
            "Loss SS33:  0.02954055722874243\n",
            "Loss SS44:  0.03856869275999252\n",
            "Loss SS55:  0.04049292963240795\n",
            "Loss SS66:  0.03287831412706795\n",
            "Loss SS77:  0.0412609527936612\n",
            "Loss SS88:  0.03725260236368325\n",
            "Loss SS11:  0.034310743123347906\n",
            "Loss SS22:  0.032763241168207786\n",
            "Loss SS33:  0.029482282856238724\n",
            "Loss SS44:  0.03852588080584563\n",
            "Loss SS55:  0.040496771845870354\n",
            "Loss SS66:  0.0328285848907439\n",
            "Loss SS77:  0.04122432073873787\n",
            "Loss SS88:  0.03719496916690876\n",
            "Loss SS11:  0.03426159568316572\n",
            "Loss SS22:  0.032768360640707817\n",
            "Loss SS33:  0.029493429529019947\n",
            "Loss SS44:  0.038489279844317575\n",
            "Loss SS55:  0.040495723300345\n",
            "Loss SS66:  0.03280745296874929\n",
            "Loss SS77:  0.04120000336465038\n",
            "Loss SS88:  0.03717455837968405\n",
            "Loss SS11:  0.034218554112970624\n",
            "Loss SS22:  0.03280555678345903\n",
            "Loss SS33:  0.02950796149543061\n",
            "Loss SS44:  0.03850859341522058\n",
            "Loss SS55:  0.040546126458857884\n",
            "Loss SS66:  0.032820090465887714\n",
            "Loss SS77:  0.04119634053500248\n",
            "Loss SS88:  0.037210732660035495\n",
            "Loss SS11:  0.03420561579894584\n",
            "Loss SS22:  0.0328572369938673\n",
            "Loss SS33:  0.02950539611948289\n",
            "Loss SS44:  0.03853350823924589\n",
            "Loss SS55:  0.04055964570504882\n",
            "Loss SS66:  0.032820241364876296\n",
            "Loss SS77:  0.041206910500791775\n",
            "Loss SS88:  0.03721058392197983\n",
            "Loss SS11:  0.03419739464327837\n",
            "Loss SS22:  0.032827878820072036\n",
            "Loss SS33:  0.029483626863149585\n",
            "Loss SS44:  0.0385005472593273\n",
            "Loss SS55:  0.040549540874275744\n",
            "Loss SS66:  0.032804706238713295\n",
            "Loss SS77:  0.041193247188805\n",
            "Loss SS88:  0.03714731455424208\n",
            "Loss SS11:  0.03414573642857535\n",
            "Loss SS22:  0.032836266569909275\n",
            "Loss SS33:  0.029483905079487328\n",
            "Loss SS44:  0.03854161358766281\n",
            "Loss SS55:  0.04054885083734061\n",
            "Loss SS66:  0.03282325889485833\n",
            "Loss SS77:  0.04123546846930483\n",
            "Loss SS88:  0.03714841388955109\n",
            "Loss SS11:  0.03411783315691523\n",
            "Loss SS22:  0.03283443743661271\n",
            "Loss SS33:  0.02947474166390759\n",
            "Loss SS44:  0.03851886707557833\n",
            "Loss SS55:  0.04053154426804122\n",
            "Loss SS66:  0.03284986559132795\n",
            "Loss SS77:  0.04123086338226889\n",
            "Loss SS88:  0.03712575193325739\n",
            "Loss SS11:  0.034137453703630354\n",
            "Loss SS22:  0.03284131537827928\n",
            "Loss SS33:  0.029512276463319827\n",
            "Loss SS44:  0.038563596150261566\n",
            "Loss SS55:  0.04056890308856964\n",
            "Loss SS66:  0.03289440624918406\n",
            "Loss SS77:  0.04125035041806635\n",
            "Loss SS88:  0.037189313967075865\n",
            "Loss SS11:  0.034172050437314215\n",
            "Loss SS22:  0.03285672043336083\n",
            "Loss SS33:  0.029545593630002434\n",
            "Loss SS44:  0.03858930857963542\n",
            "Loss SS55:  0.040612308366855664\n",
            "Loss SS66:  0.032942924287669\n",
            "Loss SS77:  0.04130255124210632\n",
            "Loss SS88:  0.03722079468100329\n",
            "Loss SS11:  0.034178208797096876\n",
            "Loss SS22:  0.03285210020840168\n",
            "Loss SS33:  0.02954935842294277\n",
            "Loss SS44:  0.038587685142981706\n",
            "Loss SS55:  0.04061346524846521\n",
            "Loss SS66:  0.03295353868682133\n",
            "Loss SS77:  0.041299973230117575\n",
            "Loss SS88:  0.037210350123510136\n",
            "Loss SS11:  0.034173254280117644\n",
            "Loss SS22:  0.03282530914058261\n",
            "Loss SS33:  0.02954985654217535\n",
            "Loss SS44:  0.03856477819664298\n",
            "Loss SS55:  0.040627382986870096\n",
            "Loss SS66:  0.03293234456802475\n",
            "Loss SS77:  0.04126077240729589\n",
            "Loss SS88:  0.037194855897775235\n",
            "Loss SS11:  0.0341456935562565\n",
            "Loss SS22:  0.03279167199580688\n",
            "Loss SS33:  0.029538598376035376\n",
            "Loss SS44:  0.038526434583340104\n",
            "Loss SS55:  0.04060448902681118\n",
            "Loss SS66:  0.03291602405350352\n",
            "Loss SS77:  0.041226226464891685\n",
            "Loss SS88:  0.03715180139744219\n",
            "Loss SS11:  0.03414542571453335\n",
            "Loss SS22:  0.032791172737813054\n",
            "Loss SS33:  0.029522505648376995\n",
            "Loss SS44:  0.03851761858043311\n",
            "Loss SS55:  0.04058804569761162\n",
            "Loss SS66:  0.03288761617811135\n",
            "Loss SS77:  0.04119751906341604\n",
            "Loss SS88:  0.03714797629133972\n",
            "Loss SS11:  0.03413474808291456\n",
            "Loss SS22:  0.0328001881162276\n",
            "Loss SS33:  0.029534324752793942\n",
            "Loss SS44:  0.0385549746017132\n",
            "Loss SS55:  0.040617602796031355\n",
            "Loss SS66:  0.03291528842915918\n",
            "Loss SS77:  0.041213251641043405\n",
            "Loss SS88:  0.03716512148424128\n",
            "Loss SS11:  0.03409017973246366\n",
            "Loss SS22:  0.03279164673674426\n",
            "Loss SS33:  0.029529905599290435\n",
            "Loss SS44:  0.03856366350249797\n",
            "Loss SS55:  0.040630625991418125\n",
            "Loss SS66:  0.0328980659399807\n",
            "Loss SS77:  0.0412056854316498\n",
            "Loss SS88:  0.03713120583109467\n",
            "Loss SS11:  0.034077074369880195\n",
            "Loss SS22:  0.03280018350538619\n",
            "Loss SS33:  0.029533363285866032\n",
            "Loss SS44:  0.03857273062925582\n",
            "Loss SS55:  0.040646144487583157\n",
            "Loss SS66:  0.0329252289134717\n",
            "Loss SS77:  0.04121549828354083\n",
            "Loss SS88:  0.037133274768034526\n",
            "Loss SS11:  0.034054033734820556\n",
            "Loss SS22:  0.03280255693311343\n",
            "Loss SS33:  0.02952236438433698\n",
            "Loss SS44:  0.03858718914336367\n",
            "Loss SS55:  0.040620999708944575\n",
            "Loss SS66:  0.03291526282089492\n",
            "Loss SS77:  0.041212623364134345\n",
            "Loss SS88:  0.037139175770903685\n",
            "Loss SS11:  0.03405262013293313\n",
            "Loss SS22:  0.03281286105834573\n",
            "Loss SS33:  0.02951911692412532\n",
            "Loss SS44:  0.038604382083428146\n",
            "Loss SS55:  0.04061116389676827\n",
            "Loss SS66:  0.03290246353567052\n",
            "Loss SS77:  0.04120225438218809\n",
            "Loss SS88:  0.037152216020050775\n",
            "Loss SS11:  0.034061313526262994\n",
            "Loss SS22:  0.032843685355822154\n",
            "Loss SS33:  0.02953169907044677\n",
            "Loss SS44:  0.038609808352166694\n",
            "Loss SS55:  0.040608451522044224\n",
            "Loss SS66:  0.03290627809642027\n",
            "Loss SS77:  0.04119785457122617\n",
            "Loss SS88:  0.037165335084243516\n",
            "Loss SS11:  0.03406480444017434\n",
            "Loss SS22:  0.03283124305550292\n",
            "Loss SS33:  0.029524511328358972\n",
            "Loss SS44:  0.03859639293847529\n",
            "Loss SS55:  0.04060053158704992\n",
            "Loss SS66:  0.032902832149069375\n",
            "Loss SS77:  0.04120900192643453\n",
            "Loss SS88:  0.037182751845027\n",
            "Loss SS11:  0.0340485334783919\n",
            "Loss SS22:  0.03282423092016749\n",
            "Loss SS33:  0.02950008754913088\n",
            "Loss SS44:  0.03858748920688963\n",
            "Loss SS55:  0.04057925856512064\n",
            "Loss SS66:  0.03288860499637395\n",
            "Loss SS77:  0.041233171892773576\n",
            "Loss SS88:  0.037157730679887875\n",
            "Loss SS11:  0.03404679872761645\n",
            "Loss SS22:  0.03282839398668486\n",
            "Loss SS33:  0.029500850001950275\n",
            "Loss SS44:  0.038587272662878536\n",
            "Loss SS55:  0.040575810892046614\n",
            "Loss SS66:  0.03289708236460874\n",
            "Loss SS77:  0.04123077437808261\n",
            "Loss SS88:  0.037128897345084656\n",
            "Loss SS11:  0.0340270570037382\n",
            "Loss SS22:  0.032791207914379124\n",
            "Loss SS33:  0.029471416720577997\n",
            "Loss SS44:  0.03856129561173455\n",
            "Loss SS55:  0.04053506831014472\n",
            "Loss SS66:  0.03285375786774513\n",
            "Loss SS77:  0.04120375718276515\n",
            "Loss SS88:  0.03708487121263128\n",
            "Validation: \n",
            " Loss SS11:  0.03215441480278969\n",
            " Loss SS22:  0.04083837941288948\n",
            " Loss SS33:  0.03859123960137367\n",
            " Loss SS44:  0.05055656284093857\n",
            " Loss SS55:  0.050046395510435104\n",
            " Loss SS66:  0.04065895080566406\n",
            " Loss SS77:  0.05710543319582939\n",
            " Loss SS88:  0.04060637950897217\n",
            " Loss SS11:  0.034547009372285435\n",
            " Loss SS22:  0.04706500683512006\n",
            " Loss SS33:  0.04430973547555152\n",
            " Loss SS44:  0.05828391281621797\n",
            " Loss SS55:  0.060979412425132024\n",
            " Loss SS66:  0.04898793853464581\n",
            " Loss SS77:  0.06532246956513041\n",
            " Loss SS88:  0.0532880737667992\n",
            " Loss SS11:  0.03459065775500565\n",
            " Loss SS22:  0.0466736479685074\n",
            " Loss SS33:  0.04440604913525465\n",
            " Loss SS44:  0.05815904324011105\n",
            " Loss SS55:  0.060848951430582415\n",
            " Loss SS66:  0.04894856590686775\n",
            " Loss SS77:  0.06486521570420847\n",
            " Loss SS88:  0.05373556575760609\n",
            " Loss SS11:  0.03446672759095176\n",
            " Loss SS22:  0.0462371402221625\n",
            " Loss SS33:  0.04377054929977558\n",
            " Loss SS44:  0.05780607840565384\n",
            " Loss SS55:  0.060003686574150304\n",
            " Loss SS66:  0.04880252046907534\n",
            " Loss SS77:  0.06423474255888188\n",
            " Loss SS88:  0.05305172319783539\n",
            " Loss SS11:  0.03444400940228392\n",
            " Loss SS22:  0.04617497205366323\n",
            " Loss SS33:  0.043859586073660556\n",
            " Loss SS44:  0.05776787402085316\n",
            " Loss SS55:  0.05980463496144907\n",
            " Loss SS66:  0.04867522758834156\n",
            " Loss SS77:  0.06392178601688808\n",
            " Loss SS88:  0.05299472427110613\n",
            "\n",
            "Epoch: 54\n",
            "Loss SS11:  0.03524769842624664\n",
            "Loss SS22:  0.0364890992641449\n",
            "Loss SS33:  0.03047395683825016\n",
            "Loss SS44:  0.04050915688276291\n",
            "Loss SS55:  0.04124964028596878\n",
            "Loss SS66:  0.03519708663225174\n",
            "Loss SS77:  0.04585098475217819\n",
            "Loss SS88:  0.0375014953315258\n",
            "Loss SS11:  0.03377997197888114\n",
            "Loss SS22:  0.031626668674024666\n",
            "Loss SS33:  0.028476633978160946\n",
            "Loss SS44:  0.03789565475149588\n",
            "Loss SS55:  0.039418735964731735\n",
            "Loss SS66:  0.03219870427115397\n",
            "Loss SS77:  0.04039282317865978\n",
            "Loss SS88:  0.0352501840415326\n",
            "Loss SS11:  0.03373128335390772\n",
            "Loss SS22:  0.0320528211692969\n",
            "Loss SS33:  0.02871722382094179\n",
            "Loss SS44:  0.03795955543007169\n",
            "Loss SS55:  0.04013697501449358\n",
            "Loss SS66:  0.03259850266788687\n",
            "Loss SS77:  0.04015918146996271\n",
            "Loss SS88:  0.035993761161253565\n",
            "Loss SS11:  0.03324273769413271\n",
            "Loss SS22:  0.0323553895878215\n",
            "Loss SS33:  0.028790096542046915\n",
            "Loss SS44:  0.037988151633931745\n",
            "Loss SS55:  0.039776686939500996\n",
            "Loss SS66:  0.032404511866550294\n",
            "Loss SS77:  0.0405569918934376\n",
            "Loss SS88:  0.036166481974144134\n",
            "Loss SS11:  0.033172178422895875\n",
            "Loss SS22:  0.03260986284330124\n",
            "Loss SS33:  0.028890027187582924\n",
            "Loss SS44:  0.03818798755727163\n",
            "Loss SS55:  0.040096372729394494\n",
            "Loss SS66:  0.03255663121618876\n",
            "Loss SS77:  0.04099481352945653\n",
            "Loss SS88:  0.03644240116019074\n",
            "Loss SS11:  0.03326374095152406\n",
            "Loss SS22:  0.032859714519159464\n",
            "Loss SS33:  0.029010892243069762\n",
            "Loss SS44:  0.03845320656603458\n",
            "Loss SS55:  0.040268746149890566\n",
            "Loss SS66:  0.03255471189086344\n",
            "Loss SS77:  0.040927610707049276\n",
            "Loss SS88:  0.03659821035084771\n",
            "Loss SS11:  0.033439403063938264\n",
            "Loss SS22:  0.03287292177193477\n",
            "Loss SS33:  0.02906660317275368\n",
            "Loss SS44:  0.0382666431489538\n",
            "Loss SS55:  0.04006514606661484\n",
            "Loss SS66:  0.03238095391969212\n",
            "Loss SS77:  0.04073822400609001\n",
            "Loss SS88:  0.036471499191200143\n",
            "Loss SS11:  0.03365634019735833\n",
            "Loss SS22:  0.032864129538057556\n",
            "Loss SS33:  0.029197592419427886\n",
            "Loss SS44:  0.03835384322094246\n",
            "Loss SS55:  0.040344753210813225\n",
            "Loss SS66:  0.03238028705015149\n",
            "Loss SS77:  0.04079738479684776\n",
            "Loss SS88:  0.036506220881997696\n",
            "Loss SS11:  0.033621398341140626\n",
            "Loss SS22:  0.03279357534591799\n",
            "Loss SS33:  0.029113952300430818\n",
            "Loss SS44:  0.03823331444535726\n",
            "Loss SS55:  0.03998584194499769\n",
            "Loss SS66:  0.03221855234400726\n",
            "Loss SS77:  0.040707702116097935\n",
            "Loss SS88:  0.03637847244555568\n",
            "Loss SS11:  0.033668404240365865\n",
            "Loss SS22:  0.03272269975271199\n",
            "Loss SS33:  0.029150204323641547\n",
            "Loss SS44:  0.0382468265245904\n",
            "Loss SS55:  0.039961444111643256\n",
            "Loss SS66:  0.03223852678150921\n",
            "Loss SS77:  0.04058107938412782\n",
            "Loss SS88:  0.0363324360696824\n",
            "Loss SS11:  0.033600847412011414\n",
            "Loss SS22:  0.03260414525497668\n",
            "Loss SS33:  0.029094806789319114\n",
            "Loss SS44:  0.03803955984882789\n",
            "Loss SS55:  0.0398573176843105\n",
            "Loss SS66:  0.03218324904763462\n",
            "Loss SS77:  0.04043489049124246\n",
            "Loss SS88:  0.03635475290293741\n",
            "Loss SS11:  0.03357749395408072\n",
            "Loss SS22:  0.03252372539996564\n",
            "Loss SS33:  0.029105871160690848\n",
            "Loss SS44:  0.0379800381860486\n",
            "Loss SS55:  0.039794121508125786\n",
            "Loss SS66:  0.03205656270320351\n",
            "Loss SS77:  0.040348343349791864\n",
            "Loss SS88:  0.036286192232961055\n",
            "Loss SS11:  0.03357188445169571\n",
            "Loss SS22:  0.032522826078386344\n",
            "Loss SS33:  0.029187947111435172\n",
            "Loss SS44:  0.03801461797176806\n",
            "Loss SS55:  0.03979966672491436\n",
            "Loss SS66:  0.03204266101983953\n",
            "Loss SS77:  0.04034701697836238\n",
            "Loss SS88:  0.03631320794320796\n",
            "Loss SS11:  0.03362414024713385\n",
            "Loss SS22:  0.032556619068593466\n",
            "Loss SS33:  0.029259166559417737\n",
            "Loss SS44:  0.03805749097000097\n",
            "Loss SS55:  0.03977588294002846\n",
            "Loss SS66:  0.03216100491242099\n",
            "Loss SS77:  0.04046079200756459\n",
            "Loss SS88:  0.03637692069222454\n",
            "Loss SS11:  0.03353710320665904\n",
            "Loss SS22:  0.03255168838651045\n",
            "Loss SS33:  0.029320327723596957\n",
            "Loss SS44:  0.038067720737968776\n",
            "Loss SS55:  0.039875028905927715\n",
            "Loss SS66:  0.0322654110425753\n",
            "Loss SS77:  0.04060579453271332\n",
            "Loss SS88:  0.03638878555849512\n",
            "Loss SS11:  0.033510484812867565\n",
            "Loss SS22:  0.03269211540849793\n",
            "Loss SS33:  0.029401705421458018\n",
            "Loss SS44:  0.03812576834087735\n",
            "Loss SS55:  0.03992786273261569\n",
            "Loss SS66:  0.03234060615646523\n",
            "Loss SS77:  0.04067108691330777\n",
            "Loss SS88:  0.03648797816146683\n",
            "Loss SS11:  0.033511164974166736\n",
            "Loss SS22:  0.03263725628440054\n",
            "Loss SS33:  0.029312870066080774\n",
            "Loss SS44:  0.03805657236703804\n",
            "Loss SS55:  0.039854096792499474\n",
            "Loss SS66:  0.03237635062671967\n",
            "Loss SS77:  0.040670132669417755\n",
            "Loss SS88:  0.036532877591548496\n",
            "Loss SS11:  0.03357256366190506\n",
            "Loss SS22:  0.03274406388140561\n",
            "Loss SS33:  0.029332653820863246\n",
            "Loss SS44:  0.03808340098033523\n",
            "Loss SS55:  0.03985179082779159\n",
            "Loss SS66:  0.03245048761934216\n",
            "Loss SS77:  0.04074640356396374\n",
            "Loss SS88:  0.036592732777285294\n",
            "Loss SS11:  0.033596540528519375\n",
            "Loss SS22:  0.03279083100718688\n",
            "Loss SS33:  0.02932660961883832\n",
            "Loss SS44:  0.03815063719961854\n",
            "Loss SS55:  0.03994625341661727\n",
            "Loss SS66:  0.032528510204006954\n",
            "Loss SS77:  0.040827059058359316\n",
            "Loss SS88:  0.03668590117431148\n",
            "Loss SS11:  0.03362146458778706\n",
            "Loss SS22:  0.032816420011807484\n",
            "Loss SS33:  0.029296478256583214\n",
            "Loss SS44:  0.03806679472522274\n",
            "Loss SS55:  0.039941328826375036\n",
            "Loss SS66:  0.03253045699593284\n",
            "Loss SS77:  0.040845536612247296\n",
            "Loss SS88:  0.03667076769728623\n",
            "Loss SS11:  0.033619791584376675\n",
            "Loss SS22:  0.032818805926771305\n",
            "Loss SS33:  0.029315242840357088\n",
            "Loss SS44:  0.03802559586862723\n",
            "Loss SS55:  0.039896549480916255\n",
            "Loss SS66:  0.032505190009204904\n",
            "Loss SS77:  0.040799762312304325\n",
            "Loss SS88:  0.03667812639689861\n",
            "Loss SS11:  0.03363650177397999\n",
            "Loss SS22:  0.03281820475419551\n",
            "Loss SS33:  0.029349910889827243\n",
            "Loss SS44:  0.03807008976196226\n",
            "Loss SS55:  0.039919213378598904\n",
            "Loss SS66:  0.0325533749085467\n",
            "Loss SS77:  0.040847246327671394\n",
            "Loss SS88:  0.03672215899549672\n",
            "Loss SS11:  0.033634973479207284\n",
            "Loss SS22:  0.03283176735481795\n",
            "Loss SS33:  0.029364398743839285\n",
            "Loss SS44:  0.03811718570700598\n",
            "Loss SS55:  0.03994790855710863\n",
            "Loss SS66:  0.03252767488292979\n",
            "Loss SS77:  0.04087792518991151\n",
            "Loss SS88:  0.0367620094548658\n",
            "Loss SS11:  0.033622719395857356\n",
            "Loss SS22:  0.03283764302601546\n",
            "Loss SS33:  0.0293693904446446\n",
            "Loss SS44:  0.03812654513172257\n",
            "Loss SS55:  0.039913010142453305\n",
            "Loss SS66:  0.03252271324138105\n",
            "Loss SS77:  0.040904759167334735\n",
            "Loss SS88:  0.03676453193254543\n",
            "Loss SS11:  0.03367431222217706\n",
            "Loss SS22:  0.032918992541204846\n",
            "Loss SS33:  0.029387209487159224\n",
            "Loss SS44:  0.038210633554770244\n",
            "Loss SS55:  0.03998583208970509\n",
            "Loss SS66:  0.03254833860763376\n",
            "Loss SS77:  0.04097760626389278\n",
            "Loss SS88:  0.036789437554993074\n",
            "Loss SS11:  0.03368042197182359\n",
            "Loss SS22:  0.032907084803063556\n",
            "Loss SS33:  0.029380657906788753\n",
            "Loss SS44:  0.03817869397807881\n",
            "Loss SS55:  0.03995868740269387\n",
            "Loss SS66:  0.0325849630547116\n",
            "Loss SS77:  0.040996347720998215\n",
            "Loss SS88:  0.036826541721524\n",
            "Loss SS11:  0.0336508492999835\n",
            "Loss SS22:  0.03287734987635265\n",
            "Loss SS33:  0.0293796099579654\n",
            "Loss SS44:  0.03817915730892013\n",
            "Loss SS55:  0.03999980155135937\n",
            "Loss SS66:  0.03256820410604221\n",
            "Loss SS77:  0.04103519533443268\n",
            "Loss SS88:  0.03683654763432526\n",
            "Loss SS11:  0.033616800046769896\n",
            "Loss SS22:  0.03285428183325102\n",
            "Loss SS33:  0.02933673297374671\n",
            "Loss SS44:  0.03815156481197839\n",
            "Loss SS55:  0.04000359629298048\n",
            "Loss SS66:  0.03256820546415019\n",
            "Loss SS77:  0.041019985816575504\n",
            "Loss SS88:  0.036809635221463285\n",
            "Loss SS11:  0.03356332807831493\n",
            "Loss SS22:  0.032848254396344846\n",
            "Loss SS33:  0.029338237222986712\n",
            "Loss SS44:  0.038148059884212195\n",
            "Loss SS55:  0.04004582931085413\n",
            "Loss SS66:  0.0325862561415523\n",
            "Loss SS77:  0.040999601333272837\n",
            "Loss SS88:  0.03682136170406995\n",
            "Loss SS11:  0.03351804653752301\n",
            "Loss SS22:  0.032865350637136866\n",
            "Loss SS33:  0.029342071316444996\n",
            "Loss SS44:  0.03813424326249005\n",
            "Loss SS55:  0.04008668749449179\n",
            "Loss SS66:  0.03256907742918562\n",
            "Loss SS77:  0.04103574049227016\n",
            "Loss SS88:  0.036838670065978546\n",
            "Loss SS11:  0.03358833022737424\n",
            "Loss SS22:  0.032880434054769946\n",
            "Loss SS33:  0.02934429290212468\n",
            "Loss SS44:  0.03814318847913679\n",
            "Loss SS55:  0.04011129204220946\n",
            "Loss SS66:  0.03261397948544287\n",
            "Loss SS77:  0.041056038768485535\n",
            "Loss SS88:  0.036841769866719595\n",
            "Loss SS11:  0.03359717584044412\n",
            "Loss SS22:  0.032863078549073055\n",
            "Loss SS33:  0.02932004507786783\n",
            "Loss SS44:  0.03813507322856851\n",
            "Loss SS55:  0.04009804717741212\n",
            "Loss SS66:  0.03258128037191089\n",
            "Loss SS77:  0.04102541284689566\n",
            "Loss SS88:  0.036811622099238196\n",
            "Loss SS11:  0.03356254798856294\n",
            "Loss SS22:  0.03284155508369857\n",
            "Loss SS33:  0.02931916846594892\n",
            "Loss SS44:  0.0381263862610606\n",
            "Loss SS55:  0.04010926482972698\n",
            "Loss SS66:  0.03257512994040953\n",
            "Loss SS77:  0.04106620412517188\n",
            "Loss SS88:  0.03683440952425434\n",
            "Loss SS11:  0.033581836903158274\n",
            "Loss SS22:  0.032809670688162394\n",
            "Loss SS33:  0.02930958989973875\n",
            "Loss SS44:  0.03814150678877384\n",
            "Loss SS55:  0.04011318975569979\n",
            "Loss SS66:  0.03259255902509675\n",
            "Loss SS77:  0.041119815280880455\n",
            "Loss SS88:  0.03684958045756349\n",
            "Loss SS11:  0.03361720251482078\n",
            "Loss SS22:  0.03283275862680502\n",
            "Loss SS33:  0.029341601321468257\n",
            "Loss SS44:  0.03816154766336215\n",
            "Loss SS55:  0.04013713945216797\n",
            "Loss SS66:  0.03261931087614155\n",
            "Loss SS77:  0.04119323845133404\n",
            "Loss SS88:  0.03690657028354857\n",
            "Loss SS11:  0.033630831198941946\n",
            "Loss SS22:  0.03286243131400174\n",
            "Loss SS33:  0.029361460395051203\n",
            "Loss SS44:  0.038161161979209324\n",
            "Loss SS55:  0.04016276838475483\n",
            "Loss SS66:  0.03262477285862818\n",
            "Loss SS77:  0.04123347887286433\n",
            "Loss SS88:  0.036899747990305265\n",
            "Loss SS11:  0.03362759424069086\n",
            "Loss SS22:  0.03285053933240535\n",
            "Loss SS33:  0.029368069167364997\n",
            "Loss SS44:  0.03816264944145884\n",
            "Loss SS55:  0.040146467933776964\n",
            "Loss SS66:  0.03262770737307224\n",
            "Loss SS77:  0.041206455792086275\n",
            "Loss SS88:  0.036903917087742495\n",
            "Loss SS11:  0.03364295926637887\n",
            "Loss SS22:  0.03286038895421272\n",
            "Loss SS33:  0.029358129847965472\n",
            "Loss SS44:  0.03814651287911395\n",
            "Loss SS55:  0.04017192481783522\n",
            "Loss SS66:  0.03263850073829655\n",
            "Loss SS77:  0.04119698090774994\n",
            "Loss SS88:  0.0368784728618163\n",
            "Loss SS11:  0.03363608951702362\n",
            "Loss SS22:  0.032851884684224765\n",
            "Loss SS33:  0.02933960684191367\n",
            "Loss SS44:  0.03812034634428387\n",
            "Loss SS55:  0.040160286712052004\n",
            "Loss SS66:  0.03264226493110338\n",
            "Loss SS77:  0.041129239998620955\n",
            "Loss SS88:  0.03685749848214347\n",
            "Loss SS11:  0.03364680330638233\n",
            "Loss SS22:  0.03284164976395305\n",
            "Loss SS33:  0.029342034536287608\n",
            "Loss SS44:  0.038120066389784484\n",
            "Loss SS55:  0.04013429266755538\n",
            "Loss SS66:  0.03261569980770121\n",
            "Loss SS77:  0.04111192124845731\n",
            "Loss SS88:  0.036831796578967665\n",
            "Loss SS11:  0.033681021557373\n",
            "Loss SS22:  0.03285451035045477\n",
            "Loss SS33:  0.02935646565403427\n",
            "Loss SS44:  0.03816547762240258\n",
            "Loss SS55:  0.040140744103606504\n",
            "Loss SS66:  0.032659803654190314\n",
            "Loss SS77:  0.04112373916734186\n",
            "Loss SS88:  0.03683794775694386\n",
            "Loss SS11:  0.0336523333478293\n",
            "Loss SS22:  0.03283291305539962\n",
            "Loss SS33:  0.029345358855605415\n",
            "Loss SS44:  0.03816488784015034\n",
            "Loss SS55:  0.040117861716634166\n",
            "Loss SS66:  0.0326367235015126\n",
            "Loss SS77:  0.041092766960969515\n",
            "Loss SS88:  0.03678075088201648\n",
            "Loss SS11:  0.033651145670624924\n",
            "Loss SS22:  0.032843485151610015\n",
            "Loss SS33:  0.029368355768487175\n",
            "Loss SS44:  0.03818753702375781\n",
            "Loss SS55:  0.040135128357206555\n",
            "Loss SS66:  0.032656097426267246\n",
            "Loss SS77:  0.04108613649838894\n",
            "Loss SS88:  0.036785674257084476\n",
            "Loss SS11:  0.033639484173737105\n",
            "Loss SS22:  0.03282653302248144\n",
            "Loss SS33:  0.02935589461429628\n",
            "Loss SS44:  0.0381868777239129\n",
            "Loss SS55:  0.04012794653493954\n",
            "Loss SS66:  0.03263854975714899\n",
            "Loss SS77:  0.04105246765431008\n",
            "Loss SS88:  0.0367710659639119\n",
            "Loss SS11:  0.033623722810593865\n",
            "Loss SS22:  0.03282237650991297\n",
            "Loss SS33:  0.029342607415519875\n",
            "Loss SS44:  0.03818268058589256\n",
            "Loss SS55:  0.04011894661147578\n",
            "Loss SS66:  0.03261760572001102\n",
            "Loss SS77:  0.041031566530871554\n",
            "Loss SS88:  0.03675951341973816\n",
            "Loss SS11:  0.03365635427908596\n",
            "Loss SS22:  0.03282437105136542\n",
            "Loss SS33:  0.029355359022111165\n",
            "Loss SS44:  0.03821359670611549\n",
            "Loss SS55:  0.04015133847914091\n",
            "Loss SS66:  0.032614397867985416\n",
            "Loss SS77:  0.04103104633892455\n",
            "Loss SS88:  0.036778535797134734\n",
            "Loss SS11:  0.03366147016059939\n",
            "Loss SS22:  0.03283044589994528\n",
            "Loss SS33:  0.029343634722612942\n",
            "Loss SS44:  0.03819743365275627\n",
            "Loss SS55:  0.0401530968926346\n",
            "Loss SS66:  0.03260774632063321\n",
            "Loss SS77:  0.04102762568733956\n",
            "Loss SS88:  0.03678569492203165\n",
            "Loss SS11:  0.03368760863291357\n",
            "Loss SS22:  0.03282976695860394\n",
            "Loss SS33:  0.029321544141273338\n",
            "Loss SS44:  0.03819997751820366\n",
            "Loss SS55:  0.040134755487211696\n",
            "Loss SS66:  0.03259039042228853\n",
            "Loss SS77:  0.04100303682488747\n",
            "Loss SS88:  0.03677068683150721\n",
            "Loss SS11:  0.03371852290528108\n",
            "Loss SS22:  0.03282024910026801\n",
            "Loss SS33:  0.029313713393470355\n",
            "Loss SS44:  0.03817954293903343\n",
            "Loss SS55:  0.04012640879301668\n",
            "Loss SS66:  0.032600508669774646\n",
            "Loss SS77:  0.04100509449498817\n",
            "Loss SS88:  0.03677118164554951\n",
            "Loss SS11:  0.03369239509454691\n",
            "Loss SS22:  0.032781676501187434\n",
            "Loss SS33:  0.029284568040682926\n",
            "Loss SS44:  0.03814050842782145\n",
            "Loss SS55:  0.04009466273084192\n",
            "Loss SS66:  0.03256968355333611\n",
            "Loss SS77:  0.040979846196662624\n",
            "Loss SS88:  0.03671077129387443\n",
            "Validation: \n",
            " Loss SS11:  0.03220179304480553\n",
            " Loss SS22:  0.04201308265328407\n",
            " Loss SS33:  0.03764287382364273\n",
            " Loss SS44:  0.05029505491256714\n",
            " Loss SS55:  0.05022227019071579\n",
            " Loss SS66:  0.04095084220170975\n",
            " Loss SS77:  0.055100467056035995\n",
            " Loss SS88:  0.04051719605922699\n",
            " Loss SS11:  0.034083357790396326\n",
            " Loss SS22:  0.04857131271135239\n",
            " Loss SS33:  0.043569675336281456\n",
            " Loss SS44:  0.05892299399489448\n",
            " Loss SS55:  0.06082074255460784\n",
            " Loss SS66:  0.049197187913315635\n",
            " Loss SS77:  0.06409326976253873\n",
            " Loss SS88:  0.05216438216822488\n",
            " Loss SS11:  0.03402200286708227\n",
            " Loss SS22:  0.04826808130232299\n",
            " Loss SS33:  0.043659496143823716\n",
            " Loss SS44:  0.058609633365782295\n",
            " Loss SS55:  0.060739078081962536\n",
            " Loss SS66:  0.04930709929364484\n",
            " Loss SS77:  0.06401580981001621\n",
            " Loss SS88:  0.05231638470800912\n",
            " Loss SS11:  0.0338528569115967\n",
            " Loss SS22:  0.04791037127619884\n",
            " Loss SS33:  0.04302916443738781\n",
            " Loss SS44:  0.05834170504183066\n",
            " Loss SS55:  0.05988980634290664\n",
            " Loss SS66:  0.04911983495608705\n",
            " Loss SS77:  0.06347468568653357\n",
            " Loss SS88:  0.05178922634632861\n",
            " Loss SS11:  0.03398069178248629\n",
            " Loss SS22:  0.04787545287866651\n",
            " Loss SS33:  0.04319352748585336\n",
            " Loss SS44:  0.05844965595522045\n",
            " Loss SS55:  0.059567320356030524\n",
            " Loss SS66:  0.048933994944816754\n",
            " Loss SS77:  0.06332624979593136\n",
            " Loss SS88:  0.05179313510472392\n",
            "\n",
            "Epoch: 55\n",
            "Loss SS11:  0.03452935814857483\n",
            "Loss SS22:  0.03501471132040024\n",
            "Loss SS33:  0.033458974212408066\n",
            "Loss SS44:  0.042267922312021255\n",
            "Loss SS55:  0.0411229282617569\n",
            "Loss SS66:  0.037782736122608185\n",
            "Loss SS77:  0.04692855849862099\n",
            "Loss SS88:  0.04138490557670593\n",
            "Loss SS11:  0.033858593214641915\n",
            "Loss SS22:  0.03229616345329718\n",
            "Loss SS33:  0.02905667166818272\n",
            "Loss SS44:  0.03820496221834963\n",
            "Loss SS55:  0.0396719906817783\n",
            "Loss SS66:  0.03189669380133802\n",
            "Loss SS77:  0.04079136252403259\n",
            "Loss SS88:  0.03618708388371901\n",
            "Loss SS11:  0.0337466284455288\n",
            "Loss SS22:  0.03248067776716891\n",
            "Loss SS33:  0.029043470997185933\n",
            "Loss SS44:  0.038300297799564544\n",
            "Loss SS55:  0.0398828224057243\n",
            "Loss SS66:  0.03238725839626221\n",
            "Loss SS77:  0.040782866023835684\n",
            "Loss SS88:  0.0368460187954562\n",
            "Loss SS11:  0.03338885187141357\n",
            "Loss SS22:  0.03265120071028509\n",
            "Loss SS33:  0.028881901995308937\n",
            "Loss SS44:  0.03818129700037741\n",
            "Loss SS55:  0.03959934389398944\n",
            "Loss SS66:  0.03229098905238413\n",
            "Loss SS77:  0.04063020694640375\n",
            "Loss SS88:  0.036687722850230434\n",
            "Loss SS11:  0.03353528505781802\n",
            "Loss SS22:  0.032665077324320634\n",
            "Loss SS33:  0.028868621578667222\n",
            "Loss SS44:  0.03821792044654125\n",
            "Loss SS55:  0.039820745133045246\n",
            "Loss SS66:  0.03246535878719353\n",
            "Loss SS77:  0.04042885924984769\n",
            "Loss SS88:  0.036724097935891736\n",
            "Loss SS11:  0.033581924716047214\n",
            "Loss SS22:  0.0327932940905585\n",
            "Loss SS33:  0.029004982544803153\n",
            "Loss SS44:  0.038501150686951244\n",
            "Loss SS55:  0.04020787768212019\n",
            "Loss SS66:  0.032594219434495066\n",
            "Loss SS77:  0.0405808743454662\n",
            "Loss SS88:  0.036757416950137005\n",
            "Loss SS11:  0.033655785695939765\n",
            "Loss SS22:  0.032732281101043106\n",
            "Loss SS33:  0.028948079397688148\n",
            "Loss SS44:  0.038331687206127604\n",
            "Loss SS55:  0.040032483331981256\n",
            "Loss SS66:  0.03241339252620447\n",
            "Loss SS77:  0.040420967108402095\n",
            "Loss SS88:  0.03656673266506586\n",
            "Loss SS11:  0.0338525230353567\n",
            "Loss SS22:  0.03272176783164622\n",
            "Loss SS33:  0.029114602553382725\n",
            "Loss SS44:  0.038523577206151585\n",
            "Loss SS55:  0.04005485993455833\n",
            "Loss SS66:  0.03244346251685015\n",
            "Loss SS77:  0.0404735750085871\n",
            "Loss SS88:  0.03678969435498748\n",
            "Loss SS11:  0.03377409723161915\n",
            "Loss SS22:  0.0325999456561274\n",
            "Loss SS33:  0.029008701926580182\n",
            "Loss SS44:  0.03839807451507191\n",
            "Loss SS55:  0.039841173147713696\n",
            "Loss SS66:  0.032456425972926764\n",
            "Loss SS77:  0.040430028911358044\n",
            "Loss SS88:  0.036610851961153525\n",
            "Loss SS11:  0.03367968167659346\n",
            "Loss SS22:  0.03248753613577439\n",
            "Loss SS33:  0.028954394980446323\n",
            "Loss SS44:  0.038235996901006486\n",
            "Loss SS55:  0.03983361286285159\n",
            "Loss SS66:  0.032270915396920924\n",
            "Loss SS77:  0.04026788104693968\n",
            "Loss SS88:  0.03653476776166276\n",
            "Loss SS11:  0.03356843391931293\n",
            "Loss SS22:  0.03240826119216952\n",
            "Loss SS33:  0.028849936966406236\n",
            "Loss SS44:  0.03808039755071744\n",
            "Loss SS55:  0.03979931495124751\n",
            "Loss SS66:  0.03222348185090145\n",
            "Loss SS77:  0.0401055479034929\n",
            "Loss SS88:  0.03642782561554767\n",
            "Loss SS11:  0.03359864960919629\n",
            "Loss SS22:  0.032410096259670215\n",
            "Loss SS33:  0.028857362079056533\n",
            "Loss SS44:  0.0380232471402164\n",
            "Loss SS55:  0.039837270975112915\n",
            "Loss SS66:  0.03214584938711948\n",
            "Loss SS77:  0.03997956471400218\n",
            "Loss SS88:  0.03638396002687849\n",
            "Loss SS11:  0.03353480069536315\n",
            "Loss SS22:  0.03247301274341\n",
            "Loss SS33:  0.028909351599733693\n",
            "Loss SS44:  0.03803647211021628\n",
            "Loss SS55:  0.0398519618575238\n",
            "Loss SS66:  0.03218005278940536\n",
            "Loss SS77:  0.039992075936853395\n",
            "Loss SS88:  0.036433928977113124\n",
            "Loss SS11:  0.033520850544894926\n",
            "Loss SS22:  0.032557431395158515\n",
            "Loss SS33:  0.028990600798421234\n",
            "Loss SS44:  0.03806309973579327\n",
            "Loss SS55:  0.03996040140285747\n",
            "Loss SS66:  0.03227492549833451\n",
            "Loss SS77:  0.040040207614425484\n",
            "Loss SS88:  0.036526862531900406\n",
            "Loss SS11:  0.033500956133642096\n",
            "Loss SS22:  0.0325818842818551\n",
            "Loss SS33:  0.029038922491331474\n",
            "Loss SS44:  0.03815232957085819\n",
            "Loss SS55:  0.04005855998248919\n",
            "Loss SS66:  0.03227427673149616\n",
            "Loss SS77:  0.04012604501653225\n",
            "Loss SS88:  0.03661113610187321\n",
            "Loss SS11:  0.033457189341945366\n",
            "Loss SS22:  0.03264258956129582\n",
            "Loss SS33:  0.02910000639690074\n",
            "Loss SS44:  0.03818927016972706\n",
            "Loss SS55:  0.04008604450435038\n",
            "Loss SS66:  0.03231827330016932\n",
            "Loss SS77:  0.04021452986542752\n",
            "Loss SS88:  0.036651464758921934\n",
            "Loss SS11:  0.033424857367547404\n",
            "Loss SS22:  0.032633588805517054\n",
            "Loss SS33:  0.029071044211524615\n",
            "Loss SS44:  0.03815413421447973\n",
            "Loss SS55:  0.03999888519785419\n",
            "Loss SS66:  0.03235790934066595\n",
            "Loss SS77:  0.04031897711087458\n",
            "Loss SS88:  0.03667911327218417\n",
            "Loss SS11:  0.03342891443106863\n",
            "Loss SS22:  0.03269626704529364\n",
            "Loss SS33:  0.0291178618933548\n",
            "Loss SS44:  0.03817397312462678\n",
            "Loss SS55:  0.04001972456163133\n",
            "Loss SS66:  0.03239630548316136\n",
            "Loss SS77:  0.04043906208193093\n",
            "Loss SS88:  0.03670090704894902\n",
            "Loss SS11:  0.03342080075108544\n",
            "Loss SS22:  0.032814767180803076\n",
            "Loss SS33:  0.029088425506626703\n",
            "Loss SS44:  0.038230229498437754\n",
            "Loss SS55:  0.04006289340843812\n",
            "Loss SS66:  0.03245916596269081\n",
            "Loss SS77:  0.04054501998490392\n",
            "Loss SS88:  0.03675005611711444\n",
            "Loss SS11:  0.03339349953408953\n",
            "Loss SS22:  0.0327933090714572\n",
            "Loss SS33:  0.02905802663747241\n",
            "Loss SS44:  0.03812951680409347\n",
            "Loss SS55:  0.0400074054121347\n",
            "Loss SS66:  0.032424485193887305\n",
            "Loss SS77:  0.04055720062546081\n",
            "Loss SS88:  0.03669433865247597\n",
            "Loss SS11:  0.03343230836204628\n",
            "Loss SS22:  0.032810217223653744\n",
            "Loss SS33:  0.02907518862714222\n",
            "Loss SS44:  0.03812235152691751\n",
            "Loss SS55:  0.04001958401345495\n",
            "Loss SS66:  0.03245031972652051\n",
            "Loss SS77:  0.04054184904813173\n",
            "Loss SS88:  0.036727465781851196\n",
            "Loss SS11:  0.0335048539696429\n",
            "Loss SS22:  0.03279740304213847\n",
            "Loss SS33:  0.029102446220073654\n",
            "Loss SS44:  0.038108184933662415\n",
            "Loss SS55:  0.040002607056314914\n",
            "Loss SS66:  0.032444352575388\n",
            "Loss SS77:  0.040601245917697655\n",
            "Loss SS88:  0.036750799777665974\n",
            "Loss SS11:  0.03350087535057672\n",
            "Loss SS22:  0.0327946106351204\n",
            "Loss SS33:  0.029126746596861208\n",
            "Loss SS44:  0.03814592269750742\n",
            "Loss SS55:  0.040025519906665405\n",
            "Loss SS66:  0.03245643680552821\n",
            "Loss SS77:  0.040632508071434446\n",
            "Loss SS88:  0.03676492363484197\n",
            "Loss SS11:  0.03348975471378147\n",
            "Loss SS22:  0.032761091350283454\n",
            "Loss SS33:  0.029135164921675925\n",
            "Loss SS44:  0.03812579530013072\n",
            "Loss SS55:  0.04000310693339352\n",
            "Loss SS66:  0.032483443738771725\n",
            "Loss SS77:  0.04065956517215415\n",
            "Loss SS88:  0.036746466379145006\n",
            "Loss SS11:  0.0335345040094803\n",
            "Loss SS22:  0.03279737766476588\n",
            "Loss SS33:  0.029145605699835476\n",
            "Loss SS44:  0.038232748918389875\n",
            "Loss SS55:  0.040032955466217025\n",
            "Loss SS66:  0.03254191204132628\n",
            "Loss SS77:  0.040728379844753575\n",
            "Loss SS88:  0.03673413496356288\n",
            "Loss SS11:  0.03351797743058537\n",
            "Loss SS22:  0.03284340740674995\n",
            "Loss SS33:  0.029175002610244125\n",
            "Loss SS44:  0.03820611376211463\n",
            "Loss SS55:  0.04002775618457699\n",
            "Loss SS66:  0.03254705298677146\n",
            "Loss SS77:  0.04076372306660352\n",
            "Loss SS88:  0.0368149453603414\n",
            "Loss SS11:  0.03349524555135504\n",
            "Loss SS22:  0.0328128463963325\n",
            "Loss SS33:  0.029189286817764414\n",
            "Loss SS44:  0.03821104542277325\n",
            "Loss SS55:  0.04004727897744526\n",
            "Loss SS66:  0.03255629376775917\n",
            "Loss SS77:  0.04075902563402022\n",
            "Loss SS88:  0.036838154483343905\n",
            "Loss SS11:  0.033464242666409905\n",
            "Loss SS22:  0.03276738215141631\n",
            "Loss SS33:  0.029160554696053158\n",
            "Loss SS44:  0.038148396005819644\n",
            "Loss SS55:  0.040014967666011016\n",
            "Loss SS66:  0.032479134639140865\n",
            "Loss SS77:  0.04077573473878012\n",
            "Loss SS88:  0.036769945882398264\n",
            "Loss SS11:  0.03344835543812806\n",
            "Loss SS22:  0.0327244712457432\n",
            "Loss SS33:  0.029164284056722056\n",
            "Loss SS44:  0.03811211430623438\n",
            "Loss SS55:  0.04001311605193013\n",
            "Loss SS66:  0.032464698467443424\n",
            "Loss SS77:  0.04073228766591523\n",
            "Loss SS88:  0.036748483624587706\n",
            "Loss SS11:  0.03345804838175626\n",
            "Loss SS22:  0.032728215148256405\n",
            "Loss SS33:  0.029196042068225823\n",
            "Loss SS44:  0.03811349388199164\n",
            "Loss SS55:  0.0400573458407343\n",
            "Loss SS66:  0.03249613449771175\n",
            "Loss SS77:  0.040756235315218006\n",
            "Loss SS88:  0.036745515789619015\n",
            "Loss SS11:  0.03344379790572827\n",
            "Loss SS22:  0.032735740642165424\n",
            "Loss SS33:  0.029200349361991566\n",
            "Loss SS44:  0.03818432288708481\n",
            "Loss SS55:  0.04005178411884165\n",
            "Loss SS66:  0.03249141639500757\n",
            "Loss SS77:  0.04074272921810118\n",
            "Loss SS88:  0.03673768374586224\n",
            "Loss SS11:  0.033463527601200284\n",
            "Loss SS22:  0.03268736885962379\n",
            "Loss SS33:  0.029172363885681344\n",
            "Loss SS44:  0.038161855586280394\n",
            "Loss SS55:  0.04003938144953305\n",
            "Loss SS66:  0.03246466869973485\n",
            "Loss SS77:  0.04071055065205626\n",
            "Loss SS88:  0.03665435805892829\n",
            "Loss SS11:  0.033476443015433545\n",
            "Loss SS22:  0.03269539860503696\n",
            "Loss SS33:  0.029192677649707066\n",
            "Loss SS44:  0.03818697943076538\n",
            "Loss SS55:  0.04007113001938921\n",
            "Loss SS66:  0.03251991346130304\n",
            "Loss SS77:  0.04077107703017297\n",
            "Loss SS88:  0.03669487067564997\n",
            "Loss SS11:  0.03345303829995707\n",
            "Loss SS22:  0.0326867128168078\n",
            "Loss SS33:  0.02918672211625785\n",
            "Loss SS44:  0.038192865801721904\n",
            "Loss SS55:  0.040058798446486\n",
            "Loss SS66:  0.03253347917918532\n",
            "Loss SS77:  0.040780947692804825\n",
            "Loss SS88:  0.036715917829480416\n",
            "Loss SS11:  0.03350594892682743\n",
            "Loss SS22:  0.03269819582640426\n",
            "Loss SS33:  0.029194040174795384\n",
            "Loss SS44:  0.03822102587768409\n",
            "Loss SS55:  0.040099543123301176\n",
            "Loss SS66:  0.03255148684261831\n",
            "Loss SS77:  0.04080646509124387\n",
            "Loss SS88:  0.036737696166611836\n",
            "Loss SS11:  0.03351314835505098\n",
            "Loss SS22:  0.03268927987930272\n",
            "Loss SS33:  0.029207792578854114\n",
            "Loss SS44:  0.03821942723884202\n",
            "Loss SS55:  0.04011389689567762\n",
            "Loss SS66:  0.03259270932748277\n",
            "Loss SS77:  0.040803050022689025\n",
            "Loss SS88:  0.036757318170321976\n",
            "Loss SS11:  0.03353931809520127\n",
            "Loss SS22:  0.03268622442914838\n",
            "Loss SS33:  0.029218964908179153\n",
            "Loss SS44:  0.038261547746585675\n",
            "Loss SS55:  0.04012397055480619\n",
            "Loss SS66:  0.03259971511867568\n",
            "Loss SS77:  0.04083140066455936\n",
            "Loss SS88:  0.036742909236132604\n",
            "Loss SS11:  0.03352676562944834\n",
            "Loss SS22:  0.032646614259907177\n",
            "Loss SS33:  0.029202558396439347\n",
            "Loss SS44:  0.038243428485171815\n",
            "Loss SS55:  0.040134173358547076\n",
            "Loss SS66:  0.03258259939356152\n",
            "Loss SS77:  0.04082783420050562\n",
            "Loss SS88:  0.03671766411904698\n",
            "Loss SS11:  0.03353483580893732\n",
            "Loss SS22:  0.032637343597732814\n",
            "Loss SS33:  0.0292134769211995\n",
            "Loss SS44:  0.038218342364225485\n",
            "Loss SS55:  0.040117036404572134\n",
            "Loss SS66:  0.032561905256758526\n",
            "Loss SS77:  0.040797383199370124\n",
            "Loss SS88:  0.036684334942987895\n",
            "Loss SS11:  0.033520367430032366\n",
            "Loss SS22:  0.03260184545784503\n",
            "Loss SS33:  0.029189359763508563\n",
            "Loss SS44:  0.03819610407132932\n",
            "Loss SS55:  0.040083189592565724\n",
            "Loss SS66:  0.03251840545298041\n",
            "Loss SS77:  0.04074837066366545\n",
            "Loss SS88:  0.036646517846361755\n",
            "Loss SS11:  0.0335085840798227\n",
            "Loss SS22:  0.03260390391280675\n",
            "Loss SS33:  0.029217823020686533\n",
            "Loss SS44:  0.03824547802718203\n",
            "Loss SS55:  0.04009779241688531\n",
            "Loss SS66:  0.032536028775193744\n",
            "Loss SS77:  0.040755199681865306\n",
            "Loss SS88:  0.03666751868929946\n",
            "Loss SS11:  0.03348156862831029\n",
            "Loss SS22:  0.032612643220491365\n",
            "Loss SS33:  0.029221042782213276\n",
            "Loss SS44:  0.03822963910490057\n",
            "Loss SS55:  0.04011447859107723\n",
            "Loss SS66:  0.032518785909144546\n",
            "Loss SS77:  0.04076082451572673\n",
            "Loss SS88:  0.03666336072151098\n",
            "Loss SS11:  0.03349208843290381\n",
            "Loss SS22:  0.03262697482778029\n",
            "Loss SS33:  0.02922111018748861\n",
            "Loss SS44:  0.03822431864654933\n",
            "Loss SS55:  0.0401503948534611\n",
            "Loss SS66:  0.03251771235533253\n",
            "Loss SS77:  0.04078795626370187\n",
            "Loss SS88:  0.03669016547235627\n",
            "Loss SS11:  0.03346767560892205\n",
            "Loss SS22:  0.032598527597820955\n",
            "Loss SS33:  0.029229095395818663\n",
            "Loss SS44:  0.038221904752979145\n",
            "Loss SS55:  0.04015032826920673\n",
            "Loss SS66:  0.03251642680655638\n",
            "Loss SS77:  0.04077981055439763\n",
            "Loss SS88:  0.03667838231870027\n",
            "Loss SS11:  0.0334669035899531\n",
            "Loss SS22:  0.032599854382291404\n",
            "Loss SS33:  0.0292381353194819\n",
            "Loss SS44:  0.038203531973351156\n",
            "Loss SS55:  0.04013396442240598\n",
            "Loss SS66:  0.032483501647030955\n",
            "Loss SS77:  0.04076141639361306\n",
            "Loss SS88:  0.03669916197252111\n",
            "Loss SS11:  0.03348071921922415\n",
            "Loss SS22:  0.032609780981905444\n",
            "Loss SS33:  0.029253817103522317\n",
            "Loss SS44:  0.03819506310603835\n",
            "Loss SS55:  0.04013619723909446\n",
            "Loss SS66:  0.0324822520494395\n",
            "Loss SS77:  0.040751108895582526\n",
            "Loss SS88:  0.0367118762331104\n",
            "Loss SS11:  0.03347720296289812\n",
            "Loss SS22:  0.03259692556444967\n",
            "Loss SS33:  0.029255706994691277\n",
            "Loss SS44:  0.038184864931374206\n",
            "Loss SS55:  0.04014441606926039\n",
            "Loss SS66:  0.032473352796311236\n",
            "Loss SS77:  0.04074708654377569\n",
            "Loss SS88:  0.036718397338646354\n",
            "Loss SS11:  0.03347286140652978\n",
            "Loss SS22:  0.032582027315618885\n",
            "Loss SS33:  0.029240558880940872\n",
            "Loss SS44:  0.038148511790387696\n",
            "Loss SS55:  0.040135815573535905\n",
            "Loss SS66:  0.03247747096903385\n",
            "Loss SS77:  0.040738607209073005\n",
            "Loss SS88:  0.036700939731929456\n",
            "Loss SS11:  0.033447193775690026\n",
            "Loss SS22:  0.03258362961952379\n",
            "Loss SS33:  0.02923770025449954\n",
            "Loss SS44:  0.03814500871960562\n",
            "Loss SS55:  0.040124695143989614\n",
            "Loss SS66:  0.03248136751198843\n",
            "Loss SS77:  0.04073995444730017\n",
            "Loss SS88:  0.036705682755074236\n",
            "Loss SS11:  0.033441601438702\n",
            "Loss SS22:  0.03256842042575543\n",
            "Loss SS33:  0.02923015785420621\n",
            "Loss SS44:  0.038125271599762554\n",
            "Loss SS55:  0.04010399597148789\n",
            "Loss SS66:  0.0324700246403817\n",
            "Loss SS77:  0.040742852211059236\n",
            "Loss SS88:  0.03669684716403606\n",
            "Validation: \n",
            " Loss SS11:  0.03136158362030983\n",
            " Loss SS22:  0.04064279794692993\n",
            " Loss SS33:  0.03862937539815903\n",
            " Loss SS44:  0.0504877083003521\n",
            " Loss SS55:  0.05041667819023132\n",
            " Loss SS66:  0.04310177266597748\n",
            " Loss SS77:  0.05668213218450546\n",
            " Loss SS88:  0.041636254638433456\n",
            " Loss SS11:  0.0330162967244784\n",
            " Loss SS22:  0.04714822946559815\n",
            " Loss SS33:  0.04392913125810169\n",
            " Loss SS44:  0.058316717722586224\n",
            " Loss SS55:  0.06187339268979572\n",
            " Loss SS66:  0.05034107732630912\n",
            " Loss SS77:  0.0646552603159632\n",
            " Loss SS88:  0.053407577176888786\n",
            " Loss SS11:  0.03300704448143157\n",
            " Loss SS22:  0.04674066503236934\n",
            " Loss SS33:  0.04401222607348024\n",
            " Loss SS44:  0.0580929523197616\n",
            " Loss SS55:  0.06173703519672882\n",
            " Loss SS66:  0.05036971245597049\n",
            " Loss SS77:  0.06444724022251803\n",
            " Loss SS88:  0.05383703394270525\n",
            " Loss SS11:  0.03278113999327675\n",
            " Loss SS22:  0.046269158359433786\n",
            " Loss SS33:  0.04340408882889591\n",
            " Loss SS44:  0.05776795677718569\n",
            " Loss SS55:  0.0609356802506525\n",
            " Loss SS66:  0.05038121556405161\n",
            " Loss SS77:  0.06379862667107192\n",
            " Loss SS88:  0.053079604675046736\n",
            " Loss SS11:  0.03289205764914736\n",
            " Loss SS22:  0.046232087192712004\n",
            " Loss SS33:  0.04354586084315806\n",
            " Loss SS44:  0.057870534973012075\n",
            " Loss SS55:  0.06065343955048808\n",
            " Loss SS66:  0.05019237703563255\n",
            " Loss SS77:  0.0637399838478477\n",
            " Loss SS88:  0.05291902218703871\n",
            "\n",
            "Epoch: 56\n",
            "Loss SS11:  0.031703487038612366\n",
            "Loss SS22:  0.03429442644119263\n",
            "Loss SS33:  0.03001667931675911\n",
            "Loss SS44:  0.041266970336437225\n",
            "Loss SS55:  0.044100649654865265\n",
            "Loss SS66:  0.03500709310173988\n",
            "Loss SS77:  0.045679692178964615\n",
            "Loss SS88:  0.03876974433660507\n",
            "Loss SS11:  0.03373109803281047\n",
            "Loss SS22:  0.03266498480330814\n",
            "Loss SS33:  0.028627408160404724\n",
            "Loss SS44:  0.03772809898311442\n",
            "Loss SS55:  0.0404701056805524\n",
            "Loss SS66:  0.031595134938305076\n",
            "Loss SS77:  0.041387120431119744\n",
            "Loss SS88:  0.03662324391982772\n",
            "Loss SS11:  0.03331787423008964\n",
            "Loss SS22:  0.032677843723268735\n",
            "Loss SS33:  0.028754675494773046\n",
            "Loss SS44:  0.03816899089586167\n",
            "Loss SS55:  0.04055254622584298\n",
            "Loss SS66:  0.031914006386484416\n",
            "Loss SS77:  0.041188862529538926\n",
            "Loss SS88:  0.03678088103021894\n",
            "Loss SS11:  0.03322505836765612\n",
            "Loss SS22:  0.03250228187009212\n",
            "Loss SS33:  0.028595672139237003\n",
            "Loss SS44:  0.03787370783186728\n",
            "Loss SS55:  0.04016480666975821\n",
            "Loss SS66:  0.031655707126182896\n",
            "Loss SS77:  0.04099808296849651\n",
            "Loss SS88:  0.03681668579097717\n",
            "Loss SS11:  0.03318737869764247\n",
            "Loss SS22:  0.03276655514065812\n",
            "Loss SS33:  0.028741446210116876\n",
            "Loss SS44:  0.03772413203629052\n",
            "Loss SS55:  0.04008272962599266\n",
            "Loss SS66:  0.03181768145139625\n",
            "Loss SS77:  0.04077069697583594\n",
            "Loss SS88:  0.0368714443430668\n",
            "Loss SS11:  0.03322763178570598\n",
            "Loss SS22:  0.032740034856924824\n",
            "Loss SS33:  0.02883511924130075\n",
            "Loss SS44:  0.037644330119969795\n",
            "Loss SS55:  0.040020425895265506\n",
            "Loss SS66:  0.031800814666876606\n",
            "Loss SS77:  0.04083819665453013\n",
            "Loss SS88:  0.03664522545010436\n",
            "Loss SS11:  0.03357570697782469\n",
            "Loss SS22:  0.03260717834116983\n",
            "Loss SS33:  0.02880838873689292\n",
            "Loss SS44:  0.03758654639613433\n",
            "Loss SS55:  0.039800803741959274\n",
            "Loss SS66:  0.03179361286466239\n",
            "Loss SS77:  0.04048752302273375\n",
            "Loss SS88:  0.036412219959692876\n",
            "Loss SS11:  0.033868973681204756\n",
            "Loss SS22:  0.03258283257904187\n",
            "Loss SS33:  0.028878356920371592\n",
            "Loss SS44:  0.037666737666012536\n",
            "Loss SS55:  0.03985277159323155\n",
            "Loss SS66:  0.031873606827477335\n",
            "Loss SS77:  0.04052244530807079\n",
            "Loss SS88:  0.03636787637648448\n",
            "Loss SS11:  0.03371152109294026\n",
            "Loss SS22:  0.03244860518585752\n",
            "Loss SS33:  0.028804022841799407\n",
            "Loss SS44:  0.03765423310759627\n",
            "Loss SS55:  0.0397194222903546\n",
            "Loss SS66:  0.031835903930995196\n",
            "Loss SS77:  0.040479503020092296\n",
            "Loss SS88:  0.03640450960324134\n",
            "Loss SS11:  0.03379514359019615\n",
            "Loss SS22:  0.0323178797428097\n",
            "Loss SS33:  0.028728577912181287\n",
            "Loss SS44:  0.03767079573411208\n",
            "Loss SS55:  0.03972026509243053\n",
            "Loss SS66:  0.031789349392056465\n",
            "Loss SS77:  0.04034672288613005\n",
            "Loss SS88:  0.03629156007625899\n",
            "Loss SS11:  0.03362677001053154\n",
            "Loss SS22:  0.03217873842188037\n",
            "Loss SS33:  0.028676957943209326\n",
            "Loss SS44:  0.03746241681499056\n",
            "Loss SS55:  0.03956062538493978\n",
            "Loss SS66:  0.03167262772964959\n",
            "Loss SS77:  0.040063932316728154\n",
            "Loss SS88:  0.036155892277855685\n",
            "Loss SS11:  0.03358439386293695\n",
            "Loss SS22:  0.03219210499108912\n",
            "Loss SS33:  0.028677662566035718\n",
            "Loss SS44:  0.03750935599610612\n",
            "Loss SS55:  0.039521107869642275\n",
            "Loss SS66:  0.031674184123272296\n",
            "Loss SS77:  0.03999073055010658\n",
            "Loss SS88:  0.03610584342869015\n",
            "Loss SS11:  0.03348828216414314\n",
            "Loss SS22:  0.03228047818007056\n",
            "Loss SS33:  0.028718566506608458\n",
            "Loss SS44:  0.03762109472978214\n",
            "Loss SS55:  0.03954418049740397\n",
            "Loss SS66:  0.031712328815016864\n",
            "Loss SS77:  0.04009357100922214\n",
            "Loss SS88:  0.03619369579679217\n",
            "Loss SS11:  0.033488998872064454\n",
            "Loss SS22:  0.03229645241535347\n",
            "Loss SS33:  0.028729590182085984\n",
            "Loss SS44:  0.03763030975602055\n",
            "Loss SS55:  0.03954893589474773\n",
            "Loss SS66:  0.031756574145823946\n",
            "Loss SS77:  0.040118409249618764\n",
            "Loss SS88:  0.036310035778024725\n",
            "Loss SS11:  0.03348647339388411\n",
            "Loss SS22:  0.0322723573086955\n",
            "Loss SS33:  0.02870738041316364\n",
            "Loss SS44:  0.03769809594179722\n",
            "Loss SS55:  0.039634566067169745\n",
            "Loss SS66:  0.031852369943092054\n",
            "Loss SS77:  0.040243178606033325\n",
            "Loss SS88:  0.036363951491971386\n",
            "Loss SS11:  0.03364472978113108\n",
            "Loss SS22:  0.032310466868001105\n",
            "Loss SS33:  0.02873751015359203\n",
            "Loss SS44:  0.03773433712637977\n",
            "Loss SS55:  0.039683542553557466\n",
            "Loss SS66:  0.03193661754346446\n",
            "Loss SS77:  0.040289544679273835\n",
            "Loss SS88:  0.03640115298084076\n",
            "Loss SS11:  0.03365570791528462\n",
            "Loss SS22:  0.03224878632087515\n",
            "Loss SS33:  0.028688781368343727\n",
            "Loss SS44:  0.03764868319404792\n",
            "Loss SS55:  0.03964810813806072\n",
            "Loss SS66:  0.031925267892779774\n",
            "Loss SS77:  0.04033897446919672\n",
            "Loss SS88:  0.036440572082441045\n",
            "Loss SS11:  0.03368131279378955\n",
            "Loss SS22:  0.03237084779692324\n",
            "Loss SS33:  0.028744398992051157\n",
            "Loss SS44:  0.03765739859980449\n",
            "Loss SS55:  0.03968861662069259\n",
            "Loss SS66:  0.03200134334334156\n",
            "Loss SS77:  0.040412974963236974\n",
            "Loss SS88:  0.03656125460800372\n",
            "Loss SS11:  0.03369366011237571\n",
            "Loss SS22:  0.032369806755395886\n",
            "Loss SS33:  0.02872679756120753\n",
            "Loss SS44:  0.037764490638648604\n",
            "Loss SS55:  0.039675467704211806\n",
            "Loss SS66:  0.032040067480166974\n",
            "Loss SS77:  0.04053798204709812\n",
            "Loss SS88:  0.036602158119994635\n",
            "Loss SS11:  0.03366956541909597\n",
            "Loss SS22:  0.032351791137020004\n",
            "Loss SS33:  0.028685196834318926\n",
            "Loss SS44:  0.037726259102840075\n",
            "Loss SS55:  0.0396411052041965\n",
            "Loss SS66:  0.03203371410009437\n",
            "Loss SS77:  0.04051830841920763\n",
            "Loss SS88:  0.036535068078658965\n",
            "Loss SS11:  0.03363669378247427\n",
            "Loss SS22:  0.032280927764909775\n",
            "Loss SS33:  0.02867794975601322\n",
            "Loss SS44:  0.03770242190331369\n",
            "Loss SS55:  0.03954329300875688\n",
            "Loss SS66:  0.03201802444658173\n",
            "Loss SS77:  0.040467182294794575\n",
            "Loss SS88:  0.03647647840689071\n",
            "Loss SS11:  0.03368972918963263\n",
            "Loss SS22:  0.03234453177106041\n",
            "Loss SS33:  0.0286800033761568\n",
            "Loss SS44:  0.03775002016416658\n",
            "Loss SS55:  0.039623657214980554\n",
            "Loss SS66:  0.03204502038198625\n",
            "Loss SS77:  0.04054168677089903\n",
            "Loss SS88:  0.03651566971181693\n",
            "Loss SS11:  0.03371507302769439\n",
            "Loss SS22:  0.03234301593441229\n",
            "Loss SS33:  0.02868521340798199\n",
            "Loss SS44:  0.03778784909197108\n",
            "Loss SS55:  0.03967511434765423\n",
            "Loss SS66:  0.032106378516055877\n",
            "Loss SS77:  0.04056798412182212\n",
            "Loss SS88:  0.03654457790549524\n",
            "Loss SS11:  0.033705107553245184\n",
            "Loss SS22:  0.032352184247854465\n",
            "Loss SS33:  0.028708805584094742\n",
            "Loss SS44:  0.03782920224906562\n",
            "Loss SS55:  0.039637627055892695\n",
            "Loss SS66:  0.03213940458412552\n",
            "Loss SS77:  0.040588098187080195\n",
            "Loss SS88:  0.03659131990753727\n",
            "Loss SS11:  0.033725146575216434\n",
            "Loss SS22:  0.03241669243642156\n",
            "Loss SS33:  0.028731311628927333\n",
            "Loss SS44:  0.03787772586236851\n",
            "Loss SS55:  0.03971161254337714\n",
            "Loss SS66:  0.0321907051665036\n",
            "Loss SS77:  0.04060249720000627\n",
            "Loss SS88:  0.036588577113087245\n",
            "Loss SS11:  0.033713866448200555\n",
            "Loss SS22:  0.03244151768693886\n",
            "Loss SS33:  0.028768362640681496\n",
            "Loss SS44:  0.03787112988501906\n",
            "Loss SS55:  0.03972390628015378\n",
            "Loss SS66:  0.03222412273169039\n",
            "Loss SS77:  0.04061654680575508\n",
            "Loss SS88:  0.03662337174275482\n",
            "Loss SS11:  0.03368148159849461\n",
            "Loss SS22:  0.03242800900734019\n",
            "Loss SS33:  0.028789677739257558\n",
            "Loss SS44:  0.037877332062328455\n",
            "Loss SS55:  0.039714764461092567\n",
            "Loss SS66:  0.032224265166283565\n",
            "Loss SS77:  0.04064140603003374\n",
            "Loss SS88:  0.03663462880014003\n",
            "Loss SS11:  0.033636395671710756\n",
            "Loss SS22:  0.03241212586305238\n",
            "Loss SS33:  0.028765004031992485\n",
            "Loss SS44:  0.03786367531247244\n",
            "Loss SS55:  0.039737044733938694\n",
            "Loss SS66:  0.03217297026756945\n",
            "Loss SS77:  0.04065987326177284\n",
            "Loss SS88:  0.03659906807420878\n",
            "Loss SS11:  0.03362029978468537\n",
            "Loss SS22:  0.03240126853947962\n",
            "Loss SS33:  0.028766513507137093\n",
            "Loss SS44:  0.037881102757627856\n",
            "Loss SS55:  0.039740462810649566\n",
            "Loss SS66:  0.03219329297303941\n",
            "Loss SS77:  0.040649001854488435\n",
            "Loss SS88:  0.03657633528855772\n",
            "Loss SS11:  0.03361254978502534\n",
            "Loss SS22:  0.03238963870222831\n",
            "Loss SS33:  0.02876172808104569\n",
            "Loss SS44:  0.03785493637073491\n",
            "Loss SS55:  0.039734837474282254\n",
            "Loss SS66:  0.03220183892604412\n",
            "Loss SS77:  0.040633317761609646\n",
            "Loss SS88:  0.03657922453384629\n",
            "Loss SS11:  0.033623535093229476\n",
            "Loss SS22:  0.03239934607424411\n",
            "Loss SS33:  0.02875013272602891\n",
            "Loss SS44:  0.03787494312192118\n",
            "Loss SS55:  0.039772165128558974\n",
            "Loss SS66:  0.03221442653144713\n",
            "Loss SS77:  0.04066019182238864\n",
            "Loss SS88:  0.03661981429223998\n",
            "Loss SS11:  0.033621961855811705\n",
            "Loss SS22:  0.03239987485134717\n",
            "Loss SS33:  0.028727611413051843\n",
            "Loss SS44:  0.03785110716603193\n",
            "Loss SS55:  0.03976355168453367\n",
            "Loss SS66:  0.032181974728007795\n",
            "Loss SS77:  0.04062596343194173\n",
            "Loss SS88:  0.036583693392598746\n",
            "Loss SS11:  0.033620809828659455\n",
            "Loss SS22:  0.03238517748093308\n",
            "Loss SS33:  0.028719606353811385\n",
            "Loss SS44:  0.03783823646071172\n",
            "Loss SS55:  0.039785129224121384\n",
            "Loss SS66:  0.03220990198632453\n",
            "Loss SS77:  0.04063904133401927\n",
            "Loss SS88:  0.036565827890487845\n",
            "Loss SS11:  0.033609365144029846\n",
            "Loss SS22:  0.032373364620878614\n",
            "Loss SS33:  0.028703964256267894\n",
            "Loss SS44:  0.037857232227066134\n",
            "Loss SS55:  0.03977258958125043\n",
            "Loss SS66:  0.032189688062874935\n",
            "Loss SS77:  0.04062937276601071\n",
            "Loss SS88:  0.0365727785974741\n",
            "Loss SS11:  0.0336231556925868\n",
            "Loss SS22:  0.03240438078221513\n",
            "Loss SS33:  0.028733273345398064\n",
            "Loss SS44:  0.03788567480803235\n",
            "Loss SS55:  0.03981228775817278\n",
            "Loss SS66:  0.032214839137063696\n",
            "Loss SS77:  0.04069130655782314\n",
            "Loss SS88:  0.03661328540497686\n",
            "Loss SS11:  0.03362687597544784\n",
            "Loss SS22:  0.03239642527814095\n",
            "Loss SS33:  0.02874783904655197\n",
            "Loss SS44:  0.037854010005749526\n",
            "Loss SS55:  0.03982790808944281\n",
            "Loss SS66:  0.03222866752781929\n",
            "Loss SS77:  0.04070071814491538\n",
            "Loss SS88:  0.03661200861072438\n",
            "Loss SS11:  0.033614933351441764\n",
            "Loss SS22:  0.03236954346030868\n",
            "Loss SS33:  0.028752840204045713\n",
            "Loss SS44:  0.03785526832446993\n",
            "Loss SS55:  0.039812631863801434\n",
            "Loss SS66:  0.03222681161459959\n",
            "Loss SS77:  0.04067500567172042\n",
            "Loss SS88:  0.03660116906579677\n",
            "Loss SS11:  0.03360125442058089\n",
            "Loss SS22:  0.03237193972976863\n",
            "Loss SS33:  0.028764361560826675\n",
            "Loss SS44:  0.037849204484222715\n",
            "Loss SS55:  0.03979742267944742\n",
            "Loss SS66:  0.0322305268213916\n",
            "Loss SS77:  0.040656152831495936\n",
            "Loss SS88:  0.03657102801227023\n",
            "Loss SS11:  0.033594684470004924\n",
            "Loss SS22:  0.0323703772830838\n",
            "Loss SS33:  0.028776803613686813\n",
            "Loss SS44:  0.03782906654606184\n",
            "Loss SS55:  0.03980000699403405\n",
            "Loss SS66:  0.03224040131302174\n",
            "Loss SS77:  0.040637155755220124\n",
            "Loss SS88:  0.0365483260950626\n",
            "Loss SS11:  0.03355782234188541\n",
            "Loss SS22:  0.03235034422610727\n",
            "Loss SS33:  0.02876332997227721\n",
            "Loss SS44:  0.0378035564509118\n",
            "Loss SS55:  0.03976580311003548\n",
            "Loss SS66:  0.0321956519323313\n",
            "Loss SS77:  0.04058639126856004\n",
            "Loss SS88:  0.036523915803455334\n",
            "Loss SS11:  0.03354865953121845\n",
            "Loss SS22:  0.03236310965755485\n",
            "Loss SS33:  0.02877300839285898\n",
            "Loss SS44:  0.037811620820527365\n",
            "Loss SS55:  0.03978182001044031\n",
            "Loss SS66:  0.03222680242820422\n",
            "Loss SS77:  0.040579666697101995\n",
            "Loss SS88:  0.036530503770917135\n",
            "Loss SS11:  0.03351522739206207\n",
            "Loss SS22:  0.03234492605348138\n",
            "Loss SS33:  0.028784414522186684\n",
            "Loss SS44:  0.03780031141229064\n",
            "Loss SS55:  0.03976928478972465\n",
            "Loss SS66:  0.03221690506558784\n",
            "Loss SS77:  0.040548277361694626\n",
            "Loss SS88:  0.03650089975564294\n",
            "Loss SS11:  0.03349945385389543\n",
            "Loss SS22:  0.03236270355693376\n",
            "Loss SS33:  0.028791808636058253\n",
            "Loss SS44:  0.03779633865733752\n",
            "Loss SS55:  0.039783560137779866\n",
            "Loss SS66:  0.032204160767645\n",
            "Loss SS77:  0.04055141808018265\n",
            "Loss SS88:  0.03650303828284463\n",
            "Loss SS11:  0.03346138979954399\n",
            "Loss SS22:  0.03235914284609725\n",
            "Loss SS33:  0.028797536046868688\n",
            "Loss SS44:  0.03780701901253719\n",
            "Loss SS55:  0.039781097651150274\n",
            "Loss SS66:  0.03220040493335773\n",
            "Loss SS77:  0.04053796903854733\n",
            "Loss SS88:  0.03648118814777609\n",
            "Loss SS11:  0.03346868386901831\n",
            "Loss SS22:  0.03234345824911751\n",
            "Loss SS33:  0.028788942666280837\n",
            "Loss SS44:  0.03779462331699\n",
            "Loss SS55:  0.03977627269163424\n",
            "Loss SS66:  0.03218688950145326\n",
            "Loss SS77:  0.04051331888521069\n",
            "Loss SS88:  0.036459346696768216\n",
            "Loss SS11:  0.03347600194854773\n",
            "Loss SS22:  0.03237177359307817\n",
            "Loss SS33:  0.028799079470088636\n",
            "Loss SS44:  0.03782949494631618\n",
            "Loss SS55:  0.039773132478582884\n",
            "Loss SS66:  0.0321993689711501\n",
            "Loss SS77:  0.04051307177464344\n",
            "Loss SS88:  0.03648099393280277\n",
            "Loss SS11:  0.033479420597505416\n",
            "Loss SS22:  0.032360327064409174\n",
            "Loss SS33:  0.02880557262037685\n",
            "Loss SS44:  0.037850526244320996\n",
            "Loss SS55:  0.03977729471444825\n",
            "Loss SS66:  0.03219948287592663\n",
            "Loss SS77:  0.04050939038816088\n",
            "Loss SS88:  0.03647981125885649\n",
            "Loss SS11:  0.03347836572067879\n",
            "Loss SS22:  0.03235934737618942\n",
            "Loss SS33:  0.028810583106429966\n",
            "Loss SS44:  0.037843851728519055\n",
            "Loss SS55:  0.03975757506464444\n",
            "Loss SS66:  0.03218669220884261\n",
            "Loss SS77:  0.040501832131557405\n",
            "Loss SS88:  0.036472250416779976\n",
            "Loss SS11:  0.033478294869036786\n",
            "Loss SS22:  0.032380583523743615\n",
            "Loss SS33:  0.028832110179406192\n",
            "Loss SS44:  0.03786784479793417\n",
            "Loss SS55:  0.03975448301236248\n",
            "Loss SS66:  0.03219271962397312\n",
            "Loss SS77:  0.040529694098382876\n",
            "Loss SS88:  0.03647480492253561\n",
            "Loss SS11:  0.03345718456701567\n",
            "Loss SS22:  0.032356299255081204\n",
            "Loss SS33:  0.02882734581054228\n",
            "Loss SS44:  0.037853492385956276\n",
            "Loss SS55:  0.039724684644740854\n",
            "Loss SS66:  0.03216666607622459\n",
            "Loss SS77:  0.040530812207708045\n",
            "Loss SS88:  0.03644664627319684\n",
            "Validation: \n",
            " Loss SS11:  0.030392169952392578\n",
            " Loss SS22:  0.04121193289756775\n",
            " Loss SS33:  0.0373094417154789\n",
            " Loss SS44:  0.05097658932209015\n",
            " Loss SS55:  0.05029064416885376\n",
            " Loss SS66:  0.041822049766778946\n",
            " Loss SS77:  0.055886562913656235\n",
            " Loss SS88:  0.041302260011434555\n",
            " Loss SS11:  0.03344472808142503\n",
            " Loss SS22:  0.048954108996050696\n",
            " Loss SS33:  0.04245397288884435\n",
            " Loss SS44:  0.058961060075532826\n",
            " Loss SS55:  0.061507193105561395\n",
            " Loss SS66:  0.04962946403594244\n",
            " Loss SS77:  0.06379697578293937\n",
            " Loss SS88:  0.05242699579823585\n",
            " Loss SS11:  0.03349766512287826\n",
            " Loss SS22:  0.04855992281582297\n",
            " Loss SS33:  0.042444372231640465\n",
            " Loss SS44:  0.058882240115142453\n",
            " Loss SS55:  0.06141698196893785\n",
            " Loss SS66:  0.049679276601570406\n",
            " Loss SS77:  0.06353542690233487\n",
            " Loss SS88:  0.0526077184008389\n",
            " Loss SS11:  0.03323921903235014\n",
            " Loss SS22:  0.0481487755037722\n",
            " Loss SS33:  0.04184980922546543\n",
            " Loss SS44:  0.058534634833941696\n",
            " Loss SS55:  0.060679931193590164\n",
            " Loss SS66:  0.049611418034698145\n",
            " Loss SS77:  0.06303379446512362\n",
            " Loss SS88:  0.051999224807883875\n",
            " Loss SS11:  0.03328222491674953\n",
            " Loss SS22:  0.048090875424720625\n",
            " Loss SS33:  0.041941532474241136\n",
            " Loss SS44:  0.05859609224178173\n",
            " Loss SS55:  0.060372179167138204\n",
            " Loss SS66:  0.04940835188752339\n",
            " Loss SS77:  0.06280842354452168\n",
            " Loss SS88:  0.05191662829415298\n",
            "\n",
            "Epoch: 57\n",
            "Loss SS11:  0.03346652537584305\n",
            "Loss SS22:  0.03544735908508301\n",
            "Loss SS33:  0.028123000636696815\n",
            "Loss SS44:  0.03989022970199585\n",
            "Loss SS55:  0.03900979086756706\n",
            "Loss SS66:  0.03366488218307495\n",
            "Loss SS77:  0.04466365650296211\n",
            "Loss SS88:  0.03623124212026596\n",
            "Loss SS11:  0.03269341063093056\n",
            "Loss SS22:  0.031694897365840996\n",
            "Loss SS33:  0.028425223956053906\n",
            "Loss SS44:  0.03673457828435031\n",
            "Loss SS55:  0.039950449358333244\n",
            "Loss SS66:  0.03125561655245044\n",
            "Loss SS77:  0.03969983756542206\n",
            "Loss SS88:  0.03504967046054927\n",
            "Loss SS11:  0.03312503679522446\n",
            "Loss SS22:  0.03174310904883203\n",
            "Loss SS33:  0.028452495201712565\n",
            "Loss SS44:  0.03747584049900373\n",
            "Loss SS55:  0.04002360201307705\n",
            "Loss SS66:  0.03184411994048527\n",
            "Loss SS77:  0.039554725090662636\n",
            "Loss SS88:  0.035628888046457655\n",
            "Loss SS11:  0.03310900091403915\n",
            "Loss SS22:  0.03222252927239864\n",
            "Loss SS33:  0.028530441464916352\n",
            "Loss SS44:  0.03769864286145856\n",
            "Loss SS55:  0.03985534992910201\n",
            "Loss SS66:  0.03165986755442235\n",
            "Loss SS77:  0.0400234587009876\n",
            "Loss SS88:  0.03582142113197234\n",
            "Loss SS11:  0.03312307028327047\n",
            "Loss SS22:  0.03235210251153969\n",
            "Loss SS33:  0.028722556035329656\n",
            "Loss SS44:  0.03767955448569321\n",
            "Loss SS55:  0.03981386052399147\n",
            "Loss SS66:  0.031967265849433296\n",
            "Loss SS77:  0.04009805619716644\n",
            "Loss SS88:  0.035869931211558785\n",
            "Loss SS11:  0.03301526876349075\n",
            "Loss SS22:  0.03261569208082031\n",
            "Loss SS33:  0.028855671667877364\n",
            "Loss SS44:  0.037843227240384796\n",
            "Loss SS55:  0.039835345292208245\n",
            "Loss SS66:  0.031944060953808764\n",
            "Loss SS77:  0.04028806039223484\n",
            "Loss SS88:  0.036003816726745345\n",
            "Loss SS11:  0.03302050925424842\n",
            "Loss SS22:  0.03246413656800497\n",
            "Loss SS33:  0.02864113975255216\n",
            "Loss SS44:  0.037819460278651754\n",
            "Loss SS55:  0.0396842573021279\n",
            "Loss SS66:  0.03187363652787248\n",
            "Loss SS77:  0.040161590595714385\n",
            "Loss SS88:  0.035845317129717495\n",
            "Loss SS11:  0.033068886944945426\n",
            "Loss SS22:  0.03249645823429168\n",
            "Loss SS33:  0.028786001123592888\n",
            "Loss SS44:  0.037952777470501374\n",
            "Loss SS55:  0.03973935429059284\n",
            "Loss SS66:  0.03186117739639652\n",
            "Loss SS77:  0.04033584803552695\n",
            "Loss SS88:  0.03605888274983621\n",
            "Loss SS11:  0.033025880332714246\n",
            "Loss SS22:  0.03249066681773574\n",
            "Loss SS33:  0.02879515823758679\n",
            "Loss SS44:  0.03789492535554333\n",
            "Loss SS55:  0.03953234823765578\n",
            "Loss SS66:  0.03175927569836746\n",
            "Loss SS77:  0.04026747081014845\n",
            "Loss SS88:  0.03597304654986034\n",
            "Loss SS11:  0.03288123787350052\n",
            "Loss SS22:  0.03239385990405476\n",
            "Loss SS33:  0.02864842303097248\n",
            "Loss SS44:  0.037911336787126874\n",
            "Loss SS55:  0.039451367334350125\n",
            "Loss SS66:  0.03172738753914178\n",
            "Loss SS77:  0.040114742591158374\n",
            "Loss SS88:  0.035867233914169634\n",
            "Loss SS11:  0.03279064785652232\n",
            "Loss SS22:  0.03222779608746566\n",
            "Loss SS33:  0.028564416056517328\n",
            "Loss SS44:  0.03771605361068603\n",
            "Loss SS55:  0.039484792195334295\n",
            "Loss SS66:  0.031671162307409954\n",
            "Loss SS77:  0.0399957503939029\n",
            "Loss SS88:  0.03580681535203268\n",
            "Loss SS11:  0.03283866100550235\n",
            "Loss SS22:  0.0321554614247771\n",
            "Loss SS33:  0.028570982954792074\n",
            "Loss SS44:  0.03774194741571272\n",
            "Loss SS55:  0.03958728986683192\n",
            "Loss SS66:  0.031666087959935\n",
            "Loss SS77:  0.04001794470188854\n",
            "Loss SS88:  0.03581902426709463\n",
            "Loss SS11:  0.03282438103140386\n",
            "Loss SS22:  0.03215683728825947\n",
            "Loss SS33:  0.02863341258946529\n",
            "Loss SS44:  0.0377455624667081\n",
            "Loss SS55:  0.03958512390078592\n",
            "Loss SS66:  0.03171150457144769\n",
            "Loss SS77:  0.04002370808489066\n",
            "Loss SS88:  0.03581876032177574\n",
            "Loss SS11:  0.032884003935765675\n",
            "Loss SS22:  0.03219395180638055\n",
            "Loss SS33:  0.028713145513684694\n",
            "Loss SS44:  0.037779127709738174\n",
            "Loss SS55:  0.03960862767150384\n",
            "Loss SS66:  0.03178653146588165\n",
            "Loss SS77:  0.04006929823574219\n",
            "Loss SS88:  0.03586981717127425\n",
            "Loss SS11:  0.03288116694447842\n",
            "Loss SS22:  0.032208023275783725\n",
            "Loss SS33:  0.02874169888384376\n",
            "Loss SS44:  0.037817642277648264\n",
            "Loss SS55:  0.03965999917886781\n",
            "Loss SS66:  0.031883602697692866\n",
            "Loss SS77:  0.04012461318403271\n",
            "Loss SS88:  0.03593886333207289\n",
            "Loss SS11:  0.03295007253078041\n",
            "Loss SS22:  0.032278380190201156\n",
            "Loss SS33:  0.02882277900168043\n",
            "Loss SS44:  0.037855110495098376\n",
            "Loss SS55:  0.03965438460770822\n",
            "Loss SS66:  0.031947778241819894\n",
            "Loss SS77:  0.04019084017699128\n",
            "Loss SS88:  0.03605026955290741\n",
            "Loss SS11:  0.03294594585895538\n",
            "Loss SS22:  0.03221216803613287\n",
            "Loss SS33:  0.028753207175072674\n",
            "Loss SS44:  0.037738955492928904\n",
            "Loss SS55:  0.039624916401153766\n",
            "Loss SS66:  0.03192591483224623\n",
            "Loss SS77:  0.04017755916770201\n",
            "Loss SS88:  0.03600322883229078\n",
            "Loss SS11:  0.03300762424866358\n",
            "Loss SS22:  0.03229828390200236\n",
            "Loss SS33:  0.028817658982517428\n",
            "Loss SS44:  0.037771716191057576\n",
            "Loss SS55:  0.03970183024716656\n",
            "Loss SS66:  0.031997026374552685\n",
            "Loss SS77:  0.040325426523796996\n",
            "Loss SS88:  0.036126126170332666\n",
            "Loss SS11:  0.03304333095721777\n",
            "Loss SS22:  0.03232720622906039\n",
            "Loss SS33:  0.02885957318486759\n",
            "Loss SS44:  0.03783199044963273\n",
            "Loss SS55:  0.03974653856612701\n",
            "Loss SS66:  0.032054333273547786\n",
            "Loss SS77:  0.040381087617979525\n",
            "Loss SS88:  0.03627311314831781\n",
            "Loss SS11:  0.033051117796314325\n",
            "Loss SS22:  0.03231031387186175\n",
            "Loss SS33:  0.028856976884197815\n",
            "Loss SS44:  0.037734742583560694\n",
            "Loss SS55:  0.03971237874545976\n",
            "Loss SS66:  0.032056849878964\n",
            "Loss SS77:  0.040426614630908866\n",
            "Loss SS88:  0.036262660750543886\n",
            "Loss SS11:  0.03305254764484232\n",
            "Loss SS22:  0.032275936839666536\n",
            "Loss SS33:  0.02884695232285196\n",
            "Loss SS44:  0.037719163116975804\n",
            "Loss SS55:  0.03967280356000312\n",
            "Loss SS66:  0.03201543116265565\n",
            "Loss SS77:  0.04039784367034091\n",
            "Loss SS88:  0.036262381265857326\n",
            "Loss SS11:  0.033088601348806895\n",
            "Loss SS22:  0.032268199010341655\n",
            "Loss SS33:  0.028857928555088022\n",
            "Loss SS44:  0.03772718775399488\n",
            "Loss SS55:  0.03968121642820643\n",
            "Loss SS66:  0.03202743271268657\n",
            "Loss SS77:  0.040415883452688914\n",
            "Loss SS88:  0.03628092965384795\n",
            "Loss SS11:  0.03307766760636239\n",
            "Loss SS22:  0.03225860104167084\n",
            "Loss SS33:  0.028834616150117027\n",
            "Loss SS44:  0.03772126952380077\n",
            "Loss SS55:  0.03966393154873028\n",
            "Loss SS66:  0.03203280256614426\n",
            "Loss SS77:  0.040380123682421255\n",
            "Loss SS88:  0.03625727176868538\n",
            "Loss SS11:  0.03308649539334691\n",
            "Loss SS22:  0.03225110039856785\n",
            "Loss SS33:  0.028850662388971875\n",
            "Loss SS44:  0.03773457548899568\n",
            "Loss SS55:  0.03964903466887288\n",
            "Loss SS66:  0.032054896123739546\n",
            "Loss SS77:  0.04036714220459843\n",
            "Loss SS88:  0.0362698080993834\n",
            "Loss SS11:  0.03315842014068884\n",
            "Loss SS22:  0.032332313204516516\n",
            "Loss SS33:  0.028868541847991744\n",
            "Loss SS44:  0.03781074313330947\n",
            "Loss SS55:  0.0397079166380449\n",
            "Loss SS66:  0.03206769030742131\n",
            "Loss SS77:  0.04045675716583165\n",
            "Loss SS88:  0.0363011262939926\n",
            "Loss SS11:  0.03316974376450734\n",
            "Loss SS22:  0.03232583551263192\n",
            "Loss SS33:  0.028912205939214543\n",
            "Loss SS44:  0.037821370217785893\n",
            "Loss SS55:  0.039698428975277214\n",
            "Loss SS66:  0.03211100243655334\n",
            "Loss SS77:  0.04045190188692385\n",
            "Loss SS88:  0.036358144609220475\n",
            "Loss SS11:  0.03317416719806834\n",
            "Loss SS22:  0.032342930142393055\n",
            "Loss SS33:  0.028940787760119786\n",
            "Loss SS44:  0.03779104012296575\n",
            "Loss SS55:  0.03971110544099662\n",
            "Loss SS66:  0.03213918591596158\n",
            "Loss SS77:  0.04050711372517535\n",
            "Loss SS88:  0.036428664050910664\n",
            "Loss SS11:  0.03312726234982814\n",
            "Loss SS22:  0.032312232829163\n",
            "Loss SS33:  0.028889395867051675\n",
            "Loss SS44:  0.03770831557217559\n",
            "Loss SS55:  0.039694944623431594\n",
            "Loss SS66:  0.03209036939203519\n",
            "Loss SS77:  0.04048039896131882\n",
            "Loss SS88:  0.036385322484528006\n",
            "Loss SS11:  0.033137610076108014\n",
            "Loss SS22:  0.03228569866632015\n",
            "Loss SS33:  0.028891706673488074\n",
            "Loss SS44:  0.03771460313495792\n",
            "Loss SS55:  0.0397039209475933\n",
            "Loss SS66:  0.03207730358048902\n",
            "Loss SS77:  0.04047606739603328\n",
            "Loss SS88:  0.03638592679915267\n",
            "Loss SS11:  0.03315160403034531\n",
            "Loss SS22:  0.03226499214665996\n",
            "Loss SS33:  0.028901817181061225\n",
            "Loss SS44:  0.037682022105684804\n",
            "Loss SS55:  0.03971945079638786\n",
            "Loss SS66:  0.032089863695793135\n",
            "Loss SS77:  0.04050671805938085\n",
            "Loss SS88:  0.03635506683326874\n",
            "Loss SS11:  0.033162344606017746\n",
            "Loss SS22:  0.03224413034404037\n",
            "Loss SS33:  0.028906094312568836\n",
            "Loss SS44:  0.03769830221997147\n",
            "Loss SS55:  0.039733237181589054\n",
            "Loss SS66:  0.032112409766033245\n",
            "Loss SS77:  0.04050085241859933\n",
            "Loss SS88:  0.03635769747692683\n",
            "Loss SS11:  0.03317950889348409\n",
            "Loss SS22:  0.03222441863136851\n",
            "Loss SS33:  0.028879648146378264\n",
            "Loss SS44:  0.037674920375899104\n",
            "Loss SS55:  0.03971406986332016\n",
            "Loss SS66:  0.03210416525434643\n",
            "Loss SS77:  0.04046747341177088\n",
            "Loss SS88:  0.036296392266917075\n",
            "Loss SS11:  0.03318310724542334\n",
            "Loss SS22:  0.03222570348983613\n",
            "Loss SS33:  0.02889846676142416\n",
            "Loss SS44:  0.03769704589266272\n",
            "Loss SS55:  0.03972496919774935\n",
            "Loss SS66:  0.032119283278551056\n",
            "Loss SS77:  0.04050576526706464\n",
            "Loss SS88:  0.036294228968869115\n",
            "Loss SS11:  0.033180786139160125\n",
            "Loss SS22:  0.03219892856667769\n",
            "Loss SS33:  0.028878317521237894\n",
            "Loss SS44:  0.037688704933464705\n",
            "Loss SS55:  0.03972567595383912\n",
            "Loss SS66:  0.0321152574436182\n",
            "Loss SS77:  0.040502780097398876\n",
            "Loss SS88:  0.03629474313169988\n",
            "Loss SS11:  0.033212928904378866\n",
            "Loss SS22:  0.0322311678327773\n",
            "Loss SS33:  0.028879143718269565\n",
            "Loss SS44:  0.03770204641121574\n",
            "Loss SS55:  0.03977795139697989\n",
            "Loss SS66:  0.0321380968732743\n",
            "Loss SS77:  0.04056127343184787\n",
            "Loss SS88:  0.03631222559974865\n",
            "Loss SS11:  0.03322495301479628\n",
            "Loss SS22:  0.03223580952489648\n",
            "Loss SS33:  0.0289014988337402\n",
            "Loss SS44:  0.03767884391070771\n",
            "Loss SS55:  0.03980471238366559\n",
            "Loss SS66:  0.0321500688058953\n",
            "Loss SS77:  0.04056571580023847\n",
            "Loss SS88:  0.036348128553341935\n",
            "Loss SS11:  0.033251721659608165\n",
            "Loss SS22:  0.03223492432168976\n",
            "Loss SS33:  0.02889735790851869\n",
            "Loss SS44:  0.037705848648772675\n",
            "Loss SS55:  0.03978636618342426\n",
            "Loss SS66:  0.03215046465355604\n",
            "Loss SS77:  0.040555936859667796\n",
            "Loss SS88:  0.03635716628685717\n",
            "Loss SS11:  0.03327444763075952\n",
            "Loss SS22:  0.032215729923140646\n",
            "Loss SS33:  0.028886995993735332\n",
            "Loss SS44:  0.037674752156930474\n",
            "Loss SS55:  0.03977536706309113\n",
            "Loss SS66:  0.03213682402216521\n",
            "Loss SS77:  0.0405452818260842\n",
            "Loss SS88:  0.03634326967769717\n",
            "Loss SS11:  0.0332682027708827\n",
            "Loss SS22:  0.032196270829931956\n",
            "Loss SS33:  0.028879514032500622\n",
            "Loss SS44:  0.0376838540213471\n",
            "Loss SS55:  0.03976435697297725\n",
            "Loss SS66:  0.032135256636995346\n",
            "Loss SS77:  0.0405044742319684\n",
            "Loss SS88:  0.03629957328200966\n",
            "Loss SS11:  0.03325763353339546\n",
            "Loss SS22:  0.03220405546314729\n",
            "Loss SS33:  0.028868510990458375\n",
            "Loss SS44:  0.037673606773090486\n",
            "Loss SS55:  0.03973340449849968\n",
            "Loss SS66:  0.0321377171417865\n",
            "Loss SS77:  0.04048293728924468\n",
            "Loss SS88:  0.03629317976381925\n",
            "Loss SS11:  0.0332371711340777\n",
            "Loss SS22:  0.03223649882467607\n",
            "Loss SS33:  0.028888142205533245\n",
            "Loss SS44:  0.03771168017699534\n",
            "Loss SS55:  0.03973770939612627\n",
            "Loss SS66:  0.03216085559746571\n",
            "Loss SS77:  0.04050706678123545\n",
            "Loss SS88:  0.03633464350274525\n",
            "Loss SS11:  0.033202255548061825\n",
            "Loss SS22:  0.03223293159981859\n",
            "Loss SS33:  0.028875576753685944\n",
            "Loss SS44:  0.03771336053083413\n",
            "Loss SS55:  0.0397073137517248\n",
            "Loss SS66:  0.03214392964694187\n",
            "Loss SS77:  0.04048244782027827\n",
            "Loss SS88:  0.03629184605830197\n",
            "Loss SS11:  0.033179107487130904\n",
            "Loss SS22:  0.03223648853600025\n",
            "Loss SS33:  0.02888310219287306\n",
            "Loss SS44:  0.03772883212764586\n",
            "Loss SS55:  0.03970936872507501\n",
            "Loss SS66:  0.032141297533577244\n",
            "Loss SS77:  0.04047888650994686\n",
            "Loss SS88:  0.036272582958613894\n",
            "Loss SS11:  0.03318135073006568\n",
            "Loss SS22:  0.03221766132506998\n",
            "Loss SS33:  0.028881191999895667\n",
            "Loss SS44:  0.037714396372195466\n",
            "Loss SS55:  0.039690117193969386\n",
            "Loss SS66:  0.032129464973533514\n",
            "Loss SS77:  0.04046121988177576\n",
            "Loss SS88:  0.03625269540587738\n",
            "Loss SS11:  0.03318411331885661\n",
            "Loss SS22:  0.032217779127108\n",
            "Loss SS33:  0.028889628177901513\n",
            "Loss SS44:  0.037708642641230236\n",
            "Loss SS55:  0.03968477141863905\n",
            "Loss SS66:  0.03211979410038784\n",
            "Loss SS77:  0.04044196110159632\n",
            "Loss SS88:  0.036236506745286806\n",
            "Loss SS11:  0.033198018448877493\n",
            "Loss SS22:  0.032244037089254\n",
            "Loss SS33:  0.02889931374983222\n",
            "Loss SS44:  0.03771656704550307\n",
            "Loss SS55:  0.039686159290654166\n",
            "Loss SS66:  0.03212317569953639\n",
            "Loss SS77:  0.04042609930534056\n",
            "Loss SS88:  0.03625993502890059\n",
            "Loss SS11:  0.03319535171714108\n",
            "Loss SS22:  0.03224457836752341\n",
            "Loss SS33:  0.0288798668322814\n",
            "Loss SS44:  0.03769842697683229\n",
            "Loss SS55:  0.03967158918202311\n",
            "Loss SS66:  0.03209744789789142\n",
            "Loss SS77:  0.040433420155221625\n",
            "Loss SS88:  0.03626683692562994\n",
            "Loss SS11:  0.03319482481593539\n",
            "Loss SS22:  0.03222806553204095\n",
            "Loss SS33:  0.02885323737583849\n",
            "Loss SS44:  0.037691852319076055\n",
            "Loss SS55:  0.039670706902593056\n",
            "Loss SS66:  0.03209940038795431\n",
            "Loss SS77:  0.04041265071450778\n",
            "Loss SS88:  0.03625017339304374\n",
            "Loss SS11:  0.03319199655878816\n",
            "Loss SS22:  0.03223126681750009\n",
            "Loss SS33:  0.028839889185778067\n",
            "Loss SS44:  0.03767986639776497\n",
            "Loss SS55:  0.039663622639840954\n",
            "Loss SS66:  0.032094489384186985\n",
            "Loss SS77:  0.04040860252265127\n",
            "Loss SS88:  0.03624733414679084\n",
            "Loss SS11:  0.03315047499801501\n",
            "Loss SS22:  0.032192585921548535\n",
            "Loss SS33:  0.02881880862497992\n",
            "Loss SS44:  0.03764308323655012\n",
            "Loss SS55:  0.039612030136245334\n",
            "Loss SS66:  0.03205657166916338\n",
            "Loss SS77:  0.040383205659945486\n",
            "Loss SS88:  0.03619825229610178\n",
            "Validation: \n",
            " Loss SS11:  0.03128638118505478\n",
            " Loss SS22:  0.04128890857100487\n",
            " Loss SS33:  0.03811243548989296\n",
            " Loss SS44:  0.04879032447934151\n",
            " Loss SS55:  0.050540607422590256\n",
            " Loss SS66:  0.043274521827697754\n",
            " Loss SS77:  0.056469932198524475\n",
            " Loss SS88:  0.04029032215476036\n",
            " Loss SS11:  0.03445538878440857\n",
            " Loss SS22:  0.0483914516156628\n",
            " Loss SS33:  0.04298914454522587\n",
            " Loss SS44:  0.05661149163331304\n",
            " Loss SS55:  0.06174447458414804\n",
            " Loss SS66:  0.049927362551291786\n",
            " Loss SS77:  0.06445451098538581\n",
            " Loss SS88:  0.052075905281872975\n",
            " Loss SS11:  0.03445081149296063\n",
            " Loss SS22:  0.04812589687545125\n",
            " Loss SS33:  0.04291171489692316\n",
            " Loss SS44:  0.05661184722330512\n",
            " Loss SS55:  0.06164107762458848\n",
            " Loss SS66:  0.049778314988787584\n",
            " Loss SS77:  0.06380546120245283\n",
            " Loss SS88:  0.05267817163612784\n",
            " Loss SS11:  0.03426976431710798\n",
            " Loss SS22:  0.04771830385825673\n",
            " Loss SS33:  0.042208850139477214\n",
            " Loss SS44:  0.0562847188872392\n",
            " Loss SS55:  0.060750266689746105\n",
            " Loss SS66:  0.04968323973847217\n",
            " Loss SS77:  0.06332100078952117\n",
            " Loss SS88:  0.05194770789048711\n",
            " Loss SS11:  0.03429454056845035\n",
            " Loss SS22:  0.04768823004431195\n",
            " Loss SS33:  0.04231315115351736\n",
            " Loss SS44:  0.05642975356291841\n",
            " Loss SS55:  0.06036959133214421\n",
            " Loss SS66:  0.04950259727092437\n",
            " Loss SS77:  0.06314645914567842\n",
            " Loss SS88:  0.05196822052936495\n",
            "\n",
            "Epoch: 58\n",
            "Loss SS11:  0.03550552949309349\n",
            "Loss SS22:  0.03637130558490753\n",
            "Loss SS33:  0.03087570145726204\n",
            "Loss SS44:  0.04205227643251419\n",
            "Loss SS55:  0.04248732328414917\n",
            "Loss SS66:  0.034257855266332626\n",
            "Loss SS77:  0.045172423124313354\n",
            "Loss SS88:  0.03840872645378113\n",
            "Loss SS11:  0.03374946185133674\n",
            "Loss SS22:  0.03177412751723419\n",
            "Loss SS33:  0.02882982451807369\n",
            "Loss SS44:  0.03781053999608213\n",
            "Loss SS55:  0.04044459794055332\n",
            "Loss SS66:  0.03151565908708356\n",
            "Loss SS77:  0.040743431923064316\n",
            "Loss SS88:  0.035944523459131066\n",
            "Loss SS11:  0.03364209413883232\n",
            "Loss SS22:  0.03205093759156409\n",
            "Loss SS33:  0.028816758521965573\n",
            "Loss SS44:  0.037678489372843786\n",
            "Loss SS55:  0.03959075255053384\n",
            "Loss SS66:  0.0318413232231424\n",
            "Loss SS77:  0.04035628072562672\n",
            "Loss SS88:  0.036197186225936526\n",
            "Loss SS11:  0.0334115037995\n",
            "Loss SS22:  0.03196281513139125\n",
            "Loss SS33:  0.028619568975221728\n",
            "Loss SS44:  0.03783438330696475\n",
            "Loss SS55:  0.03931665276327441\n",
            "Loss SS66:  0.03182576795018489\n",
            "Loss SS77:  0.040354768595387856\n",
            "Loss SS88:  0.0360304988440006\n",
            "Loss SS11:  0.033356714539411594\n",
            "Loss SS22:  0.032074753558490335\n",
            "Loss SS33:  0.028641570041455875\n",
            "Loss SS44:  0.037677440247157724\n",
            "Loss SS55:  0.03923719140087686\n",
            "Loss SS66:  0.03186562498349969\n",
            "Loss SS77:  0.04035617074951893\n",
            "Loss SS88:  0.0357471999780434\n",
            "Loss SS11:  0.03340603283369074\n",
            "Loss SS22:  0.03222424087717252\n",
            "Loss SS33:  0.028802637522127115\n",
            "Loss SS44:  0.03793253735000012\n",
            "Loss SS55:  0.03928972758791026\n",
            "Loss SS66:  0.03188371877459919\n",
            "Loss SS77:  0.04034155804444762\n",
            "Loss SS88:  0.03605375942938468\n",
            "Loss SS11:  0.03345283282706972\n",
            "Loss SS22:  0.03218579182370764\n",
            "Loss SS33:  0.028587364179433368\n",
            "Loss SS44:  0.03760823833404994\n",
            "Loss SS55:  0.03919931922535427\n",
            "Loss SS66:  0.03175913637168095\n",
            "Loss SS77:  0.03999939579211297\n",
            "Loss SS88:  0.03592608398834213\n",
            "Loss SS11:  0.033498864804565064\n",
            "Loss SS22:  0.03216804417086319\n",
            "Loss SS33:  0.0287075772829039\n",
            "Loss SS44:  0.037786866472640505\n",
            "Loss SS55:  0.039396817153188546\n",
            "Loss SS66:  0.03188346976965246\n",
            "Loss SS77:  0.04006510941495358\n",
            "Loss SS88:  0.03599913407799224\n",
            "Loss SS11:  0.03341387575607241\n",
            "Loss SS22:  0.03206460541229189\n",
            "Loss SS33:  0.02858837355894071\n",
            "Loss SS44:  0.03759154318659394\n",
            "Loss SS55:  0.03914047359132472\n",
            "Loss SS66:  0.03179166456799448\n",
            "Loss SS77:  0.0398359005741867\n",
            "Loss SS88:  0.03593496874802642\n",
            "Loss SS11:  0.033303005109121514\n",
            "Loss SS22:  0.03187949134671426\n",
            "Loss SS33:  0.028501245019200084\n",
            "Loss SS44:  0.0375180656683969\n",
            "Loss SS55:  0.03916760267955916\n",
            "Loss SS66:  0.031659239677937476\n",
            "Loss SS77:  0.03968555672646879\n",
            "Loss SS88:  0.03596222713835292\n",
            "Loss SS11:  0.03322560574379888\n",
            "Loss SS22:  0.03177400730033912\n",
            "Loss SS33:  0.02840749355088366\n",
            "Loss SS44:  0.03736704008856622\n",
            "Loss SS55:  0.0390260981598703\n",
            "Loss SS66:  0.03156395262051927\n",
            "Loss SS77:  0.03965546928419925\n",
            "Loss SS88:  0.035819956829937376\n",
            "Loss SS11:  0.03319261743275969\n",
            "Loss SS22:  0.03176816403530203\n",
            "Loss SS33:  0.02836529472591104\n",
            "Loss SS44:  0.03739550387537157\n",
            "Loss SS55:  0.03905000071133579\n",
            "Loss SS66:  0.031548637007405095\n",
            "Loss SS77:  0.039563805066250464\n",
            "Loss SS88:  0.03579773216902673\n",
            "Loss SS11:  0.033180662977301385\n",
            "Loss SS22:  0.0317390361600671\n",
            "Loss SS33:  0.028351860675067942\n",
            "Loss SS44:  0.03734931583739509\n",
            "Loss SS55:  0.03909069853873292\n",
            "Loss SS66:  0.03153535848375687\n",
            "Loss SS77:  0.03957692275116266\n",
            "Loss SS88:  0.03580561471133193\n",
            "Loss SS11:  0.03324086942802404\n",
            "Loss SS22:  0.03181262152226827\n",
            "Loss SS33:  0.02843520308325764\n",
            "Loss SS44:  0.03738318983721369\n",
            "Loss SS55:  0.03913750306113076\n",
            "Loss SS66:  0.03158955440948938\n",
            "Loss SS77:  0.039659184211765536\n",
            "Loss SS88:  0.035890900480155724\n",
            "Loss SS11:  0.03321687690913677\n",
            "Loss SS22:  0.03182299354556182\n",
            "Loss SS33:  0.028462999520149638\n",
            "Loss SS44:  0.037407508185992006\n",
            "Loss SS55:  0.03913916935417669\n",
            "Loss SS66:  0.031629263654562596\n",
            "Loss SS77:  0.039759439145419614\n",
            "Loss SS88:  0.03592482325455821\n",
            "Loss SS11:  0.03316390736410949\n",
            "Loss SS22:  0.03189837096246662\n",
            "Loss SS33:  0.028505734334520947\n",
            "Loss SS44:  0.03747958319866105\n",
            "Loss SS55:  0.03923818927924365\n",
            "Loss SS66:  0.03171150686083645\n",
            "Loss SS77:  0.039832732313318756\n",
            "Loss SS88:  0.03598209189263401\n",
            "Loss SS11:  0.03322640157403042\n",
            "Loss SS22:  0.031908702718452635\n",
            "Loss SS33:  0.028466484769716027\n",
            "Loss SS44:  0.03739167923420112\n",
            "Loss SS55:  0.039184898809061286\n",
            "Loss SS66:  0.03172105516058318\n",
            "Loss SS77:  0.039852430893582584\n",
            "Loss SS88:  0.03599878296441173\n",
            "Loss SS11:  0.03328053983287853\n",
            "Loss SS22:  0.03198194447142339\n",
            "Loss SS33:  0.028570335761409753\n",
            "Loss SS44:  0.037373901916700494\n",
            "Loss SS55:  0.03926133403652593\n",
            "Loss SS66:  0.0317717123754889\n",
            "Loss SS77:  0.0400024596492798\n",
            "Loss SS88:  0.03605632226892382\n",
            "Loss SS11:  0.03327138704448444\n",
            "Loss SS22:  0.031968012102236404\n",
            "Loss SS33:  0.028563310623745234\n",
            "Loss SS44:  0.03745231413133237\n",
            "Loss SS55:  0.03932125109006028\n",
            "Loss SS66:  0.03180196454223678\n",
            "Loss SS77:  0.040028644077356346\n",
            "Loss SS88:  0.036167670082486135\n",
            "Loss SS11:  0.03323242188241157\n",
            "Loss SS22:  0.03197697374521126\n",
            "Loss SS33:  0.028556060543313076\n",
            "Loss SS44:  0.03739366978796989\n",
            "Loss SS55:  0.03932539551123899\n",
            "Loss SS66:  0.03183332837493944\n",
            "Loss SS77:  0.0400681503468159\n",
            "Loss SS88:  0.036141130468608194\n",
            "Loss SS11:  0.0332173655337807\n",
            "Loss SS22:  0.03199143815248167\n",
            "Loss SS33:  0.028549727738200137\n",
            "Loss SS44:  0.03735154422361459\n",
            "Loss SS55:  0.039319006809547766\n",
            "Loss SS66:  0.031845026033286435\n",
            "Loss SS77:  0.040079524879580115\n",
            "Loss SS88:  0.03613341505180544\n",
            "Loss SS11:  0.033246784325318315\n",
            "Loss SS22:  0.03203361326060589\n",
            "Loss SS33:  0.028570197831609804\n",
            "Loss SS44:  0.037365571211723354\n",
            "Loss SS55:  0.039329097540033935\n",
            "Loss SS66:  0.031908841150419975\n",
            "Loss SS77:  0.04015469208571583\n",
            "Loss SS88:  0.03617161367600563\n",
            "Loss SS11:  0.033244281619517514\n",
            "Loss SS22:  0.032036162702139145\n",
            "Loss SS33:  0.028585465321511166\n",
            "Loss SS44:  0.037375761389867215\n",
            "Loss SS55:  0.03935406567756407\n",
            "Loss SS66:  0.03191217499452209\n",
            "Loss SS77:  0.0401481341284055\n",
            "Loss SS88:  0.03616919683722349\n",
            "Loss SS11:  0.033221003530926003\n",
            "Loss SS22:  0.032012122945287526\n",
            "Loss SS33:  0.028583176490483864\n",
            "Loss SS44:  0.037383105218797535\n",
            "Loss SS55:  0.03931742287301398\n",
            "Loss SS66:  0.03187801011584022\n",
            "Loss SS77:  0.040107304157761786\n",
            "Loss SS88:  0.03620627440221898\n",
            "Loss SS11:  0.03321836476071247\n",
            "Loss SS22:  0.03204417518529407\n",
            "Loss SS33:  0.028586953723826348\n",
            "Loss SS44:  0.03745386478997365\n",
            "Loss SS55:  0.0393474960846525\n",
            "Loss SS66:  0.031874659034535596\n",
            "Loss SS77:  0.04015950979162548\n",
            "Loss SS88:  0.036190177835678655\n",
            "Loss SS11:  0.0331943844968223\n",
            "Loss SS22:  0.03205001349022901\n",
            "Loss SS33:  0.028586540853537887\n",
            "Loss SS44:  0.03743811475625076\n",
            "Loss SS55:  0.03935348316730256\n",
            "Loss SS66:  0.03191334556832731\n",
            "Loss SS77:  0.04017783725166701\n",
            "Loss SS88:  0.0362590270495391\n",
            "Loss SS11:  0.03314066237245483\n",
            "Loss SS22:  0.03205319974300277\n",
            "Loss SS33:  0.028565663323377285\n",
            "Loss SS44:  0.037458804232635715\n",
            "Loss SS55:  0.039340445721857395\n",
            "Loss SS66:  0.031916211476956294\n",
            "Loss SS77:  0.040184084911227685\n",
            "Loss SS88:  0.03627784957688202\n",
            "Loss SS11:  0.03311446890564862\n",
            "Loss SS22:  0.03203866305001547\n",
            "Loss SS33:  0.028532588765080125\n",
            "Loss SS44:  0.03745508567873402\n",
            "Loss SS55:  0.03932524209917691\n",
            "Loss SS66:  0.031914105240214354\n",
            "Loss SS77:  0.04014670637150972\n",
            "Loss SS88:  0.036272533492243596\n",
            "Loss SS11:  0.033127867368139406\n",
            "Loss SS22:  0.03205245307040172\n",
            "Loss SS33:  0.028557402794365357\n",
            "Loss SS44:  0.03748074514638911\n",
            "Loss SS55:  0.03935915918430824\n",
            "Loss SS66:  0.03193517480695163\n",
            "Loss SS77:  0.04013897445437323\n",
            "Loss SS88:  0.036287078686933504\n",
            "Loss SS11:  0.033119403766756206\n",
            "Loss SS22:  0.03206596858927475\n",
            "Loss SS33:  0.028558994083316466\n",
            "Loss SS44:  0.03748805271749644\n",
            "Loss SS55:  0.03942081722658115\n",
            "Loss SS66:  0.03197251634089807\n",
            "Loss SS77:  0.04016935150699107\n",
            "Loss SS88:  0.036291993360748814\n",
            "Loss SS11:  0.03315801183349666\n",
            "Loss SS22:  0.032094935153261374\n",
            "Loss SS33:  0.028565148621847065\n",
            "Loss SS44:  0.0375157168562032\n",
            "Loss SS55:  0.03945084786246782\n",
            "Loss SS66:  0.031985536483781676\n",
            "Loss SS77:  0.04017638614357904\n",
            "Loss SS88:  0.03628192374179529\n",
            "Loss SS11:  0.03319840813564717\n",
            "Loss SS22:  0.03208147921267047\n",
            "Loss SS33:  0.028556158215142905\n",
            "Loss SS44:  0.037505914807511295\n",
            "Loss SS55:  0.0394557237002244\n",
            "Loss SS66:  0.0319741086621089\n",
            "Loss SS77:  0.040182248368811374\n",
            "Loss SS88:  0.03628600224660907\n",
            "Loss SS11:  0.03319332551942251\n",
            "Loss SS22:  0.03208266868839197\n",
            "Loss SS33:  0.028573885084219812\n",
            "Loss SS44:  0.03752101223184683\n",
            "Loss SS55:  0.03947053341386474\n",
            "Loss SS66:  0.032010088526756963\n",
            "Loss SS77:  0.0402126106494498\n",
            "Loss SS88:  0.03628544553510868\n",
            "Loss SS11:  0.03318387735916409\n",
            "Loss SS22:  0.03207954611302863\n",
            "Loss SS33:  0.028564936951298008\n",
            "Loss SS44:  0.03752387297027421\n",
            "Loss SS55:  0.03947335398359241\n",
            "Loss SS66:  0.03201717881097534\n",
            "Loss SS77:  0.04023356839249141\n",
            "Loss SS88:  0.03626736413316064\n",
            "Loss SS11:  0.03319264905762113\n",
            "Loss SS22:  0.03210908422503304\n",
            "Loss SS33:  0.028601570650279698\n",
            "Loss SS44:  0.03755685192180519\n",
            "Loss SS55:  0.03955967952647517\n",
            "Loss SS66:  0.032030080786504704\n",
            "Loss SS77:  0.04028696898928136\n",
            "Loss SS88:  0.03633184259063687\n",
            "Loss SS11:  0.03320548196782244\n",
            "Loss SS22:  0.03209113399166497\n",
            "Loss SS33:  0.028643182592938767\n",
            "Loss SS44:  0.03755211134963905\n",
            "Loss SS55:  0.03956628250133278\n",
            "Loss SS66:  0.03207173183072157\n",
            "Loss SS77:  0.04030986734137915\n",
            "Loss SS88:  0.03634027198508934\n",
            "Loss SS11:  0.03318237027777694\n",
            "Loss SS22:  0.032095570839590644\n",
            "Loss SS33:  0.02862932125410875\n",
            "Loss SS44:  0.03754893114527177\n",
            "Loss SS55:  0.03960484615075621\n",
            "Loss SS66:  0.03205607107058787\n",
            "Loss SS77:  0.04030606732639249\n",
            "Loss SS88:  0.03633477293223225\n",
            "Loss SS11:  0.033171477157753114\n",
            "Loss SS22:  0.03207448414072515\n",
            "Loss SS33:  0.02861006534846645\n",
            "Loss SS44:  0.03753754036686813\n",
            "Loss SS55:  0.03959481039579024\n",
            "Loss SS66:  0.03204460118237049\n",
            "Loss SS77:  0.04030234000151048\n",
            "Loss SS88:  0.03629373905633016\n",
            "Loss SS11:  0.03316011116534393\n",
            "Loss SS22:  0.032054361787055105\n",
            "Loss SS33:  0.028596265666832134\n",
            "Loss SS44:  0.03752503333872385\n",
            "Loss SS55:  0.0396085141759532\n",
            "Loss SS66:  0.03199830743193313\n",
            "Loss SS77:  0.04026174096493271\n",
            "Loss SS88:  0.036268670454112875\n",
            "Loss SS11:  0.03314484138508587\n",
            "Loss SS22:  0.0320491062053253\n",
            "Loss SS33:  0.028586497844751837\n",
            "Loss SS44:  0.03750990669402625\n",
            "Loss SS55:  0.039579713150211006\n",
            "Loss SS66:  0.03197837701005399\n",
            "Loss SS77:  0.04021935645119309\n",
            "Loss SS88:  0.03622917652778004\n",
            "Loss SS11:  0.03314702702393556\n",
            "Loss SS22:  0.03204533241299025\n",
            "Loss SS33:  0.028608032671776494\n",
            "Loss SS44:  0.03753340449602229\n",
            "Loss SS55:  0.039602131979944104\n",
            "Loss SS66:  0.03200428124675132\n",
            "Loss SS77:  0.04023561744655754\n",
            "Loss SS88:  0.036243245144957616\n",
            "Loss SS11:  0.0331109243007762\n",
            "Loss SS22:  0.03204228805600146\n",
            "Loss SS33:  0.02862338907110285\n",
            "Loss SS44:  0.03752135644030107\n",
            "Loss SS55:  0.03959610450043005\n",
            "Loss SS66:  0.031987529456470425\n",
            "Loss SS77:  0.04020842696809711\n",
            "Loss SS88:  0.03621612278939454\n",
            "Loss SS11:  0.033134637422629606\n",
            "Loss SS22:  0.03206409637389302\n",
            "Loss SS33:  0.02862476330082377\n",
            "Loss SS44:  0.037532331939618545\n",
            "Loss SS55:  0.03961546883659521\n",
            "Loss SS66:  0.03202088814308411\n",
            "Loss SS77:  0.040227323213056825\n",
            "Loss SS88:  0.03623634065344328\n",
            "Loss SS11:  0.0331486297507712\n",
            "Loss SS22:  0.03205457619302787\n",
            "Loss SS33:  0.028635077731653597\n",
            "Loss SS44:  0.03752019077435723\n",
            "Loss SS55:  0.03958749128051811\n",
            "Loss SS66:  0.032022624481858344\n",
            "Loss SS77:  0.040226727801865605\n",
            "Loss SS88:  0.03623861197615583\n",
            "Loss SS11:  0.03313827335986556\n",
            "Loss SS22:  0.03205518154195106\n",
            "Loss SS33:  0.02864010703840764\n",
            "Loss SS44:  0.03753102853759076\n",
            "Loss SS55:  0.03958444896232244\n",
            "Loss SS66:  0.03201619269170053\n",
            "Loss SS77:  0.04021630582321528\n",
            "Loss SS88:  0.03625188705821832\n",
            "Loss SS11:  0.03319531283909228\n",
            "Loss SS22:  0.032094137445753006\n",
            "Loss SS33:  0.02865114421098433\n",
            "Loss SS44:  0.03754551676484011\n",
            "Loss SS55:  0.039610959845543435\n",
            "Loss SS66:  0.03203622508637128\n",
            "Loss SS77:  0.04021358977334198\n",
            "Loss SS88:  0.03626906862296577\n",
            "Loss SS11:  0.0331605713887225\n",
            "Loss SS22:  0.032093808592303194\n",
            "Loss SS33:  0.028623071064821034\n",
            "Loss SS44:  0.03752716534534659\n",
            "Loss SS55:  0.039602059208325345\n",
            "Loss SS66:  0.03201647643040292\n",
            "Loss SS77:  0.04020897038038537\n",
            "Loss SS88:  0.0362704155235319\n",
            "Loss SS11:  0.03315227114518357\n",
            "Loss SS22:  0.032088640049068286\n",
            "Loss SS33:  0.028621578727494885\n",
            "Loss SS44:  0.03752633352267641\n",
            "Loss SS55:  0.039607488235850245\n",
            "Loss SS66:  0.03202210993935095\n",
            "Loss SS77:  0.040219253431333354\n",
            "Loss SS88:  0.036283171690225345\n",
            "Loss SS11:  0.03311804108507544\n",
            "Loss SS22:  0.03207508531900553\n",
            "Loss SS33:  0.02861980901035846\n",
            "Loss SS44:  0.037524873445008\n",
            "Loss SS55:  0.03962105840227708\n",
            "Loss SS66:  0.03202767205074151\n",
            "Loss SS77:  0.040237344940833884\n",
            "Loss SS88:  0.036260478535430844\n",
            "Loss SS11:  0.03309852222470062\n",
            "Loss SS22:  0.03205920748655398\n",
            "Loss SS33:  0.028593834076084332\n",
            "Loss SS44:  0.037488422973001805\n",
            "Loss SS55:  0.03958568939760843\n",
            "Loss SS66:  0.03200679978018137\n",
            "Loss SS77:  0.04021199301734961\n",
            "Loss SS88:  0.036244061553581425\n",
            "Validation: \n",
            " Loss SS11:  0.030185040086507797\n",
            " Loss SS22:  0.0418015718460083\n",
            " Loss SS33:  0.03732745721936226\n",
            " Loss SS44:  0.04858915135264397\n",
            " Loss SS55:  0.0477072075009346\n",
            " Loss SS66:  0.04048986732959747\n",
            " Loss SS77:  0.055890344083309174\n",
            " Loss SS88:  0.04074550420045853\n",
            " Loss SS11:  0.03271433090170225\n",
            " Loss SS22:  0.04743458508026032\n",
            " Loss SS33:  0.043562255267586024\n",
            " Loss SS44:  0.05636237455265863\n",
            " Loss SS55:  0.05972882040909359\n",
            " Loss SS66:  0.047877875821931024\n",
            " Loss SS77:  0.06306302316841625\n",
            " Loss SS88:  0.05295664834834281\n",
            " Loss SS11:  0.03282679562888494\n",
            " Loss SS22:  0.04716172841627423\n",
            " Loss SS33:  0.04354526674965533\n",
            " Loss SS44:  0.05627381329129382\n",
            " Loss SS55:  0.0595804776360349\n",
            " Loss SS66:  0.047829212938866966\n",
            " Loss SS77:  0.0625640699957929\n",
            " Loss SS88:  0.05329805476272979\n",
            " Loss SS11:  0.03261473289400828\n",
            " Loss SS22:  0.046751431632237356\n",
            " Loss SS33:  0.04296069005962278\n",
            " Loss SS44:  0.05595577257822772\n",
            " Loss SS55:  0.0588903584685482\n",
            " Loss SS66:  0.04758616964348027\n",
            " Loss SS77:  0.062007338236101335\n",
            " Loss SS88:  0.05260690851289718\n",
            " Loss SS11:  0.03257718047610036\n",
            " Loss SS22:  0.04658472294240822\n",
            " Loss SS33:  0.04306320114820092\n",
            " Loss SS44:  0.056025147530031794\n",
            " Loss SS55:  0.05853773120008869\n",
            " Loss SS66:  0.047407132377963006\n",
            " Loss SS77:  0.06175180565979746\n",
            " Loss SS88:  0.05252567067006488\n",
            "\n",
            "Epoch: 59\n",
            "Loss SS11:  0.034557636827230453\n",
            "Loss SS22:  0.034286245703697205\n",
            "Loss SS33:  0.029015550389885902\n",
            "Loss SS44:  0.03951697424054146\n",
            "Loss SS55:  0.0405849814414978\n",
            "Loss SS66:  0.032074760645627975\n",
            "Loss SS77:  0.04532073065638542\n",
            "Loss SS88:  0.03775569796562195\n",
            "Loss SS11:  0.03271736018359661\n",
            "Loss SS22:  0.031065163964574986\n",
            "Loss SS33:  0.028239925984631885\n",
            "Loss SS44:  0.03707211498509754\n",
            "Loss SS55:  0.03928331082517451\n",
            "Loss SS66:  0.031422531232237816\n",
            "Loss SS77:  0.040306705981492996\n",
            "Loss SS88:  0.03594751148061319\n",
            "Loss SS11:  0.03298761598056271\n",
            "Loss SS22:  0.031522840971038454\n",
            "Loss SS33:  0.028440081913556372\n",
            "Loss SS44:  0.037069463304110935\n",
            "Loss SS55:  0.03918667048925445\n",
            "Loss SS66:  0.03166563684741656\n",
            "Loss SS77:  0.040078851261309216\n",
            "Loss SS88:  0.03630517121581804\n",
            "Loss SS11:  0.03262681069393312\n",
            "Loss SS22:  0.03166501181981256\n",
            "Loss SS33:  0.028485217281887607\n",
            "Loss SS44:  0.03701975280719419\n",
            "Loss SS55:  0.03919280248303567\n",
            "Loss SS66:  0.03164270940807558\n",
            "Loss SS77:  0.040245978221777945\n",
            "Loss SS88:  0.036019445667343754\n",
            "Loss SS11:  0.03243535852468595\n",
            "Loss SS22:  0.031805331841474625\n",
            "Loss SS33:  0.02858127858035448\n",
            "Loss SS44:  0.0369923109324967\n",
            "Loss SS55:  0.039495073168016065\n",
            "Loss SS66:  0.03162520864933002\n",
            "Loss SS77:  0.040077562284905735\n",
            "Loss SS88:  0.036095765124006966\n",
            "Loss SS11:  0.032493376812221954\n",
            "Loss SS22:  0.032122278476462644\n",
            "Loss SS33:  0.02880084207829307\n",
            "Loss SS44:  0.037324929354237577\n",
            "Loss SS55:  0.03955221088493571\n",
            "Loss SS66:  0.03170955608434537\n",
            "Loss SS77:  0.04014527447083417\n",
            "Loss SS88:  0.036065241139309076\n",
            "Loss SS11:  0.03272224913855068\n",
            "Loss SS22:  0.03203490089441909\n",
            "Loss SS33:  0.028678480902167618\n",
            "Loss SS44:  0.037222141125163095\n",
            "Loss SS55:  0.03938348609648767\n",
            "Loss SS66:  0.031535040831468145\n",
            "Loss SS77:  0.03986370062730352\n",
            "Loss SS88:  0.03578399246955504\n",
            "Loss SS11:  0.032649888696385104\n",
            "Loss SS22:  0.03198755388213715\n",
            "Loss SS33:  0.02877538632863844\n",
            "Loss SS44:  0.037411078915629586\n",
            "Loss SS55:  0.039415727723652205\n",
            "Loss SS66:  0.031581535961636355\n",
            "Loss SS77:  0.03996487234679746\n",
            "Loss SS88:  0.035804596821397125\n",
            "Loss SS11:  0.03257472054273994\n",
            "Loss SS22:  0.03185018673999074\n",
            "Loss SS33:  0.028599557570285268\n",
            "Loss SS44:  0.03724686116164113\n",
            "Loss SS55:  0.03920604858869388\n",
            "Loss SS66:  0.03154618490809276\n",
            "Loss SS77:  0.03992167009431639\n",
            "Loss SS88:  0.0357100092748433\n",
            "Loss SS11:  0.03246817948644633\n",
            "Loss SS22:  0.031764758382361015\n",
            "Loss SS33:  0.028547807484046443\n",
            "Loss SS44:  0.037318193773319436\n",
            "Loss SS55:  0.039125491302091996\n",
            "Loss SS66:  0.03144816552790312\n",
            "Loss SS77:  0.03981691132207493\n",
            "Loss SS88:  0.03561557684052777\n",
            "Loss SS11:  0.03237151105583894\n",
            "Loss SS22:  0.03158607049891264\n",
            "Loss SS33:  0.028447901073953893\n",
            "Loss SS44:  0.03713024769089009\n",
            "Loss SS55:  0.03907747179417327\n",
            "Loss SS66:  0.0313616166684297\n",
            "Loss SS77:  0.03964755219398158\n",
            "Loss SS88:  0.03552218013913325\n",
            "Loss SS11:  0.0322848597356865\n",
            "Loss SS22:  0.03164970440169176\n",
            "Loss SS33:  0.028412080781014117\n",
            "Loss SS44:  0.03715485972058666\n",
            "Loss SS55:  0.03906184914815533\n",
            "Loss SS66:  0.031303515590660205\n",
            "Loss SS77:  0.03954767093465135\n",
            "Loss SS88:  0.035420114224826965\n",
            "Loss SS11:  0.03228360811664053\n",
            "Loss SS22:  0.0317078699657986\n",
            "Loss SS33:  0.028436135916301043\n",
            "Loss SS44:  0.037096775298522525\n",
            "Loss SS55:  0.039047487911360326\n",
            "Loss SS66:  0.03126477999689658\n",
            "Loss SS77:  0.03959660943258892\n",
            "Loss SS88:  0.03543973378529233\n",
            "Loss SS11:  0.03236056050953974\n",
            "Loss SS22:  0.03173397976251049\n",
            "Loss SS33:  0.028495015860855125\n",
            "Loss SS44:  0.03712258775156873\n",
            "Loss SS55:  0.03909810845747249\n",
            "Loss SS66:  0.0312751637672195\n",
            "Loss SS77:  0.039554157500503624\n",
            "Loss SS88:  0.0355494289964665\n",
            "Loss SS11:  0.03236766241438\n",
            "Loss SS22:  0.031771699296878585\n",
            "Loss SS33:  0.028532813270146964\n",
            "Loss SS44:  0.03719114319335484\n",
            "Loss SS55:  0.039127339311736696\n",
            "Loss SS66:  0.0313565399207122\n",
            "Loss SS77:  0.03955523199434822\n",
            "Loss SS88:  0.03557299531943409\n",
            "Loss SS11:  0.032428902950113185\n",
            "Loss SS22:  0.03182917434894881\n",
            "Loss SS33:  0.028587504074174836\n",
            "Loss SS44:  0.037240066127666574\n",
            "Loss SS55:  0.03921495777684332\n",
            "Loss SS66:  0.031399236757628964\n",
            "Loss SS77:  0.03961962392393327\n",
            "Loss SS88:  0.03561573742043893\n",
            "Loss SS11:  0.03256746389619682\n",
            "Loss SS22:  0.03178765802927639\n",
            "Loss SS33:  0.028560681187588234\n",
            "Loss SS44:  0.03712337586727942\n",
            "Loss SS55:  0.03917314725355332\n",
            "Loss SS66:  0.03137378200240757\n",
            "Loss SS77:  0.0396120204410938\n",
            "Loss SS88:  0.0356360315489843\n",
            "Loss SS11:  0.03270535093694054\n",
            "Loss SS22:  0.03190791817131447\n",
            "Loss SS33:  0.028631633539733133\n",
            "Loss SS44:  0.03717326438217832\n",
            "Loss SS55:  0.03918620706563108\n",
            "Loss SS66:  0.031479185196565604\n",
            "Loss SS77:  0.0397439604265648\n",
            "Loss SS88:  0.035748076107766896\n",
            "Loss SS11:  0.03270613099577019\n",
            "Loss SS22:  0.03194444850910434\n",
            "Loss SS33:  0.02863819423795405\n",
            "Loss SS44:  0.03723477872695712\n",
            "Loss SS55:  0.039283874264573526\n",
            "Loss SS66:  0.031522459705560904\n",
            "Loss SS77:  0.039810348034563646\n",
            "Loss SS88:  0.03583086794499534\n",
            "Loss SS11:  0.032684222439118706\n",
            "Loss SS22:  0.03191279572147037\n",
            "Loss SS33:  0.02856451157890065\n",
            "Loss SS44:  0.03715912712962215\n",
            "Loss SS55:  0.03924013377796293\n",
            "Loss SS66:  0.03149984069948733\n",
            "Loss SS77:  0.03979212952611958\n",
            "Loss SS88:  0.03578339698505027\n",
            "Loss SS11:  0.03267602248126594\n",
            "Loss SS22:  0.03190528593994492\n",
            "Loss SS33:  0.02858746568880864\n",
            "Loss SS44:  0.03715492380940499\n",
            "Loss SS55:  0.03918448112793823\n",
            "Loss SS66:  0.03149808196360199\n",
            "Loss SS77:  0.039801852527394224\n",
            "Loss SS88:  0.03581190631905599\n",
            "Loss SS11:  0.03270678931043894\n",
            "Loss SS22:  0.03195164862896594\n",
            "Loss SS33:  0.028579405266173644\n",
            "Loss SS44:  0.03717076432365942\n",
            "Loss SS55:  0.039130096487100655\n",
            "Loss SS66:  0.0315085701662076\n",
            "Loss SS77:  0.03986697128457481\n",
            "Loss SS88:  0.03583190065777697\n",
            "Loss SS11:  0.03273570168409412\n",
            "Loss SS22:  0.031944832844634403\n",
            "Loss SS33:  0.02858586759386559\n",
            "Loss SS44:  0.03716121512840236\n",
            "Loss SS55:  0.03920448879917822\n",
            "Loss SS66:  0.031515435429450074\n",
            "Loss SS77:  0.03992858689227795\n",
            "Loss SS88:  0.035842179223827646\n",
            "Loss SS11:  0.032723552446602744\n",
            "Loss SS22:  0.03189475622302268\n",
            "Loss SS33:  0.028579322274500156\n",
            "Loss SS44:  0.037205258489171146\n",
            "Loss SS55:  0.03917787404674472\n",
            "Loss SS66:  0.03151340819734\n",
            "Loss SS77:  0.03991421318544454\n",
            "Loss SS88:  0.0358509776299392\n",
            "Loss SS11:  0.03270632976132804\n",
            "Loss SS22:  0.031936323532981495\n",
            "Loss SS33:  0.028589100070699616\n",
            "Loss SS44:  0.03723628468100461\n",
            "Loss SS55:  0.03918902223535593\n",
            "Loss SS66:  0.0315370997452637\n",
            "Loss SS77:  0.039945488617875266\n",
            "Loss SS88:  0.0358380955871952\n",
            "Loss SS11:  0.03270532342186012\n",
            "Loss SS22:  0.03197333100159092\n",
            "Loss SS33:  0.028637145768361263\n",
            "Loss SS44:  0.03721297425459106\n",
            "Loss SS55:  0.03919687156242679\n",
            "Loss SS66:  0.03160923511621012\n",
            "Loss SS77:  0.039992799470386656\n",
            "Loss SS88:  0.03591853890760961\n",
            "Loss SS11:  0.03268758563674501\n",
            "Loss SS22:  0.03196610489653216\n",
            "Loss SS33:  0.02863629418514469\n",
            "Loss SS44:  0.037216676820169464\n",
            "Loss SS55:  0.03920225510766223\n",
            "Loss SS66:  0.03156553583497974\n",
            "Loss SS77:  0.04001823975436989\n",
            "Loss SS88:  0.0359117870158391\n",
            "Loss SS11:  0.0326619737908739\n",
            "Loss SS22:  0.03191970871603357\n",
            "Loss SS33:  0.02860884276318374\n",
            "Loss SS44:  0.037204568064685674\n",
            "Loss SS55:  0.03917108213604596\n",
            "Loss SS66:  0.031539733717527337\n",
            "Loss SS77:  0.04002195595320301\n",
            "Loss SS88:  0.03587761763964412\n",
            "Loss SS11:  0.032614580339013044\n",
            "Loss SS22:  0.03192766476091117\n",
            "Loss SS33:  0.028598220435590932\n",
            "Loss SS44:  0.03716654625466074\n",
            "Loss SS55:  0.039159974368335515\n",
            "Loss SS66:  0.031548323885295736\n",
            "Loss SS77:  0.03996584323165255\n",
            "Loss SS88:  0.0358404386656034\n",
            "Loss SS11:  0.03260377729215573\n",
            "Loss SS22:  0.03193434108931994\n",
            "Loss SS33:  0.0285915079433791\n",
            "Loss SS44:  0.03715694834609417\n",
            "Loss SS55:  0.03918596637464061\n",
            "Loss SS66:  0.031614457458718534\n",
            "Loss SS77:  0.03999511851477869\n",
            "Loss SS88:  0.035851535409740155\n",
            "Loss SS11:  0.03263028267387338\n",
            "Loss SS22:  0.03194255610785611\n",
            "Loss SS33:  0.028606498910986704\n",
            "Loss SS44:  0.03720284028727549\n",
            "Loss SS55:  0.039223689177503615\n",
            "Loss SS66:  0.03164618074324638\n",
            "Loss SS77:  0.0400292680682336\n",
            "Loss SS88:  0.03586464796673222\n",
            "Loss SS11:  0.032645254141360616\n",
            "Loss SS22:  0.03191404412365803\n",
            "Loss SS33:  0.02859343145749385\n",
            "Loss SS44:  0.03717305104568649\n",
            "Loss SS55:  0.03920344106397828\n",
            "Loss SS66:  0.03162844522206346\n",
            "Loss SS77:  0.03997760450533348\n",
            "Loss SS88:  0.03583692493953314\n",
            "Loss SS11:  0.03263110052388034\n",
            "Loss SS22:  0.03191295482009371\n",
            "Loss SS33:  0.028589041526443863\n",
            "Loss SS44:  0.037191248635218895\n",
            "Loss SS55:  0.03919839777771929\n",
            "Loss SS66:  0.03167864118015098\n",
            "Loss SS77:  0.0400289826303999\n",
            "Loss SS88:  0.035824967275527406\n",
            "Loss SS11:  0.032592048210139724\n",
            "Loss SS22:  0.03188563537224002\n",
            "Loss SS33:  0.028588179037470112\n",
            "Loss SS44:  0.03720038024612964\n",
            "Loss SS55:  0.039187140841588516\n",
            "Loss SS66:  0.03169580063926129\n",
            "Loss SS77:  0.040012344976836464\n",
            "Loss SS88:  0.03584364990723457\n",
            "Loss SS11:  0.032641739663708945\n",
            "Loss SS22:  0.03191439132820651\n",
            "Loss SS33:  0.02861568717903231\n",
            "Loss SS44:  0.03726204236166393\n",
            "Loss SS55:  0.03923728934262505\n",
            "Loss SS66:  0.03173456234513315\n",
            "Loss SS77:  0.0400450676351873\n",
            "Loss SS88:  0.03589277973404029\n",
            "Loss SS11:  0.032669816983391414\n",
            "Loss SS22:  0.031894871121288366\n",
            "Loss SS33:  0.028623570271494383\n",
            "Loss SS44:  0.037246358493327074\n",
            "Loss SS55:  0.03925618605735975\n",
            "Loss SS66:  0.031763027419476765\n",
            "Loss SS77:  0.04005401813958445\n",
            "Loss SS88:  0.035883867248171074\n",
            "Loss SS11:  0.03275048176027896\n",
            "Loss SS22:  0.031885321146274535\n",
            "Loss SS33:  0.028648353493940137\n",
            "Loss SS44:  0.03726087797980064\n",
            "Loss SS55:  0.03925796069844608\n",
            "Loss SS66:  0.03176792256967513\n",
            "Loss SS77:  0.040043241811343507\n",
            "Loss SS88:  0.03590986828146879\n",
            "Loss SS11:  0.032753006221269663\n",
            "Loss SS22:  0.031870593228992425\n",
            "Loss SS33:  0.028622344763813957\n",
            "Loss SS44:  0.03724057142524867\n",
            "Loss SS55:  0.039249680012544856\n",
            "Loss SS66:  0.031756038318144664\n",
            "Loss SS77:  0.040028855154376146\n",
            "Loss SS88:  0.03588535730046725\n",
            "Loss SS11:  0.032756460273242374\n",
            "Loss SS22:  0.031870881550190015\n",
            "Loss SS33:  0.028609767869075764\n",
            "Loss SS44:  0.037239285968146296\n",
            "Loss SS55:  0.03923653546241638\n",
            "Loss SS66:  0.031751149424456865\n",
            "Loss SS77:  0.04002737840564232\n",
            "Loss SS88:  0.03584540397750111\n",
            "Loss SS11:  0.0327514355425792\n",
            "Loss SS22:  0.03186821014813297\n",
            "Loss SS33:  0.02859022213941645\n",
            "Loss SS44:  0.037236124367627035\n",
            "Loss SS55:  0.039235265987456\n",
            "Loss SS66:  0.0317464030974204\n",
            "Loss SS77:  0.040030002641632124\n",
            "Loss SS88:  0.035848468842218296\n",
            "Loss SS11:  0.0327317888089026\n",
            "Loss SS22:  0.031879426758188265\n",
            "Loss SS33:  0.028600543570191487\n",
            "Loss SS44:  0.037270010952974796\n",
            "Loss SS55:  0.03925061958872172\n",
            "Loss SS66:  0.03178060447281584\n",
            "Loss SS77:  0.0400115367944847\n",
            "Loss SS88:  0.03586541642516183\n",
            "Loss SS11:  0.03271535677051312\n",
            "Loss SS22:  0.03188347379130894\n",
            "Loss SS33:  0.028612438154264088\n",
            "Loss SS44:  0.03725175627047273\n",
            "Loss SS55:  0.03924747620802146\n",
            "Loss SS66:  0.03177934482584904\n",
            "Loss SS77:  0.039989826824615764\n",
            "Loss SS88:  0.03585785275230007\n",
            "Loss SS11:  0.032719092501101756\n",
            "Loss SS22:  0.031908010900232965\n",
            "Loss SS33:  0.028622590899113523\n",
            "Loss SS44:  0.037258852366136824\n",
            "Loss SS55:  0.039259479197534135\n",
            "Loss SS66:  0.031800227761799534\n",
            "Loss SS77:  0.03998489023989283\n",
            "Loss SS88:  0.03584792134916131\n",
            "Loss SS11:  0.032713903395109\n",
            "Loss SS22:  0.031892551338243924\n",
            "Loss SS33:  0.028615336121857304\n",
            "Loss SS44:  0.03724049683876353\n",
            "Loss SS55:  0.039242479435909375\n",
            "Loss SS66:  0.031792273762227485\n",
            "Loss SS77:  0.039944789257859964\n",
            "Loss SS88:  0.03582441907056246\n",
            "Loss SS11:  0.03272784712920789\n",
            "Loss SS22:  0.03191796634472957\n",
            "Loss SS33:  0.02861591254924836\n",
            "Loss SS44:  0.03725297213901603\n",
            "Loss SS55:  0.03922917867446838\n",
            "Loss SS66:  0.03177970529971074\n",
            "Loss SS77:  0.039934877559644026\n",
            "Loss SS88:  0.03583787036151572\n",
            "Loss SS11:  0.032779172744329385\n",
            "Loss SS22:  0.03193478213990739\n",
            "Loss SS33:  0.02863129452201321\n",
            "Loss SS44:  0.03726002020955482\n",
            "Loss SS55:  0.039243546998058085\n",
            "Loss SS66:  0.0317817792098168\n",
            "Loss SS77:  0.0399361569393104\n",
            "Loss SS88:  0.03586382882260695\n",
            "Loss SS11:  0.03277570688026329\n",
            "Loss SS22:  0.031940952072691764\n",
            "Loss SS33:  0.028619257563527133\n",
            "Loss SS44:  0.03725416730855008\n",
            "Loss SS55:  0.03925596360610131\n",
            "Loss SS66:  0.031772850889413576\n",
            "Loss SS77:  0.03994348549047675\n",
            "Loss SS88:  0.03587242805776782\n",
            "Loss SS11:  0.0327787940408204\n",
            "Loss SS22:  0.03193808999110306\n",
            "Loss SS33:  0.02861095792485397\n",
            "Loss SS44:  0.037246283036982936\n",
            "Loss SS55:  0.03927451401015629\n",
            "Loss SS66:  0.03177821004640651\n",
            "Loss SS77:  0.039955512472208896\n",
            "Loss SS88:  0.0358783622448295\n",
            "Loss SS11:  0.032759653880665554\n",
            "Loss SS22:  0.03194861009856396\n",
            "Loss SS33:  0.028606346078418398\n",
            "Loss SS44:  0.037248614242226814\n",
            "Loss SS55:  0.03926974000204006\n",
            "Loss SS66:  0.03180266950442117\n",
            "Loss SS77:  0.03995995132907017\n",
            "Loss SS88:  0.03587809666263214\n",
            "Loss SS11:  0.0327345059346218\n",
            "Loss SS22:  0.031911556940912716\n",
            "Loss SS33:  0.028583532798648364\n",
            "Loss SS44:  0.03722246634999628\n",
            "Loss SS55:  0.03922948595933055\n",
            "Loss SS66:  0.031766036517781544\n",
            "Loss SS77:  0.039945953588196796\n",
            "Loss SS88:  0.035860747383440826\n",
            "Validation: \n",
            " Loss SS11:  0.03096630983054638\n",
            " Loss SS22:  0.039671674370765686\n",
            " Loss SS33:  0.038473501801490784\n",
            " Loss SS44:  0.049293216317892075\n",
            " Loss SS55:  0.04888470098376274\n",
            " Loss SS66:  0.041281651705503464\n",
            " Loss SS77:  0.05647990480065346\n",
            " Loss SS88:  0.04075530543923378\n",
            " Loss SS11:  0.03280872266207423\n",
            " Loss SS22:  0.04627687590462821\n",
            " Loss SS33:  0.0427493905382497\n",
            " Loss SS44:  0.057389368081376665\n",
            " Loss SS55:  0.05973155832006818\n",
            " Loss SS66:  0.04818488125290189\n",
            " Loss SS77:  0.06411349081567355\n",
            " Loss SS88:  0.051576646665732064\n",
            " Loss SS11:  0.03283090630500782\n",
            " Loss SS22:  0.04614181543995694\n",
            " Loss SS33:  0.0427778257829387\n",
            " Loss SS44:  0.057345241035630067\n",
            " Loss SS55:  0.05978999486783656\n",
            " Loss SS66:  0.04803915785216704\n",
            " Loss SS77:  0.0639517106479261\n",
            " Loss SS88:  0.05214633592745153\n",
            " Loss SS11:  0.0325886323803761\n",
            " Loss SS22:  0.04571752384549282\n",
            " Loss SS33:  0.04232304109657397\n",
            " Loss SS44:  0.05689789733437241\n",
            " Loss SS55:  0.05896500925548741\n",
            " Loss SS66:  0.04784430477951394\n",
            " Loss SS77:  0.0632388501382265\n",
            " Loss SS88:  0.05156659591393393\n",
            " Loss SS11:  0.03262117907496882\n",
            " Loss SS22:  0.04561447982250908\n",
            " Loss SS33:  0.042351197727300505\n",
            " Loss SS44:  0.05705639862535912\n",
            " Loss SS55:  0.05867737609847092\n",
            " Loss SS66:  0.04762577132126431\n",
            " Loss SS77:  0.06313381987957307\n",
            " Loss SS88:  0.05144969833853804\n",
            "\n",
            "Epoch: 60\n",
            "Loss SS11:  0.03544709086418152\n",
            "Loss SS22:  0.03208861127495766\n",
            "Loss SS33:  0.029491230845451355\n",
            "Loss SS44:  0.04153072088956833\n",
            "Loss SS55:  0.041181161999702454\n",
            "Loss SS66:  0.032904334366321564\n",
            "Loss SS77:  0.04187292978167534\n",
            "Loss SS88:  0.039236441254615784\n",
            "Loss SS11:  0.03360032493417913\n",
            "Loss SS22:  0.03141784820366989\n",
            "Loss SS33:  0.02844693020663478\n",
            "Loss SS44:  0.037089387801560486\n",
            "Loss SS55:  0.038728423077951775\n",
            "Loss SS66:  0.03182878548448736\n",
            "Loss SS77:  0.0390370251102881\n",
            "Loss SS88:  0.0359483195299452\n",
            "Loss SS11:  0.033295201669846265\n",
            "Loss SS22:  0.031810681734766276\n",
            "Loss SS33:  0.028290240714947384\n",
            "Loss SS44:  0.037244782206558046\n",
            "Loss SS55:  0.03856706938573292\n",
            "Loss SS66:  0.032073178905106726\n",
            "Loss SS77:  0.03944781901580947\n",
            "Loss SS88:  0.036176947078534534\n",
            "Loss SS11:  0.03289184899580094\n",
            "Loss SS22:  0.03168689874151061\n",
            "Loss SS33:  0.02819259657013801\n",
            "Loss SS44:  0.03694008783467354\n",
            "Loss SS55:  0.03830449042781707\n",
            "Loss SS66:  0.03187591804852409\n",
            "Loss SS77:  0.03948490417772724\n",
            "Loss SS88:  0.03566903705077787\n",
            "Loss SS11:  0.03260233262326659\n",
            "Loss SS22:  0.03164181749268276\n",
            "Loss SS33:  0.02814063938652597\n",
            "Loss SS44:  0.037156637031130674\n",
            "Loss SS55:  0.038443081716938714\n",
            "Loss SS66:  0.032017741079737504\n",
            "Loss SS77:  0.039660682765448964\n",
            "Loss SS88:  0.03576557292807393\n",
            "Loss SS11:  0.032765817408468206\n",
            "Loss SS22:  0.032032874723275505\n",
            "Loss SS33:  0.028433266661915126\n",
            "Loss SS44:  0.03738525582879197\n",
            "Loss SS55:  0.03882395614888154\n",
            "Loss SS66:  0.03206408323318351\n",
            "Loss SS77:  0.03973056419807322\n",
            "Loss SS88:  0.03606393392763886\n",
            "Loss SS11:  0.03287149358113281\n",
            "Loss SS22:  0.031957832668892676\n",
            "Loss SS33:  0.02827046668065376\n",
            "Loss SS44:  0.037138418584573465\n",
            "Loss SS55:  0.038761866752241476\n",
            "Loss SS66:  0.03176382242045442\n",
            "Loss SS77:  0.03953806378069471\n",
            "Loss SS88:  0.0357207220169853\n",
            "Loss SS11:  0.03291985728371311\n",
            "Loss SS22:  0.03202506414496563\n",
            "Loss SS33:  0.02844922380967879\n",
            "Loss SS44:  0.03745810336000483\n",
            "Loss SS55:  0.03886927753477029\n",
            "Loss SS66:  0.03179569731295948\n",
            "Loss SS77:  0.03973007726837212\n",
            "Loss SS88:  0.035828184572533825\n",
            "Loss SS11:  0.032734143582207186\n",
            "Loss SS22:  0.03180454337946427\n",
            "Loss SS33:  0.02834845150326505\n",
            "Loss SS44:  0.037254872651379785\n",
            "Loss SS55:  0.038688748569032295\n",
            "Loss SS66:  0.031715586896479867\n",
            "Loss SS77:  0.03952230545289723\n",
            "Loss SS88:  0.0358186325541249\n",
            "Loss SS11:  0.0325942536263348\n",
            "Loss SS22:  0.03164160789942348\n",
            "Loss SS33:  0.02822603496139521\n",
            "Loss SS44:  0.03716148624380866\n",
            "Loss SS55:  0.03860361595730205\n",
            "Loss SS66:  0.03164196219090577\n",
            "Loss SS77:  0.039344689184492763\n",
            "Loss SS88:  0.03573529645391218\n",
            "Loss SS11:  0.032568790092326626\n",
            "Loss SS22:  0.031615727922261354\n",
            "Loss SS33:  0.02820919675402122\n",
            "Loss SS44:  0.03704127794740224\n",
            "Loss SS55:  0.03853035187072093\n",
            "Loss SS66:  0.031514908159428305\n",
            "Loss SS77:  0.0392293152906517\n",
            "Loss SS88:  0.03562605176305417\n",
            "Loss SS11:  0.0325346708398413\n",
            "Loss SS22:  0.03152548134125568\n",
            "Loss SS33:  0.028133719236598357\n",
            "Loss SS44:  0.03698716799283887\n",
            "Loss SS55:  0.03859755470677539\n",
            "Loss SS66:  0.03144755390648906\n",
            "Loss SS77:  0.03915005076575924\n",
            "Loss SS88:  0.035531933528480227\n",
            "Loss SS11:  0.03258494958897268\n",
            "Loss SS22:  0.031535194524802454\n",
            "Loss SS33:  0.028136610954014724\n",
            "Loss SS44:  0.03701866020964197\n",
            "Loss SS55:  0.03863891426566218\n",
            "Loss SS66:  0.031482366521742716\n",
            "Loss SS77:  0.03927299214049804\n",
            "Loss SS88:  0.03556732831846091\n",
            "Loss SS11:  0.03271470844518137\n",
            "Loss SS22:  0.031597838559337246\n",
            "Loss SS33:  0.02822057244488756\n",
            "Loss SS44:  0.03709852152312075\n",
            "Loss SS55:  0.03870609985621831\n",
            "Loss SS66:  0.0315636450865569\n",
            "Loss SS77:  0.03937870155991489\n",
            "Loss SS88:  0.035688479111053564\n",
            "Loss SS11:  0.03272538974299921\n",
            "Loss SS22:  0.03165769123914817\n",
            "Loss SS33:  0.028267101100678985\n",
            "Loss SS44:  0.037115485083760946\n",
            "Loss SS55:  0.03871349554429663\n",
            "Loss SS66:  0.031598764663258345\n",
            "Loss SS77:  0.03941861006385046\n",
            "Loss SS88:  0.03574173275302065\n",
            "Loss SS11:  0.032706048536971705\n",
            "Loss SS22:  0.03174337893971149\n",
            "Loss SS33:  0.028363578847998026\n",
            "Loss SS44:  0.03718067360240103\n",
            "Loss SS55:  0.03883041949658994\n",
            "Loss SS66:  0.031718346234781064\n",
            "Loss SS77:  0.03953338241735042\n",
            "Loss SS88:  0.03582456754828921\n",
            "Loss SS11:  0.032726446583126644\n",
            "Loss SS22:  0.031695774368802956\n",
            "Loss SS33:  0.02828676919944538\n",
            "Loss SS44:  0.03710309286480364\n",
            "Loss SS55:  0.038785827108977\n",
            "Loss SS66:  0.031716609177011884\n",
            "Loss SS77:  0.03960171124394636\n",
            "Loss SS88:  0.03582775146231888\n",
            "Loss SS11:  0.03275156607143363\n",
            "Loss SS22:  0.031707077521329737\n",
            "Loss SS33:  0.02828837986708733\n",
            "Loss SS44:  0.03709886994278222\n",
            "Loss SS55:  0.03881038435631328\n",
            "Loss SS66:  0.031773755806144215\n",
            "Loss SS77:  0.03972756718857247\n",
            "Loss SS88:  0.03596477299841524\n",
            "Loss SS11:  0.0327105235143755\n",
            "Loss SS22:  0.03176634332669374\n",
            "Loss SS33:  0.02827816851725236\n",
            "Loss SS44:  0.037189063240480685\n",
            "Loss SS55:  0.038865757643023906\n",
            "Loss SS66:  0.031819312622286995\n",
            "Loss SS77:  0.039818419400828975\n",
            "Loss SS88:  0.0360357857911297\n",
            "Loss SS11:  0.03275850377196729\n",
            "Loss SS22:  0.031783037931117086\n",
            "Loss SS33:  0.028280952574772984\n",
            "Loss SS44:  0.03714924241313759\n",
            "Loss SS55:  0.03888873342444135\n",
            "Loss SS66:  0.03182432322680014\n",
            "Loss SS77:  0.03986127463934933\n",
            "Loss SS88:  0.03599009233775563\n",
            "Loss SS11:  0.032773268450774364\n",
            "Loss SS22:  0.031775490932203644\n",
            "Loss SS33:  0.028320941684851005\n",
            "Loss SS44:  0.037154226438767875\n",
            "Loss SS55:  0.03884502915452369\n",
            "Loss SS66:  0.03180100665015368\n",
            "Loss SS77:  0.03982807447512945\n",
            "Loss SS88:  0.03593655790559095\n",
            "Loss SS11:  0.032795062509334484\n",
            "Loss SS22:  0.031773360772720444\n",
            "Loss SS33:  0.02836357592053323\n",
            "Loss SS44:  0.03717830416998027\n",
            "Loss SS55:  0.03887093805165087\n",
            "Loss SS66:  0.031824415072939975\n",
            "Loss SS77:  0.03987373958972958\n",
            "Loss SS88:  0.03594266175694093\n",
            "Loss SS11:  0.03281553623970278\n",
            "Loss SS22:  0.03173866540159845\n",
            "Loss SS33:  0.02836237645526817\n",
            "Loss SS44:  0.03721416490447467\n",
            "Loss SS55:  0.03884408368446708\n",
            "Loss SS66:  0.03182817533679677\n",
            "Loss SS77:  0.03987738660355499\n",
            "Loss SS88:  0.03593501129200286\n",
            "Loss SS11:  0.0327670547579016\n",
            "Loss SS22:  0.0317217062642693\n",
            "Loss SS33:  0.028376001711467126\n",
            "Loss SS44:  0.037213040991520986\n",
            "Loss SS55:  0.03880911367215636\n",
            "Loss SS66:  0.0317917364322907\n",
            "Loss SS77:  0.03987196247492518\n",
            "Loss SS88:  0.03592546194695033\n",
            "Loss SS11:  0.032771295083385284\n",
            "Loss SS22:  0.03176752392867294\n",
            "Loss SS33:  0.028390770577110692\n",
            "Loss SS44:  0.03726996212455742\n",
            "Loss SS55:  0.03883971864380777\n",
            "Loss SS66:  0.031795553414777106\n",
            "Loss SS77:  0.03995124136263899\n",
            "Loss SS88:  0.035920767036278216\n",
            "Loss SS11:  0.03275724742338952\n",
            "Loss SS22:  0.03178561533025774\n",
            "Loss SS33:  0.028393031710172555\n",
            "Loss SS44:  0.03727009037576349\n",
            "Loss SS55:  0.03883910830692941\n",
            "Loss SS66:  0.03180416311282086\n",
            "Loss SS77:  0.03998928831749228\n",
            "Loss SS88:  0.03595890622007182\n",
            "Loss SS11:  0.03274992847693834\n",
            "Loss SS22:  0.03179710820117444\n",
            "Loss SS33:  0.028397607199590783\n",
            "Loss SS44:  0.03727056338698014\n",
            "Loss SS55:  0.03886517536937049\n",
            "Loss SS66:  0.03181615401187848\n",
            "Loss SS77:  0.04000765486295652\n",
            "Loss SS88:  0.03599370003346054\n",
            "Loss SS11:  0.03269908530465791\n",
            "Loss SS22:  0.03177189316971715\n",
            "Loss SS33:  0.028361758471525025\n",
            "Loss SS44:  0.037264457941605156\n",
            "Loss SS55:  0.038866771474425646\n",
            "Loss SS66:  0.03180434067503333\n",
            "Loss SS77:  0.040013097693999314\n",
            "Loss SS88:  0.035966357841702844\n",
            "Loss SS11:  0.03269482575012272\n",
            "Loss SS22:  0.03175446121983257\n",
            "Loss SS33:  0.028344651916167064\n",
            "Loss SS44:  0.037236323414430075\n",
            "Loss SS55:  0.03888772884085509\n",
            "Loss SS66:  0.03180798603838237\n",
            "Loss SS77:  0.039975222637644865\n",
            "Loss SS88:  0.03592327099916562\n",
            "Loss SS11:  0.032684426793118115\n",
            "Loss SS22:  0.03177419535278045\n",
            "Loss SS33:  0.028331175203278304\n",
            "Loss SS44:  0.0371859640753556\n",
            "Loss SS55:  0.03891426762661983\n",
            "Loss SS66:  0.031823443479298316\n",
            "Loss SS77:  0.04000731229884518\n",
            "Loss SS88:  0.03593284186756693\n",
            "Loss SS11:  0.032664887959824446\n",
            "Loss SS22:  0.03178682642991955\n",
            "Loss SS33:  0.028318419477026725\n",
            "Loss SS44:  0.037192753593489974\n",
            "Loss SS55:  0.038923636103587296\n",
            "Loss SS66:  0.031845774625325916\n",
            "Loss SS77:  0.04001054687555446\n",
            "Loss SS88:  0.035925422946727556\n",
            "Loss SS11:  0.03265272876265731\n",
            "Loss SS22:  0.03177223494126674\n",
            "Loss SS33:  0.028298473692352364\n",
            "Loss SS44:  0.03718334159714999\n",
            "Loss SS55:  0.038897993013119005\n",
            "Loss SS66:  0.03183553932635922\n",
            "Loss SS77:  0.0399683574364308\n",
            "Loss SS88:  0.03586933326419335\n",
            "Loss SS11:  0.032638538820303487\n",
            "Loss SS22:  0.03173514839110902\n",
            "Loss SS33:  0.02828226781035324\n",
            "Loss SS44:  0.03719425832742471\n",
            "Loss SS55:  0.03890134541555729\n",
            "Loss SS66:  0.03182747530115542\n",
            "Loss SS77:  0.039949060855810514\n",
            "Loss SS88:  0.035861019654390966\n",
            "Loss SS11:  0.03263506412033408\n",
            "Loss SS22:  0.031717195122128526\n",
            "Loss SS33:  0.028273042792234897\n",
            "Loss SS44:  0.037188950869792174\n",
            "Loss SS55:  0.03890604322095292\n",
            "Loss SS66:  0.03182846047209289\n",
            "Loss SS77:  0.03995089294822195\n",
            "Loss SS88:  0.03587952482983965\n",
            "Loss SS11:  0.03265084568247243\n",
            "Loss SS22:  0.03171202790024868\n",
            "Loss SS33:  0.028290926934905414\n",
            "Loss SS44:  0.03721234779204092\n",
            "Loss SS55:  0.038947662420549\n",
            "Loss SS66:  0.031847849062198764\n",
            "Loss SS77:  0.03999016321430807\n",
            "Loss SS88:  0.0358886058521498\n",
            "Loss SS11:  0.03267592578553236\n",
            "Loss SS22:  0.03169106178388976\n",
            "Loss SS33:  0.028320403843905852\n",
            "Loss SS44:  0.03722115585904176\n",
            "Loss SS55:  0.03897828552500475\n",
            "Loss SS66:  0.03185613395545388\n",
            "Loss SS77:  0.03997798527238036\n",
            "Loss SS88:  0.035911649120626624\n",
            "Loss SS11:  0.032677245174129586\n",
            "Loss SS22:  0.03170407297724337\n",
            "Loss SS33:  0.028335536533445532\n",
            "Loss SS44:  0.03721281998581834\n",
            "Loss SS55:  0.038978538592023534\n",
            "Loss SS66:  0.03185270964542089\n",
            "Loss SS77:  0.039994274346039235\n",
            "Loss SS88:  0.03592032554799335\n",
            "Loss SS11:  0.032659694371679725\n",
            "Loss SS22:  0.03170304627953514\n",
            "Loss SS33:  0.028333206281989733\n",
            "Loss SS44:  0.03720943799438181\n",
            "Loss SS55:  0.03898591797226523\n",
            "Loss SS66:  0.031828092235439226\n",
            "Loss SS77:  0.03999356851182537\n",
            "Loss SS88:  0.03591118553638298\n",
            "Loss SS11:  0.032663703864327875\n",
            "Loss SS22:  0.031717070361252216\n",
            "Loss SS33:  0.02833342901247693\n",
            "Loss SS44:  0.03721731680193598\n",
            "Loss SS55:  0.0390025447770523\n",
            "Loss SS66:  0.031829974362308895\n",
            "Loss SS77:  0.039982228571661504\n",
            "Loss SS88:  0.03589792955872111\n",
            "Loss SS11:  0.03265689422026315\n",
            "Loss SS22:  0.0317216294262644\n",
            "Loss SS33:  0.02834392654354615\n",
            "Loss SS44:  0.03719880436654286\n",
            "Loss SS55:  0.03896734021280123\n",
            "Loss SS66:  0.0317860379948488\n",
            "Loss SS77:  0.0399649947824533\n",
            "Loss SS88:  0.03588665259139769\n",
            "Loss SS11:  0.03266066585451439\n",
            "Loss SS22:  0.03172407108835151\n",
            "Loss SS33:  0.028366405944202904\n",
            "Loss SS44:  0.037247163148369275\n",
            "Loss SS55:  0.03896021335722502\n",
            "Loss SS66:  0.03181237828664649\n",
            "Loss SS77:  0.03998516672790199\n",
            "Loss SS88:  0.035903600845811075\n",
            "Loss SS11:  0.03262021121118046\n",
            "Loss SS22:  0.031709197944895774\n",
            "Loss SS33:  0.028363549098408717\n",
            "Loss SS44:  0.03724826478066236\n",
            "Loss SS55:  0.03893855347359267\n",
            "Loss SS66:  0.03179318920104173\n",
            "Loss SS77:  0.03995297964290691\n",
            "Loss SS88:  0.03588262032218948\n",
            "Loss SS11:  0.032600070533482024\n",
            "Loss SS22:  0.03173498164269534\n",
            "Loss SS33:  0.028361541610609324\n",
            "Loss SS44:  0.03723713283973465\n",
            "Loss SS55:  0.038953306131294954\n",
            "Loss SS66:  0.03180186021320894\n",
            "Loss SS77:  0.03994040535386838\n",
            "Loss SS88:  0.03587559429649927\n",
            "Loss SS11:  0.032602155598403404\n",
            "Loss SS22:  0.03172473627598546\n",
            "Loss SS33:  0.028368447071023442\n",
            "Loss SS44:  0.03721066337865356\n",
            "Loss SS55:  0.03893485693844455\n",
            "Loss SS66:  0.031787033056680396\n",
            "Loss SS77:  0.03989133062467774\n",
            "Loss SS88:  0.03585386300014371\n",
            "Loss SS11:  0.03260527796147902\n",
            "Loss SS22:  0.031737170969885764\n",
            "Loss SS33:  0.028368617452326276\n",
            "Loss SS44:  0.03721484447282458\n",
            "Loss SS55:  0.038925118271320586\n",
            "Loss SS66:  0.03176407771113238\n",
            "Loss SS77:  0.03987910824905988\n",
            "Loss SS88:  0.03585221424546777\n",
            "Loss SS11:  0.032637056223469935\n",
            "Loss SS22:  0.0317561709654503\n",
            "Loss SS33:  0.028395493764702868\n",
            "Loss SS44:  0.03722502266422344\n",
            "Loss SS55:  0.03891755492651542\n",
            "Loss SS66:  0.031751175313989764\n",
            "Loss SS77:  0.039863572730589866\n",
            "Loss SS88:  0.03586138990148076\n",
            "Loss SS11:  0.0326401960314807\n",
            "Loss SS22:  0.03174741404159338\n",
            "Loss SS33:  0.028399868227880844\n",
            "Loss SS44:  0.03722312926858211\n",
            "Loss SS55:  0.038946992905564005\n",
            "Loss SS66:  0.03174856511589116\n",
            "Loss SS77:  0.039884602655040464\n",
            "Loss SS88:  0.035871933430530245\n",
            "Loss SS11:  0.032653319453642624\n",
            "Loss SS22:  0.031739912298359686\n",
            "Loss SS33:  0.028407546234833207\n",
            "Loss SS44:  0.03722606034605366\n",
            "Loss SS55:  0.038946653445371904\n",
            "Loss SS66:  0.03175085064442927\n",
            "Loss SS77:  0.03990606562631905\n",
            "Loss SS88:  0.035868205624432825\n",
            "Loss SS11:  0.0326555622025178\n",
            "Loss SS22:  0.031734601069791646\n",
            "Loss SS33:  0.028407154112992317\n",
            "Loss SS44:  0.03721194499774435\n",
            "Loss SS55:  0.038956803205789\n",
            "Loss SS66:  0.03174748588856813\n",
            "Loss SS77:  0.03990286777644048\n",
            "Loss SS88:  0.035877640101586214\n",
            "Loss SS11:  0.03262840698107076\n",
            "Loss SS22:  0.03170162885091334\n",
            "Loss SS33:  0.028393504795923737\n",
            "Loss SS44:  0.03719316099726741\n",
            "Loss SS55:  0.03892447443606897\n",
            "Loss SS66:  0.03173205358129898\n",
            "Loss SS77:  0.03989072078023326\n",
            "Loss SS88:  0.03584510668763197\n",
            "Validation: \n",
            " Loss SS11:  0.030878257006406784\n",
            " Loss SS22:  0.03990497812628746\n",
            " Loss SS33:  0.03827541321516037\n",
            " Loss SS44:  0.049776267260313034\n",
            " Loss SS55:  0.05013372376561165\n",
            " Loss SS66:  0.043643560260534286\n",
            " Loss SS77:  0.05303485319018364\n",
            " Loss SS88:  0.04122199863195419\n",
            " Loss SS11:  0.032380638200612294\n",
            " Loss SS22:  0.047061563070331304\n",
            " Loss SS33:  0.043756053561256045\n",
            " Loss SS44:  0.05852079320521582\n",
            " Loss SS55:  0.06167169837724595\n",
            " Loss SS66:  0.050628947537569774\n",
            " Loss SS77:  0.06327201656642414\n",
            " Loss SS88:  0.05273151592839332\n",
            " Loss SS11:  0.03253062133018563\n",
            " Loss SS22:  0.04689709605967126\n",
            " Loss SS33:  0.04368965009727129\n",
            " Loss SS44:  0.058377870790115215\n",
            " Loss SS55:  0.06149813423796398\n",
            " Loss SS66:  0.05051590820274702\n",
            " Loss SS77:  0.06322807536982908\n",
            " Loss SS88:  0.0530708896495947\n",
            " Loss SS11:  0.032331430063140196\n",
            " Loss SS22:  0.0463310677985676\n",
            " Loss SS33:  0.04315385552214795\n",
            " Loss SS44:  0.05808894794251098\n",
            " Loss SS55:  0.060580670528235986\n",
            " Loss SS66:  0.050288783905447505\n",
            " Loss SS77:  0.062424924285685426\n",
            " Loss SS88:  0.05246969196395796\n",
            " Loss SS11:  0.032402386934852895\n",
            " Loss SS22:  0.04627552886068085\n",
            " Loss SS33:  0.043223183786059605\n",
            " Loss SS44:  0.058200277710034526\n",
            " Loss SS55:  0.060284140393321896\n",
            " Loss SS66:  0.05008770842795019\n",
            " Loss SS77:  0.06222725139907849\n",
            " Loss SS88:  0.0523487003405153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net4(nn.Module):\n",
        "    def __init__(self, ss11,ss22,ss33,ss44,ss55,ss66,ss77,ss88):\n",
        "        super(Net4, self).__init__()\n",
        "        self.ss11 = ss11\n",
        "        self.ss22 = ss22\n",
        "        self.ss33 = ss33\n",
        "        self.ss44 = ss44\n",
        "        self.ss55 = ss55\n",
        "        self.ss66 = ss66\n",
        "        self.ss77 = ss77\n",
        "        self.ss88 = ss88\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.ss11(x)\n",
        "        out2 = self.ss22(x)\n",
        "        out3 = self.ss33(x)\n",
        "        out4 = self.ss44(x)\n",
        "        out5 = self.ss55(x)\n",
        "        out6 = self.ss66(x)\n",
        "        out7 = self.ss77(x)\n",
        "        out8 = self.ss88(x)\n",
        "\n",
        "        out = torch.cat((out1,out2,out3,out4,out5,out6,out7,out8),1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "net8students = Net4( ss11,ss22,ss33,ss44,ss55,ss66,ss77,ss88)\n",
        "net8students.to(device)\n",
        "summary(net8students, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPbCl6H0KSJX",
        "outputId": "0371451e-4e70-4250-a9b2-cdb2a75fbe54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "              VGG-40                   [-1, 64]               0\n",
            "           Conv2d-41           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-42           [-1, 32, 32, 32]              64\n",
            "             ReLU-43           [-1, 32, 32, 32]               0\n",
            "           Conv2d-44           [-1, 32, 32, 32]           9,248\n",
            "      BatchNorm2d-45           [-1, 32, 32, 32]              64\n",
            "             ReLU-46           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-47           [-1, 32, 16, 16]               0\n",
            "           Conv2d-48           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-49           [-1, 32, 16, 16]              64\n",
            "             ReLU-50           [-1, 32, 16, 16]               0\n",
            "           Conv2d-51           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-52           [-1, 32, 16, 16]              64\n",
            "             ReLU-53           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-54             [-1, 32, 8, 8]               0\n",
            "           Conv2d-55             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-56             [-1, 32, 8, 8]              64\n",
            "             ReLU-57             [-1, 32, 8, 8]               0\n",
            "           Conv2d-58             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-59             [-1, 64, 8, 8]             128\n",
            "             ReLU-60             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-61             [-1, 64, 4, 4]               0\n",
            "           Conv2d-62             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-63             [-1, 64, 4, 4]             128\n",
            "             ReLU-64             [-1, 64, 4, 4]               0\n",
            "           Conv2d-65             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-66             [-1, 64, 4, 4]             128\n",
            "             ReLU-67             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-68             [-1, 64, 2, 2]               0\n",
            "           Conv2d-69             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-70             [-1, 64, 2, 2]             128\n",
            "             ReLU-71             [-1, 64, 2, 2]               0\n",
            "           Conv2d-72             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-73             [-1, 64, 2, 2]             128\n",
            "             ReLU-74             [-1, 64, 2, 2]               0\n",
            "           Conv2d-75             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-76             [-1, 64, 2, 2]             128\n",
            "             ReLU-77             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-78             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-79             [-1, 64, 1, 1]               0\n",
            "              VGG-80                   [-1, 64]               0\n",
            "           Conv2d-81           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-82           [-1, 32, 32, 32]              64\n",
            "             ReLU-83           [-1, 32, 32, 32]               0\n",
            "           Conv2d-84           [-1, 32, 32, 32]           9,248\n",
            "      BatchNorm2d-85           [-1, 32, 32, 32]              64\n",
            "             ReLU-86           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-87           [-1, 32, 16, 16]               0\n",
            "           Conv2d-88           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-89           [-1, 32, 16, 16]              64\n",
            "             ReLU-90           [-1, 32, 16, 16]               0\n",
            "           Conv2d-91           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-92           [-1, 32, 16, 16]              64\n",
            "             ReLU-93           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-94             [-1, 32, 8, 8]               0\n",
            "           Conv2d-95             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-96             [-1, 32, 8, 8]              64\n",
            "             ReLU-97             [-1, 32, 8, 8]               0\n",
            "           Conv2d-98             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-99             [-1, 64, 8, 8]             128\n",
            "            ReLU-100             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-101             [-1, 64, 4, 4]               0\n",
            "          Conv2d-102             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-103             [-1, 64, 4, 4]             128\n",
            "            ReLU-104             [-1, 64, 4, 4]               0\n",
            "          Conv2d-105             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-106             [-1, 64, 4, 4]             128\n",
            "            ReLU-107             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-108             [-1, 64, 2, 2]               0\n",
            "          Conv2d-109             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-110             [-1, 64, 2, 2]             128\n",
            "            ReLU-111             [-1, 64, 2, 2]               0\n",
            "          Conv2d-112             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-113             [-1, 64, 2, 2]             128\n",
            "            ReLU-114             [-1, 64, 2, 2]               0\n",
            "          Conv2d-115             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-116             [-1, 64, 2, 2]             128\n",
            "            ReLU-117             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-118             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-119             [-1, 64, 1, 1]               0\n",
            "             VGG-120                   [-1, 64]               0\n",
            "          Conv2d-121           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-122           [-1, 32, 32, 32]              64\n",
            "            ReLU-123           [-1, 32, 32, 32]               0\n",
            "          Conv2d-124           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-125           [-1, 32, 32, 32]              64\n",
            "            ReLU-126           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-127           [-1, 32, 16, 16]               0\n",
            "          Conv2d-128           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-129           [-1, 32, 16, 16]              64\n",
            "            ReLU-130           [-1, 32, 16, 16]               0\n",
            "          Conv2d-131           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-132           [-1, 32, 16, 16]              64\n",
            "            ReLU-133           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-134             [-1, 32, 8, 8]               0\n",
            "          Conv2d-135             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-136             [-1, 32, 8, 8]              64\n",
            "            ReLU-137             [-1, 32, 8, 8]               0\n",
            "          Conv2d-138             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-139             [-1, 64, 8, 8]             128\n",
            "            ReLU-140             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-141             [-1, 64, 4, 4]               0\n",
            "          Conv2d-142             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-143             [-1, 64, 4, 4]             128\n",
            "            ReLU-144             [-1, 64, 4, 4]               0\n",
            "          Conv2d-145             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-146             [-1, 64, 4, 4]             128\n",
            "            ReLU-147             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-148             [-1, 64, 2, 2]               0\n",
            "          Conv2d-149             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-150             [-1, 64, 2, 2]             128\n",
            "            ReLU-151             [-1, 64, 2, 2]               0\n",
            "          Conv2d-152             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-153             [-1, 64, 2, 2]             128\n",
            "            ReLU-154             [-1, 64, 2, 2]               0\n",
            "          Conv2d-155             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-156             [-1, 64, 2, 2]             128\n",
            "            ReLU-157             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-158             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-159             [-1, 64, 1, 1]               0\n",
            "             VGG-160                   [-1, 64]               0\n",
            "          Conv2d-161           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-162           [-1, 32, 32, 32]              64\n",
            "            ReLU-163           [-1, 32, 32, 32]               0\n",
            "          Conv2d-164           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-165           [-1, 32, 32, 32]              64\n",
            "            ReLU-166           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-167           [-1, 32, 16, 16]               0\n",
            "          Conv2d-168           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-169           [-1, 32, 16, 16]              64\n",
            "            ReLU-170           [-1, 32, 16, 16]               0\n",
            "          Conv2d-171           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-172           [-1, 32, 16, 16]              64\n",
            "            ReLU-173           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-174             [-1, 32, 8, 8]               0\n",
            "          Conv2d-175             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-176             [-1, 32, 8, 8]              64\n",
            "            ReLU-177             [-1, 32, 8, 8]               0\n",
            "          Conv2d-178             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-179             [-1, 64, 8, 8]             128\n",
            "            ReLU-180             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-181             [-1, 64, 4, 4]               0\n",
            "          Conv2d-182             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-183             [-1, 64, 4, 4]             128\n",
            "            ReLU-184             [-1, 64, 4, 4]               0\n",
            "          Conv2d-185             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-186             [-1, 64, 4, 4]             128\n",
            "            ReLU-187             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-188             [-1, 64, 2, 2]               0\n",
            "          Conv2d-189             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-190             [-1, 64, 2, 2]             128\n",
            "            ReLU-191             [-1, 64, 2, 2]               0\n",
            "          Conv2d-192             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-193             [-1, 64, 2, 2]             128\n",
            "            ReLU-194             [-1, 64, 2, 2]               0\n",
            "          Conv2d-195             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-196             [-1, 64, 2, 2]             128\n",
            "            ReLU-197             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-198             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-199             [-1, 64, 1, 1]               0\n",
            "             VGG-200                   [-1, 64]               0\n",
            "          Conv2d-201           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-202           [-1, 32, 32, 32]              64\n",
            "            ReLU-203           [-1, 32, 32, 32]               0\n",
            "          Conv2d-204           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-205           [-1, 32, 32, 32]              64\n",
            "            ReLU-206           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-207           [-1, 32, 16, 16]               0\n",
            "          Conv2d-208           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-209           [-1, 32, 16, 16]              64\n",
            "            ReLU-210           [-1, 32, 16, 16]               0\n",
            "          Conv2d-211           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-212           [-1, 32, 16, 16]              64\n",
            "            ReLU-213           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-214             [-1, 32, 8, 8]               0\n",
            "          Conv2d-215             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-216             [-1, 32, 8, 8]              64\n",
            "            ReLU-217             [-1, 32, 8, 8]               0\n",
            "          Conv2d-218             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-219             [-1, 64, 8, 8]             128\n",
            "            ReLU-220             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-221             [-1, 64, 4, 4]               0\n",
            "          Conv2d-222             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-223             [-1, 64, 4, 4]             128\n",
            "            ReLU-224             [-1, 64, 4, 4]               0\n",
            "          Conv2d-225             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-226             [-1, 64, 4, 4]             128\n",
            "            ReLU-227             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-228             [-1, 64, 2, 2]               0\n",
            "          Conv2d-229             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-230             [-1, 64, 2, 2]             128\n",
            "            ReLU-231             [-1, 64, 2, 2]               0\n",
            "          Conv2d-232             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-233             [-1, 64, 2, 2]             128\n",
            "            ReLU-234             [-1, 64, 2, 2]               0\n",
            "          Conv2d-235             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-236             [-1, 64, 2, 2]             128\n",
            "            ReLU-237             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-238             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-239             [-1, 64, 1, 1]               0\n",
            "             VGG-240                   [-1, 64]               0\n",
            "          Conv2d-241           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-242           [-1, 32, 32, 32]              64\n",
            "            ReLU-243           [-1, 32, 32, 32]               0\n",
            "          Conv2d-244           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-245           [-1, 32, 32, 32]              64\n",
            "            ReLU-246           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-247           [-1, 32, 16, 16]               0\n",
            "          Conv2d-248           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-249           [-1, 32, 16, 16]              64\n",
            "            ReLU-250           [-1, 32, 16, 16]               0\n",
            "          Conv2d-251           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-252           [-1, 32, 16, 16]              64\n",
            "            ReLU-253           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-254             [-1, 32, 8, 8]               0\n",
            "          Conv2d-255             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-256             [-1, 32, 8, 8]              64\n",
            "            ReLU-257             [-1, 32, 8, 8]               0\n",
            "          Conv2d-258             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-259             [-1, 64, 8, 8]             128\n",
            "            ReLU-260             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-261             [-1, 64, 4, 4]               0\n",
            "          Conv2d-262             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-263             [-1, 64, 4, 4]             128\n",
            "            ReLU-264             [-1, 64, 4, 4]               0\n",
            "          Conv2d-265             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-266             [-1, 64, 4, 4]             128\n",
            "            ReLU-267             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-268             [-1, 64, 2, 2]               0\n",
            "          Conv2d-269             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-270             [-1, 64, 2, 2]             128\n",
            "            ReLU-271             [-1, 64, 2, 2]               0\n",
            "          Conv2d-272             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-273             [-1, 64, 2, 2]             128\n",
            "            ReLU-274             [-1, 64, 2, 2]               0\n",
            "          Conv2d-275             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-276             [-1, 64, 2, 2]             128\n",
            "            ReLU-277             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-278             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-279             [-1, 64, 1, 1]               0\n",
            "             VGG-280                   [-1, 64]               0\n",
            "          Conv2d-281           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-282           [-1, 32, 32, 32]              64\n",
            "            ReLU-283           [-1, 32, 32, 32]               0\n",
            "          Conv2d-284           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-285           [-1, 32, 32, 32]              64\n",
            "            ReLU-286           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-287           [-1, 32, 16, 16]               0\n",
            "          Conv2d-288           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-289           [-1, 32, 16, 16]              64\n",
            "            ReLU-290           [-1, 32, 16, 16]               0\n",
            "          Conv2d-291           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-292           [-1, 32, 16, 16]              64\n",
            "            ReLU-293           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-294             [-1, 32, 8, 8]               0\n",
            "          Conv2d-295             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-296             [-1, 32, 8, 8]              64\n",
            "            ReLU-297             [-1, 32, 8, 8]               0\n",
            "          Conv2d-298             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-299             [-1, 64, 8, 8]             128\n",
            "            ReLU-300             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-301             [-1, 64, 4, 4]               0\n",
            "          Conv2d-302             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-303             [-1, 64, 4, 4]             128\n",
            "            ReLU-304             [-1, 64, 4, 4]               0\n",
            "          Conv2d-305             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-306             [-1, 64, 4, 4]             128\n",
            "            ReLU-307             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-308             [-1, 64, 2, 2]               0\n",
            "          Conv2d-309             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-310             [-1, 64, 2, 2]             128\n",
            "            ReLU-311             [-1, 64, 2, 2]               0\n",
            "          Conv2d-312             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-313             [-1, 64, 2, 2]             128\n",
            "            ReLU-314             [-1, 64, 2, 2]               0\n",
            "          Conv2d-315             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-316             [-1, 64, 2, 2]             128\n",
            "            ReLU-317             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-318             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-319             [-1, 64, 1, 1]               0\n",
            "             VGG-320                   [-1, 64]               0\n",
            "          Linear-321                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 1,942,026\n",
            "Trainable params: 1,942,026\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 17.36\n",
            "Params size (MB): 7.41\n",
            "Estimated Total Size (MB): 24.78\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for m in net8students.modules():\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        m.weight.requires_grad = False\n",
        "        if m.bias is not None:\n",
        "            m.bias.requires_grad = False\n",
        "net8students = net8students.to(device)\n",
        "summary(net8students, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZAB4QuQLbvG",
        "outputId": "2a7873f7-9c7f-4d23-a6d4-f2bb209d5e92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "              VGG-40                   [-1, 64]               0\n",
            "           Conv2d-41           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-42           [-1, 32, 32, 32]              64\n",
            "             ReLU-43           [-1, 32, 32, 32]               0\n",
            "           Conv2d-44           [-1, 32, 32, 32]           9,248\n",
            "      BatchNorm2d-45           [-1, 32, 32, 32]              64\n",
            "             ReLU-46           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-47           [-1, 32, 16, 16]               0\n",
            "           Conv2d-48           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-49           [-1, 32, 16, 16]              64\n",
            "             ReLU-50           [-1, 32, 16, 16]               0\n",
            "           Conv2d-51           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-52           [-1, 32, 16, 16]              64\n",
            "             ReLU-53           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-54             [-1, 32, 8, 8]               0\n",
            "           Conv2d-55             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-56             [-1, 32, 8, 8]              64\n",
            "             ReLU-57             [-1, 32, 8, 8]               0\n",
            "           Conv2d-58             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-59             [-1, 64, 8, 8]             128\n",
            "             ReLU-60             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-61             [-1, 64, 4, 4]               0\n",
            "           Conv2d-62             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-63             [-1, 64, 4, 4]             128\n",
            "             ReLU-64             [-1, 64, 4, 4]               0\n",
            "           Conv2d-65             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-66             [-1, 64, 4, 4]             128\n",
            "             ReLU-67             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-68             [-1, 64, 2, 2]               0\n",
            "           Conv2d-69             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-70             [-1, 64, 2, 2]             128\n",
            "             ReLU-71             [-1, 64, 2, 2]               0\n",
            "           Conv2d-72             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-73             [-1, 64, 2, 2]             128\n",
            "             ReLU-74             [-1, 64, 2, 2]               0\n",
            "           Conv2d-75             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-76             [-1, 64, 2, 2]             128\n",
            "             ReLU-77             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-78             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-79             [-1, 64, 1, 1]               0\n",
            "              VGG-80                   [-1, 64]               0\n",
            "           Conv2d-81           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-82           [-1, 32, 32, 32]              64\n",
            "             ReLU-83           [-1, 32, 32, 32]               0\n",
            "           Conv2d-84           [-1, 32, 32, 32]           9,248\n",
            "      BatchNorm2d-85           [-1, 32, 32, 32]              64\n",
            "             ReLU-86           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-87           [-1, 32, 16, 16]               0\n",
            "           Conv2d-88           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-89           [-1, 32, 16, 16]              64\n",
            "             ReLU-90           [-1, 32, 16, 16]               0\n",
            "           Conv2d-91           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-92           [-1, 32, 16, 16]              64\n",
            "             ReLU-93           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-94             [-1, 32, 8, 8]               0\n",
            "           Conv2d-95             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-96             [-1, 32, 8, 8]              64\n",
            "             ReLU-97             [-1, 32, 8, 8]               0\n",
            "           Conv2d-98             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-99             [-1, 64, 8, 8]             128\n",
            "            ReLU-100             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-101             [-1, 64, 4, 4]               0\n",
            "          Conv2d-102             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-103             [-1, 64, 4, 4]             128\n",
            "            ReLU-104             [-1, 64, 4, 4]               0\n",
            "          Conv2d-105             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-106             [-1, 64, 4, 4]             128\n",
            "            ReLU-107             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-108             [-1, 64, 2, 2]               0\n",
            "          Conv2d-109             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-110             [-1, 64, 2, 2]             128\n",
            "            ReLU-111             [-1, 64, 2, 2]               0\n",
            "          Conv2d-112             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-113             [-1, 64, 2, 2]             128\n",
            "            ReLU-114             [-1, 64, 2, 2]               0\n",
            "          Conv2d-115             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-116             [-1, 64, 2, 2]             128\n",
            "            ReLU-117             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-118             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-119             [-1, 64, 1, 1]               0\n",
            "             VGG-120                   [-1, 64]               0\n",
            "          Conv2d-121           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-122           [-1, 32, 32, 32]              64\n",
            "            ReLU-123           [-1, 32, 32, 32]               0\n",
            "          Conv2d-124           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-125           [-1, 32, 32, 32]              64\n",
            "            ReLU-126           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-127           [-1, 32, 16, 16]               0\n",
            "          Conv2d-128           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-129           [-1, 32, 16, 16]              64\n",
            "            ReLU-130           [-1, 32, 16, 16]               0\n",
            "          Conv2d-131           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-132           [-1, 32, 16, 16]              64\n",
            "            ReLU-133           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-134             [-1, 32, 8, 8]               0\n",
            "          Conv2d-135             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-136             [-1, 32, 8, 8]              64\n",
            "            ReLU-137             [-1, 32, 8, 8]               0\n",
            "          Conv2d-138             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-139             [-1, 64, 8, 8]             128\n",
            "            ReLU-140             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-141             [-1, 64, 4, 4]               0\n",
            "          Conv2d-142             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-143             [-1, 64, 4, 4]             128\n",
            "            ReLU-144             [-1, 64, 4, 4]               0\n",
            "          Conv2d-145             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-146             [-1, 64, 4, 4]             128\n",
            "            ReLU-147             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-148             [-1, 64, 2, 2]               0\n",
            "          Conv2d-149             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-150             [-1, 64, 2, 2]             128\n",
            "            ReLU-151             [-1, 64, 2, 2]               0\n",
            "          Conv2d-152             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-153             [-1, 64, 2, 2]             128\n",
            "            ReLU-154             [-1, 64, 2, 2]               0\n",
            "          Conv2d-155             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-156             [-1, 64, 2, 2]             128\n",
            "            ReLU-157             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-158             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-159             [-1, 64, 1, 1]               0\n",
            "             VGG-160                   [-1, 64]               0\n",
            "          Conv2d-161           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-162           [-1, 32, 32, 32]              64\n",
            "            ReLU-163           [-1, 32, 32, 32]               0\n",
            "          Conv2d-164           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-165           [-1, 32, 32, 32]              64\n",
            "            ReLU-166           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-167           [-1, 32, 16, 16]               0\n",
            "          Conv2d-168           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-169           [-1, 32, 16, 16]              64\n",
            "            ReLU-170           [-1, 32, 16, 16]               0\n",
            "          Conv2d-171           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-172           [-1, 32, 16, 16]              64\n",
            "            ReLU-173           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-174             [-1, 32, 8, 8]               0\n",
            "          Conv2d-175             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-176             [-1, 32, 8, 8]              64\n",
            "            ReLU-177             [-1, 32, 8, 8]               0\n",
            "          Conv2d-178             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-179             [-1, 64, 8, 8]             128\n",
            "            ReLU-180             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-181             [-1, 64, 4, 4]               0\n",
            "          Conv2d-182             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-183             [-1, 64, 4, 4]             128\n",
            "            ReLU-184             [-1, 64, 4, 4]               0\n",
            "          Conv2d-185             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-186             [-1, 64, 4, 4]             128\n",
            "            ReLU-187             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-188             [-1, 64, 2, 2]               0\n",
            "          Conv2d-189             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-190             [-1, 64, 2, 2]             128\n",
            "            ReLU-191             [-1, 64, 2, 2]               0\n",
            "          Conv2d-192             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-193             [-1, 64, 2, 2]             128\n",
            "            ReLU-194             [-1, 64, 2, 2]               0\n",
            "          Conv2d-195             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-196             [-1, 64, 2, 2]             128\n",
            "            ReLU-197             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-198             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-199             [-1, 64, 1, 1]               0\n",
            "             VGG-200                   [-1, 64]               0\n",
            "          Conv2d-201           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-202           [-1, 32, 32, 32]              64\n",
            "            ReLU-203           [-1, 32, 32, 32]               0\n",
            "          Conv2d-204           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-205           [-1, 32, 32, 32]              64\n",
            "            ReLU-206           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-207           [-1, 32, 16, 16]               0\n",
            "          Conv2d-208           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-209           [-1, 32, 16, 16]              64\n",
            "            ReLU-210           [-1, 32, 16, 16]               0\n",
            "          Conv2d-211           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-212           [-1, 32, 16, 16]              64\n",
            "            ReLU-213           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-214             [-1, 32, 8, 8]               0\n",
            "          Conv2d-215             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-216             [-1, 32, 8, 8]              64\n",
            "            ReLU-217             [-1, 32, 8, 8]               0\n",
            "          Conv2d-218             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-219             [-1, 64, 8, 8]             128\n",
            "            ReLU-220             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-221             [-1, 64, 4, 4]               0\n",
            "          Conv2d-222             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-223             [-1, 64, 4, 4]             128\n",
            "            ReLU-224             [-1, 64, 4, 4]               0\n",
            "          Conv2d-225             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-226             [-1, 64, 4, 4]             128\n",
            "            ReLU-227             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-228             [-1, 64, 2, 2]               0\n",
            "          Conv2d-229             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-230             [-1, 64, 2, 2]             128\n",
            "            ReLU-231             [-1, 64, 2, 2]               0\n",
            "          Conv2d-232             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-233             [-1, 64, 2, 2]             128\n",
            "            ReLU-234             [-1, 64, 2, 2]               0\n",
            "          Conv2d-235             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-236             [-1, 64, 2, 2]             128\n",
            "            ReLU-237             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-238             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-239             [-1, 64, 1, 1]               0\n",
            "             VGG-240                   [-1, 64]               0\n",
            "          Conv2d-241           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-242           [-1, 32, 32, 32]              64\n",
            "            ReLU-243           [-1, 32, 32, 32]               0\n",
            "          Conv2d-244           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-245           [-1, 32, 32, 32]              64\n",
            "            ReLU-246           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-247           [-1, 32, 16, 16]               0\n",
            "          Conv2d-248           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-249           [-1, 32, 16, 16]              64\n",
            "            ReLU-250           [-1, 32, 16, 16]               0\n",
            "          Conv2d-251           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-252           [-1, 32, 16, 16]              64\n",
            "            ReLU-253           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-254             [-1, 32, 8, 8]               0\n",
            "          Conv2d-255             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-256             [-1, 32, 8, 8]              64\n",
            "            ReLU-257             [-1, 32, 8, 8]               0\n",
            "          Conv2d-258             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-259             [-1, 64, 8, 8]             128\n",
            "            ReLU-260             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-261             [-1, 64, 4, 4]               0\n",
            "          Conv2d-262             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-263             [-1, 64, 4, 4]             128\n",
            "            ReLU-264             [-1, 64, 4, 4]               0\n",
            "          Conv2d-265             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-266             [-1, 64, 4, 4]             128\n",
            "            ReLU-267             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-268             [-1, 64, 2, 2]               0\n",
            "          Conv2d-269             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-270             [-1, 64, 2, 2]             128\n",
            "            ReLU-271             [-1, 64, 2, 2]               0\n",
            "          Conv2d-272             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-273             [-1, 64, 2, 2]             128\n",
            "            ReLU-274             [-1, 64, 2, 2]               0\n",
            "          Conv2d-275             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-276             [-1, 64, 2, 2]             128\n",
            "            ReLU-277             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-278             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-279             [-1, 64, 1, 1]               0\n",
            "             VGG-280                   [-1, 64]               0\n",
            "          Conv2d-281           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-282           [-1, 32, 32, 32]              64\n",
            "            ReLU-283           [-1, 32, 32, 32]               0\n",
            "          Conv2d-284           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-285           [-1, 32, 32, 32]              64\n",
            "            ReLU-286           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-287           [-1, 32, 16, 16]               0\n",
            "          Conv2d-288           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-289           [-1, 32, 16, 16]              64\n",
            "            ReLU-290           [-1, 32, 16, 16]               0\n",
            "          Conv2d-291           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-292           [-1, 32, 16, 16]              64\n",
            "            ReLU-293           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-294             [-1, 32, 8, 8]               0\n",
            "          Conv2d-295             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-296             [-1, 32, 8, 8]              64\n",
            "            ReLU-297             [-1, 32, 8, 8]               0\n",
            "          Conv2d-298             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-299             [-1, 64, 8, 8]             128\n",
            "            ReLU-300             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-301             [-1, 64, 4, 4]               0\n",
            "          Conv2d-302             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-303             [-1, 64, 4, 4]             128\n",
            "            ReLU-304             [-1, 64, 4, 4]               0\n",
            "          Conv2d-305             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-306             [-1, 64, 4, 4]             128\n",
            "            ReLU-307             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-308             [-1, 64, 2, 2]               0\n",
            "          Conv2d-309             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-310             [-1, 64, 2, 2]             128\n",
            "            ReLU-311             [-1, 64, 2, 2]               0\n",
            "          Conv2d-312             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-313             [-1, 64, 2, 2]             128\n",
            "            ReLU-314             [-1, 64, 2, 2]               0\n",
            "          Conv2d-315             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-316             [-1, 64, 2, 2]             128\n",
            "            ReLU-317             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-318             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-319             [-1, 64, 1, 1]               0\n",
            "             VGG-320                   [-1, 64]               0\n",
            "          Linear-321                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 1,942,026\n",
            "Trainable params: 13,834\n",
            "Non-trainable params: 1,928,192\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 17.36\n",
            "Params size (MB): 7.41\n",
            "Estimated Total Size (MB): 24.78\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net8students.parameters(), lr=0.0001)\n",
        "\n",
        "def train81(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    net8students.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        net8students.zero_grad()\n",
        "        outputs = net8students(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if(batch_idx % 200 == 0):\n",
        "          print(\"Accuracy : \",100.*correct/total,\" Loss : \", train_loss/(batch_idx+1))\n",
        "def test82(epoch):\n",
        "    net8students.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net8students(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\"Accuracy : \",100.*correct/total,\" Loss : \", test_loss/(batch_idx+1))"
      ],
      "metadata": {
        "id": "mrvLxB_eNVcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "    train81(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test82(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tGEXdFBNpfV",
        "outputId": "094f941a-a964-4047-ca12-53af50d8cae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Accuracy :  10.0  Loss :  2.6411001682281494\n",
            "Accuracy :  64.66666666666667  Loss :  1.5341597134201088\n",
            "Accuracy :  73.356608478803  Loss :  1.1754024572800519\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.6132931709289551\n",
            "Accuracy :  82.9047619047619  Loss :  0.6218314185028985\n",
            "Accuracy :  82.34146341463415  Loss :  0.6272539547303828\n",
            "Accuracy :  82.45901639344262  Loss :  0.6230301412402607\n",
            "Accuracy :  82.46913580246914  Loss :  0.6209356645007192\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy :  81.0  Loss :  0.6154868602752686\n",
            "Accuracy :  82.96019900497512  Loss :  0.5849587328694946\n",
            "Accuracy :  83.24438902743142  Loss :  0.5587492741254202\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.4913302958011627\n",
            "Accuracy :  83.28571428571429  Loss :  0.5138541474228814\n",
            "Accuracy :  82.92682926829268  Loss :  0.5211354341448807\n",
            "Accuracy :  83.0  Loss :  0.515239635940458\n",
            "Accuracy :  82.96296296296296  Loss :  0.5131170065314682\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy :  83.0  Loss :  0.45704376697540283\n",
            "Accuracy :  83.71641791044776  Loss :  0.49672597883945674\n",
            "Accuracy :  83.84788029925187  Loss :  0.48983111791777195\n",
            "Validation: \n",
            "Accuracy :  89.0  Loss :  0.4553029537200928\n",
            "Accuracy :  83.66666666666667  Loss :  0.4889447476182665\n",
            "Accuracy :  83.17073170731707  Loss :  0.49722078224507776\n",
            "Accuracy :  83.14754098360656  Loss :  0.4904933102795335\n",
            "Accuracy :  83.1604938271605  Loss :  0.48863152552534034\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy :  85.0  Loss :  0.3975190818309784\n",
            "Accuracy :  83.93532338308458  Loss :  0.4745362836923172\n",
            "Accuracy :  84.07231920199501  Loss :  0.46947600322768573\n",
            "Validation: \n",
            "Accuracy :  90.0  Loss :  0.4395003020763397\n",
            "Accuracy :  83.66666666666667  Loss :  0.480935819092251\n",
            "Accuracy :  83.1951219512195  Loss :  0.4889268075547567\n",
            "Accuracy :  83.32786885245902  Loss :  0.48192672455897095\n",
            "Accuracy :  83.39506172839506  Loss :  0.4800146417117413\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy :  86.0  Loss :  0.3799375593662262\n",
            "Accuracy :  84.21890547263682  Loss :  0.461022816116537\n",
            "Accuracy :  84.27680798004988  Loss :  0.4590237373277136\n",
            "Validation: \n",
            "Accuracy :  90.0  Loss :  0.42664605379104614\n",
            "Accuracy :  84.0  Loss :  0.4768499433994293\n",
            "Accuracy :  83.46341463414635  Loss :  0.48512500524520874\n",
            "Accuracy :  83.47540983606558  Loss :  0.47757219975111914\n",
            "Accuracy :  83.5925925925926  Loss :  0.4756247415954684\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy :  84.0  Loss :  0.4218336045742035\n",
            "Accuracy :  84.34328358208955  Loss :  0.4575044835236535\n",
            "Accuracy :  84.44887780548629  Loss :  0.45506665262944085\n",
            "Validation: \n",
            "Accuracy :  89.0  Loss :  0.4191950559616089\n",
            "Accuracy :  84.23809523809524  Loss :  0.4740473088764009\n",
            "Accuracy :  83.58536585365853  Loss :  0.48182017192607973\n",
            "Accuracy :  83.63934426229508  Loss :  0.47422192575501615\n",
            "Accuracy :  83.66666666666667  Loss :  0.47235623002052307\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy :  83.0  Loss :  0.41606032848358154\n",
            "Accuracy :  84.41293532338308  Loss :  0.45186449246323523\n",
            "Accuracy :  84.51620947630923  Loss :  0.4495413944756895\n",
            "Validation: \n",
            "Accuracy :  88.0  Loss :  0.41645899415016174\n",
            "Accuracy :  84.19047619047619  Loss :  0.47258785367012024\n",
            "Accuracy :  83.6829268292683  Loss :  0.48027252133299664\n",
            "Accuracy :  83.77049180327869  Loss :  0.4722916576705995\n",
            "Accuracy :  83.82716049382717  Loss :  0.47027080993593473\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy :  87.0  Loss :  0.43032580614089966\n",
            "Accuracy :  84.25373134328358  Loss :  0.45040081952934835\n",
            "Accuracy :  84.4713216957606  Loss :  0.447547447911819\n",
            "Validation: \n",
            "Accuracy :  88.0  Loss :  0.40890106558799744\n",
            "Accuracy :  84.33333333333333  Loss :  0.4701936812627883\n",
            "Accuracy :  83.73170731707317  Loss :  0.4781356221292077\n",
            "Accuracy :  83.80327868852459  Loss :  0.47030350614766603\n",
            "Accuracy :  83.8395061728395  Loss :  0.46854310050422765\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy :  88.0  Loss :  0.369022935628891\n",
            "Accuracy :  84.65174129353234  Loss :  0.44633857320197184\n",
            "Accuracy :  84.74812967581047  Loss :  0.4443733852700402\n",
            "Validation: \n",
            "Accuracy :  88.0  Loss :  0.4080425202846527\n",
            "Accuracy :  84.28571428571429  Loss :  0.46928590607075465\n",
            "Accuracy :  83.82926829268293  Loss :  0.47656784224800947\n",
            "Accuracy :  83.98360655737704  Loss :  0.4685433323754639\n",
            "Accuracy :  84.01234567901234  Loss :  0.46676688889662427\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy :  83.0  Loss :  0.3795682489871979\n",
            "Accuracy :  84.5273631840796  Loss :  0.44511423493499186\n",
            "Accuracy :  84.60847880299252  Loss :  0.44361363652341085\n",
            "Validation: \n",
            "Accuracy :  88.0  Loss :  0.4029063284397125\n",
            "Accuracy :  84.14285714285714  Loss :  0.467584680943262\n",
            "Accuracy :  83.6829268292683  Loss :  0.4752211272716522\n",
            "Accuracy :  83.93442622950819  Loss :  0.4671540284743074\n",
            "Accuracy :  83.93827160493827  Loss :  0.46548740952103224\n"
          ]
        }
      ]
    }
  ]
}