{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALL run on Teacher",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1433c7cd29854e3b8a93289628f9e6d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f574f890d2964300829ab5b9b22d9bff",
              "IPY_MODEL_6374f245b27d472d9fd008818a251e38",
              "IPY_MODEL_17dbcd61f0a24057bcddddb6d2df5589"
            ],
            "layout": "IPY_MODEL_60dfd89905c54156b5bd57bd96eaa50e"
          }
        },
        "f574f890d2964300829ab5b9b22d9bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ca5b8ffac8443d0abacd7757ed66998",
            "placeholder": "​",
            "style": "IPY_MODEL_46378cc2eae74dbf8a90ae886f405edc",
            "value": ""
          }
        },
        "6374f245b27d472d9fd008818a251e38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43df067c8fff427c9005d52efd00ad97",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3d4f57623f2422fb47c55d51e208316",
            "value": 170498071
          }
        },
        "17dbcd61f0a24057bcddddb6d2df5589": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cfb9a1c38ae4b35b38ef8ac1598ab5d",
            "placeholder": "​",
            "style": "IPY_MODEL_daff5d5c73d84e2a88d7f1b630d03919",
            "value": " 170499072/? [00:02&lt;00:00, 80180322.23it/s]"
          }
        },
        "60dfd89905c54156b5bd57bd96eaa50e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ca5b8ffac8443d0abacd7757ed66998": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46378cc2eae74dbf8a90ae886f405edc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43df067c8fff427c9005d52efd00ad97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3d4f57623f2422fb47c55d51e208316": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9cfb9a1c38ae4b35b38ef8ac1598ab5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daff5d5c73d84e2a88d7f1b630d03919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Knowledge Distillation \n",
        "We will impliment [TCN](https://www.bmvc2021-virtualconference.com/conference/papers/paper_0831.html) paper. It is a varient of knowledge distillation which uses dense feature vactors instead of logits to transfer knowledge from teacher to student.  "
      ],
      "metadata": {
        "id": "NM1cxLfnctDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71XqWTiofkMi",
        "outputId": "100c40d2-78e3-44df-84cf-29d194ae9e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training base teacher network\n",
        "This section is not graded"
      ],
      "metadata": {
        "id": "sOqQ-lRNesRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "import tqdm\n",
        "\n",
        "batch_size = 100\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "1433c7cd29854e3b8a93289628f9e6d1",
            "f574f890d2964300829ab5b9b22d9bff",
            "6374f245b27d472d9fd008818a251e38",
            "17dbcd61f0a24057bcddddb6d2df5589",
            "60dfd89905c54156b5bd57bd96eaa50e",
            "4ca5b8ffac8443d0abacd7757ed66998",
            "46378cc2eae74dbf8a90ae886f405edc",
            "43df067c8fff427c9005d52efd00ad97",
            "a3d4f57623f2422fb47c55d51e208316",
            "9cfb9a1c38ae4b35b38ef8ac1598ab5d",
            "daff5d5c73d84e2a88d7f1b630d03919"
          ]
        },
        "id": "KSnbXgTWmFsJ",
        "outputId": "89f3b0df-81c4-498c-eeaa-5ea388d7feff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1433c7cd29854e3b8a93289628f9e6d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "teacher = VGG('VGG16')\n",
        "teacher = teacher.to(device)"
      ],
      "metadata": {
        "id": "Up5ob6ujFKex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(teacher.parameters(), lr=0.0001)\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    teacher.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        teacher.zero_grad()\n",
        "        outputs = teacher(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if(batch_idx % 200 == 0):\n",
        "          print(\"Accuracy : \",100.*correct/total,\" Loss : \", train_loss/(batch_idx+1))\n",
        "def test(epoch):\n",
        "    teacher.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = teacher(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\"Accuracy : \",100.*correct/total,\" Loss : \", test_loss/(batch_idx+1))"
      ],
      "metadata": {
        "id": "2wyVCEgqFxxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "    train(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xfNOBETGFNR",
        "outputId": "f8111f75-8c9b-4c37-c9b5-bdabfe9c5b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Accuracy :  14.0  Loss :  2.3733975887298584\n",
            "Accuracy :  41.19900497512438  Loss :  1.5865661386233658\n",
            "Accuracy :  47.80548628428928  Loss :  1.41774071199341\n",
            "Validation: \n",
            "Accuracy :  67.0  Loss :  0.9107257127761841\n",
            "Accuracy :  65.33333333333333  Loss :  0.9950999220212301\n",
            "Accuracy :  64.17073170731707  Loss :  1.0056527911162958\n",
            "Accuracy :  64.47540983606558  Loss :  0.9981735944747925\n",
            "Accuracy :  64.24691358024691  Loss :  1.005907082999194\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy :  64.0  Loss :  0.9784829616546631\n",
            "Accuracy :  63.975124378109456  Loss :  1.007334947289519\n",
            "Accuracy :  65.65336658354114  Loss :  0.9614940865378725\n",
            "Validation: \n",
            "Accuracy :  75.0  Loss :  0.7087078094482422\n",
            "Accuracy :  73.0952380952381  Loss :  0.7782325758820489\n",
            "Accuracy :  72.07317073170732  Loss :  0.8009776901908037\n",
            "Accuracy :  72.45901639344262  Loss :  0.7891016011355353\n",
            "Accuracy :  72.55555555555556  Loss :  0.7947008230803926\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy :  64.0  Loss :  0.8603017330169678\n",
            "Accuracy :  72.07462686567165  Loss :  0.7997423499377806\n",
            "Accuracy :  72.94513715710723  Loss :  0.7742086663805041\n",
            "Validation: \n",
            "Accuracy :  79.0  Loss :  0.6711878180503845\n",
            "Accuracy :  74.76190476190476  Loss :  0.7303563171908969\n",
            "Accuracy :  74.34146341463415  Loss :  0.7372472046352014\n",
            "Accuracy :  74.85245901639344  Loss :  0.7317987011104333\n",
            "Accuracy :  74.92592592592592  Loss :  0.7305009972166132\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy :  79.0  Loss :  0.5780044794082642\n",
            "Accuracy :  76.72139303482587  Loss :  0.6739745083732984\n",
            "Accuracy :  77.12219451371571  Loss :  0.6625515817852686\n",
            "Validation: \n",
            "Accuracy :  77.0  Loss :  0.6510322093963623\n",
            "Accuracy :  76.38095238095238  Loss :  0.6998276596977597\n",
            "Accuracy :  76.53658536585365  Loss :  0.7156145616275508\n",
            "Accuracy :  76.21311475409836  Loss :  0.7109470562856706\n",
            "Accuracy :  76.12345679012346  Loss :  0.7078349800021561\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy :  77.0  Loss :  0.6177147030830383\n",
            "Accuracy :  79.4776119402985  Loss :  0.5986515867769422\n",
            "Accuracy :  79.75062344139651  Loss :  0.5906961390799715\n",
            "Validation: \n",
            "Accuracy :  84.0  Loss :  0.5090551376342773\n",
            "Accuracy :  78.66666666666667  Loss :  0.6368099678130377\n",
            "Accuracy :  78.0  Loss :  0.6627424199406694\n",
            "Accuracy :  77.98360655737704  Loss :  0.6582581303158744\n",
            "Accuracy :  78.0246913580247  Loss :  0.6549958369614165\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy :  75.0  Loss :  0.5102257132530212\n",
            "Accuracy :  81.50248756218906  Loss :  0.5391105981311988\n",
            "Accuracy :  81.57107231920199  Loss :  0.5350365822303325\n",
            "Validation: \n",
            "Accuracy :  83.0  Loss :  0.49757125973701477\n",
            "Accuracy :  82.61904761904762  Loss :  0.5151596452508654\n",
            "Accuracy :  82.0  Loss :  0.5324131686513017\n",
            "Accuracy :  82.01639344262296  Loss :  0.5297190204995578\n",
            "Accuracy :  82.06172839506173  Loss :  0.5265540598351278\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy :  83.0  Loss :  0.44234907627105713\n",
            "Accuracy :  83.60199004975124  Loss :  0.4862030011356173\n",
            "Accuracy :  83.44389027431421  Loss :  0.48413379181947497\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.3825371563434601\n",
            "Accuracy :  82.42857142857143  Loss :  0.5258803821745373\n",
            "Accuracy :  81.60975609756098  Loss :  0.5497385663230244\n",
            "Accuracy :  81.62295081967213  Loss :  0.5458922777019564\n",
            "Accuracy :  81.23456790123457  Loss :  0.5467918978797065\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy :  84.0  Loss :  0.36962124705314636\n",
            "Accuracy :  84.67164179104478  Loss :  0.44874195969519926\n",
            "Accuracy :  84.55860349127182  Loss :  0.44864371693936966\n",
            "Validation: \n",
            "Accuracy :  80.0  Loss :  0.5324685573577881\n",
            "Accuracy :  81.42857142857143  Loss :  0.5608684661842528\n",
            "Accuracy :  81.1219512195122  Loss :  0.5700179571058692\n",
            "Accuracy :  80.9672131147541  Loss :  0.57261781272341\n",
            "Accuracy :  81.07407407407408  Loss :  0.5733076157393279\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy :  85.0  Loss :  0.4485841691493988\n",
            "Accuracy :  85.57711442786069  Loss :  0.41721999919533137\n",
            "Accuracy :  85.56109725685785  Loss :  0.41495818572300036\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4822770953178406\n",
            "Accuracy :  82.52380952380952  Loss :  0.5443572174935114\n",
            "Accuracy :  82.2439024390244  Loss :  0.549246766218325\n",
            "Accuracy :  82.19672131147541  Loss :  0.547752359851462\n",
            "Accuracy :  82.18518518518519  Loss :  0.5443754104184516\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy :  93.0  Loss :  0.21290676295757294\n",
            "Accuracy :  86.90049751243781  Loss :  0.38545506681079295\n",
            "Accuracy :  86.8927680798005  Loss :  0.38235597234414404\n",
            "Validation: \n",
            "Accuracy :  85.0  Loss :  0.49091336131095886\n",
            "Accuracy :  83.23809523809524  Loss :  0.5201689849297205\n",
            "Accuracy :  82.97560975609755  Loss :  0.5335028255131187\n",
            "Accuracy :  83.0  Loss :  0.5326328812564005\n",
            "Accuracy :  82.88888888888889  Loss :  0.5320950560731652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_name = 'teacher.pt'\n",
        "path = F\"/content/gdrive/MyDrive/{model_save_name}\" \n",
        "#torch.save(teacher.state_dict(), path)\n",
        "teacher.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKgOR2afg0Ea",
        "outputId": "ea0a409d-8197-47b4-af48-93d79d2daf52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Dense Feature Dataset\n",
        "1.1 In this cell we remove the head of teacher network(i.e: last fullyconnected layer) and add a flatten layer at the end."
      ],
      "metadata": {
        "id": "mERdZFH7fBXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(teacher, (3, 32, 32))"
      ],
      "metadata": {
        "id": "Ubp5SeKSCRDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4601a0a-e656-47c7-a5c1-8c6f6d9229b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          36,928\n",
            "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
            "              ReLU-6           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 64, 16, 16]               0\n",
            "            Conv2d-8          [-1, 128, 16, 16]          73,856\n",
            "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
            "             ReLU-10          [-1, 128, 16, 16]               0\n",
            "           Conv2d-11          [-1, 128, 16, 16]         147,584\n",
            "      BatchNorm2d-12          [-1, 128, 16, 16]             256\n",
            "             ReLU-13          [-1, 128, 16, 16]               0\n",
            "        MaxPool2d-14            [-1, 128, 8, 8]               0\n",
            "           Conv2d-15            [-1, 256, 8, 8]         295,168\n",
            "      BatchNorm2d-16            [-1, 256, 8, 8]             512\n",
            "             ReLU-17            [-1, 256, 8, 8]               0\n",
            "           Conv2d-18            [-1, 256, 8, 8]         590,080\n",
            "      BatchNorm2d-19            [-1, 256, 8, 8]             512\n",
            "             ReLU-20            [-1, 256, 8, 8]               0\n",
            "           Conv2d-21            [-1, 256, 8, 8]         590,080\n",
            "      BatchNorm2d-22            [-1, 256, 8, 8]             512\n",
            "             ReLU-23            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 512, 4, 4]       1,180,160\n",
            "      BatchNorm2d-26            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-27            [-1, 512, 4, 4]               0\n",
            "           Conv2d-28            [-1, 512, 4, 4]       2,359,808\n",
            "      BatchNorm2d-29            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-30            [-1, 512, 4, 4]               0\n",
            "           Conv2d-31            [-1, 512, 4, 4]       2,359,808\n",
            "      BatchNorm2d-32            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-33            [-1, 512, 4, 4]               0\n",
            "        MaxPool2d-34            [-1, 512, 2, 2]               0\n",
            "           Conv2d-35            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-36            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-37            [-1, 512, 2, 2]               0\n",
            "           Conv2d-38            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-39            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-40            [-1, 512, 2, 2]               0\n",
            "           Conv2d-41            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-42            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-43            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-44            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-45            [-1, 512, 1, 1]               0\n",
            "           Linear-46                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 14,728,266\n",
            "Trainable params: 14,728,266\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 6.57\n",
            "Params size (MB): 56.18\n",
            "Estimated Total Size (MB): 62.77\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_WOH = nn.Sequential(*list(teacher.children())[:-1],nn.Flatten())"
      ],
      "metadata": {
        "id": "OIK5Vcfo8Y19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The summery of the new teacher without head :"
      ],
      "metadata": {
        "id": "JG2u-KMYbvI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "55V5zBa_vZjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from torchsummary import summary\n",
        "summary(teacher_WOH, (3, 32, 32))"
      ],
      "metadata": {
        "id": "rFtzu7DD8uev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "618c2479-143d-4eee-b30e-4016ffbb7734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          36,928\n",
            "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
            "              ReLU-6           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 64, 16, 16]               0\n",
            "            Conv2d-8          [-1, 128, 16, 16]          73,856\n",
            "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
            "             ReLU-10          [-1, 128, 16, 16]               0\n",
            "           Conv2d-11          [-1, 128, 16, 16]         147,584\n",
            "      BatchNorm2d-12          [-1, 128, 16, 16]             256\n",
            "             ReLU-13          [-1, 128, 16, 16]               0\n",
            "        MaxPool2d-14            [-1, 128, 8, 8]               0\n",
            "           Conv2d-15            [-1, 256, 8, 8]         295,168\n",
            "      BatchNorm2d-16            [-1, 256, 8, 8]             512\n",
            "             ReLU-17            [-1, 256, 8, 8]               0\n",
            "           Conv2d-18            [-1, 256, 8, 8]         590,080\n",
            "      BatchNorm2d-19            [-1, 256, 8, 8]             512\n",
            "             ReLU-20            [-1, 256, 8, 8]               0\n",
            "           Conv2d-21            [-1, 256, 8, 8]         590,080\n",
            "      BatchNorm2d-22            [-1, 256, 8, 8]             512\n",
            "             ReLU-23            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 512, 4, 4]       1,180,160\n",
            "      BatchNorm2d-26            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-27            [-1, 512, 4, 4]               0\n",
            "           Conv2d-28            [-1, 512, 4, 4]       2,359,808\n",
            "      BatchNorm2d-29            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-30            [-1, 512, 4, 4]               0\n",
            "           Conv2d-31            [-1, 512, 4, 4]       2,359,808\n",
            "      BatchNorm2d-32            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-33            [-1, 512, 4, 4]               0\n",
            "        MaxPool2d-34            [-1, 512, 2, 2]               0\n",
            "           Conv2d-35            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-36            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-37            [-1, 512, 2, 2]               0\n",
            "           Conv2d-38            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-39            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-40            [-1, 512, 2, 2]               0\n",
            "           Conv2d-41            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-42            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-43            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-44            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-45            [-1, 512, 1, 1]               0\n",
            "          Flatten-46                  [-1, 512]               0\n",
            "================================================================\n",
            "Total params: 14,723,136\n",
            "Trainable params: 14,723,136\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 6.57\n",
            "Params size (MB): 56.16\n",
            "Estimated Total Size (MB): 62.75\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 In this cell you have to create dense feature labels dataset(i.e: the outputs of teacher network without head). For that you have to do forward pass on whole dataset and append the outputs in a variable. "
      ],
      "metadata": {
        "id": "FJsum1ioc2tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_WOH.eval()\n",
        "DenseTrain = None\n",
        "DenseTest = None\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = teacher_WOH(inputs)\n",
        "        if(DenseTrain == None):\n",
        "            DenseTrain = outputs\n",
        "        else:\n",
        "            DenseTrain = torch.cat((DenseTrain,outputs))\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = teacher_WOH(inputs)\n",
        "        if(DenseTest == None):\n",
        "            DenseTest = outputs\n",
        "        else:\n",
        "            DenseTest = torch.cat((DenseTest,outputs))"
      ],
      "metadata": {
        "id": "jb8FytxoHM62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating ad-hoc student network\n",
        "we create an ad-hoc student network "
      ],
      "metadata": {
        "id": "xzcPi5zuhqOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "    'VGGS': [32, 32, 'M', 64, 64, 'M', 128, 128, 'M', 256, 256, 'M',512,'M'],\n",
        "\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(512, 512)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "s1 = VGG('VGGS')\n",
        "s1 = s1.to(device)\n",
        "summary(s1, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9mnHRpJhpl5",
        "outputId": "1b150742-cfeb-413b-c206-76e05c7efd5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "           Conv2d-29            [-1, 512, 2, 2]       1,180,160\n",
            "      BatchNorm2d-30            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-31            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-32            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-33            [-1, 512, 1, 1]               0\n",
            "           Linear-34                  [-1, 512]         262,656\n",
            "================================================================\n",
            "Total params: 2,618,016\n",
            "Trainable params: 2,618,016\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.99\n",
            "Params size (MB): 9.99\n",
            "Estimated Total Size (MB): 12.99\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Student\n",
        "We will train the student network using Dense Features that we created.\n",
        "Dataset datagen will provide data in batches so we need to extract the corresponding batch of targets from our Dense feature variable from 1.2, for this we use the following formula:\n",
        "\n",
        "batch_index * batch_size --> (batch_index * batch_size) + batch_size"
      ],
      "metadata": {
        "id": "8z6cgCZ7h8F5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(s1.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    s1.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        targets = DenseTrain[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "        \n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        \n",
        "        s1.zero_grad()\n",
        "        outputs = s1(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        if(batch_idx % 10 == 0):\n",
        "          print(\"Loss : \", train_loss/(batch_idx+1))\n",
        "def test(epoch):\n",
        "    s1.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            targets = DenseTest[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "            outputs = s1(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\" Loss : \", test_loss/(batch_idx+1))"
      ],
      "metadata": {
        "id": "eXXtMar7h6jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+60):\n",
        "    train(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1IThg_HiK6q",
        "outputId": "53886561-b8c4-44d3-d2b6-42c049ccad86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Loss :  3.6982531547546387\n",
            "Loss :  2.6693457798524336\n",
            "Loss :  2.040993488970257\n",
            "Loss :  1.6936085147242392\n",
            "Loss :  1.467016842307114\n",
            "Loss :  1.3230298921173693\n",
            "Loss :  1.2173858148152712\n",
            "Loss :  1.1410718870834566\n",
            "Loss :  1.0773069829116633\n",
            "Loss :  1.0279522839483324\n",
            "Loss :  0.9826088010674656\n",
            "Loss :  0.9441749147466711\n",
            "Loss :  0.913011798188706\n",
            "Loss :  0.8847250028420951\n",
            "Loss :  0.8595928154515882\n",
            "Loss :  0.8373164838513002\n",
            "Loss :  0.8162185294287545\n",
            "Loss :  0.7974991399293755\n",
            "Loss :  0.7824804616567179\n",
            "Loss :  0.7662618968499268\n",
            "Loss :  0.751274244405737\n",
            "Loss :  0.7378353629067046\n",
            "Loss :  0.7251876513073348\n",
            "Loss :  0.7137788181955164\n",
            "Loss :  0.7028869803515707\n",
            "Loss :  0.6934796735584974\n",
            "Loss :  0.683782813078599\n",
            "Loss :  0.6741902095805234\n",
            "Loss :  0.6649867340763268\n",
            "Loss :  0.65718315843864\n",
            "Loss :  0.6493550655849748\n",
            "Loss :  0.642017930650251\n",
            "Loss :  0.634771172595544\n",
            "Loss :  0.6277699426401778\n",
            "Loss :  0.6213034062791081\n",
            "Loss :  0.6148867536643972\n",
            "Loss :  0.6086351433123908\n",
            "Loss :  0.60276378952268\n",
            "Loss :  0.5964537517135851\n",
            "Loss :  0.590879879949038\n",
            "Loss :  0.5854624561092205\n",
            "Loss :  0.5800766229194446\n",
            "Loss :  0.5750415806532473\n",
            "Loss :  0.5699751790604293\n",
            "Loss :  0.565402856692165\n",
            "Loss :  0.5606944153287723\n",
            "Loss :  0.556215103950004\n",
            "Loss :  0.5516340666136165\n",
            "Loss :  0.5476758462103886\n",
            "Loss :  0.5432978083669775\n",
            "Validation: \n",
            " Loss :  0.30649250745773315\n",
            " Loss :  0.3378171239580427\n",
            " Loss :  0.3386595416359785\n",
            " Loss :  0.33641758463421806\n",
            " Loss :  0.33542635374599034\n",
            "\n",
            "Epoch: 2\n",
            "Loss :  0.3453124165534973\n",
            "Loss :  0.3395089100707661\n",
            "Loss :  0.3359364285355523\n",
            "Loss :  0.33559507323849586\n",
            "Loss :  0.33323589912274987\n",
            "Loss :  0.33278703689575195\n",
            "Loss :  0.3289629880522118\n",
            "Loss :  0.32678995669727595\n",
            "Loss :  0.32343184727209584\n",
            "Loss :  0.32060641115838356\n",
            "Loss :  0.3180653718438479\n",
            "Loss :  0.3157094601575319\n",
            "Loss :  0.3135690538843801\n",
            "Loss :  0.3123232349184633\n",
            "Loss :  0.31033879666463704\n",
            "Loss :  0.3090059044740058\n",
            "Loss :  0.30723855778667497\n",
            "Loss :  0.30615733546471735\n",
            "Loss :  0.30530184449741193\n",
            "Loss :  0.3039969348626611\n",
            "Loss :  0.3027485862596711\n",
            "Loss :  0.30187874192027686\n",
            "Loss :  0.3004751420533495\n",
            "Loss :  0.2992846324588313\n",
            "Loss :  0.29758340893197355\n",
            "Loss :  0.29689817373971067\n",
            "Loss :  0.295948617131774\n",
            "Loss :  0.2945846964732307\n",
            "Loss :  0.2932850057226059\n",
            "Loss :  0.2925554873914653\n",
            "Loss :  0.29147096908963793\n",
            "Loss :  0.2903700003765787\n",
            "Loss :  0.2892078389167043\n",
            "Loss :  0.2882967150553476\n",
            "Loss :  0.2874440227872815\n",
            "Loss :  0.2865203149043597\n",
            "Loss :  0.28566781999001545\n",
            "Loss :  0.28465449532890574\n",
            "Loss :  0.28340626944081365\n",
            "Loss :  0.282232231381909\n",
            "Loss :  0.2812218281470629\n",
            "Loss :  0.2803345363578077\n",
            "Loss :  0.2795526964364312\n",
            "Loss :  0.2785109686395132\n",
            "Loss :  0.277784326280596\n",
            "Loss :  0.27703229719545786\n",
            "Loss :  0.27627762644642606\n",
            "Loss :  0.2753514422226357\n",
            "Loss :  0.27471211986333566\n",
            "Loss :  0.2736089423511276\n",
            "Validation: \n",
            " Loss :  0.20474176108837128\n",
            " Loss :  0.22717477026439847\n",
            " Loss :  0.22691406709391895\n",
            " Loss :  0.22566280233078315\n",
            " Loss :  0.22603972053822177\n",
            "\n",
            "Epoch: 3\n",
            "Loss :  0.2517645061016083\n",
            "Loss :  0.22620063613761554\n",
            "Loss :  0.23045008948871068\n",
            "Loss :  0.22976429039432156\n",
            "Loss :  0.2289476885301311\n",
            "Loss :  0.22875907315927393\n",
            "Loss :  0.22587587330185\n",
            "Loss :  0.22627878126124262\n",
            "Loss :  0.22464136834497805\n",
            "Loss :  0.2226736226252147\n",
            "Loss :  0.22040974695493679\n",
            "Loss :  0.2191299000033387\n",
            "Loss :  0.21863673528856484\n",
            "Loss :  0.21862557302904492\n",
            "Loss :  0.2181914688636225\n",
            "Loss :  0.21798042124075606\n",
            "Loss :  0.2171848788394691\n",
            "Loss :  0.2169962457397528\n",
            "Loss :  0.21721155653342358\n",
            "Loss :  0.2165050810856345\n",
            "Loss :  0.21610040821839327\n",
            "Loss :  0.2164073916973096\n",
            "Loss :  0.21600888696461243\n",
            "Loss :  0.2158858058772562\n",
            "Loss :  0.215392130044486\n",
            "Loss :  0.21514970111656948\n",
            "Loss :  0.21464041172316248\n",
            "Loss :  0.2141067688645472\n",
            "Loss :  0.21369533505940352\n",
            "Loss :  0.21372181694327352\n",
            "Loss :  0.213568601497384\n",
            "Loss :  0.21276638434055917\n",
            "Loss :  0.21218505873115634\n",
            "Loss :  0.21164354465879343\n",
            "Loss :  0.21120504619788563\n",
            "Loss :  0.21093432074598437\n",
            "Loss :  0.21070822251965796\n",
            "Loss :  0.2101616445738029\n",
            "Loss :  0.20954788634626884\n",
            "Loss :  0.2090496803488573\n",
            "Loss :  0.2085881682108168\n",
            "Loss :  0.20824384174497748\n",
            "Loss :  0.20806160063047024\n",
            "Loss :  0.20756165617579928\n",
            "Loss :  0.2072549958348004\n",
            "Loss :  0.20698499643221135\n",
            "Loss :  0.2066955699256082\n",
            "Loss :  0.20638451445254552\n",
            "Loss :  0.20625571853901392\n",
            "Loss :  0.2056330046682882\n",
            "Validation: \n",
            " Loss :  0.16627654433250427\n",
            " Loss :  0.17772453881445385\n",
            " Loss :  0.1773947651793317\n",
            " Loss :  0.17455215800981053\n",
            " Loss :  0.17496148506064474\n",
            "\n",
            "Epoch: 4\n",
            "Loss :  0.21935003995895386\n",
            "Loss :  0.1865910440683365\n",
            "Loss :  0.18488500444662004\n",
            "Loss :  0.18518177203593716\n",
            "Loss :  0.18538091567958273\n",
            "Loss :  0.1855055888493856\n",
            "Loss :  0.18343696071476231\n",
            "Loss :  0.18322345383570227\n",
            "Loss :  0.1828488924621064\n",
            "Loss :  0.18176368877782925\n",
            "Loss :  0.18060693572653402\n",
            "Loss :  0.1795629073639174\n",
            "Loss :  0.1792623013996881\n",
            "Loss :  0.17950734621240894\n",
            "Loss :  0.1788713018944923\n",
            "Loss :  0.1789802455941573\n",
            "Loss :  0.17889064689230474\n",
            "Loss :  0.17880696624691722\n",
            "Loss :  0.17884602286539025\n",
            "Loss :  0.17856750032664592\n",
            "Loss :  0.17821484136937268\n",
            "Loss :  0.17833745204159435\n",
            "Loss :  0.17827772231123565\n",
            "Loss :  0.17806470994051402\n",
            "Loss :  0.17770380721547296\n",
            "Loss :  0.17773440438675214\n",
            "Loss :  0.17753417281812178\n",
            "Loss :  0.17732164065776276\n",
            "Loss :  0.1770701704818583\n",
            "Loss :  0.17712825413831731\n",
            "Loss :  0.17713158095001777\n",
            "Loss :  0.17660198549940656\n",
            "Loss :  0.17630622329370255\n",
            "Loss :  0.1762032539012569\n",
            "Loss :  0.17605535266511252\n",
            "Loss :  0.17594606817787528\n",
            "Loss :  0.17578614992283056\n",
            "Loss :  0.1754848935372746\n",
            "Loss :  0.1751738318542796\n",
            "Loss :  0.1746587021957578\n",
            "Loss :  0.1745346425254446\n",
            "Loss :  0.1744564466740383\n",
            "Loss :  0.17427110441931637\n",
            "Loss :  0.1739059499618349\n",
            "Loss :  0.17376816992451544\n",
            "Loss :  0.17365499243900148\n",
            "Loss :  0.17347887446073545\n",
            "Loss :  0.17324877070013883\n",
            "Loss :  0.17322360428356084\n",
            "Loss :  0.1727492909749511\n",
            "Validation: \n",
            " Loss :  0.1424711048603058\n",
            " Loss :  0.15739396924064272\n",
            " Loss :  0.15585817687395143\n",
            " Loss :  0.1532519733319517\n",
            " Loss :  0.1537072899532907\n",
            "\n",
            "Epoch: 5\n",
            "Loss :  0.16409868001937866\n",
            "Loss :  0.16059130836616864\n",
            "Loss :  0.15991567217168354\n",
            "Loss :  0.1606549471616745\n",
            "Loss :  0.16112094245305875\n",
            "Loss :  0.16101580975102445\n",
            "Loss :  0.15973594638167835\n",
            "Loss :  0.15948726908421854\n",
            "Loss :  0.1595777330207236\n",
            "Loss :  0.15900212215198264\n",
            "Loss :  0.15725805979258944\n",
            "Loss :  0.15625733841915387\n",
            "Loss :  0.1559887137417951\n",
            "Loss :  0.1567234058530276\n",
            "Loss :  0.15609430088430432\n",
            "Loss :  0.15579261038674425\n",
            "Loss :  0.15539481083613746\n",
            "Loss :  0.15553669017135052\n",
            "Loss :  0.15571787743443283\n",
            "Loss :  0.15529670285460836\n",
            "Loss :  0.15508080069995045\n",
            "Loss :  0.15516545662382766\n",
            "Loss :  0.155306575592287\n",
            "Loss :  0.15562725802520652\n",
            "Loss :  0.15557571571644907\n",
            "Loss :  0.1555275253921866\n",
            "Loss :  0.15535833050008022\n",
            "Loss :  0.1550825900807152\n",
            "Loss :  0.15501729682671217\n",
            "Loss :  0.15522647620886051\n",
            "Loss :  0.15512931802344085\n",
            "Loss :  0.15482126703407986\n",
            "Loss :  0.15475051541380422\n",
            "Loss :  0.154660213637388\n",
            "Loss :  0.15453173397136225\n",
            "Loss :  0.15447380750352502\n",
            "Loss :  0.15429217867821537\n",
            "Loss :  0.15411954983627058\n",
            "Loss :  0.1539685192305272\n",
            "Loss :  0.153705810189552\n",
            "Loss :  0.15372302072898408\n",
            "Loss :  0.15370737063333645\n",
            "Loss :  0.15374057513447104\n",
            "Loss :  0.15354740464078853\n",
            "Loss :  0.15349682060634198\n",
            "Loss :  0.15351899754578152\n",
            "Loss :  0.15338837647903508\n",
            "Loss :  0.15328124137061416\n",
            "Loss :  0.15321606837538324\n",
            "Loss :  0.1529051237976721\n",
            "Validation: \n",
            " Loss :  0.13179747760295868\n",
            " Loss :  0.13752470449322746\n",
            " Loss :  0.1359674525333614\n",
            " Loss :  0.1341034897038194\n",
            " Loss :  0.13475935492250654\n",
            "\n",
            "Epoch: 6\n",
            "Loss :  0.14827799797058105\n",
            "Loss :  0.1437982056628574\n",
            "Loss :  0.14505051111891157\n",
            "Loss :  0.14470099994251806\n",
            "Loss :  0.1451499385804665\n",
            "Loss :  0.14495146230739706\n",
            "Loss :  0.1436963276784928\n",
            "Loss :  0.14316638831941175\n",
            "Loss :  0.1431932766680364\n",
            "Loss :  0.14282486080140858\n",
            "Loss :  0.14173391284328876\n",
            "Loss :  0.14132884225329836\n",
            "Loss :  0.14120402174793983\n",
            "Loss :  0.14162348898995014\n",
            "Loss :  0.14117331147616638\n",
            "Loss :  0.141192353916484\n",
            "Loss :  0.1407812335383818\n",
            "Loss :  0.1409287738956903\n",
            "Loss :  0.1412605203235347\n",
            "Loss :  0.14111157327266263\n",
            "Loss :  0.14082104560747669\n",
            "Loss :  0.14094954672582907\n",
            "Loss :  0.1410381220179985\n",
            "Loss :  0.14112157387521876\n",
            "Loss :  0.14101073065486686\n",
            "Loss :  0.14107714504359728\n",
            "Loss :  0.14096271143905048\n",
            "Loss :  0.14076785721242208\n",
            "Loss :  0.14065795031200523\n",
            "Loss :  0.14096498125812032\n",
            "Loss :  0.14109533745486078\n",
            "Loss :  0.14073718190672313\n",
            "Loss :  0.14060810932489198\n",
            "Loss :  0.14054650241604744\n",
            "Loss :  0.1404488789808016\n",
            "Loss :  0.140471987192787\n",
            "Loss :  0.14019374103592375\n",
            "Loss :  0.14004386288698792\n",
            "Loss :  0.13985743005992235\n",
            "Loss :  0.13959431244284295\n",
            "Loss :  0.13952624935313057\n",
            "Loss :  0.13962376665169884\n",
            "Loss :  0.13967574536446437\n",
            "Loss :  0.13941064003310458\n",
            "Loss :  0.13936562224580587\n",
            "Loss :  0.13930684503192647\n",
            "Loss :  0.13917868626415084\n",
            "Loss :  0.1391327909002132\n",
            "Loss :  0.13912163958294227\n",
            "Loss :  0.13885151684405606\n",
            "Validation: \n",
            " Loss :  0.12932391464710236\n",
            " Loss :  0.1358499512785957\n",
            " Loss :  0.1348891563531829\n",
            " Loss :  0.13253659925988462\n",
            " Loss :  0.1327569474592621\n",
            "\n",
            "Epoch: 7\n",
            "Loss :  0.13597576320171356\n",
            "Loss :  0.13024542412974618\n",
            "Loss :  0.1310151606088593\n",
            "Loss :  0.1325588291210513\n",
            "Loss :  0.13295839090899722\n",
            "Loss :  0.13311483637959348\n",
            "Loss :  0.13298852558507293\n",
            "Loss :  0.13329672383170732\n",
            "Loss :  0.13322548669429474\n",
            "Loss :  0.1323074358668956\n",
            "Loss :  0.13145954921694086\n",
            "Loss :  0.1307041459792369\n",
            "Loss :  0.13060527393394264\n",
            "Loss :  0.13104587110854288\n",
            "Loss :  0.13077264791684792\n",
            "Loss :  0.13078807294368744\n",
            "Loss :  0.13065168455890988\n",
            "Loss :  0.13069212807026523\n",
            "Loss :  0.13092404871520416\n",
            "Loss :  0.1309756896414682\n",
            "Loss :  0.13089775360787093\n",
            "Loss :  0.13119302287485926\n",
            "Loss :  0.13126077098409514\n",
            "Loss :  0.13129143491064832\n",
            "Loss :  0.13099271915389293\n",
            "Loss :  0.13095081947832943\n",
            "Loss :  0.13080127223241375\n",
            "Loss :  0.13072508276608596\n",
            "Loss :  0.13078069374018292\n",
            "Loss :  0.13101862386329888\n",
            "Loss :  0.13103461728440566\n",
            "Loss :  0.13091330544070798\n",
            "Loss :  0.1308497591321342\n",
            "Loss :  0.13077299312612442\n",
            "Loss :  0.1307375231359012\n",
            "Loss :  0.1308022104522102\n",
            "Loss :  0.13077195013494042\n",
            "Loss :  0.13060384880339682\n",
            "Loss :  0.1304738837003395\n",
            "Loss :  0.1302210011369432\n",
            "Loss :  0.13026587384523003\n",
            "Loss :  0.13025994640559754\n",
            "Loss :  0.1302757352909113\n",
            "Loss :  0.12996512397328552\n",
            "Loss :  0.1299294952869145\n",
            "Loss :  0.12986884576716604\n",
            "Loss :  0.12971255697164516\n",
            "Loss :  0.12971358896567312\n",
            "Loss :  0.12967860611957224\n",
            "Loss :  0.12947430933565812\n",
            "Validation: \n",
            " Loss :  0.11391229182481766\n",
            " Loss :  0.11811758223034087\n",
            " Loss :  0.11731488148613674\n",
            " Loss :  0.11565628134813465\n",
            " Loss :  0.11571743238119432\n",
            "\n",
            "Epoch: 8\n",
            "Loss :  0.13323912024497986\n",
            "Loss :  0.12093311074105176\n",
            "Loss :  0.12363736295983904\n",
            "Loss :  0.12420976931048978\n",
            "Loss :  0.1248838327279905\n",
            "Loss :  0.12464200179366504\n",
            "Loss :  0.1241217249485313\n",
            "Loss :  0.12407046818817166\n",
            "Loss :  0.12398980585513292\n",
            "Loss :  0.1231826350584135\n",
            "Loss :  0.12256217489738276\n",
            "Loss :  0.12176832160702697\n",
            "Loss :  0.12150856095158365\n",
            "Loss :  0.12178630415947382\n",
            "Loss :  0.12138040881630377\n",
            "Loss :  0.12166766506551907\n",
            "Loss :  0.12170328431247925\n",
            "Loss :  0.12180798448491514\n",
            "Loss :  0.12191058324845457\n",
            "Loss :  0.12174774604942162\n",
            "Loss :  0.12191334848676748\n",
            "Loss :  0.1222617561924514\n",
            "Loss :  0.12245674522349198\n",
            "Loss :  0.12273382772872975\n",
            "Loss :  0.12272571902181104\n",
            "Loss :  0.12290545719197071\n",
            "Loss :  0.12280567292966148\n",
            "Loss :  0.12269491831534902\n",
            "Loss :  0.12268715805218314\n",
            "Loss :  0.12286042959726963\n",
            "Loss :  0.12291892635267834\n",
            "Loss :  0.12269085842121835\n",
            "Loss :  0.12262530005145296\n",
            "Loss :  0.12258157654021082\n",
            "Loss :  0.12250126668872022\n",
            "Loss :  0.12258568401859696\n",
            "Loss :  0.12248538529443609\n",
            "Loss :  0.12237706069515722\n",
            "Loss :  0.12231478357096044\n",
            "Loss :  0.1222008813143996\n",
            "Loss :  0.12222296809615042\n",
            "Loss :  0.1223637917761095\n",
            "Loss :  0.1224421995057063\n",
            "Loss :  0.12225772499208384\n",
            "Loss :  0.12222888803874013\n",
            "Loss :  0.12222995747814157\n",
            "Loss :  0.12210855443927575\n",
            "Loss :  0.12207993347743515\n",
            "Loss :  0.12215269366259882\n",
            "Loss :  0.12190466624534786\n",
            "Validation: \n",
            " Loss :  0.10476729273796082\n",
            " Loss :  0.10889840126037598\n",
            " Loss :  0.10774664362756217\n",
            " Loss :  0.10608530178910396\n",
            " Loss :  0.10633854733573066\n",
            "\n",
            "Epoch: 9\n",
            "Loss :  0.11848090589046478\n",
            "Loss :  0.11764514378525993\n",
            "Loss :  0.11629418141785122\n",
            "Loss :  0.11622757584817948\n",
            "Loss :  0.11681063782151152\n",
            "Loss :  0.11721871705616221\n",
            "Loss :  0.11653888872900947\n",
            "Loss :  0.11645869879235685\n",
            "Loss :  0.11685530684980346\n",
            "Loss :  0.11623121838975739\n",
            "Loss :  0.11554212750184654\n",
            "Loss :  0.11519978987472551\n",
            "Loss :  0.11500131443512342\n",
            "Loss :  0.11555326275015605\n",
            "Loss :  0.11546383206303237\n",
            "Loss :  0.11554630337566729\n",
            "Loss :  0.11532654428148861\n",
            "Loss :  0.11561876975479182\n",
            "Loss :  0.11570192428912905\n",
            "Loss :  0.1157693386233914\n",
            "Loss :  0.11583050870480228\n",
            "Loss :  0.11605520620589008\n",
            "Loss :  0.1162050449335737\n",
            "Loss :  0.11635008831689884\n",
            "Loss :  0.1163569376552748\n",
            "Loss :  0.11639445144460496\n",
            "Loss :  0.11641852266487034\n",
            "Loss :  0.11627164829481132\n",
            "Loss :  0.11635031508699431\n",
            "Loss :  0.11649125509757766\n",
            "Loss :  0.11643568230823821\n",
            "Loss :  0.11624590204459678\n",
            "Loss :  0.11626666956044432\n",
            "Loss :  0.11628627221119728\n",
            "Loss :  0.11637957278322264\n",
            "Loss :  0.11651989280583172\n",
            "Loss :  0.11652364119515855\n",
            "Loss :  0.11646460097112424\n",
            "Loss :  0.11637278575831511\n",
            "Loss :  0.11620346142355438\n",
            "Loss :  0.11618176847696304\n",
            "Loss :  0.11622809528071805\n",
            "Loss :  0.11631317985312672\n",
            "Loss :  0.1161998267827897\n",
            "Loss :  0.11612402235502019\n",
            "Loss :  0.11615894318552081\n",
            "Loss :  0.11609638000711185\n",
            "Loss :  0.11606722519655896\n",
            "Loss :  0.1160886118385995\n",
            "Loss :  0.11589666882381905\n",
            "Validation: \n",
            " Loss :  0.1098499670624733\n",
            " Loss :  0.1123905795670691\n",
            " Loss :  0.11113504520276697\n",
            " Loss :  0.10966949103797069\n",
            " Loss :  0.11011484099758996\n",
            "\n",
            "Epoch: 10\n",
            "Loss :  0.11409494280815125\n",
            "Loss :  0.11226073584773323\n",
            "Loss :  0.11172286704892204\n",
            "Loss :  0.11031674449482272\n",
            "Loss :  0.11094995133760499\n",
            "Loss :  0.11090283022791732\n",
            "Loss :  0.11046934933936009\n",
            "Loss :  0.11027780741872922\n",
            "Loss :  0.11036868908523041\n",
            "Loss :  0.11013228798305595\n",
            "Loss :  0.10974029001623097\n",
            "Loss :  0.10949227838097392\n",
            "Loss :  0.10939839617772536\n",
            "Loss :  0.10974175551237951\n",
            "Loss :  0.1098088504786187\n",
            "Loss :  0.10993059295297458\n",
            "Loss :  0.1098590945790273\n",
            "Loss :  0.11006738319557313\n",
            "Loss :  0.11034117231546844\n",
            "Loss :  0.11050956048734525\n",
            "Loss :  0.11044743497721592\n",
            "Loss :  0.11070024850667935\n",
            "Loss :  0.11087112391696256\n",
            "Loss :  0.11108262914341765\n",
            "Loss :  0.11107646587355008\n",
            "Loss :  0.1110851814250547\n",
            "Loss :  0.11106350795290935\n",
            "Loss :  0.11095808748829408\n",
            "Loss :  0.11108023392242045\n",
            "Loss :  0.11126602924976152\n",
            "Loss :  0.11131375599838174\n",
            "Loss :  0.1111408375754617\n",
            "Loss :  0.11110779828743028\n",
            "Loss :  0.11110063192077274\n",
            "Loss :  0.11112888065601025\n",
            "Loss :  0.11120156173267935\n",
            "Loss :  0.11116459800596053\n",
            "Loss :  0.11107027122996888\n",
            "Loss :  0.11091980715514481\n",
            "Loss :  0.11077159776559571\n",
            "Loss :  0.11071180838376209\n",
            "Loss :  0.11069293331055746\n",
            "Loss :  0.11079055716830025\n",
            "Loss :  0.11067678739604153\n",
            "Loss :  0.11067709990707385\n",
            "Loss :  0.11063761411114965\n",
            "Loss :  0.11053154866439402\n",
            "Loss :  0.11050880062858517\n",
            "Loss :  0.11051436347921773\n",
            "Loss :  0.1103187920241152\n",
            "Validation: \n",
            " Loss :  0.10402581095695496\n",
            " Loss :  0.10551298303263527\n",
            " Loss :  0.10396078864975673\n",
            " Loss :  0.10258800915030182\n",
            " Loss :  0.10261549993797585\n",
            "\n",
            "Epoch: 11\n",
            "Loss :  0.11310729384422302\n",
            "Loss :  0.10588148371739821\n",
            "Loss :  0.10528435451643807\n",
            "Loss :  0.10607156469937294\n",
            "Loss :  0.10681230938289224\n",
            "Loss :  0.10713371064733057\n",
            "Loss :  0.10679354152229965\n",
            "Loss :  0.10615617099782111\n",
            "Loss :  0.1060699564807209\n",
            "Loss :  0.10549907844800216\n",
            "Loss :  0.10500273477322984\n",
            "Loss :  0.10485859779087273\n",
            "Loss :  0.10513615195662522\n",
            "Loss :  0.10576163516699813\n",
            "Loss :  0.10574171808383144\n",
            "Loss :  0.10579938436580809\n",
            "Loss :  0.10590631880375169\n",
            "Loss :  0.10623265341011404\n",
            "Loss :  0.10638441289492075\n",
            "Loss :  0.10617539606481323\n",
            "Loss :  0.1061900947520982\n",
            "Loss :  0.10633663328196766\n",
            "Loss :  0.10655550763213256\n",
            "Loss :  0.10682613118773415\n",
            "Loss :  0.10678964813096889\n",
            "Loss :  0.10684307447942605\n",
            "Loss :  0.10682833060565122\n",
            "Loss :  0.10671664548975955\n",
            "Loss :  0.10667795987527989\n",
            "Loss :  0.10689149128714788\n",
            "Loss :  0.10686891964107653\n",
            "Loss :  0.10670722589807112\n",
            "Loss :  0.1067109492801803\n",
            "Loss :  0.10664896851579948\n",
            "Loss :  0.10669127753403179\n",
            "Loss :  0.10669305190401539\n",
            "Loss :  0.10663426190697255\n",
            "Loss :  0.10651551534464417\n",
            "Loss :  0.10641184026800742\n",
            "Loss :  0.10630530246612056\n",
            "Loss :  0.10630765564721124\n",
            "Loss :  0.10642584563751871\n",
            "Loss :  0.10657905873946509\n",
            "Loss :  0.1064469813090466\n",
            "Loss :  0.10647278899103065\n",
            "Loss :  0.10644439407965033\n",
            "Loss :  0.1064384292463677\n",
            "Loss :  0.10645947831578062\n",
            "Loss :  0.10641267460435938\n",
            "Loss :  0.10626733551928565\n",
            "Validation: \n",
            " Loss :  0.10214588791131973\n",
            " Loss :  0.10314772810254778\n",
            " Loss :  0.10236600804619672\n",
            " Loss :  0.1010466068983078\n",
            " Loss :  0.10096087637874815\n",
            "\n",
            "Epoch: 12\n",
            "Loss :  0.10775481909513474\n",
            "Loss :  0.10163313759998842\n",
            "Loss :  0.10047505689518792\n",
            "Loss :  0.10115605376420482\n",
            "Loss :  0.10178683316562234\n",
            "Loss :  0.10264534430176604\n",
            "Loss :  0.10192531970192174\n",
            "Loss :  0.1018570279571372\n",
            "Loss :  0.10182068395761797\n",
            "Loss :  0.10181486115350828\n",
            "Loss :  0.10143459632550136\n",
            "Loss :  0.10090581985475781\n",
            "Loss :  0.10098464405241091\n",
            "Loss :  0.10139187658561095\n",
            "Loss :  0.1012873278653368\n",
            "Loss :  0.10153810665110088\n",
            "Loss :  0.10150838809909288\n",
            "Loss :  0.10170034245092269\n",
            "Loss :  0.10189406952475974\n",
            "Loss :  0.10186187649896632\n",
            "Loss :  0.10179175336414309\n",
            "Loss :  0.10202278871248119\n",
            "Loss :  0.10221463374422686\n",
            "Loss :  0.10256991245142826\n",
            "Loss :  0.10250960982934074\n",
            "Loss :  0.1024437543168011\n",
            "Loss :  0.10242033549994801\n",
            "Loss :  0.10235327652679599\n",
            "Loss :  0.10214950899954793\n",
            "Loss :  0.10237380741900186\n",
            "Loss :  0.10239301775381018\n",
            "Loss :  0.10225597922345833\n",
            "Loss :  0.10218124421214761\n",
            "Loss :  0.10211974092030453\n",
            "Loss :  0.102184423559572\n",
            "Loss :  0.10232922823255898\n",
            "Loss :  0.102224241168215\n",
            "Loss :  0.10214648423892148\n",
            "Loss :  0.10206235978468822\n",
            "Loss :  0.10199990035856471\n",
            "Loss :  0.1020563067380627\n",
            "Loss :  0.10221178871364199\n",
            "Loss :  0.10230286318516787\n",
            "Loss :  0.10211794277147461\n",
            "Loss :  0.10211665164721526\n",
            "Loss :  0.10213663615699353\n",
            "Loss :  0.10210117503774399\n",
            "Loss :  0.1021156610482058\n",
            "Loss :  0.10217527823673712\n",
            "Loss :  0.10198778328305592\n",
            "Validation: \n",
            " Loss :  0.09562219679355621\n",
            " Loss :  0.09895302000499907\n",
            " Loss :  0.09793091856124925\n",
            " Loss :  0.09680646987723522\n",
            " Loss :  0.09715473348343814\n",
            "\n",
            "Epoch: 13\n",
            "Loss :  0.11168307811021805\n",
            "Loss :  0.09971408884633672\n",
            "Loss :  0.0996245387054625\n",
            "Loss :  0.10031460297684516\n",
            "Loss :  0.10047508358228498\n",
            "Loss :  0.10065568662157245\n",
            "Loss :  0.10022102613918117\n",
            "Loss :  0.09996584844841085\n",
            "Loss :  0.09968176585288695\n",
            "Loss :  0.09941382697977863\n",
            "Loss :  0.09880324342463276\n",
            "Loss :  0.09842540907698709\n",
            "Loss :  0.09823511473157188\n",
            "Loss :  0.09855724757409277\n",
            "Loss :  0.09850397642622603\n",
            "Loss :  0.09858364049369926\n",
            "Loss :  0.09864963813227896\n",
            "Loss :  0.09886323843608823\n",
            "Loss :  0.09903690815795192\n",
            "Loss :  0.09895876674127828\n",
            "Loss :  0.09889340497071471\n",
            "Loss :  0.09896148529380419\n",
            "Loss :  0.09903303786640254\n",
            "Loss :  0.09955390636281018\n",
            "Loss :  0.09955003409341164\n",
            "Loss :  0.0995820760133257\n",
            "Loss :  0.09955585719410943\n",
            "Loss :  0.09949006772569184\n",
            "Loss :  0.09954800006015445\n",
            "Loss :  0.09963791072368622\n",
            "Loss :  0.09978373529408065\n",
            "Loss :  0.0997165530872115\n",
            "Loss :  0.09976293466915594\n",
            "Loss :  0.09984920185138092\n",
            "Loss :  0.09992674624552825\n",
            "Loss :  0.09998768319686253\n",
            "Loss :  0.09993275700761341\n",
            "Loss :  0.099854648595229\n",
            "Loss :  0.09974873173502799\n",
            "Loss :  0.09970016310663174\n",
            "Loss :  0.0996820380414216\n",
            "Loss :  0.09970571537595016\n",
            "Loss :  0.0997364454130662\n",
            "Loss :  0.09959009692522323\n",
            "Loss :  0.09966408964795591\n",
            "Loss :  0.09966230446972497\n",
            "Loss :  0.09951638452983472\n",
            "Loss :  0.09954575431827781\n",
            "Loss :  0.09958491544094006\n",
            "Loss :  0.09945219098125602\n",
            "Validation: \n",
            " Loss :  0.09092225879430771\n",
            " Loss :  0.09507480050836291\n",
            " Loss :  0.09418621750139608\n",
            " Loss :  0.09285164832091722\n",
            " Loss :  0.09316060682873667\n",
            "\n",
            "Epoch: 14\n",
            "Loss :  0.11124810576438904\n",
            "Loss :  0.09388496184890921\n",
            "Loss :  0.09592266096955254\n",
            "Loss :  0.09621082054030511\n",
            "Loss :  0.0966532959080324\n",
            "Loss :  0.0965125803269592\n",
            "Loss :  0.09612272582093223\n",
            "Loss :  0.09620935249496514\n",
            "Loss :  0.09587118268748861\n",
            "Loss :  0.09576422557398513\n",
            "Loss :  0.09550566063954098\n",
            "Loss :  0.09512330772908958\n",
            "Loss :  0.09509326763882124\n",
            "Loss :  0.0953652385536951\n",
            "Loss :  0.09554611500484723\n",
            "Loss :  0.09589076757628397\n",
            "Loss :  0.09607901692575549\n",
            "Loss :  0.09619107611520945\n",
            "Loss :  0.09632400522080574\n",
            "Loss :  0.09634588752429522\n",
            "Loss :  0.09617332698990456\n",
            "Loss :  0.09631116246866389\n",
            "Loss :  0.09647696687759857\n",
            "Loss :  0.09689632467764281\n",
            "Loss :  0.09673408870256787\n",
            "Loss :  0.09667580491636854\n",
            "Loss :  0.09671599046823165\n",
            "Loss :  0.0966917954907646\n",
            "Loss :  0.09666356216333939\n",
            "Loss :  0.09689590365616317\n",
            "Loss :  0.09697736151194651\n",
            "Loss :  0.0968119351618543\n",
            "Loss :  0.09680002008643106\n",
            "Loss :  0.09682408391619017\n",
            "Loss :  0.09691015381879471\n",
            "Loss :  0.09711696706351391\n",
            "Loss :  0.09711187378273776\n",
            "Loss :  0.09711100673177493\n",
            "Loss :  0.09716451048772792\n",
            "Loss :  0.09698627267957038\n",
            "Loss :  0.09697991390329347\n",
            "Loss :  0.09697396761382003\n",
            "Loss :  0.0970011059362928\n",
            "Loss :  0.0969000633659053\n",
            "Loss :  0.09694696775301784\n",
            "Loss :  0.09690052470782907\n",
            "Loss :  0.09685736605311682\n",
            "Loss :  0.0968519702648661\n",
            "Loss :  0.09690856287670235\n",
            "Loss :  0.096815204562699\n",
            "Validation: \n",
            " Loss :  0.0954829752445221\n",
            " Loss :  0.09802990122919991\n",
            " Loss :  0.09702519799877958\n",
            " Loss :  0.09591804261578889\n",
            " Loss :  0.0962790267335044\n",
            "\n",
            "Epoch: 15\n",
            "Loss :  0.10563857853412628\n",
            "Loss :  0.09243084558031776\n",
            "Loss :  0.09097885943594433\n",
            "Loss :  0.09292769984852883\n",
            "Loss :  0.09344408642954943\n",
            "Loss :  0.09372395759119707\n",
            "Loss :  0.09294453739631371\n",
            "Loss :  0.09297954752831392\n",
            "Loss :  0.09303490265651986\n",
            "Loss :  0.0929893685893698\n",
            "Loss :  0.09254539558793058\n",
            "Loss :  0.09198852769426398\n",
            "Loss :  0.09213349099986809\n",
            "Loss :  0.09236280386912003\n",
            "Loss :  0.09221892803907394\n",
            "Loss :  0.09245222180292306\n",
            "Loss :  0.09249691623523369\n",
            "Loss :  0.09269187551492836\n",
            "Loss :  0.09288467705579094\n",
            "Loss :  0.09292062223737776\n",
            "Loss :  0.0928780406861756\n",
            "Loss :  0.09291132906743135\n",
            "Loss :  0.09308303693705554\n",
            "Loss :  0.09344024794958371\n",
            "Loss :  0.09341794230754939\n",
            "Loss :  0.09334352924173096\n",
            "Loss :  0.09338058990879534\n",
            "Loss :  0.09336303985536758\n",
            "Loss :  0.09331022648412562\n",
            "Loss :  0.09353537773041382\n",
            "Loss :  0.0936036929488182\n",
            "Loss :  0.09351790229223932\n",
            "Loss :  0.09353252716153582\n",
            "Loss :  0.09363745044905614\n",
            "Loss :  0.09372256005789179\n",
            "Loss :  0.09387541299107408\n",
            "Loss :  0.09379258343222399\n",
            "Loss :  0.09369622345642259\n",
            "Loss :  0.093611832776564\n",
            "Loss :  0.09352725854767557\n",
            "Loss :  0.09349365092052189\n",
            "Loss :  0.0935426661676734\n",
            "Loss :  0.09359488797301069\n",
            "Loss :  0.093507846208017\n",
            "Loss :  0.09351313767992721\n",
            "Loss :  0.09352269025308858\n",
            "Loss :  0.09345353493333639\n",
            "Loss :  0.09349534499227621\n",
            "Loss :  0.09350562442612995\n",
            "Loss :  0.0933491883992906\n",
            "Validation: \n",
            " Loss :  0.09083449840545654\n",
            " Loss :  0.09288446073021207\n",
            " Loss :  0.09233852494053724\n",
            " Loss :  0.09133500434824678\n",
            " Loss :  0.09149585093980954\n",
            "\n",
            "Epoch: 16\n",
            "Loss :  0.10070464760065079\n",
            "Loss :  0.09381381896409122\n",
            "Loss :  0.09175448190598261\n",
            "Loss :  0.0923921360123542\n",
            "Loss :  0.09241960252203592\n",
            "Loss :  0.092754554076522\n",
            "Loss :  0.09211906754091138\n",
            "Loss :  0.09191476207383921\n",
            "Loss :  0.09192634962591124\n",
            "Loss :  0.09176270311677849\n",
            "Loss :  0.09144776875134741\n",
            "Loss :  0.0911682292416289\n",
            "Loss :  0.09090330113064159\n",
            "Loss :  0.09101334254022773\n",
            "Loss :  0.09117659741471\n",
            "Loss :  0.09122555234179591\n",
            "Loss :  0.09132802731687238\n",
            "Loss :  0.09165008928169284\n",
            "Loss :  0.09187917304302447\n",
            "Loss :  0.09180035629353599\n",
            "Loss :  0.09181694074797986\n",
            "Loss :  0.09198496815576372\n",
            "Loss :  0.09200889162078701\n",
            "Loss :  0.09219853815449265\n",
            "Loss :  0.09212681202843971\n",
            "Loss :  0.09215625093515176\n",
            "Loss :  0.09207546571091217\n",
            "Loss :  0.0919113137735212\n",
            "Loss :  0.09196700789432084\n",
            "Loss :  0.09205304764697642\n",
            "Loss :  0.09211643857021268\n",
            "Loss :  0.09196859983865088\n",
            "Loss :  0.09194556222155087\n",
            "Loss :  0.09194239465311575\n",
            "Loss :  0.09195363386110826\n",
            "Loss :  0.09200515757259141\n",
            "Loss :  0.09196327007543348\n",
            "Loss :  0.09193856176742003\n",
            "Loss :  0.09195915480532985\n",
            "Loss :  0.09189918707779911\n",
            "Loss :  0.09192536377401424\n",
            "Loss :  0.09195791904581144\n",
            "Loss :  0.09200457111885882\n",
            "Loss :  0.09190753138604685\n",
            "Loss :  0.0919466492067389\n",
            "Loss :  0.09195746706092436\n",
            "Loss :  0.09188594790316973\n",
            "Loss :  0.09192361703217662\n",
            "Loss :  0.09196110636677415\n",
            "Loss :  0.09185994281424038\n",
            "Validation: \n",
            " Loss :  0.08358583599328995\n",
            " Loss :  0.08835762561786742\n",
            " Loss :  0.08734325192323546\n",
            " Loss :  0.08633270395583793\n",
            " Loss :  0.08646364068543469\n",
            "\n",
            "Epoch: 17\n",
            "Loss :  0.09448143094778061\n",
            "Loss :  0.08685297993096439\n",
            "Loss :  0.08747539598317373\n",
            "Loss :  0.08779562120476077\n",
            "Loss :  0.08863666998903925\n",
            "Loss :  0.0888843815408501\n",
            "Loss :  0.08874520525091985\n",
            "Loss :  0.08855634287629328\n",
            "Loss :  0.08881460148611187\n",
            "Loss :  0.08875358866138773\n",
            "Loss :  0.08866502337231494\n",
            "Loss :  0.08816408124324437\n",
            "Loss :  0.08814588336905171\n",
            "Loss :  0.08839513594640121\n",
            "Loss :  0.08832340243648976\n",
            "Loss :  0.08855013381566433\n",
            "Loss :  0.08865609718775898\n",
            "Loss :  0.08890403946589308\n",
            "Loss :  0.08876845191196842\n",
            "Loss :  0.08862543675599922\n",
            "Loss :  0.08868302126873785\n",
            "Loss :  0.08873828814775458\n",
            "Loss :  0.0888299470708381\n",
            "Loss :  0.08906337744616843\n",
            "Loss :  0.08911739521625131\n",
            "Loss :  0.0890576815996987\n",
            "Loss :  0.08902251001062064\n",
            "Loss :  0.08887654595159517\n",
            "Loss :  0.08895802924747569\n",
            "Loss :  0.08912248038958848\n",
            "Loss :  0.0892266101664879\n",
            "Loss :  0.08917457763692574\n",
            "Loss :  0.08916538603302103\n",
            "Loss :  0.0892320925150393\n",
            "Loss :  0.08930274767697381\n",
            "Loss :  0.08945907849786627\n",
            "Loss :  0.08942193279966423\n",
            "Loss :  0.08934915840947082\n",
            "Loss :  0.08927876240234049\n",
            "Loss :  0.08920512291247887\n",
            "Loss :  0.08920118090071881\n",
            "Loss :  0.08925769287739357\n",
            "Loss :  0.08930174581653431\n",
            "Loss :  0.08920465516933432\n",
            "Loss :  0.08923727152298908\n",
            "Loss :  0.08925901832443119\n",
            "Loss :  0.08920252250422625\n",
            "Loss :  0.08924308327479474\n",
            "Loss :  0.08935196319761494\n",
            "Loss :  0.08925744466286327\n",
            "Validation: \n",
            " Loss :  0.08796583861112595\n",
            " Loss :  0.09163175984507516\n",
            " Loss :  0.09164272793909399\n",
            " Loss :  0.09061363107356869\n",
            " Loss :  0.09078306649570111\n",
            "\n",
            "Epoch: 18\n",
            "Loss :  0.08913469314575195\n",
            "Loss :  0.08605759319933978\n",
            "Loss :  0.0850853721300761\n",
            "Loss :  0.08583508359809075\n",
            "Loss :  0.08648070529466723\n",
            "Loss :  0.0868408649283297\n",
            "Loss :  0.0864842068709311\n",
            "Loss :  0.08626423975531484\n",
            "Loss :  0.0864046636370965\n",
            "Loss :  0.08635440542475208\n",
            "Loss :  0.08588355337039079\n",
            "Loss :  0.08563203002149994\n",
            "Loss :  0.0854298753186691\n",
            "Loss :  0.08589012158736017\n",
            "Loss :  0.08596993005233454\n",
            "Loss :  0.08641440100622493\n",
            "Loss :  0.08646989484196124\n",
            "Loss :  0.08677706531962456\n",
            "Loss :  0.08700169779319131\n",
            "Loss :  0.08690038541848746\n",
            "Loss :  0.08700955536828112\n",
            "Loss :  0.08721214818869721\n",
            "Loss :  0.08732228368790441\n",
            "Loss :  0.08766844097799037\n",
            "Loss :  0.08763899202787036\n",
            "Loss :  0.08766412114598361\n",
            "Loss :  0.08772923075147972\n",
            "Loss :  0.08760262143238004\n",
            "Loss :  0.08757298333683047\n",
            "Loss :  0.08770985947441809\n",
            "Loss :  0.08767154892021635\n",
            "Loss :  0.08761623609583477\n",
            "Loss :  0.0875746779306284\n",
            "Loss :  0.087673927249325\n",
            "Loss :  0.08765277303646038\n",
            "Loss :  0.08778607870778467\n",
            "Loss :  0.08774621281102093\n",
            "Loss :  0.08772459543859862\n",
            "Loss :  0.0876604150247386\n",
            "Loss :  0.08760468547453966\n",
            "Loss :  0.08754787092419931\n",
            "Loss :  0.08758490092127863\n",
            "Loss :  0.0876307654196746\n",
            "Loss :  0.08744743668631999\n",
            "Loss :  0.08750491173697167\n",
            "Loss :  0.08757784990738342\n",
            "Loss :  0.08754279189733001\n",
            "Loss :  0.08762643895964208\n",
            "Loss :  0.08765946942034977\n",
            "Loss :  0.08757629482238696\n",
            "Validation: \n",
            " Loss :  0.08501657098531723\n",
            " Loss :  0.0840288024573099\n",
            " Loss :  0.08307420407853476\n",
            " Loss :  0.0818054698041228\n",
            " Loss :  0.08210221567639599\n",
            "\n",
            "Epoch: 19\n",
            "Loss :  0.09487462788820267\n",
            "Loss :  0.08307448300448331\n",
            "Loss :  0.08318474995238441\n",
            "Loss :  0.08408753670031024\n",
            "Loss :  0.08516337359096945\n",
            "Loss :  0.08555452014301337\n",
            "Loss :  0.08533364851943782\n",
            "Loss :  0.08564977494763656\n",
            "Loss :  0.08575439775063667\n",
            "Loss :  0.0855621554694333\n",
            "Loss :  0.08520646135110667\n",
            "Loss :  0.08474774846622536\n",
            "Loss :  0.08442901462809113\n",
            "Loss :  0.0850257849079052\n",
            "Loss :  0.0851150837244717\n",
            "Loss :  0.08539066054173652\n",
            "Loss :  0.08559086789255557\n",
            "Loss :  0.08571281458376444\n",
            "Loss :  0.08574856126176718\n",
            "Loss :  0.08565944522931314\n",
            "Loss :  0.08575755669109857\n",
            "Loss :  0.0858967904160373\n",
            "Loss :  0.08594038570089038\n",
            "Loss :  0.08613518567441346\n",
            "Loss :  0.08617889998612067\n",
            "Loss :  0.0861472951107291\n",
            "Loss :  0.08621491734438015\n",
            "Loss :  0.08613820371262702\n",
            "Loss :  0.08612358996982676\n",
            "Loss :  0.08619180536761727\n",
            "Loss :  0.08627747345802396\n",
            "Loss :  0.08620027629987986\n",
            "Loss :  0.08614853479409143\n",
            "Loss :  0.08618591361175491\n",
            "Loss :  0.0862167696755303\n",
            "Loss :  0.08635260776067391\n",
            "Loss :  0.08634762765215374\n",
            "Loss :  0.08634827237807194\n",
            "Loss :  0.08628551941609446\n",
            "Loss :  0.08620059703622023\n",
            "Loss :  0.0861244111732949\n",
            "Loss :  0.08617936832481347\n",
            "Loss :  0.08620051535249039\n",
            "Loss :  0.08604173848012207\n",
            "Loss :  0.08602332969168687\n",
            "Loss :  0.08603264665656501\n",
            "Loss :  0.08603160466328101\n",
            "Loss :  0.08599803527132974\n",
            "Loss :  0.08604417487266405\n",
            "Loss :  0.08595607123221982\n",
            "Validation: \n",
            " Loss :  0.08526955544948578\n",
            " Loss :  0.0867279622526396\n",
            " Loss :  0.0864815304918987\n",
            " Loss :  0.08541325736241262\n",
            " Loss :  0.08580832615678693\n",
            "\n",
            "Epoch: 20\n",
            "Loss :  0.09164559841156006\n",
            "Loss :  0.08422259783202951\n",
            "Loss :  0.0844541419120062\n",
            "Loss :  0.0849248291023316\n",
            "Loss :  0.08464042587978084\n",
            "Loss :  0.08435756625498042\n",
            "Loss :  0.08421914572598505\n",
            "Loss :  0.08399781301407747\n",
            "Loss :  0.08412259725140936\n",
            "Loss :  0.08417897728773263\n",
            "Loss :  0.08389661129158323\n",
            "Loss :  0.08357103457590481\n",
            "Loss :  0.08349114933536073\n",
            "Loss :  0.08394641986557545\n",
            "Loss :  0.08383702132718783\n",
            "Loss :  0.08383160010473617\n",
            "Loss :  0.08400818733324916\n",
            "Loss :  0.08420394452517493\n",
            "Loss :  0.08435618543987117\n",
            "Loss :  0.08424234909068852\n",
            "Loss :  0.08407326865552077\n",
            "Loss :  0.08412664447209281\n",
            "Loss :  0.08415396761031173\n",
            "Loss :  0.08440797211545886\n",
            "Loss :  0.08454722274264855\n",
            "Loss :  0.0844014036762287\n",
            "Loss :  0.0843668537521271\n",
            "Loss :  0.0842446797008444\n",
            "Loss :  0.0842696285120533\n",
            "Loss :  0.08445555081789437\n",
            "Loss :  0.08439188878797614\n",
            "Loss :  0.08436537232142169\n",
            "Loss :  0.08436920036593702\n",
            "Loss :  0.08449414275654132\n",
            "Loss :  0.08459251186103066\n",
            "Loss :  0.08476196736776591\n",
            "Loss :  0.08473159044650783\n",
            "Loss :  0.08464480625249626\n",
            "Loss :  0.08461692712203724\n",
            "Loss :  0.08455894502532452\n",
            "Loss :  0.08454061468640468\n",
            "Loss :  0.08454867218532701\n",
            "Loss :  0.0845451579913674\n",
            "Loss :  0.08446374613350893\n",
            "Loss :  0.08451219494380648\n",
            "Loss :  0.08445570890496416\n",
            "Loss :  0.08439941510331864\n",
            "Loss :  0.08447021709442645\n",
            "Loss :  0.08446889541067354\n",
            "Loss :  0.084400371141929\n",
            "Validation: \n",
            " Loss :  0.08863324671983719\n",
            " Loss :  0.09235123580410368\n",
            " Loss :  0.0925036403464108\n",
            " Loss :  0.09163288735463972\n",
            " Loss :  0.09214266132057448\n",
            "\n",
            "Epoch: 21\n",
            "Loss :  0.08409183472394943\n",
            "Loss :  0.08177218247543681\n",
            "Loss :  0.08128265539805095\n",
            "Loss :  0.08263157812818404\n",
            "Loss :  0.08325800208783732\n",
            "Loss :  0.08271752750756693\n",
            "Loss :  0.08278821921739422\n",
            "Loss :  0.0828063945535203\n",
            "Loss :  0.08313064810670452\n",
            "Loss :  0.08272481603281838\n",
            "Loss :  0.08255010902291478\n",
            "Loss :  0.08224999931481508\n",
            "Loss :  0.08195336760321925\n",
            "Loss :  0.08245701740943749\n",
            "Loss :  0.0825843907205771\n",
            "Loss :  0.08286435482715139\n",
            "Loss :  0.08297857484832313\n",
            "Loss :  0.08316500049236922\n",
            "Loss :  0.08326777358904727\n",
            "Loss :  0.08313113271565961\n",
            "Loss :  0.08323253583700503\n",
            "Loss :  0.08336601593483116\n",
            "Loss :  0.08337038907125525\n",
            "Loss :  0.08361872914549592\n",
            "Loss :  0.08365254765724245\n",
            "Loss :  0.08353653977591678\n",
            "Loss :  0.08350752776495798\n",
            "Loss :  0.08339560914413516\n",
            "Loss :  0.08334910294233268\n",
            "Loss :  0.08344883182409293\n",
            "Loss :  0.08348141717059272\n",
            "Loss :  0.08338538993305715\n",
            "Loss :  0.08337637048644068\n",
            "Loss :  0.08341215523888337\n",
            "Loss :  0.08350893900978251\n",
            "Loss :  0.08365268502225223\n",
            "Loss :  0.08362476917762836\n",
            "Loss :  0.08355559117469505\n",
            "Loss :  0.08347064485465448\n",
            "Loss :  0.08342344060425869\n",
            "Loss :  0.08337346592597533\n",
            "Loss :  0.0833942148553484\n",
            "Loss :  0.0833995970864194\n",
            "Loss :  0.08326224498710057\n",
            "Loss :  0.08325270943495693\n",
            "Loss :  0.08327090138870967\n",
            "Loss :  0.08323401589261735\n",
            "Loss :  0.08323006760605835\n",
            "Loss :  0.08328668937365875\n",
            "Loss :  0.08318100344624879\n",
            "Validation: \n",
            " Loss :  0.08460782468318939\n",
            " Loss :  0.08787946651379268\n",
            " Loss :  0.0868438382337733\n",
            " Loss :  0.08627789638570098\n",
            " Loss :  0.08656421036999902\n",
            "\n",
            "Epoch: 22\n",
            "Loss :  0.0891968160867691\n",
            "Loss :  0.08325265077027408\n",
            "Loss :  0.08202553966215678\n",
            "Loss :  0.08155435684227175\n",
            "Loss :  0.08097003291292888\n",
            "Loss :  0.08103123876978369\n",
            "Loss :  0.08062809644663921\n",
            "Loss :  0.08070691517541106\n",
            "Loss :  0.08035994468279826\n",
            "Loss :  0.08024732703036004\n",
            "Loss :  0.0797878687482069\n",
            "Loss :  0.0796239174701072\n",
            "Loss :  0.0794066307096442\n",
            "Loss :  0.07980201939362606\n",
            "Loss :  0.07992671979657302\n",
            "Loss :  0.08027726959511144\n",
            "Loss :  0.08057224537645068\n",
            "Loss :  0.08074600070889233\n",
            "Loss :  0.08097416870501818\n",
            "Loss :  0.08086093156281567\n",
            "Loss :  0.08090394286818765\n",
            "Loss :  0.08091302060685451\n",
            "Loss :  0.08106844794696273\n",
            "Loss :  0.08141956781541114\n",
            "Loss :  0.08146241186689045\n",
            "Loss :  0.08130601277033171\n",
            "Loss :  0.08127419073919684\n",
            "Loss :  0.08123170007088967\n",
            "Loss :  0.0813270445077869\n",
            "Loss :  0.0814178162526429\n",
            "Loss :  0.08150584072468685\n",
            "Loss :  0.08150562579321324\n",
            "Loss :  0.08144506942074618\n",
            "Loss :  0.08153532994657844\n",
            "Loss :  0.08157712203666262\n",
            "Loss :  0.08167997435626821\n",
            "Loss :  0.08161187607413184\n",
            "Loss :  0.08156380109549212\n",
            "Loss :  0.0815295374260487\n",
            "Loss :  0.08144444204352395\n",
            "Loss :  0.08151732587680555\n",
            "Loss :  0.08162639830306789\n",
            "Loss :  0.08168861924752487\n",
            "Loss :  0.08157325760740139\n",
            "Loss :  0.08161394196708187\n",
            "Loss :  0.08161157364657608\n",
            "Loss :  0.0815195909776036\n",
            "Loss :  0.08158508601208908\n",
            "Loss :  0.08161426882362167\n",
            "Loss :  0.08154644521215054\n",
            "Validation: \n",
            " Loss :  0.08013299852609634\n",
            " Loss :  0.08723082961071105\n",
            " Loss :  0.08698170141475957\n",
            " Loss :  0.08625415297316723\n",
            " Loss :  0.08660691792582288\n",
            "\n",
            "Epoch: 23\n",
            "Loss :  0.08974447101354599\n",
            "Loss :  0.07956567677584561\n",
            "Loss :  0.07843526239906039\n",
            "Loss :  0.07980941524428706\n",
            "Loss :  0.08004729704159062\n",
            "Loss :  0.08030186681186452\n",
            "Loss :  0.07949420431109726\n",
            "Loss :  0.07938404097943239\n",
            "Loss :  0.07953670004635681\n",
            "Loss :  0.07947396405123092\n",
            "Loss :  0.07912211968462066\n",
            "Loss :  0.07879641430603491\n",
            "Loss :  0.07852531070551597\n",
            "Loss :  0.0788868413740442\n",
            "Loss :  0.07897084486399981\n",
            "Loss :  0.07925001674929992\n",
            "Loss :  0.07944450064660599\n",
            "Loss :  0.0798069900500844\n",
            "Loss :  0.07983478158712387\n",
            "Loss :  0.07980583210266073\n",
            "Loss :  0.0797968287002388\n",
            "Loss :  0.0799212282373442\n",
            "Loss :  0.08009457652250566\n",
            "Loss :  0.08034923256604702\n",
            "Loss :  0.08037107617157624\n",
            "Loss :  0.08034252621738085\n",
            "Loss :  0.08029597923445062\n",
            "Loss :  0.08020273527316062\n",
            "Loss :  0.08034479000284155\n",
            "Loss :  0.08041988830386158\n",
            "Loss :  0.08042869862923986\n",
            "Loss :  0.08039323726842641\n",
            "Loss :  0.08039360449321545\n",
            "Loss :  0.08048693243740548\n",
            "Loss :  0.08052213085798923\n",
            "Loss :  0.08065154087169897\n",
            "Loss :  0.08067298152192477\n",
            "Loss :  0.08063938743823944\n",
            "Loss :  0.08060053201133185\n",
            "Loss :  0.08052656893878032\n",
            "Loss :  0.08049764527830103\n",
            "Loss :  0.08052365515390161\n",
            "Loss :  0.08056448008024494\n",
            "Loss :  0.08043265625350436\n",
            "Loss :  0.08050569509263752\n",
            "Loss :  0.0805213317985413\n",
            "Loss :  0.08048768571846655\n",
            "Loss :  0.08042986011587384\n",
            "Loss :  0.0804586164467424\n",
            "Loss :  0.08039259439421526\n",
            "Validation: \n",
            " Loss :  0.0762125626206398\n",
            " Loss :  0.07926873720827557\n",
            " Loss :  0.07891941615721075\n",
            " Loss :  0.07833290210024255\n",
            " Loss :  0.07868301344138605\n",
            "\n",
            "Epoch: 24\n",
            "Loss :  0.08455126732587814\n",
            "Loss :  0.0788967643271793\n",
            "Loss :  0.07724528937112718\n",
            "Loss :  0.07803626430611457\n",
            "Loss :  0.0779679297673993\n",
            "Loss :  0.07820281196458667\n",
            "Loss :  0.07813157397704046\n",
            "Loss :  0.0780326077635859\n",
            "Loss :  0.07808296180065767\n",
            "Loss :  0.07795350286331805\n",
            "Loss :  0.07780831203897401\n",
            "Loss :  0.07754117740435643\n",
            "Loss :  0.07759376048795448\n",
            "Loss :  0.07791491347642346\n",
            "Loss :  0.07789868422856568\n",
            "Loss :  0.07819785187575991\n",
            "Loss :  0.07813905137851372\n",
            "Loss :  0.07843926779882253\n",
            "Loss :  0.07852879022531088\n",
            "Loss :  0.07854980366860384\n",
            "Loss :  0.07861878682131791\n",
            "Loss :  0.0787080863217042\n",
            "Loss :  0.07872490588221615\n",
            "Loss :  0.0789219963795695\n",
            "Loss :  0.07897612709103778\n",
            "Loss :  0.07890559991161186\n",
            "Loss :  0.07885699022661223\n",
            "Loss :  0.07873417225381105\n",
            "Loss :  0.07871796726332016\n",
            "Loss :  0.07884093906563991\n",
            "Loss :  0.07878784278401504\n",
            "Loss :  0.07878295116581717\n",
            "Loss :  0.07883921541807436\n",
            "Loss :  0.07888951370903373\n",
            "Loss :  0.07896926688833321\n",
            "Loss :  0.07906499137820681\n",
            "Loss :  0.07900203289747898\n",
            "Loss :  0.07898684095018957\n",
            "Loss :  0.0789591021777138\n",
            "Loss :  0.07889533418295024\n",
            "Loss :  0.07891630009745719\n",
            "Loss :  0.07891874405081835\n",
            "Loss :  0.07890735957727296\n",
            "Loss :  0.07878200243078086\n",
            "Loss :  0.07876536854946153\n",
            "Loss :  0.07878513019655867\n",
            "Loss :  0.07870575984426796\n",
            "Loss :  0.07873001980996688\n",
            "Loss :  0.07879166376318109\n",
            "Loss :  0.07868036069355283\n",
            "Validation: \n",
            " Loss :  0.08166925609111786\n",
            " Loss :  0.08407757466747648\n",
            " Loss :  0.08339841173189443\n",
            " Loss :  0.08265969987775458\n",
            " Loss :  0.08311525474727889\n",
            "\n",
            "Epoch: 25\n",
            "Loss :  0.08559553325176239\n",
            "Loss :  0.07578678564591841\n",
            "Loss :  0.07595315362725939\n",
            "Loss :  0.07643855218925784\n",
            "Loss :  0.07709665451107955\n",
            "Loss :  0.07715697279747795\n",
            "Loss :  0.07707599355060546\n",
            "Loss :  0.0773838576926312\n",
            "Loss :  0.0774732768351649\n",
            "Loss :  0.07744308945896862\n",
            "Loss :  0.07720459599306087\n",
            "Loss :  0.07691961667827658\n",
            "Loss :  0.07659765262125938\n",
            "Loss :  0.07685621580436029\n",
            "Loss :  0.07701701921879822\n",
            "Loss :  0.0771813764428066\n",
            "Loss :  0.07723175977235255\n",
            "Loss :  0.07750213632022428\n",
            "Loss :  0.07762642589639564\n",
            "Loss :  0.07759333793951578\n",
            "Loss :  0.0774590603599501\n",
            "Loss :  0.07759397173238591\n",
            "Loss :  0.07757522037665768\n",
            "Loss :  0.07776905360805007\n",
            "Loss :  0.07778012879285574\n",
            "Loss :  0.07776477101789528\n",
            "Loss :  0.07776205786914205\n",
            "Loss :  0.07764985569068866\n",
            "Loss :  0.07766285689806175\n",
            "Loss :  0.07774258883753184\n",
            "Loss :  0.07783362633267114\n",
            "Loss :  0.07789009737623466\n",
            "Loss :  0.07788983572309262\n",
            "Loss :  0.07792568812287466\n",
            "Loss :  0.07801675925419128\n",
            "Loss :  0.07817446518997181\n",
            "Loss :  0.07816487755058875\n",
            "Loss :  0.0781244698239144\n",
            "Loss :  0.07809606403738183\n",
            "Loss :  0.07802494290425345\n",
            "Loss :  0.07801756759794276\n",
            "Loss :  0.07796351585328724\n",
            "Loss :  0.0780374631728101\n",
            "Loss :  0.07800805344831915\n",
            "Loss :  0.07805675549470649\n",
            "Loss :  0.078071652364374\n",
            "Loss :  0.07797033864188609\n",
            "Loss :  0.0780120813020855\n",
            "Loss :  0.07805959964771281\n",
            "Loss :  0.07797260246336339\n",
            "Validation: \n",
            " Loss :  0.08386821299791336\n",
            " Loss :  0.08596976740019661\n",
            " Loss :  0.08505835856606321\n",
            " Loss :  0.08449612948738161\n",
            " Loss :  0.08500881014782706\n",
            "\n",
            "Epoch: 26\n",
            "Loss :  0.07582227140665054\n",
            "Loss :  0.07399629124186256\n",
            "Loss :  0.0742921130288215\n",
            "Loss :  0.07547304779291153\n",
            "Loss :  0.07611007043501226\n",
            "Loss :  0.07603286133677352\n",
            "Loss :  0.07583877248842208\n",
            "Loss :  0.0755783676786322\n",
            "Loss :  0.07570796390926396\n",
            "Loss :  0.07560150980294406\n",
            "Loss :  0.07528105569948064\n",
            "Loss :  0.07500982687279985\n",
            "Loss :  0.07497336506104667\n",
            "Loss :  0.07508598876590947\n",
            "Loss :  0.07516044853849614\n",
            "Loss :  0.07532184901616432\n",
            "Loss :  0.07547792815458701\n",
            "Loss :  0.07586992871865891\n",
            "Loss :  0.07593650350254544\n",
            "Loss :  0.07576264729674574\n",
            "Loss :  0.07584222928801579\n",
            "Loss :  0.07593841568271131\n",
            "Loss :  0.07600090079582654\n",
            "Loss :  0.07631782945487406\n",
            "Loss :  0.07646018721132358\n",
            "Loss :  0.0764924688524459\n",
            "Loss :  0.07643067953801247\n",
            "Loss :  0.07643828249615497\n",
            "Loss :  0.07645481633970322\n",
            "Loss :  0.07652833983558151\n",
            "Loss :  0.0765808227400843\n",
            "Loss :  0.07660003880883337\n",
            "Loss :  0.07659487270770414\n",
            "Loss :  0.07661323411407067\n",
            "Loss :  0.07670405111617007\n",
            "Loss :  0.07682967236918262\n",
            "Loss :  0.07676795892768289\n",
            "Loss :  0.0767355287532922\n",
            "Loss :  0.07669251937213845\n",
            "Loss :  0.07663905816843443\n",
            "Loss :  0.0766329388145794\n",
            "Loss :  0.07675767706258453\n",
            "Loss :  0.0767673961906705\n",
            "Loss :  0.0767215812392965\n",
            "Loss :  0.07674557036485802\n",
            "Loss :  0.07678254697050593\n",
            "Loss :  0.07671682659431549\n",
            "Loss :  0.0767554020204615\n",
            "Loss :  0.07684824866651249\n",
            "Loss :  0.07679703323333667\n",
            "Validation: \n",
            " Loss :  0.08294347673654556\n",
            " Loss :  0.08245222980067843\n",
            " Loss :  0.08233491985536204\n",
            " Loss :  0.08161912174498448\n",
            " Loss :  0.08167994307515061\n",
            "\n",
            "Epoch: 27\n",
            "Loss :  0.09296771883964539\n",
            "Loss :  0.07718444480137392\n",
            "Loss :  0.07580847293138504\n",
            "Loss :  0.07591409620738798\n",
            "Loss :  0.07631994402263223\n",
            "Loss :  0.07625163448791877\n",
            "Loss :  0.07600314463259744\n",
            "Loss :  0.07598577534228983\n",
            "Loss :  0.0760428473169421\n",
            "Loss :  0.07590765113031471\n",
            "Loss :  0.07566887582882796\n",
            "Loss :  0.07530116061638067\n",
            "Loss :  0.07520087007894989\n",
            "Loss :  0.07555294594236912\n",
            "Loss :  0.07557666079795107\n",
            "Loss :  0.07574701013154542\n",
            "Loss :  0.07586694124692715\n",
            "Loss :  0.0760656554709401\n",
            "Loss :  0.07609929971767394\n",
            "Loss :  0.07603735465029772\n",
            "Loss :  0.0759624342494343\n",
            "Loss :  0.0760270957945365\n",
            "Loss :  0.07612727860587215\n",
            "Loss :  0.07634845577599682\n",
            "Loss :  0.07635697597228146\n",
            "Loss :  0.07627842315580266\n",
            "Loss :  0.07622763555910853\n",
            "Loss :  0.07612665408596782\n",
            "Loss :  0.07621977344367428\n",
            "Loss :  0.0763290855406281\n",
            "Loss :  0.07636897722599514\n",
            "Loss :  0.07623374311199525\n",
            "Loss :  0.07616704335650923\n",
            "Loss :  0.07618574939915781\n",
            "Loss :  0.07618667220265285\n",
            "Loss :  0.07626003748670943\n",
            "Loss :  0.07623605857347848\n",
            "Loss :  0.07621662198211948\n",
            "Loss :  0.0761602898164997\n",
            "Loss :  0.07606470659184639\n",
            "Loss :  0.07606308440912395\n",
            "Loss :  0.07612624994667197\n",
            "Loss :  0.0761539079868312\n",
            "Loss :  0.07609088015321236\n",
            "Loss :  0.07611124508099762\n",
            "Loss :  0.07613356603090621\n",
            "Loss :  0.07606933484108484\n",
            "Loss :  0.07609043681634206\n",
            "Loss :  0.07614431724633844\n",
            "Loss :  0.07603749635111533\n",
            "Validation: \n",
            " Loss :  0.0827886164188385\n",
            " Loss :  0.0846114701458386\n",
            " Loss :  0.08446143549389956\n",
            " Loss :  0.08354986837652863\n",
            " Loss :  0.08396186642808678\n",
            "\n",
            "Epoch: 28\n",
            "Loss :  0.08918624371290207\n",
            "Loss :  0.07442126219922846\n",
            "Loss :  0.07346191292717344\n",
            "Loss :  0.07373554331641044\n",
            "Loss :  0.07430306949266573\n",
            "Loss :  0.07435522389178183\n",
            "Loss :  0.07431473521912685\n",
            "Loss :  0.07418223706559396\n",
            "Loss :  0.07383155404233638\n",
            "Loss :  0.07384609672558176\n",
            "Loss :  0.07364490971264273\n",
            "Loss :  0.07337771681649191\n",
            "Loss :  0.07315793538019677\n",
            "Loss :  0.07345808067053329\n",
            "Loss :  0.07341616928366058\n",
            "Loss :  0.07373657531493547\n",
            "Loss :  0.07380672912142291\n",
            "Loss :  0.07403978524588005\n",
            "Loss :  0.07404197808046367\n",
            "Loss :  0.07393114684217887\n",
            "Loss :  0.07386996287537452\n",
            "Loss :  0.0740194848045636\n",
            "Loss :  0.07415118242078776\n",
            "Loss :  0.07444925220820295\n",
            "Loss :  0.07454672630458947\n",
            "Loss :  0.07446580693363193\n",
            "Loss :  0.0745209133059814\n",
            "Loss :  0.07441273522717927\n",
            "Loss :  0.07444645844903705\n",
            "Loss :  0.07464301246906474\n",
            "Loss :  0.07479802413883796\n",
            "Loss :  0.07477889503457155\n",
            "Loss :  0.07482657181186096\n",
            "Loss :  0.07487034699752972\n",
            "Loss :  0.07495582224992363\n",
            "Loss :  0.07513965595184569\n",
            "Loss :  0.07515606945612754\n",
            "Loss :  0.07512443324907449\n",
            "Loss :  0.07512643779786866\n",
            "Loss :  0.07508675827432776\n",
            "Loss :  0.07507335659385916\n",
            "Loss :  0.07506859791068556\n",
            "Loss :  0.07509079231325351\n",
            "Loss :  0.07503756700113866\n",
            "Loss :  0.07503974712253157\n",
            "Loss :  0.07507455237457598\n",
            "Loss :  0.0750538233966605\n",
            "Loss :  0.07506087959413822\n",
            "Loss :  0.0751128395682437\n",
            "Loss :  0.07502348904636387\n",
            "Validation: \n",
            " Loss :  0.08130841702222824\n",
            " Loss :  0.08537928476220086\n",
            " Loss :  0.0847086506645854\n",
            " Loss :  0.08428862202362936\n",
            " Loss :  0.08463043636745876\n",
            "\n",
            "Epoch: 29\n",
            "Loss :  0.07977738976478577\n",
            "Loss :  0.07425885647535324\n",
            "Loss :  0.07337139546871185\n",
            "Loss :  0.07316236342153241\n",
            "Loss :  0.07308810985669857\n",
            "Loss :  0.07327099093327336\n",
            "Loss :  0.07328260024307204\n",
            "Loss :  0.07327273288663004\n",
            "Loss :  0.0732920859698896\n",
            "Loss :  0.07319033186841797\n",
            "Loss :  0.07317245972923714\n",
            "Loss :  0.07302481130705224\n",
            "Loss :  0.07272297670402803\n",
            "Loss :  0.07309515253846882\n",
            "Loss :  0.07310216797264756\n",
            "Loss :  0.0733441925354746\n",
            "Loss :  0.07347814417126015\n",
            "Loss :  0.07373634941483799\n",
            "Loss :  0.07385703787536911\n",
            "Loss :  0.0737611975774403\n",
            "Loss :  0.07368002641037923\n",
            "Loss :  0.07376650026977345\n",
            "Loss :  0.07391265117272533\n",
            "Loss :  0.07422320356036162\n",
            "Loss :  0.07433411619660765\n",
            "Loss :  0.07428072857488674\n",
            "Loss :  0.07425679407756904\n",
            "Loss :  0.07419135296421737\n",
            "Loss :  0.07412827167381596\n",
            "Loss :  0.07424385654916059\n",
            "Loss :  0.07427631812386734\n",
            "Loss :  0.07418015001527366\n",
            "Loss :  0.07415516854400205\n",
            "Loss :  0.07424686667917359\n",
            "Loss :  0.07430274335563707\n",
            "Loss :  0.07446813172636887\n",
            "Loss :  0.07443707237159446\n",
            "Loss :  0.07439612377324516\n",
            "Loss :  0.074353428415739\n",
            "Loss :  0.07434105397680836\n",
            "Loss :  0.07434797917778355\n",
            "Loss :  0.07443121428218491\n",
            "Loss :  0.07443756481849383\n",
            "Loss :  0.0743010186021953\n",
            "Loss :  0.07427072414363864\n",
            "Loss :  0.07426671664790145\n",
            "Loss :  0.07420786483104214\n",
            "Loss :  0.07423986496123032\n",
            "Loss :  0.07429383746241829\n",
            "Loss :  0.07418366222663227\n",
            "Validation: \n",
            " Loss :  0.08231557905673981\n",
            " Loss :  0.08371000062851679\n",
            " Loss :  0.08333573926512788\n",
            " Loss :  0.0828242521794116\n",
            " Loss :  0.08338530048911955\n",
            "\n",
            "Epoch: 30\n",
            "Loss :  0.07369528710842133\n",
            "Loss :  0.07094934718175368\n",
            "Loss :  0.07034516086181004\n",
            "Loss :  0.07170169127564277\n",
            "Loss :  0.07179012476671033\n",
            "Loss :  0.07164994583410375\n",
            "Loss :  0.07157112847341866\n",
            "Loss :  0.07155817196192876\n",
            "Loss :  0.07158804211167642\n",
            "Loss :  0.07175482477951836\n",
            "Loss :  0.0716040823055376\n",
            "Loss :  0.07150948705436948\n",
            "Loss :  0.07141247631843425\n",
            "Loss :  0.07174392459729245\n",
            "Loss :  0.07178551069599517\n",
            "Loss :  0.07195457298037232\n",
            "Loss :  0.07195394351985884\n",
            "Loss :  0.07222818949243479\n",
            "Loss :  0.0724268386047848\n",
            "Loss :  0.0722755220040913\n",
            "Loss :  0.07240739807634805\n",
            "Loss :  0.07255820866444665\n",
            "Loss :  0.07263090643542924\n",
            "Loss :  0.07280698989505892\n",
            "Loss :  0.07281500884791628\n",
            "Loss :  0.0727563879909031\n",
            "Loss :  0.07289897551995585\n",
            "Loss :  0.07292466097927182\n",
            "Loss :  0.07289453491144333\n",
            "Loss :  0.07296450829126991\n",
            "Loss :  0.0729788593774618\n",
            "Loss :  0.07298373131506695\n",
            "Loss :  0.07297668773158689\n",
            "Loss :  0.07310774846964732\n",
            "Loss :  0.07316502665450846\n",
            "Loss :  0.07328430959089868\n",
            "Loss :  0.07325377727025434\n",
            "Loss :  0.07323986756793573\n",
            "Loss :  0.07317301495064275\n",
            "Loss :  0.07314479632107802\n",
            "Loss :  0.0731476254463939\n",
            "Loss :  0.07324729708460706\n",
            "Loss :  0.07324435052211947\n",
            "Loss :  0.07311207948040796\n",
            "Loss :  0.07311385924043028\n",
            "Loss :  0.07308077549524687\n",
            "Loss :  0.07300796106037503\n",
            "Loss :  0.07304101415184147\n",
            "Loss :  0.07309807265436823\n",
            "Loss :  0.07305869179381372\n",
            "Validation: \n",
            " Loss :  0.08340305835008621\n",
            " Loss :  0.08424407527560279\n",
            " Loss :  0.08425937611155393\n",
            " Loss :  0.083634866187807\n",
            " Loss :  0.08403245249280224\n",
            "\n",
            "Epoch: 31\n",
            "Loss :  0.07915142178535461\n",
            "Loss :  0.07103596830909903\n",
            "Loss :  0.07085686017360006\n",
            "Loss :  0.071379738349107\n",
            "Loss :  0.0719851392616586\n",
            "Loss :  0.0719462178063159\n",
            "Loss :  0.07202513778551681\n",
            "Loss :  0.07215698918616267\n",
            "Loss :  0.07215331398226597\n",
            "Loss :  0.07221563811321835\n",
            "Loss :  0.0720421428432559\n",
            "Loss :  0.07170239768855206\n",
            "Loss :  0.07155494887597305\n",
            "Loss :  0.0718355784548148\n",
            "Loss :  0.07198951837230236\n",
            "Loss :  0.07214242688670064\n",
            "Loss :  0.07242289791751352\n",
            "Loss :  0.07262325661572797\n",
            "Loss :  0.07265161932831969\n",
            "Loss :  0.07261863046603677\n",
            "Loss :  0.07266027382134799\n",
            "Loss :  0.07279125920616054\n",
            "Loss :  0.07288741467023327\n",
            "Loss :  0.07303295360305609\n",
            "Loss :  0.07296471268371428\n",
            "Loss :  0.0729167517497245\n",
            "Loss :  0.07291675557407383\n",
            "Loss :  0.07283279730000179\n",
            "Loss :  0.07292828784453487\n",
            "Loss :  0.07308850218493913\n",
            "Loss :  0.07313962652041667\n",
            "Loss :  0.07311961373428057\n",
            "Loss :  0.07313272296639618\n",
            "Loss :  0.07319008859487819\n",
            "Loss :  0.07325110343063682\n",
            "Loss :  0.07330474980239175\n",
            "Loss :  0.07324737045201898\n",
            "Loss :  0.07320966355683348\n",
            "Loss :  0.0731486585333435\n",
            "Loss :  0.07306142548656525\n",
            "Loss :  0.07307472183111302\n",
            "Loss :  0.07307269778601154\n",
            "Loss :  0.07309818128898138\n",
            "Loss :  0.0729808400789159\n",
            "Loss :  0.07299504197643998\n",
            "Loss :  0.07296797053314366\n",
            "Loss :  0.07290498426672952\n",
            "Loss :  0.07289656894696746\n",
            "Loss :  0.07291712406674195\n",
            "Loss :  0.07285658880127423\n",
            "Validation: \n",
            " Loss :  0.07603345066308975\n",
            " Loss :  0.07613119163683482\n",
            " Loss :  0.07581730350488569\n",
            " Loss :  0.07535257483603525\n",
            " Loss :  0.07562508719202912\n",
            "\n",
            "Epoch: 32\n",
            "Loss :  0.07528532296419144\n",
            "Loss :  0.06933298774740913\n",
            "Loss :  0.06890518856900078\n",
            "Loss :  0.0694461427628994\n",
            "Loss :  0.06977914346427452\n",
            "Loss :  0.06995540696616266\n",
            "Loss :  0.06986370157511508\n",
            "Loss :  0.06989495996648157\n",
            "Loss :  0.06992673266817022\n",
            "Loss :  0.06986028656035989\n",
            "Loss :  0.06966914952096373\n",
            "Loss :  0.06949579568059595\n",
            "Loss :  0.06925112763342779\n",
            "Loss :  0.0695944644851994\n",
            "Loss :  0.0697705218608075\n",
            "Loss :  0.07012210704928992\n",
            "Loss :  0.07016690262818928\n",
            "Loss :  0.070465347724177\n",
            "Loss :  0.07045176903104913\n",
            "Loss :  0.07048593670207792\n",
            "Loss :  0.07060561629373636\n",
            "Loss :  0.07089524867975316\n",
            "Loss :  0.07092565582955584\n",
            "Loss :  0.0711500993183939\n",
            "Loss :  0.07119790169398814\n",
            "Loss :  0.07115033705277272\n",
            "Loss :  0.07114174433937474\n",
            "Loss :  0.07108944035874082\n",
            "Loss :  0.07115077439470223\n",
            "Loss :  0.07131758042934425\n",
            "Loss :  0.0713164806242203\n",
            "Loss :  0.07130230990780512\n",
            "Loss :  0.07128590607661696\n",
            "Loss :  0.0713297656745709\n",
            "Loss :  0.07139796931897441\n",
            "Loss :  0.07142749456343828\n",
            "Loss :  0.07144032263937419\n",
            "Loss :  0.07143712545822894\n",
            "Loss :  0.07137351377507833\n",
            "Loss :  0.07130955626516391\n",
            "Loss :  0.07130510057148494\n",
            "Loss :  0.07132939582831088\n",
            "Loss :  0.07134197706316825\n",
            "Loss :  0.07126035290638422\n",
            "Loss :  0.07126782530424547\n",
            "Loss :  0.07124560026497376\n",
            "Loss :  0.07118165065532393\n",
            "Loss :  0.07118016312930994\n",
            "Loss :  0.07123289821890189\n",
            "Loss :  0.07116699274894905\n",
            "Validation: \n",
            " Loss :  0.08003101497888565\n",
            " Loss :  0.08833418360778264\n",
            " Loss :  0.08772366784694718\n",
            " Loss :  0.08715434164785948\n",
            " Loss :  0.08753168509330278\n",
            "\n",
            "Epoch: 33\n",
            "Loss :  0.07570164650678635\n",
            "Loss :  0.07092757421461018\n",
            "Loss :  0.06995029179822831\n",
            "Loss :  0.07026349128253999\n",
            "Loss :  0.06998745751817052\n",
            "Loss :  0.07044632564864907\n",
            "Loss :  0.07034038880565127\n",
            "Loss :  0.06991725304806737\n",
            "Loss :  0.06982998407365364\n",
            "Loss :  0.06969336956575677\n",
            "Loss :  0.06953792093266355\n",
            "Loss :  0.06948204241223163\n",
            "Loss :  0.06947806403656637\n",
            "Loss :  0.06997778494166963\n",
            "Loss :  0.07015912176658076\n",
            "Loss :  0.07041143436009521\n",
            "Loss :  0.07049029836465853\n",
            "Loss :  0.07079431909131027\n",
            "Loss :  0.0706836662116301\n",
            "Loss :  0.07061087150926365\n",
            "Loss :  0.0705938686556484\n",
            "Loss :  0.0707177160334248\n",
            "Loss :  0.07073798448656479\n",
            "Loss :  0.07094600516093241\n",
            "Loss :  0.07095277402163541\n",
            "Loss :  0.07085757597212297\n",
            "Loss :  0.07082653281161155\n",
            "Loss :  0.07080461204436872\n",
            "Loss :  0.07082144641632288\n",
            "Loss :  0.07092643376836662\n",
            "Loss :  0.07091278090231443\n",
            "Loss :  0.07093759901653915\n",
            "Loss :  0.07086780478139161\n",
            "Loss :  0.0709511736616087\n",
            "Loss :  0.07099977117086435\n",
            "Loss :  0.07112925713006248\n",
            "Loss :  0.07110493078084863\n",
            "Loss :  0.07111145665422283\n",
            "Loss :  0.07109552774373001\n",
            "Loss :  0.07104237716826027\n",
            "Loss :  0.07102972365674236\n",
            "Loss :  0.07103307570564196\n",
            "Loss :  0.07101581696058112\n",
            "Loss :  0.07095517779067331\n",
            "Loss :  0.07097813303744982\n",
            "Loss :  0.07100922311191549\n",
            "Loss :  0.07093974581865065\n",
            "Loss :  0.07100146411742121\n",
            "Loss :  0.07101825701378735\n",
            "Loss :  0.070947933304759\n",
            "Validation: \n",
            " Loss :  0.0797184631228447\n",
            " Loss :  0.08657772306885038\n",
            " Loss :  0.08572421913466803\n",
            " Loss :  0.08498552319456319\n",
            " Loss :  0.08533781582926526\n",
            "\n",
            "Epoch: 34\n",
            "Loss :  0.07571611553430557\n",
            "Loss :  0.06958352367986333\n",
            "Loss :  0.06840742627779643\n",
            "Loss :  0.06908859360602594\n",
            "Loss :  0.06918081804746534\n",
            "Loss :  0.06893786846422682\n",
            "Loss :  0.06840006793375875\n",
            "Loss :  0.06831704289980338\n",
            "Loss :  0.06871033009187674\n",
            "Loss :  0.06878297489423019\n",
            "Loss :  0.06869593144643425\n",
            "Loss :  0.0685623100883252\n",
            "Loss :  0.06846632131121376\n",
            "Loss :  0.06877668543171336\n",
            "Loss :  0.06911917279163997\n",
            "Loss :  0.06920780887864283\n",
            "Loss :  0.06951642041065678\n",
            "Loss :  0.06963429547715605\n",
            "Loss :  0.0696368805693658\n",
            "Loss :  0.06956829659211698\n",
            "Loss :  0.06957893966887127\n",
            "Loss :  0.06965517777049146\n",
            "Loss :  0.0698154925524649\n",
            "Loss :  0.07004223762330039\n",
            "Loss :  0.070213531943896\n",
            "Loss :  0.07005610708815169\n",
            "Loss :  0.0700380845402164\n",
            "Loss :  0.07004792169011387\n",
            "Loss :  0.070078508285441\n",
            "Loss :  0.07020530690954313\n",
            "Loss :  0.07023330738279114\n",
            "Loss :  0.07017507061506008\n",
            "Loss :  0.07016756745328041\n",
            "Loss :  0.07021080553981833\n",
            "Loss :  0.0702671695314894\n",
            "Loss :  0.07039496311095365\n",
            "Loss :  0.07038938610136014\n",
            "Loss :  0.0703569238257376\n",
            "Loss :  0.0703228437231751\n",
            "Loss :  0.07024965238998003\n",
            "Loss :  0.07022343713446448\n",
            "Loss :  0.07017571575160154\n",
            "Loss :  0.07013195100950516\n",
            "Loss :  0.07004434324092212\n",
            "Loss :  0.07008893868110888\n",
            "Loss :  0.07008354627669783\n",
            "Loss :  0.0699983624632343\n",
            "Loss :  0.07003337342584716\n",
            "Loss :  0.07004999146330133\n",
            "Loss :  0.06997280705970563\n",
            "Validation: \n",
            " Loss :  0.08283742517232895\n",
            " Loss :  0.0861883486310641\n",
            " Loss :  0.08550560856010855\n",
            " Loss :  0.08485474803897201\n",
            " Loss :  0.08509792350692513\n",
            "\n",
            "Epoch: 35\n",
            "Loss :  0.0743994489312172\n",
            "Loss :  0.0677421350370754\n",
            "Loss :  0.06818484053725288\n",
            "Loss :  0.0687807607314279\n",
            "Loss :  0.06873465774626267\n",
            "Loss :  0.068613544471708\n",
            "Loss :  0.06853698261204313\n",
            "Loss :  0.06832183456756699\n",
            "Loss :  0.06869161657897042\n",
            "Loss :  0.06864459841297223\n",
            "Loss :  0.06838901285635363\n",
            "Loss :  0.0679987952583008\n",
            "Loss :  0.06788113907225861\n",
            "Loss :  0.06816358682308488\n",
            "Loss :  0.0682948926556195\n",
            "Loss :  0.06844585818170712\n",
            "Loss :  0.06858872867519071\n",
            "Loss :  0.06884526762000301\n",
            "Loss :  0.06891014966187556\n",
            "Loss :  0.06886069833530181\n",
            "Loss :  0.06884243778550803\n",
            "Loss :  0.0689213956525258\n",
            "Loss :  0.06905937162796837\n",
            "Loss :  0.06932339909208285\n",
            "Loss :  0.06950606849369172\n",
            "Loss :  0.06949988316254786\n",
            "Loss :  0.06947445692458828\n",
            "Loss :  0.06940969466008383\n",
            "Loss :  0.06936889185546981\n",
            "Loss :  0.06953354729051442\n",
            "Loss :  0.06952325601217359\n",
            "Loss :  0.06953810242834199\n",
            "Loss :  0.06949669151709087\n",
            "Loss :  0.06951641033648365\n",
            "Loss :  0.06951140801359482\n",
            "Loss :  0.06955168292223558\n",
            "Loss :  0.06951093161865615\n",
            "Loss :  0.06951341679636037\n",
            "Loss :  0.06952023517897749\n",
            "Loss :  0.06948224384613964\n",
            "Loss :  0.0694833042522767\n",
            "Loss :  0.06948896716836016\n",
            "Loss :  0.06947700556305696\n",
            "Loss :  0.06935103854765196\n",
            "Loss :  0.06936895431709938\n",
            "Loss :  0.06937960842562356\n",
            "Loss :  0.06929373854282363\n",
            "Loss :  0.06930892310261473\n",
            "Loss :  0.06939335700256165\n",
            "Loss :  0.06932139460399535\n",
            "Validation: \n",
            " Loss :  0.08420433849096298\n",
            " Loss :  0.0877522627512614\n",
            " Loss :  0.08661668620458464\n",
            " Loss :  0.0858353259133511\n",
            " Loss :  0.0861503349410163\n",
            "\n",
            "Epoch: 36\n",
            "Loss :  0.07030883431434631\n",
            "Loss :  0.06869870288805528\n",
            "Loss :  0.06765028390856016\n",
            "Loss :  0.06789750305394973\n",
            "Loss :  0.06759278112795294\n",
            "Loss :  0.06733620429740232\n",
            "Loss :  0.0671997172910659\n",
            "Loss :  0.06727788314013414\n",
            "Loss :  0.06755536058802664\n",
            "Loss :  0.06771472374816517\n",
            "Loss :  0.06752933923265722\n",
            "Loss :  0.06720927535547866\n",
            "Loss :  0.06710856752701042\n",
            "Loss :  0.06743841385113374\n",
            "Loss :  0.06753951194861256\n",
            "Loss :  0.06775857508182526\n",
            "Loss :  0.06793868995231131\n",
            "Loss :  0.06822218976871312\n",
            "Loss :  0.06820192309456635\n",
            "Loss :  0.06808034129713843\n",
            "Loss :  0.06814225535116979\n",
            "Loss :  0.0681420999327542\n",
            "Loss :  0.06821998638602403\n",
            "Loss :  0.06839983534567799\n",
            "Loss :  0.06850736135330933\n",
            "Loss :  0.0683584532949079\n",
            "Loss :  0.06842113477516905\n",
            "Loss :  0.06834076709248044\n",
            "Loss :  0.06834415091844641\n",
            "Loss :  0.06847240894879263\n",
            "Loss :  0.06857746273922762\n",
            "Loss :  0.06856349520265481\n",
            "Loss :  0.0685387746636927\n",
            "Loss :  0.06857377294102104\n",
            "Loss :  0.06864807932826081\n",
            "Loss :  0.06874216578242785\n",
            "Loss :  0.06876014567230547\n",
            "Loss :  0.06863837963286436\n",
            "Loss :  0.06864306441168459\n",
            "Loss :  0.06863363412068323\n",
            "Loss :  0.06860904460610297\n",
            "Loss :  0.06871061201078178\n",
            "Loss :  0.06863556378421477\n",
            "Loss :  0.06858727028226078\n",
            "Loss :  0.06857710287752065\n",
            "Loss :  0.06861553749197602\n",
            "Loss :  0.0685514264422875\n",
            "Loss :  0.06854443512303308\n",
            "Loss :  0.06859102267423439\n",
            "Loss :  0.06857520093014673\n",
            "Validation: \n",
            " Loss :  0.08282167464494705\n",
            " Loss :  0.08433806186630613\n",
            " Loss :  0.08345920574374316\n",
            " Loss :  0.08287372513384116\n",
            " Loss :  0.0829470216492076\n",
            "\n",
            "Epoch: 37\n",
            "Loss :  0.06700814515352249\n",
            "Loss :  0.06630723652514545\n",
            "Loss :  0.06610686704516411\n",
            "Loss :  0.06728210816940954\n",
            "Loss :  0.06772443043386064\n",
            "Loss :  0.06763921188665371\n",
            "Loss :  0.06773099822343373\n",
            "Loss :  0.06746932224068843\n",
            "Loss :  0.06751538537166736\n",
            "Loss :  0.06755600750937567\n",
            "Loss :  0.06747511663649342\n",
            "Loss :  0.06731878929175772\n",
            "Loss :  0.06711708695804777\n",
            "Loss :  0.06741546694445245\n",
            "Loss :  0.06752017843173751\n",
            "Loss :  0.06780392720999308\n",
            "Loss :  0.06799088140822346\n",
            "Loss :  0.0681102986049931\n",
            "Loss :  0.0681315719957839\n",
            "Loss :  0.06803744762199711\n",
            "Loss :  0.06812276932137523\n",
            "Loss :  0.06825707609195845\n",
            "Loss :  0.06836007006162971\n",
            "Loss :  0.06851466611285746\n",
            "Loss :  0.06851401370157839\n",
            "Loss :  0.06844525162382904\n",
            "Loss :  0.06842367225451487\n",
            "Loss :  0.06835812412724723\n",
            "Loss :  0.06831282190315664\n",
            "Loss :  0.06836891139751856\n",
            "Loss :  0.06839759442398319\n",
            "Loss :  0.06834103177837621\n",
            "Loss :  0.06837032446376631\n",
            "Loss :  0.06839603582942955\n",
            "Loss :  0.06846569639103503\n",
            "Loss :  0.06854927895373089\n",
            "Loss :  0.06857235590382957\n",
            "Loss :  0.06853444636309886\n",
            "Loss :  0.0684803314115901\n",
            "Loss :  0.06840299359520378\n",
            "Loss :  0.0684044186555388\n",
            "Loss :  0.06843247029866906\n",
            "Loss :  0.06846267001752049\n",
            "Loss :  0.0684044181764679\n",
            "Loss :  0.06837979603617911\n",
            "Loss :  0.06837946724997392\n",
            "Loss :  0.06828891796235666\n",
            "Loss :  0.06831263000987897\n",
            "Loss :  0.06833241940350146\n",
            "Loss :  0.06829320333609756\n",
            "Validation: \n",
            " Loss :  0.07253504544496536\n",
            " Loss :  0.07981230673335847\n",
            " Loss :  0.07911324446521155\n",
            " Loss :  0.07872889007701249\n",
            " Loss :  0.07897996930060563\n",
            "\n",
            "Epoch: 38\n",
            "Loss :  0.08097332715988159\n",
            "Loss :  0.06649654392491687\n",
            "Loss :  0.06658672151111421\n",
            "Loss :  0.0668232170564513\n",
            "Loss :  0.06688620568048663\n",
            "Loss :  0.06743053361481312\n",
            "Loss :  0.06722554955326143\n",
            "Loss :  0.06691757857169904\n",
            "Loss :  0.06703071664144963\n",
            "Loss :  0.06712393249784197\n",
            "Loss :  0.06710971061988633\n",
            "Loss :  0.06666430280552255\n",
            "Loss :  0.0663921918011894\n",
            "Loss :  0.06668522388084244\n",
            "Loss :  0.06679001931391709\n",
            "Loss :  0.06692445968950031\n",
            "Loss :  0.06699879039426028\n",
            "Loss :  0.0672882957198815\n",
            "Loss :  0.06730688147205674\n",
            "Loss :  0.06721708231181374\n",
            "Loss :  0.06730257590018694\n",
            "Loss :  0.06729360297322273\n",
            "Loss :  0.0673311657327063\n",
            "Loss :  0.06760966310124376\n",
            "Loss :  0.06761256143203415\n",
            "Loss :  0.06746915968290838\n",
            "Loss :  0.06743587850382501\n",
            "Loss :  0.06740983152191578\n",
            "Loss :  0.06740878631105626\n",
            "Loss :  0.0675199305273823\n",
            "Loss :  0.0676034851946506\n",
            "Loss :  0.06763487611073773\n",
            "Loss :  0.0676286369213991\n",
            "Loss :  0.06764810896982239\n",
            "Loss :  0.06771858886452364\n",
            "Loss :  0.06778821637827447\n",
            "Loss :  0.06778307761635807\n",
            "Loss :  0.06773811561680547\n",
            "Loss :  0.0676931672854217\n",
            "Loss :  0.06760672545608352\n",
            "Loss :  0.06761784450223024\n",
            "Loss :  0.06762154503677884\n",
            "Loss :  0.06763687897075384\n",
            "Loss :  0.06752268810110414\n",
            "Loss :  0.06753165174334769\n",
            "Loss :  0.06754122494395715\n",
            "Loss :  0.06742172285817451\n",
            "Loss :  0.06741453389263456\n",
            "Loss :  0.06743189200696975\n",
            "Loss :  0.06738850211083525\n",
            "Validation: \n",
            " Loss :  0.08193344622850418\n",
            " Loss :  0.08933911472558975\n",
            " Loss :  0.08882156850361242\n",
            " Loss :  0.08851815393713654\n",
            " Loss :  0.08909708500644307\n",
            "\n",
            "Epoch: 39\n",
            "Loss :  0.07646527141332626\n",
            "Loss :  0.06866294992240993\n",
            "Loss :  0.06687559755075545\n",
            "Loss :  0.06683253184441597\n",
            "Loss :  0.06695035081811068\n",
            "Loss :  0.06686684956737593\n",
            "Loss :  0.06654992467555844\n",
            "Loss :  0.06657331468353808\n",
            "Loss :  0.06666516760985057\n",
            "Loss :  0.0666890595476706\n",
            "Loss :  0.06656504669549441\n",
            "Loss :  0.06618016169549108\n",
            "Loss :  0.06592463680412158\n",
            "Loss :  0.06617022191977683\n",
            "Loss :  0.06633719114969808\n",
            "Loss :  0.06654159626029185\n",
            "Loss :  0.06658044485201747\n",
            "Loss :  0.0667374782830651\n",
            "Loss :  0.06684461777299149\n",
            "Loss :  0.06681062930619529\n",
            "Loss :  0.06677603680844331\n",
            "Loss :  0.0667861034625797\n",
            "Loss :  0.06688383295794957\n",
            "Loss :  0.06709418813516567\n",
            "Loss :  0.06716263430247169\n",
            "Loss :  0.0670158741216498\n",
            "Loss :  0.06693729631231662\n",
            "Loss :  0.06677157521192878\n",
            "Loss :  0.0668261953069434\n",
            "Loss :  0.06685491327413988\n",
            "Loss :  0.06690331667265623\n",
            "Loss :  0.06692442246428257\n",
            "Loss :  0.06688062883797465\n",
            "Loss :  0.06686374874858697\n",
            "Loss :  0.0669216572845087\n",
            "Loss :  0.06701568103371522\n",
            "Loss :  0.06696925979943487\n",
            "Loss :  0.06695238977791164\n",
            "Loss :  0.0669239087719617\n",
            "Loss :  0.06683421210216745\n",
            "Loss :  0.0668193743171686\n",
            "Loss :  0.06685072385735466\n",
            "Loss :  0.0668456377439431\n",
            "Loss :  0.0668031309413661\n",
            "Loss :  0.06681729332795219\n",
            "Loss :  0.06681963527149742\n",
            "Loss :  0.06677814413856012\n",
            "Loss :  0.06682932943959904\n",
            "Loss :  0.06683489983115276\n",
            "Loss :  0.06682628385586069\n",
            "Validation: \n",
            " Loss :  0.07133565098047256\n",
            " Loss :  0.07645606657578832\n",
            " Loss :  0.0761252624414316\n",
            " Loss :  0.07587842234089727\n",
            " Loss :  0.07603433999566385\n",
            "\n",
            "Epoch: 40\n",
            "Loss :  0.07586167007684708\n",
            "Loss :  0.06579279188405383\n",
            "Loss :  0.06512586222518058\n",
            "Loss :  0.06538622213467475\n",
            "Loss :  0.06591615426104243\n",
            "Loss :  0.06587045296442275\n",
            "Loss :  0.06562401519202796\n",
            "Loss :  0.06549903063077323\n",
            "Loss :  0.06563819397562816\n",
            "Loss :  0.06556572617737802\n",
            "Loss :  0.06533969573602819\n",
            "Loss :  0.0651844972828487\n",
            "Loss :  0.0649397173696313\n",
            "Loss :  0.06517732333932214\n",
            "Loss :  0.06547824285448865\n",
            "Loss :  0.06554450785482166\n",
            "Loss :  0.06549274400802133\n",
            "Loss :  0.06566175162705065\n",
            "Loss :  0.0656544077338764\n",
            "Loss :  0.06557028736743628\n",
            "Loss :  0.06566477203695335\n",
            "Loss :  0.06578588206762385\n",
            "Loss :  0.06583965844495804\n",
            "Loss :  0.066085441755655\n",
            "Loss :  0.06617059373138356\n",
            "Loss :  0.06616397387537348\n",
            "Loss :  0.06624466728890079\n",
            "Loss :  0.06619995910798052\n",
            "Loss :  0.0662425975464417\n",
            "Loss :  0.06636451101538651\n",
            "Loss :  0.0664212540089094\n",
            "Loss :  0.0663391710573454\n",
            "Loss :  0.06627668831439404\n",
            "Loss :  0.06626789651113335\n",
            "Loss :  0.06633917194220328\n",
            "Loss :  0.06643302641023598\n",
            "Loss :  0.06640332254098723\n",
            "Loss :  0.0663199458080482\n",
            "Loss :  0.06630423149763756\n",
            "Loss :  0.06620919299514397\n",
            "Loss :  0.06619888805131663\n",
            "Loss :  0.06621184570764684\n",
            "Loss :  0.06620977322555495\n",
            "Loss :  0.06614178985253565\n",
            "Loss :  0.06614529525611947\n",
            "Loss :  0.06608343228466496\n",
            "Loss :  0.06602502236882156\n",
            "Loss :  0.06605939892162184\n",
            "Loss :  0.0660805246035671\n",
            "Loss :  0.06603953233197361\n",
            "Validation: \n",
            " Loss :  0.07271931320428848\n",
            " Loss :  0.07845862458149593\n",
            " Loss :  0.07820109114414309\n",
            " Loss :  0.07756709747138571\n",
            " Loss :  0.07796547322729487\n",
            "\n",
            "Epoch: 41\n",
            "Loss :  0.07041511684656143\n",
            "Loss :  0.06418561021035368\n",
            "Loss :  0.06411368932042803\n",
            "Loss :  0.06467021893589728\n",
            "Loss :  0.06469862453821229\n",
            "Loss :  0.06526286257248298\n",
            "Loss :  0.0650860065685921\n",
            "Loss :  0.06480877746788549\n",
            "Loss :  0.06494363852673107\n",
            "Loss :  0.06504718234742081\n",
            "Loss :  0.06471208619451758\n",
            "Loss :  0.06443839765212557\n",
            "Loss :  0.06428256578558733\n",
            "Loss :  0.0643585451402282\n",
            "Loss :  0.06454371988561981\n",
            "Loss :  0.06466476962167696\n",
            "Loss :  0.06467174923753147\n",
            "Loss :  0.06474870319167773\n",
            "Loss :  0.06485364674616255\n",
            "Loss :  0.06495984156094296\n",
            "Loss :  0.06499422308224351\n",
            "Loss :  0.06504739317778163\n",
            "Loss :  0.06504758516520397\n",
            "Loss :  0.0652988874131725\n",
            "Loss :  0.0653342829082022\n",
            "Loss :  0.0652890765512607\n",
            "Loss :  0.06523044124759476\n",
            "Loss :  0.06514002966979773\n",
            "Loss :  0.06517472969255414\n",
            "Loss :  0.06527412436979334\n",
            "Loss :  0.06531274112009527\n",
            "Loss :  0.06533254468603916\n",
            "Loss :  0.06530607650723784\n",
            "Loss :  0.06528802990373168\n",
            "Loss :  0.0654344867008173\n",
            "Loss :  0.06554650495152528\n",
            "Loss :  0.06559042150665519\n",
            "Loss :  0.065587012754939\n",
            "Loss :  0.06551538664955167\n",
            "Loss :  0.0654457834980372\n",
            "Loss :  0.06541314364371455\n",
            "Loss :  0.06544144244053358\n",
            "Loss :  0.065390023723634\n",
            "Loss :  0.06532293582322149\n",
            "Loss :  0.06530981300538088\n",
            "Loss :  0.06538532272873326\n",
            "Loss :  0.06539832125872438\n",
            "Loss :  0.06539628127957606\n",
            "Loss :  0.06544592392605704\n",
            "Loss :  0.0654154587423243\n",
            "Validation: \n",
            " Loss :  0.08064386248588562\n",
            " Loss :  0.09008865058422089\n",
            " Loss :  0.08870286094706233\n",
            " Loss :  0.08850441714290713\n",
            " Loss :  0.0883986563594253\n",
            "\n",
            "Epoch: 42\n",
            "Loss :  0.0686054602265358\n",
            "Loss :  0.06415171447125348\n",
            "Loss :  0.06394490688329652\n",
            "Loss :  0.06433098102288862\n",
            "Loss :  0.06431139451338024\n",
            "Loss :  0.06475944619844942\n",
            "Loss :  0.06468912922456617\n",
            "Loss :  0.06467218025469444\n",
            "Loss :  0.06472253256741865\n",
            "Loss :  0.06466864753555465\n",
            "Loss :  0.0647001530200538\n",
            "Loss :  0.0643245822808764\n",
            "Loss :  0.06419945455902865\n",
            "Loss :  0.06435636214855063\n",
            "Loss :  0.06455387537043991\n",
            "Loss :  0.0647986437508602\n",
            "Loss :  0.06489566840833018\n",
            "Loss :  0.06513658372892274\n",
            "Loss :  0.0651698949751933\n",
            "Loss :  0.06508804048309151\n",
            "Loss :  0.06508481167071495\n",
            "Loss :  0.06514100456760392\n",
            "Loss :  0.0651739791912191\n",
            "Loss :  0.06527975124198121\n",
            "Loss :  0.06539838482853783\n",
            "Loss :  0.06535725341553232\n",
            "Loss :  0.06532580085755307\n",
            "Loss :  0.06524864475610512\n",
            "Loss :  0.0652591953869392\n",
            "Loss :  0.06534572270830062\n",
            "Loss :  0.06535762663546987\n",
            "Loss :  0.06534374082443031\n",
            "Loss :  0.06531449540799653\n",
            "Loss :  0.06530640168492527\n",
            "Loss :  0.06534186068955056\n",
            "Loss :  0.06543076394969581\n",
            "Loss :  0.06538319346234409\n",
            "Loss :  0.06538670321278817\n",
            "Loss :  0.06540036295342633\n",
            "Loss :  0.065313903181373\n",
            "Loss :  0.06532850341309336\n",
            "Loss :  0.06532384919279104\n",
            "Loss :  0.0652655323971762\n",
            "Loss :  0.06518791685974239\n",
            "Loss :  0.06521136760542723\n",
            "Loss :  0.06524695198313889\n",
            "Loss :  0.06517425695042289\n",
            "Loss :  0.06517006422050946\n",
            "Loss :  0.06521946968599814\n",
            "Loss :  0.06519245223546222\n",
            "Validation: \n",
            " Loss :  0.08482123911380768\n",
            " Loss :  0.0883462056517601\n",
            " Loss :  0.08746573201766829\n",
            " Loss :  0.08685100811426757\n",
            " Loss :  0.08718054346096368\n",
            "\n",
            "Epoch: 43\n",
            "Loss :  0.06363862007856369\n",
            "Loss :  0.06483078002929688\n",
            "Loss :  0.06368013968070348\n",
            "Loss :  0.06384977626223717\n",
            "Loss :  0.06404853139708681\n",
            "Loss :  0.06391751839249742\n",
            "Loss :  0.06381639942038254\n",
            "Loss :  0.06368362310696656\n",
            "Loss :  0.06364835913718482\n",
            "Loss :  0.06377328928191583\n",
            "Loss :  0.06378011239489706\n",
            "Loss :  0.06341218814119562\n",
            "Loss :  0.06324031752003126\n",
            "Loss :  0.06353996563276262\n",
            "Loss :  0.06377896280787515\n",
            "Loss :  0.06396893920093183\n",
            "Loss :  0.06412440637901703\n",
            "Loss :  0.06430509045981524\n",
            "Loss :  0.06436725487695873\n",
            "Loss :  0.06417727979455942\n",
            "Loss :  0.0641993629880509\n",
            "Loss :  0.06419742525824439\n",
            "Loss :  0.06423034662237534\n",
            "Loss :  0.06444671417856629\n",
            "Loss :  0.06450772126247774\n",
            "Loss :  0.06449660587595754\n",
            "Loss :  0.06447375148991516\n",
            "Loss :  0.0644627037660882\n",
            "Loss :  0.06453626305925464\n",
            "Loss :  0.06465532168541167\n",
            "Loss :  0.06468700539580612\n",
            "Loss :  0.06464424608365132\n",
            "Loss :  0.06464526138480207\n",
            "Loss :  0.06465170342261338\n",
            "Loss :  0.06471543563557161\n",
            "Loss :  0.06473835845321332\n",
            "Loss :  0.06469396006450098\n",
            "Loss :  0.06467315216229932\n",
            "Loss :  0.06466793539760307\n",
            "Loss :  0.06456721906581193\n",
            "Loss :  0.06454658831592808\n",
            "Loss :  0.06453406194410764\n",
            "Loss :  0.06449296279432089\n",
            "Loss :  0.06442598315720768\n",
            "Loss :  0.06440982390659737\n",
            "Loss :  0.06440038319900136\n",
            "Loss :  0.0643132725354777\n",
            "Loss :  0.0643613840000518\n",
            "Loss :  0.06440014103253269\n",
            "Loss :  0.06433096266296634\n",
            "Validation: \n",
            " Loss :  0.07865402847528458\n",
            " Loss :  0.08120575405302502\n",
            " Loss :  0.08071929979615095\n",
            " Loss :  0.08041639613812088\n",
            " Loss :  0.08060559446429029\n",
            "\n",
            "Epoch: 44\n",
            "Loss :  0.06969815492630005\n",
            "Loss :  0.06217741695317355\n",
            "Loss :  0.06242312863469124\n",
            "Loss :  0.0629498505544278\n",
            "Loss :  0.06291723587527508\n",
            "Loss :  0.06298044404270602\n",
            "Loss :  0.06282213989828454\n",
            "Loss :  0.06284002513742783\n",
            "Loss :  0.06313824552444765\n",
            "Loss :  0.06336610608703487\n",
            "Loss :  0.06320546228106659\n",
            "Loss :  0.06314816859525603\n",
            "Loss :  0.06311653766873454\n",
            "Loss :  0.06344788669402363\n",
            "Loss :  0.06346746431069171\n",
            "Loss :  0.06355411697499799\n",
            "Loss :  0.063585763438518\n",
            "Loss :  0.06379482274254163\n",
            "Loss :  0.06379716442023194\n",
            "Loss :  0.06381781286126031\n",
            "Loss :  0.06384060357637074\n",
            "Loss :  0.06394842265270897\n",
            "Loss :  0.06398988494910805\n",
            "Loss :  0.06416730340812113\n",
            "Loss :  0.06423148903918464\n",
            "Loss :  0.06410558227880067\n",
            "Loss :  0.06408877720720924\n",
            "Loss :  0.06397127144286113\n",
            "Loss :  0.06394848940741549\n",
            "Loss :  0.06405046017667682\n",
            "Loss :  0.06403583653295951\n",
            "Loss :  0.06401741394467676\n",
            "Loss :  0.0639983344695464\n",
            "Loss :  0.06407047643852377\n",
            "Loss :  0.06416287140980843\n",
            "Loss :  0.06424194251709854\n",
            "Loss :  0.064260254389609\n",
            "Loss :  0.06425988027509653\n",
            "Loss :  0.06420839706000693\n",
            "Loss :  0.06418589033815257\n",
            "Loss :  0.06413365671053492\n",
            "Loss :  0.06416552339809654\n",
            "Loss :  0.06415800022894866\n",
            "Loss :  0.06410599181000705\n",
            "Loss :  0.06410624017588405\n",
            "Loss :  0.06410323151133278\n",
            "Loss :  0.06403335140222065\n",
            "Loss :  0.06402198702191851\n",
            "Loss :  0.06407798208652564\n",
            "Loss :  0.06404067253477705\n",
            "Validation: \n",
            " Loss :  0.08052976429462433\n",
            " Loss :  0.08063127597173055\n",
            " Loss :  0.08018687094857053\n",
            " Loss :  0.07995509624969764\n",
            " Loss :  0.08044543236861994\n",
            "\n",
            "Epoch: 45\n",
            "Loss :  0.06985964626073837\n",
            "Loss :  0.06284679912708023\n",
            "Loss :  0.06175516989259493\n",
            "Loss :  0.06180152681566054\n",
            "Loss :  0.06232056030776442\n",
            "Loss :  0.06253810847798984\n",
            "Loss :  0.06232600152248242\n",
            "Loss :  0.06204585416216246\n",
            "Loss :  0.06224629365735584\n",
            "Loss :  0.062296434745683776\n",
            "Loss :  0.06226776536590982\n",
            "Loss :  0.062109751039528635\n",
            "Loss :  0.06212860847677081\n",
            "Loss :  0.06253483385302638\n",
            "Loss :  0.06267150531106806\n",
            "Loss :  0.06282711371976808\n",
            "Loss :  0.06298920433528675\n",
            "Loss :  0.06317849866828026\n",
            "Loss :  0.06313035188458901\n",
            "Loss :  0.06308358090476215\n",
            "Loss :  0.06303028341623682\n",
            "Loss :  0.06311569110406519\n",
            "Loss :  0.06309954161287973\n",
            "Loss :  0.06321339734963008\n",
            "Loss :  0.06337673332990452\n",
            "Loss :  0.06343738394845054\n",
            "Loss :  0.06342689229096946\n",
            "Loss :  0.06333401287833702\n",
            "Loss :  0.06327902179648867\n",
            "Loss :  0.0634161594909491\n",
            "Loss :  0.06340539519770994\n",
            "Loss :  0.06336362741384476\n",
            "Loss :  0.06339981362827099\n",
            "Loss :  0.06339101973932315\n",
            "Loss :  0.06346062492153162\n",
            "Loss :  0.06350492891932485\n",
            "Loss :  0.06347515470144491\n",
            "Loss :  0.06342198389439249\n",
            "Loss :  0.0634223482329545\n",
            "Loss :  0.06334289271965661\n",
            "Loss :  0.06330553162610739\n",
            "Loss :  0.06332484782285934\n",
            "Loss :  0.06333772791217739\n",
            "Loss :  0.06324581183026395\n",
            "Loss :  0.0632671023594414\n",
            "Loss :  0.06324284027295472\n",
            "Loss :  0.06322879713521604\n",
            "Loss :  0.06323668183541349\n",
            "Loss :  0.06324983832741973\n",
            "Loss :  0.06317193003451023\n",
            "Validation: \n",
            " Loss :  0.0766502246260643\n",
            " Loss :  0.08255001263959068\n",
            " Loss :  0.0823402604678782\n",
            " Loss :  0.08193093489428036\n",
            " Loss :  0.08196891209593525\n",
            "\n",
            "Epoch: 46\n",
            "Loss :  0.07431937009096146\n",
            "Loss :  0.06218510967763988\n",
            "Loss :  0.06216879774417196\n",
            "Loss :  0.06222710217679701\n",
            "Loss :  0.0625811685330984\n",
            "Loss :  0.06264897231377807\n",
            "Loss :  0.06254304401942941\n",
            "Loss :  0.06224492890104442\n",
            "Loss :  0.062236902936373226\n",
            "Loss :  0.062234226696111346\n",
            "Loss :  0.06211624312961456\n",
            "Loss :  0.06181359911958376\n",
            "Loss :  0.0614927789458066\n",
            "Loss :  0.06178067567694278\n",
            "Loss :  0.06190079278874059\n",
            "Loss :  0.06205107034831647\n",
            "Loss :  0.062235859286340865\n",
            "Loss :  0.06241081413208393\n",
            "Loss :  0.06249259914482496\n",
            "Loss :  0.062346331170560175\n",
            "Loss :  0.062401397644880396\n",
            "Loss :  0.06264671137759471\n",
            "Loss :  0.06270256110917928\n",
            "Loss :  0.06295730638039576\n",
            "Loss :  0.06301913147645373\n",
            "Loss :  0.06299528991618004\n",
            "Loss :  0.06300751685068526\n",
            "Loss :  0.06298656926603775\n",
            "Loss :  0.06290356508618572\n",
            "Loss :  0.06300890978259319\n",
            "Loss :  0.0631152567178308\n",
            "Loss :  0.06310822551177629\n",
            "Loss :  0.06309572400705094\n",
            "Loss :  0.06311947293016845\n",
            "Loss :  0.06322960443030005\n",
            "Loss :  0.06337675069089968\n",
            "Loss :  0.06335364272843767\n",
            "Loss :  0.06332427554455086\n",
            "Loss :  0.06327562348970904\n",
            "Loss :  0.06318645067798817\n",
            "Loss :  0.063196761733353\n",
            "Loss :  0.06324989324177269\n",
            "Loss :  0.06326034205861726\n",
            "Loss :  0.06319952330912902\n",
            "Loss :  0.06319260678323758\n",
            "Loss :  0.06320501554203932\n",
            "Loss :  0.06314604990426734\n",
            "Loss :  0.0631257808448909\n",
            "Loss :  0.06316948926033696\n",
            "Loss :  0.06309929216193327\n",
            "Validation: \n",
            " Loss :  0.08147046715021133\n",
            " Loss :  0.08376903548127129\n",
            " Loss :  0.0832309595695356\n",
            " Loss :  0.08293139946753861\n",
            " Loss :  0.08303670539164248\n",
            "\n",
            "Epoch: 47\n",
            "Loss :  0.07280800491571426\n",
            "Loss :  0.06282267821106044\n",
            "Loss :  0.0613771735557488\n",
            "Loss :  0.062167494768096555\n",
            "Loss :  0.0622675041781693\n",
            "Loss :  0.06207308896324214\n",
            "Loss :  0.06188810549554278\n",
            "Loss :  0.061807702964460344\n",
            "Loss :  0.0621380703408777\n",
            "Loss :  0.06209465787633435\n",
            "Loss :  0.06216558275541457\n",
            "Loss :  0.062142746606925585\n",
            "Loss :  0.06184092841365121\n",
            "Loss :  0.062018976866743944\n",
            "Loss :  0.062215480213681014\n",
            "Loss :  0.06234141066670418\n",
            "Loss :  0.06244552850445605\n",
            "Loss :  0.06253120775895508\n",
            "Loss :  0.06254049481113971\n",
            "Loss :  0.06244724635912486\n",
            "Loss :  0.062450820014844484\n",
            "Loss :  0.06249786860428715\n",
            "Loss :  0.06259299915840183\n",
            "Loss :  0.06283433674088804\n",
            "Loss :  0.06286834699731644\n",
            "Loss :  0.06282765479142448\n",
            "Loss :  0.0627737210000155\n",
            "Loss :  0.0626314704571043\n",
            "Loss :  0.06258485950131858\n",
            "Loss :  0.06267228023600332\n",
            "Loss :  0.0626988724814697\n",
            "Loss :  0.06271477013682629\n",
            "Loss :  0.06271468642576833\n",
            "Loss :  0.06271035247834067\n",
            "Loss :  0.06279384275991197\n",
            "Loss :  0.06288224973442548\n",
            "Loss :  0.06287823103554031\n",
            "Loss :  0.0628316332548455\n",
            "Loss :  0.06274640827080397\n",
            "Loss :  0.06270149072913257\n",
            "Loss :  0.06268377429306358\n",
            "Loss :  0.06269936618868742\n",
            "Loss :  0.06267975583991076\n",
            "Loss :  0.06259664201681132\n",
            "Loss :  0.06258889460036544\n",
            "Loss :  0.06260543290170492\n",
            "Loss :  0.06251609195306607\n",
            "Loss :  0.062536917903684\n",
            "Loss :  0.06253126198561425\n",
            "Loss :  0.062485110045025646\n",
            "Validation: \n",
            " Loss :  0.07678256928920746\n",
            " Loss :  0.07946072022120158\n",
            " Loss :  0.0788998156785965\n",
            " Loss :  0.07888466009839637\n",
            " Loss :  0.07908000502689386\n",
            "\n",
            "Epoch: 48\n",
            "Loss :  0.07267727702856064\n",
            "Loss :  0.06216052987358787\n",
            "Loss :  0.06099817121312732\n",
            "Loss :  0.06201472337688169\n",
            "Loss :  0.061823384427442785\n",
            "Loss :  0.06229493547888363\n",
            "Loss :  0.061891959033540035\n",
            "Loss :  0.061761344507546494\n",
            "Loss :  0.0617034318914384\n",
            "Loss :  0.06181049829983449\n",
            "Loss :  0.0618226356287994\n",
            "Loss :  0.061596965299801784\n",
            "Loss :  0.06151809527977439\n",
            "Loss :  0.061738584901540335\n",
            "Loss :  0.06189244414897675\n",
            "Loss :  0.06194618556475797\n",
            "Loss :  0.062027247356516976\n",
            "Loss :  0.06223696971323058\n",
            "Loss :  0.06226886942653366\n",
            "Loss :  0.062194312059598446\n",
            "Loss :  0.062254955704828994\n",
            "Loss :  0.06239367144014598\n",
            "Loss :  0.06253567895940526\n",
            "Loss :  0.06275897939161305\n",
            "Loss :  0.06274876646171962\n",
            "Loss :  0.06268023755920836\n",
            "Loss :  0.06263058813881144\n",
            "Loss :  0.06259882887241146\n",
            "Loss :  0.0625589854511502\n",
            "Loss :  0.06255735637447268\n",
            "Loss :  0.06251430056023835\n",
            "Loss :  0.062490939327374914\n",
            "Loss :  0.062478666933618976\n",
            "Loss :  0.06242396015099889\n",
            "Loss :  0.06251564837140072\n",
            "Loss :  0.06265039607203245\n",
            "Loss :  0.06263834787042517\n",
            "Loss :  0.06256442092298818\n",
            "Loss :  0.06249491569251213\n",
            "Loss :  0.06243489605500875\n",
            "Loss :  0.06240933335518599\n",
            "Loss :  0.062410030332722514\n",
            "Loss :  0.0624139731984382\n",
            "Loss :  0.06235357642761516\n",
            "Loss :  0.062344413723737474\n",
            "Loss :  0.062369197904402825\n",
            "Loss :  0.06228370241287474\n",
            "Loss :  0.062321606042721724\n",
            "Loss :  0.06240259745233768\n",
            "Loss :  0.062336500969292434\n",
            "Validation: \n",
            " Loss :  0.07554326951503754\n",
            " Loss :  0.0831608594883056\n",
            " Loss :  0.08205293137125852\n",
            " Loss :  0.08184684263389619\n",
            " Loss :  0.08181488421964056\n",
            "\n",
            "Epoch: 49\n",
            "Loss :  0.06645891070365906\n",
            "Loss :  0.059276332570747894\n",
            "Loss :  0.05918520440657934\n",
            "Loss :  0.06045086261245512\n",
            "Loss :  0.06053031235933304\n",
            "Loss :  0.06070742383599281\n",
            "Loss :  0.060742767558234635\n",
            "Loss :  0.06032572161983436\n",
            "Loss :  0.06058962712133372\n",
            "Loss :  0.06079143838404299\n",
            "Loss :  0.06061053670721479\n",
            "Loss :  0.060344232430866174\n",
            "Loss :  0.060247370711535464\n",
            "Loss :  0.060515934871580766\n",
            "Loss :  0.06081838982430755\n",
            "Loss :  0.06085997015632541\n",
            "Loss :  0.06098864451976296\n",
            "Loss :  0.061231497239473964\n",
            "Loss :  0.06119425502188956\n",
            "Loss :  0.06135451038860526\n",
            "Loss :  0.06135104978410759\n",
            "Loss :  0.06143141306618943\n",
            "Loss :  0.061478634950666945\n",
            "Loss :  0.0616583481463261\n",
            "Loss :  0.0617132247947311\n",
            "Loss :  0.0617460794153204\n",
            "Loss :  0.061697494581170464\n",
            "Loss :  0.06166245441925042\n",
            "Loss :  0.06169418964097508\n",
            "Loss :  0.061750408671668304\n",
            "Loss :  0.061805997712172545\n",
            "Loss :  0.06177138647370017\n",
            "Loss :  0.06174756133946303\n",
            "Loss :  0.06179827767961695\n",
            "Loss :  0.061800800100012605\n",
            "Loss :  0.061875973331962215\n",
            "Loss :  0.061896459733515236\n",
            "Loss :  0.06186320990163361\n",
            "Loss :  0.06183795891995505\n",
            "Loss :  0.061791519584405756\n",
            "Loss :  0.06181859450781732\n",
            "Loss :  0.06181215221139346\n",
            "Loss :  0.06178430144867535\n",
            "Loss :  0.06173425981139639\n",
            "Loss :  0.061743277921225206\n",
            "Loss :  0.06177471300582928\n",
            "Loss :  0.06173337600042401\n",
            "Loss :  0.061752052491257904\n",
            "Loss :  0.06177961454569922\n",
            "Loss :  0.061715133351609566\n",
            "Validation: \n",
            " Loss :  0.07193418592214584\n",
            " Loss :  0.07929912404645056\n",
            " Loss :  0.07868061605386617\n",
            " Loss :  0.07824637832455948\n",
            " Loss :  0.07829943386676871\n",
            "\n",
            "Epoch: 50\n",
            "Loss :  0.06673481315374374\n",
            "Loss :  0.05902540616013787\n",
            "Loss :  0.05965711921453476\n",
            "Loss :  0.06030485911234733\n",
            "Loss :  0.060426277812661196\n",
            "Loss :  0.06035890707782671\n",
            "Loss :  0.06046626576390423\n",
            "Loss :  0.060191065492764324\n",
            "Loss :  0.06039433503224526\n",
            "Loss :  0.06051962593427071\n",
            "Loss :  0.06052815939972896\n",
            "Loss :  0.06036700670783584\n",
            "Loss :  0.06020025890474477\n",
            "Loss :  0.06043576983777621\n",
            "Loss :  0.060583523137772335\n",
            "Loss :  0.06080198243556433\n",
            "Loss :  0.06091859541046694\n",
            "Loss :  0.06098519729679091\n",
            "Loss :  0.06106996283099796\n",
            "Loss :  0.06113299782051466\n",
            "Loss :  0.061185986174279776\n",
            "Loss :  0.06123677104429046\n",
            "Loss :  0.06126492558156743\n",
            "Loss :  0.06140265249328696\n",
            "Loss :  0.061415881335858985\n",
            "Loss :  0.06121695197495331\n",
            "Loss :  0.0611884748969955\n",
            "Loss :  0.0611368364266144\n",
            "Loss :  0.061112622402316734\n",
            "Loss :  0.0611901832712475\n",
            "Loss :  0.06122359898300266\n",
            "Loss :  0.06124700181833034\n",
            "Loss :  0.06121220800596234\n",
            "Loss :  0.06122678160397308\n",
            "Loss :  0.061274781954253524\n",
            "Loss :  0.06133172335235821\n",
            "Loss :  0.06131861235048632\n",
            "Loss :  0.061301528403379844\n",
            "Loss :  0.06126551050448355\n",
            "Loss :  0.06126490509723458\n",
            "Loss :  0.0612307883000136\n",
            "Loss :  0.06124913715134282\n",
            "Loss :  0.06123156589843032\n",
            "Loss :  0.06115418949149324\n",
            "Loss :  0.061159983446268266\n",
            "Loss :  0.06118735709039705\n",
            "Loss :  0.061125902479227605\n",
            "Loss :  0.061128223042009745\n",
            "Loss :  0.06119864541657749\n",
            "Loss :  0.06114506592515044\n",
            "Validation: \n",
            " Loss :  0.07326560467481613\n",
            " Loss :  0.07818414164440972\n",
            " Loss :  0.07794152545492823\n",
            " Loss :  0.07748992716679808\n",
            " Loss :  0.07767526409876199\n",
            "\n",
            "Epoch: 51\n",
            "Loss :  0.05889204889535904\n",
            "Loss :  0.058606517924503845\n",
            "Loss :  0.058739040401719865\n",
            "Loss :  0.05918894816310175\n",
            "Loss :  0.05977459415429976\n",
            "Loss :  0.06013831810331812\n",
            "Loss :  0.06007758882202086\n",
            "Loss :  0.05966215727614685\n",
            "Loss :  0.059957204225622576\n",
            "Loss :  0.06016064848709893\n",
            "Loss :  0.06015086716206947\n",
            "Loss :  0.05995301700927116\n",
            "Loss :  0.05986816688510012\n",
            "Loss :  0.060151212628333624\n",
            "Loss :  0.060270501287482306\n",
            "Loss :  0.06042095542644823\n",
            "Loss :  0.06055676805214112\n",
            "Loss :  0.060721698995919254\n",
            "Loss :  0.06083072302091187\n",
            "Loss :  0.06077967931306799\n",
            "Loss :  0.060744771872883414\n",
            "Loss :  0.06074742531465693\n",
            "Loss :  0.06075661017546826\n",
            "Loss :  0.060912575475968324\n",
            "Loss :  0.06101494324219672\n",
            "Loss :  0.060944804528438715\n",
            "Loss :  0.06092791541897018\n",
            "Loss :  0.06088230259908961\n",
            "Loss :  0.06087441180312336\n",
            "Loss :  0.06098638361006258\n",
            "Loss :  0.0609795613913639\n",
            "Loss :  0.06101659764430914\n",
            "Loss :  0.061032076514212885\n",
            "Loss :  0.0611059718576803\n",
            "Loss :  0.06118515810090775\n",
            "Loss :  0.06120200492335521\n",
            "Loss :  0.06120548067082989\n",
            "Loss :  0.061201177246246054\n",
            "Loss :  0.06117249150171367\n",
            "Loss :  0.0611188604837031\n",
            "Loss :  0.061080951782459036\n",
            "Loss :  0.06107709517842952\n",
            "Loss :  0.06102251016364528\n",
            "Loss :  0.06094843409350466\n",
            "Loss :  0.06094293919753055\n",
            "Loss :  0.06097771718007762\n",
            "Loss :  0.060948082421645684\n",
            "Loss :  0.060937528880095536\n",
            "Loss :  0.060971226612892074\n",
            "Loss :  0.06089119352172689\n",
            "Validation: \n",
            " Loss :  0.07207517325878143\n",
            " Loss :  0.07998184398526237\n",
            " Loss :  0.07926152065032865\n",
            " Loss :  0.07929178376178272\n",
            " Loss :  0.07951823851944488\n",
            "\n",
            "Epoch: 52\n",
            "Loss :  0.06371849775314331\n",
            "Loss :  0.06026010316881267\n",
            "Loss :  0.05949334906680243\n",
            "Loss :  0.059975300224558\n",
            "Loss :  0.059891147402728474\n",
            "Loss :  0.059628052308278924\n",
            "Loss :  0.06001316994184353\n",
            "Loss :  0.05975823329997734\n",
            "Loss :  0.059767382978289214\n",
            "Loss :  0.05994745962076135\n",
            "Loss :  0.05982268446742898\n",
            "Loss :  0.05984192942311098\n",
            "Loss :  0.0597573694857684\n",
            "Loss :  0.0600197704238746\n",
            "Loss :  0.06003695036819641\n",
            "Loss :  0.06011920037451169\n",
            "Loss :  0.06024678856689737\n",
            "Loss :  0.060252973559307074\n",
            "Loss :  0.060224958361018426\n",
            "Loss :  0.06016447607681389\n",
            "Loss :  0.0601944577560496\n",
            "Loss :  0.06027879744273791\n",
            "Loss :  0.060398245828723475\n",
            "Loss :  0.060485727552856715\n",
            "Loss :  0.060479889287617196\n",
            "Loss :  0.060479881338389274\n",
            "Loss :  0.06052333917254689\n",
            "Loss :  0.06048209826389802\n",
            "Loss :  0.060481136578576\n",
            "Loss :  0.06053159720043546\n",
            "Loss :  0.06062414739714112\n",
            "Loss :  0.06058381305007306\n",
            "Loss :  0.06052719470914279\n",
            "Loss :  0.06053696603168174\n",
            "Loss :  0.0605705796557962\n",
            "Loss :  0.060627034692852584\n",
            "Loss :  0.06062231386219696\n",
            "Loss :  0.06056580724661562\n",
            "Loss :  0.06052071125957909\n",
            "Loss :  0.060472186921578845\n",
            "Loss :  0.060443578459824114\n",
            "Loss :  0.06042184334456776\n",
            "Loss :  0.06039031132206214\n",
            "Loss :  0.06030288131220435\n",
            "Loss :  0.060360144806894854\n",
            "Loss :  0.060362560108733546\n",
            "Loss :  0.06028157870998352\n",
            "Loss :  0.06028392646786007\n",
            "Loss :  0.06030021842545878\n",
            "Loss :  0.06026502534018757\n",
            "Validation: \n",
            " Loss :  0.07955572754144669\n",
            " Loss :  0.08507920979034334\n",
            " Loss :  0.08464853810827906\n",
            " Loss :  0.08439190624678722\n",
            " Loss :  0.08454943494296369\n",
            "\n",
            "Epoch: 53\n",
            "Loss :  0.06244907155632973\n",
            "Loss :  0.05969602479176088\n",
            "Loss :  0.05929256478945414\n",
            "Loss :  0.059812184783720204\n",
            "Loss :  0.059608897421418165\n",
            "Loss :  0.059468139036028995\n",
            "Loss :  0.059218185114078836\n",
            "Loss :  0.0596061707277533\n",
            "Loss :  0.059668914035514546\n",
            "Loss :  0.05973643154560865\n",
            "Loss :  0.05957516673767921\n",
            "Loss :  0.05928291121984387\n",
            "Loss :  0.05906305890068535\n",
            "Loss :  0.0592104499406032\n",
            "Loss :  0.059226146350938376\n",
            "Loss :  0.05937166911679388\n",
            "Loss :  0.05956826315070531\n",
            "Loss :  0.059849843731400565\n",
            "Loss :  0.059822702255532226\n",
            "Loss :  0.05974070281180412\n",
            "Loss :  0.05968566813427417\n",
            "Loss :  0.05971423112809376\n",
            "Loss :  0.05989739138683582\n",
            "Loss :  0.06005608701667228\n",
            "Loss :  0.060124782451208204\n",
            "Loss :  0.05999208562283877\n",
            "Loss :  0.060019185159969145\n",
            "Loss :  0.05992748272155044\n",
            "Loss :  0.059867466246424195\n",
            "Loss :  0.059944064730835946\n",
            "Loss :  0.060006834665604206\n",
            "Loss :  0.059954898521735356\n",
            "Loss :  0.05994828503776191\n",
            "Loss :  0.059954534777161575\n",
            "Loss :  0.05995426487581821\n",
            "Loss :  0.060041645005812334\n",
            "Loss :  0.06000026411420751\n",
            "Loss :  0.05998315004446114\n",
            "Loss :  0.059981438979076276\n",
            "Loss :  0.059968959864066994\n",
            "Loss :  0.05995340758651272\n",
            "Loss :  0.05997170009389701\n",
            "Loss :  0.05997425539865913\n",
            "Loss :  0.059914989591405454\n",
            "Loss :  0.05994621583190905\n",
            "Loss :  0.059941331498704835\n",
            "Loss :  0.05988151020897702\n",
            "Loss :  0.05988747194694106\n",
            "Loss :  0.05990535889804487\n",
            "Loss :  0.059821539036853735\n",
            "Validation: \n",
            " Loss :  0.07001801580190659\n",
            " Loss :  0.07752219339211781\n",
            " Loss :  0.07708821700113576\n",
            " Loss :  0.07660254665085527\n",
            " Loss :  0.07659421796783988\n",
            "\n",
            "Epoch: 54\n",
            "Loss :  0.0693332850933075\n",
            "Loss :  0.060062272982163864\n",
            "Loss :  0.05850391923671677\n",
            "Loss :  0.059028991407925085\n",
            "Loss :  0.059451833094765504\n",
            "Loss :  0.05931144215020479\n",
            "Loss :  0.05916225152914641\n",
            "Loss :  0.05900359017328477\n",
            "Loss :  0.05899224112983103\n",
            "Loss :  0.058864511467598295\n",
            "Loss :  0.05883008640001316\n",
            "Loss :  0.05852118228469883\n",
            "Loss :  0.05831498723507913\n",
            "Loss :  0.058595254107286\n",
            "Loss :  0.058677585148219524\n",
            "Loss :  0.05886933020013847\n",
            "Loss :  0.05894573130037473\n",
            "Loss :  0.05904642331321337\n",
            "Loss :  0.059067474194488476\n",
            "Loss :  0.0590743121949478\n",
            "Loss :  0.05902604780980011\n",
            "Loss :  0.05913345773496899\n",
            "Loss :  0.059308008286613145\n",
            "Loss :  0.05946826871919941\n",
            "Loss :  0.05952644311046205\n",
            "Loss :  0.05948377365016368\n",
            "Loss :  0.059431230499484074\n",
            "Loss :  0.059346069846425986\n",
            "Loss :  0.059333966600831296\n",
            "Loss :  0.059401044334025724\n",
            "Loss :  0.05945726222918675\n",
            "Loss :  0.059362447003076316\n",
            "Loss :  0.05940575435804058\n",
            "Loss :  0.05943639700236277\n",
            "Loss :  0.0594963079586057\n",
            "Loss :  0.05951094678324512\n",
            "Loss :  0.05948549770128364\n",
            "Loss :  0.05949425541687847\n",
            "Loss :  0.059456734519618075\n",
            "Loss :  0.05940807141992442\n",
            "Loss :  0.05939536988549399\n",
            "Loss :  0.05938573418198711\n",
            "Loss :  0.0593797629013339\n",
            "Loss :  0.05932712824604627\n",
            "Loss :  0.05931256875361986\n",
            "Loss :  0.05935096661426011\n",
            "Loss :  0.05933242536871138\n",
            "Loss :  0.05936451828119102\n",
            "Loss :  0.05942126951352722\n",
            "Loss :  0.059381553345566614\n",
            "Validation: \n",
            " Loss :  0.07348376512527466\n",
            " Loss :  0.07958054844112623\n",
            " Loss :  0.07906072850270969\n",
            " Loss :  0.07878142503685638\n",
            " Loss :  0.07885957725438071\n",
            "\n",
            "Epoch: 55\n",
            "Loss :  0.06093607470393181\n",
            "Loss :  0.05925774303349582\n",
            "Loss :  0.05755300873092243\n",
            "Loss :  0.05812262335131245\n",
            "Loss :  0.05834873720276647\n",
            "Loss :  0.05819205742548494\n",
            "Loss :  0.058072104134031986\n",
            "Loss :  0.057737189069600174\n",
            "Loss :  0.05779944879957187\n",
            "Loss :  0.057882290824756516\n",
            "Loss :  0.057906197549978104\n",
            "Loss :  0.057860591121622035\n",
            "Loss :  0.05770879559034158\n",
            "Loss :  0.057905599133658954\n",
            "Loss :  0.05797319824919633\n",
            "Loss :  0.05814787207652401\n",
            "Loss :  0.05825788951160745\n",
            "Loss :  0.05846336616845856\n",
            "Loss :  0.058470721052990435\n",
            "Loss :  0.05840109489585093\n",
            "Loss :  0.05853876291741779\n",
            "Loss :  0.05875641284113247\n",
            "Loss :  0.058906787644009785\n",
            "Loss :  0.05909460834610514\n",
            "Loss :  0.05911944352121274\n",
            "Loss :  0.05908188643861577\n",
            "Loss :  0.05897661625368385\n",
            "Loss :  0.058894393271405755\n",
            "Loss :  0.05889853242506336\n",
            "Loss :  0.058912653040742544\n",
            "Loss :  0.0589147520768286\n",
            "Loss :  0.058945082643120233\n",
            "Loss :  0.05894799742882497\n",
            "Loss :  0.05897626701708647\n",
            "Loss :  0.05900715763443027\n",
            "Loss :  0.05907277122904093\n",
            "Loss :  0.05906418608785336\n",
            "Loss :  0.05905251135683124\n",
            "Loss :  0.05903905701488647\n",
            "Loss :  0.058993252399175064\n",
            "Loss :  0.05901336141320833\n",
            "Loss :  0.058987799270527205\n",
            "Loss :  0.058949247798981974\n",
            "Loss :  0.058888223221019914\n",
            "Loss :  0.05887687525578907\n",
            "Loss :  0.05888599283712402\n",
            "Loss :  0.05882781642184082\n",
            "Loss :  0.05884949652569056\n",
            "Loss :  0.05887752368643477\n",
            "Loss :  0.05883073543488615\n",
            "Validation: \n",
            " Loss :  0.07911770045757294\n",
            " Loss :  0.0855942712653251\n",
            " Loss :  0.0850142603967248\n",
            " Loss :  0.08434321914539962\n",
            " Loss :  0.08458456728193495\n",
            "\n",
            "Epoch: 56\n",
            "Loss :  0.060372285544872284\n",
            "Loss :  0.05580813369967721\n",
            "Loss :  0.056749657683429267\n",
            "Loss :  0.057520470071223476\n",
            "Loss :  0.05759804050733403\n",
            "Loss :  0.05725873641523661\n",
            "Loss :  0.05704114146408488\n",
            "Loss :  0.05710445407410743\n",
            "Loss :  0.05751842582299386\n",
            "Loss :  0.05756401217409542\n",
            "Loss :  0.057698341768861995\n",
            "Loss :  0.0575559352365163\n",
            "Loss :  0.05735065615620495\n",
            "Loss :  0.05756715261412941\n",
            "Loss :  0.0578634131514857\n",
            "Loss :  0.058072774936229185\n",
            "Loss :  0.058208933898380825\n",
            "Loss :  0.05821708825073744\n",
            "Loss :  0.0581910792758781\n",
            "Loss :  0.05810357462517254\n",
            "Loss :  0.05813124243966976\n",
            "Loss :  0.058226397781857946\n",
            "Loss :  0.05830241872916394\n",
            "Loss :  0.05845831513598368\n",
            "Loss :  0.05855438023558296\n",
            "Loss :  0.058516847377873035\n",
            "Loss :  0.0585761658447684\n",
            "Loss :  0.058529753100498134\n",
            "Loss :  0.05851190351612627\n",
            "Loss :  0.058482822709272\n",
            "Loss :  0.058536604083950734\n",
            "Loss :  0.05854935084029409\n",
            "Loss :  0.05853751355149664\n",
            "Loss :  0.05862301355087145\n",
            "Loss :  0.05866855146947844\n",
            "Loss :  0.058735564849909896\n",
            "Loss :  0.05867905636160658\n",
            "Loss :  0.058638047336568085\n",
            "Loss :  0.05858530811085476\n",
            "Loss :  0.05853315535218209\n",
            "Loss :  0.05848921298460473\n",
            "Loss :  0.05849167578593078\n",
            "Loss :  0.05844609815441514\n",
            "Loss :  0.05842898690126611\n",
            "Loss :  0.05840456208674545\n",
            "Loss :  0.05842838849368487\n",
            "Loss :  0.05840281432660179\n",
            "Loss :  0.058394250200938774\n",
            "Loss :  0.058404373752414064\n",
            "Loss :  0.05836670551365602\n",
            "Validation: \n",
            " Loss :  0.07652238756418228\n",
            " Loss :  0.08096530714205333\n",
            " Loss :  0.0803944790145246\n",
            " Loss :  0.08000298357400738\n",
            " Loss :  0.08022667550378376\n",
            "\n",
            "Epoch: 57\n",
            "Loss :  0.06429212540388107\n",
            "Loss :  0.05669154870239171\n",
            "Loss :  0.05684882455638477\n",
            "Loss :  0.05744878563188737\n",
            "Loss :  0.05699993897138572\n",
            "Loss :  0.056965828234074166\n",
            "Loss :  0.057104700657187916\n",
            "Loss :  0.05705239476872162\n",
            "Loss :  0.05719316736967475\n",
            "Loss :  0.0571116050193598\n",
            "Loss :  0.05715149338587676\n",
            "Loss :  0.05692572881643836\n",
            "Loss :  0.05683500615279537\n",
            "Loss :  0.05703088857290399\n",
            "Loss :  0.0573028842291088\n",
            "Loss :  0.05741032496686803\n",
            "Loss :  0.057543361547941006\n",
            "Loss :  0.05765739954703036\n",
            "Loss :  0.05772331182057686\n",
            "Loss :  0.05760014525258728\n",
            "Loss :  0.05766386462755464\n",
            "Loss :  0.057755660089130086\n",
            "Loss :  0.057762205196182116\n",
            "Loss :  0.05802200304029824\n",
            "Loss :  0.05804725608081244\n",
            "Loss :  0.05804665512832038\n",
            "Loss :  0.058062104729514466\n",
            "Loss :  0.057974263159661275\n",
            "Loss :  0.05802576791402284\n",
            "Loss :  0.0581480436718341\n",
            "Loss :  0.05824565651062319\n",
            "Loss :  0.05823678929370699\n",
            "Loss :  0.05822895389526061\n",
            "Loss :  0.05821097429470353\n",
            "Loss :  0.05829238137803819\n",
            "Loss :  0.05835701205749118\n",
            "Loss :  0.05833346509165711\n",
            "Loss :  0.05836378669521879\n",
            "Loss :  0.058412801161447536\n",
            "Loss :  0.05840145382086944\n",
            "Loss :  0.05841072612532654\n",
            "Loss :  0.058400717294274165\n",
            "Loss :  0.05836179236618187\n",
            "Loss :  0.05830268182396336\n",
            "Loss :  0.05832384250500575\n",
            "Loss :  0.05835538336846094\n",
            "Loss :  0.05831077129230582\n",
            "Loss :  0.0583195625596745\n",
            "Loss :  0.058346434986207195\n",
            "Loss :  0.05828730284773162\n",
            "Validation: \n",
            " Loss :  0.07748492062091827\n",
            " Loss :  0.07922624548276265\n",
            " Loss :  0.07839377715093333\n",
            " Loss :  0.07811969176667635\n",
            " Loss :  0.07804014653335382\n",
            "\n",
            "Epoch: 58\n",
            "Loss :  0.05859695374965668\n",
            "Loss :  0.056692360138351265\n",
            "Loss :  0.05571597300115086\n",
            "Loss :  0.0561500929536358\n",
            "Loss :  0.05597222560062641\n",
            "Loss :  0.05628523223248182\n",
            "Loss :  0.056248203408522685\n",
            "Loss :  0.056232516666952996\n",
            "Loss :  0.05630414799591641\n",
            "Loss :  0.056388397342883624\n",
            "Loss :  0.056411239549075023\n",
            "Loss :  0.0562384332286882\n",
            "Loss :  0.05618924111866754\n",
            "Loss :  0.05643149115543329\n",
            "Loss :  0.056661830673403774\n",
            "Loss :  0.05673510657734429\n",
            "Loss :  0.05691780874170132\n",
            "Loss :  0.05718679859014283\n",
            "Loss :  0.05721593421796409\n",
            "Loss :  0.057220528477617584\n",
            "Loss :  0.057233478503292474\n",
            "Loss :  0.057354649996729255\n",
            "Loss :  0.057431002817542307\n",
            "Loss :  0.05768667879107194\n",
            "Loss :  0.057815137616826294\n",
            "Loss :  0.057789458429433435\n",
            "Loss :  0.05776784446276011\n",
            "Loss :  0.057739063767489475\n",
            "Loss :  0.05773898552161943\n",
            "Loss :  0.0578224091467374\n",
            "Loss :  0.057853586204424254\n",
            "Loss :  0.057816146259043376\n",
            "Loss :  0.05782697853455291\n",
            "Loss :  0.05786024283876592\n",
            "Loss :  0.05795644276128137\n",
            "Loss :  0.058037935466932776\n",
            "Loss :  0.05803367932045889\n",
            "Loss :  0.05806979526968979\n",
            "Loss :  0.05805662436669893\n",
            "Loss :  0.0580178129360499\n",
            "Loss :  0.05801143777786645\n",
            "Loss :  0.05799372072746284\n",
            "Loss :  0.0579777888759015\n",
            "Loss :  0.057919442463501974\n",
            "Loss :  0.057915319762532676\n",
            "Loss :  0.057900404933418775\n",
            "Loss :  0.05784574930036921\n",
            "Loss :  0.057828781829707944\n",
            "Loss :  0.05785530316110956\n",
            "Loss :  0.057824760371944325\n",
            "Validation: \n",
            " Loss :  0.08194702863693237\n",
            " Loss :  0.08728968174684615\n",
            " Loss :  0.08681955152168506\n",
            " Loss :  0.0867576683398153\n",
            " Loss :  0.08677850930411139\n",
            "\n",
            "Epoch: 59\n",
            "Loss :  0.0613485611975193\n",
            "Loss :  0.05710194869474931\n",
            "Loss :  0.05648813645044962\n",
            "Loss :  0.05714664716393717\n",
            "Loss :  0.05727773641304272\n",
            "Loss :  0.05716419862765892\n",
            "Loss :  0.05712439699983988\n",
            "Loss :  0.0568739277586131\n",
            "Loss :  0.0572016512354215\n",
            "Loss :  0.057250319847038815\n",
            "Loss :  0.05711741513102361\n",
            "Loss :  0.05706060130719666\n",
            "Loss :  0.05699190996156251\n",
            "Loss :  0.05736275096886031\n",
            "Loss :  0.05740334378912094\n",
            "Loss :  0.05758361607197894\n",
            "Loss :  0.05761224459509672\n",
            "Loss :  0.05771240642109112\n",
            "Loss :  0.05768290069468772\n",
            "Loss :  0.05767649895857766\n",
            "Loss :  0.05772799436948193\n",
            "Loss :  0.05782198152059062\n",
            "Loss :  0.0578484168250906\n",
            "Loss :  0.05795069666200386\n",
            "Loss :  0.0579743334613895\n",
            "Loss :  0.057879722495359255\n",
            "Loss :  0.05793243940648448\n",
            "Loss :  0.057896323873218136\n",
            "Loss :  0.05790898415965966\n",
            "Loss :  0.057961034022041204\n",
            "Loss :  0.05796442721719758\n",
            "Loss :  0.05797309216435316\n",
            "Loss :  0.05798190074314209\n",
            "Loss :  0.05798982666788505\n",
            "Loss :  0.057982108449219256\n",
            "Loss :  0.058022619641659265\n",
            "Loss :  0.05796954919096506\n",
            "Loss :  0.05795616210711934\n",
            "Loss :  0.05785244105877526\n",
            "Loss :  0.05781633255388731\n",
            "Loss :  0.05777904436809761\n",
            "Loss :  0.0577721963398648\n",
            "Loss :  0.057752042911468945\n",
            "Loss :  0.05767119318903737\n",
            "Loss :  0.05767076947386303\n",
            "Loss :  0.05763477409989501\n",
            "Loss :  0.05756663956119802\n",
            "Loss :  0.05755344160581344\n",
            "Loss :  0.057556592152606924\n",
            "Loss :  0.05751264413966666\n",
            "Validation: \n",
            " Loss :  0.0772971361875534\n",
            " Loss :  0.08232522436550685\n",
            " Loss :  0.08159851410040041\n",
            " Loss :  0.08172662995877814\n",
            " Loss :  0.08200494475938656\n",
            "\n",
            "Epoch: 60\n",
            "Loss :  0.06250016391277313\n",
            "Loss :  0.055087213489142334\n",
            "Loss :  0.055307810327836444\n",
            "Loss :  0.05591589524861305\n",
            "Loss :  0.05606709157184857\n",
            "Loss :  0.05620090385862425\n",
            "Loss :  0.056378592721751476\n",
            "Loss :  0.05610567955693729\n",
            "Loss :  0.05619988070777905\n",
            "Loss :  0.05651773299489703\n",
            "Loss :  0.05643323296219996\n",
            "Loss :  0.05635807082593978\n",
            "Loss :  0.05627159016930367\n",
            "Loss :  0.05662991004135772\n",
            "Loss :  0.05679886753783158\n",
            "Loss :  0.05699940442742891\n",
            "Loss :  0.05705802815948954\n",
            "Loss :  0.057234405708766124\n",
            "Loss :  0.057295625298721356\n",
            "Loss :  0.05720687480107028\n",
            "Loss :  0.05720861495208385\n",
            "Loss :  0.05737143214703736\n",
            "Loss :  0.05742351704053749\n",
            "Loss :  0.057602745871910284\n",
            "Loss :  0.05770213257413188\n",
            "Loss :  0.05765917012653503\n",
            "Loss :  0.05761483214595765\n",
            "Loss :  0.057525272408974565\n",
            "Loss :  0.05748761504941564\n",
            "Loss :  0.05750945899187494\n",
            "Loss :  0.057498953308278936\n",
            "Loss :  0.05742784855450081\n",
            "Loss :  0.05739495391461337\n",
            "Loss :  0.057379940901728196\n",
            "Loss :  0.057463913773598906\n",
            "Loss :  0.05753063576089011\n",
            "Loss :  0.057504348002807584\n",
            "Loss :  0.05751131843484315\n",
            "Loss :  0.05746609196457963\n",
            "Loss :  0.057436468246419105\n",
            "Loss :  0.05739999582010909\n",
            "Loss :  0.057374725956696376\n",
            "Loss :  0.057363428078113995\n",
            "Loss :  0.057296596519355154\n",
            "Loss :  0.0572864567256974\n",
            "Loss :  0.05729469837013739\n",
            "Loss :  0.05719881144366399\n",
            "Loss :  0.05716784451860784\n",
            "Loss :  0.05715609129649934\n",
            "Loss :  0.05710230719927858\n",
            "Validation: \n",
            " Loss :  0.07652295380830765\n",
            " Loss :  0.08058544070947737\n",
            " Loss :  0.08002650556040973\n",
            " Loss :  0.07992396491472839\n",
            " Loss :  0.07989441787387118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Finetuning student on crossentropy\n",
        "Now the student is trained. In this cell we need to replace the classifier(i.e: Fully Connected layer) of student network from one with output shape of dense feature to one with shape of classes. e.g: nn.Linear(256,512) to nn.Linear(256,10). After this we need to freez Conv layers in the network and finetune the network using orignal dataset. "
      ],
      "metadata": {
        "id": "N5rEK4VmiN_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s1.classifier = nn.Linear(512, 10)\n",
        "for m in s1.modules():\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        m.weight.requires_grad = False\n",
        "        if m.bias is not None:\n",
        "            m.bias.requires_grad = False\n",
        "s1 = s1.to(device)\n",
        "summary(s1, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv_cGp5aiSbS",
        "outputId": "6104876f-028e-4505-9a9b-d0ab3c8954b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "           Conv2d-29            [-1, 512, 2, 2]       1,180,160\n",
            "      BatchNorm2d-30            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-31            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-32            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-33            [-1, 512, 1, 1]               0\n",
            "           Linear-34                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 2,360,490\n",
            "Trainable params: 8,074\n",
            "Non-trainable params: 2,352,416\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.98\n",
            "Params size (MB): 9.00\n",
            "Estimated Total Size (MB): 12.00\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(s1.parameters(), lr=0.0001)\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    s1.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        s1.zero_grad()\n",
        "        outputs = s1(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if(batch_idx % 200 == 0):\n",
        "          print(\"Accuracy : \",100.*correct/total,\" Loss : \", train_loss/(batch_idx+1))\n",
        "def test(epoch):\n",
        "    s1.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = s1(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\"Accuracy : \",100.*correct/total,\" Loss : \", test_loss/(batch_idx+1))"
      ],
      "metadata": {
        "id": "t7gwqasZiseD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "    train(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGgfdwA-iue-",
        "outputId": "042f3ea0-48aa-4d54-9cb0-acbe9756a4b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Accuracy :  13.0  Loss :  2.3406362533569336\n",
            "Accuracy :  69.44278606965175  Loss :  1.4717139343717205\n",
            "Accuracy :  77.3640897755611  Loss :  1.1176168737268806\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.5915505886077881\n",
            "Accuracy :  84.14285714285714  Loss :  0.5783692726067134\n",
            "Accuracy :  83.82926829268293  Loss :  0.5829475471159307\n",
            "Accuracy :  84.09836065573771  Loss :  0.5820076895541832\n",
            "Accuracy :  84.11111111111111  Loss :  0.5806706735381374\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy :  82.0  Loss :  0.5544735193252563\n",
            "Accuracy :  85.71641791044776  Loss :  0.5207162867138042\n",
            "Accuracy :  86.07481296758105  Loss :  0.49043705888519856\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.4678022861480713\n",
            "Accuracy :  85.0  Loss :  0.4648697092419579\n",
            "Accuracy :  84.60975609756098  Loss :  0.4711946045480123\n",
            "Accuracy :  84.75409836065573  Loss :  0.46949492419352296\n",
            "Accuracy :  84.85185185185185  Loss :  0.4675458275977476\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy :  87.0  Loss :  0.40074536204338074\n",
            "Accuracy :  86.33830845771145  Loss :  0.4234069590248279\n",
            "Accuracy :  86.60598503740648  Loss :  0.41434788826546465\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.4359573721885681\n",
            "Accuracy :  85.04761904761905  Loss :  0.4403695875690097\n",
            "Accuracy :  84.85365853658537  Loss :  0.4470213804303146\n",
            "Accuracy :  85.06557377049181  Loss :  0.4447761070532877\n",
            "Accuracy :  85.1604938271605  Loss :  0.4426679692150634\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy :  89.0  Loss :  0.36340785026550293\n",
            "Accuracy :  86.72636815920399  Loss :  0.39733397708603396\n",
            "Accuracy :  86.88778054862843  Loss :  0.39068682811355354\n",
            "Validation: \n",
            "Accuracy :  88.0  Loss :  0.4188281297683716\n",
            "Accuracy :  85.23809523809524  Loss :  0.43245758754866465\n",
            "Accuracy :  85.09756097560975  Loss :  0.4392263460450056\n",
            "Accuracy :  85.21311475409836  Loss :  0.43702534378552044\n",
            "Accuracy :  85.32098765432099  Loss :  0.43483704917224836\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy :  88.0  Loss :  0.3661419153213501\n",
            "Accuracy :  86.8905472636816  Loss :  0.38450111826853967\n",
            "Accuracy :  87.01995012468828  Loss :  0.38024007675802324\n",
            "Validation: \n",
            "Accuracy :  88.0  Loss :  0.40844354033470154\n",
            "Accuracy :  85.42857142857143  Loss :  0.4295568572623389\n",
            "Accuracy :  85.1951219512195  Loss :  0.43673316044051474\n",
            "Accuracy :  85.22950819672131  Loss :  0.43440292824487214\n",
            "Accuracy :  85.33333333333333  Loss :  0.4321039092761499\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy :  92.0  Loss :  0.33555862307548523\n",
            "Accuracy :  87.12437810945273  Loss :  0.380191825960406\n",
            "Accuracy :  87.24688279301746  Loss :  0.375374685573459\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4065747857093811\n",
            "Accuracy :  85.23809523809524  Loss :  0.4284237083934602\n",
            "Accuracy :  85.1219512195122  Loss :  0.4354355538763651\n",
            "Accuracy :  85.26229508196721  Loss :  0.4327308882455357\n",
            "Accuracy :  85.4074074074074  Loss :  0.4303741723666956\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy :  91.0  Loss :  0.36680299043655396\n",
            "Accuracy :  86.96517412935323  Loss :  0.37606080691909316\n",
            "Accuracy :  87.23690773067332  Loss :  0.372890130354282\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4015743136405945\n",
            "Accuracy :  85.28571428571429  Loss :  0.427399259238016\n",
            "Accuracy :  85.17073170731707  Loss :  0.43431568727260683\n",
            "Accuracy :  85.31147540983606  Loss :  0.43208492583915836\n",
            "Accuracy :  85.46913580246914  Loss :  0.4296861316686795\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy :  89.0  Loss :  0.3054676055908203\n",
            "Accuracy :  87.13930348258707  Loss :  0.3697983798251223\n",
            "Accuracy :  87.28678304239402  Loss :  0.368000478741534\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.40024876594543457\n",
            "Accuracy :  85.38095238095238  Loss :  0.42641933049474445\n",
            "Accuracy :  85.21951219512195  Loss :  0.43326170197347313\n",
            "Accuracy :  85.39344262295081  Loss :  0.43038281651793936\n",
            "Accuracy :  85.50617283950618  Loss :  0.4277764750115665\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy :  90.0  Loss :  0.2875247299671173\n",
            "Accuracy :  87.28358208955224  Loss :  0.368846304603477\n",
            "Accuracy :  87.3790523690773  Loss :  0.3667525038531891\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.3947552442550659\n",
            "Accuracy :  85.28571428571429  Loss :  0.42599410599186305\n",
            "Accuracy :  85.29268292682927  Loss :  0.43284123009297903\n",
            "Accuracy :  85.42622950819673  Loss :  0.4302906450189528\n",
            "Accuracy :  85.60493827160494  Loss :  0.4276731468645143\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy :  88.0  Loss :  0.30968278646469116\n",
            "Accuracy :  87.45273631840796  Loss :  0.36800066370572615\n",
            "Accuracy :  87.53615960099751  Loss :  0.36524396882092863\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.3901783227920532\n",
            "Accuracy :  85.42857142857143  Loss :  0.42506549117111025\n",
            "Accuracy :  85.29268292682927  Loss :  0.4321807902760622\n",
            "Accuracy :  85.49180327868852  Loss :  0.42954785730995115\n",
            "Accuracy :  85.60493827160494  Loss :  0.4268613422726407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_name = '1student.pt'\n",
        "path = F\"/content/gdrive/MyDrive/{model_save_name}\" \n",
        "torch.save(s1.state_dict(), path)"
      ],
      "metadata": {
        "id": "285Zfq11iwx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_save_name = '1student.pt'\n",
        "# path = F\"/content/gdrive/MyDrive/{model_save_name}\" \n",
        "# s1.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "eeygMyzeeKF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extract denseFeatures from s1"
      ],
      "metadata": {
        "id": "SsHS6Sb4k4t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# S1_AS_TA_WOH = nn.Sequential(*list(s1.children())[:-1],nn.Flatten())\n",
        "# summary(s1, (3, 32, 32))\n",
        "# summary(S1_AS_TA_WOH, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rSy2A3uk8cF",
        "outputId": "2d73cd61-8ce6-4122-c283-4ef3dcbdc803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "           Conv2d-29            [-1, 512, 2, 2]       1,180,160\n",
            "      BatchNorm2d-30            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-31            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-32            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-33            [-1, 512, 1, 1]               0\n",
            "           Linear-34                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 2,360,490\n",
            "Trainable params: 8,074\n",
            "Non-trainable params: 2,352,416\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.98\n",
            "Params size (MB): 9.00\n",
            "Estimated Total Size (MB): 12.00\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "           Conv2d-29            [-1, 512, 2, 2]       1,180,160\n",
            "      BatchNorm2d-30            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-31            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-32            [-1, 512, 1, 1]               0\n",
            "        AvgPool2d-33            [-1, 512, 1, 1]               0\n",
            "          Flatten-34                  [-1, 512]               0\n",
            "================================================================\n",
            "Total params: 2,355,360\n",
            "Trainable params: 2,944\n",
            "Non-trainable params: 2,352,416\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.99\n",
            "Params size (MB): 8.98\n",
            "Estimated Total Size (MB): 11.98\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# S1_AS_TA_WOH.eval()\n",
        "# S1DenseTrain = None\n",
        "# s1DenseTest = None\n",
        "# with torch.no_grad():\n",
        "#     for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "#         inputs, targets = inputs.to(device), targets.to(device)\n",
        "#         outputs = S1_AS_TA_WOH(inputs)\n",
        "#         if(S1DenseTrain == None):\n",
        "#             S1DenseTrain = outputs\n",
        "#         else:\n",
        "#             S1DenseTrain = torch.cat((S1DenseTrain,outputs))\n",
        "# with torch.no_grad():\n",
        "#     for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "#         inputs, targets = inputs.to(device), targets.to(device)\n",
        "#         outputs = S1_AS_TA_WOH(inputs)\n",
        "#         if(s1DenseTest == None):\n",
        "#             s1DenseTest = outputs\n",
        "#         else:\n",
        "#             s1DenseTest = torch.cat((s1DenseTest,outputs))"
      ],
      "metadata": {
        "id": "4w6BzS6LlRvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating 2 more students "
      ],
      "metadata": {
        "id": "guODYUgtigU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "    'VGGS': [32, 32, 'M', 64, 64, 'M', 128, 128, 'M', 256, 256, 'M','M'],\n",
        "    \n",
        "\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(256, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "s01 = VGG('VGGS')\n",
        "s01 = s01.to(device)\n",
        "summary(s01, (3, 32, 32))\n",
        "s2 = VGG('VGGS')\n",
        "s2 = s2.to(device)\n",
        "summary(s2, (3, 32, 32))"
      ],
      "metadata": {
        "id": "5TbTN57Ke7ZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "812379b2-8219-4a51-cb0b-3183861a4dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 256, 1, 1]               0\n",
            "           Linear-31                  [-1, 256]          65,792\n",
            "================================================================\n",
            "Total params: 1,239,968\n",
            "Trainable params: 1,239,968\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.94\n",
            "Params size (MB): 4.73\n",
            "Estimated Total Size (MB): 7.68\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 256, 1, 1]               0\n",
            "           Linear-31                  [-1, 256]          65,792\n",
            "================================================================\n",
            "Total params: 1,239,968\n",
            "Trainable params: 1,239,968\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.94\n",
            "Params size (MB): 4.73\n",
            "Estimated Total Size (MB): 7.68\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Multi Students\n",
        "1.6 In this step you will train two students instead of one. In the training loop you will pass the input from both students and then backwark both the losses. "
      ],
      "metadata": {
        "id": "ZYIy2HShm_Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer1 = optim.Adam(s01.parameters(), lr=0.0001)\n",
        "optimizer2 = optim.Adam(s2.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    s01.train()\n",
        "    train_loss1 = 0\n",
        "    train_loss2 = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        #code here\n",
        "        targets = DenseTrain[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "        targets.to(device)\n",
        "        s01.zero_grad()\n",
        "        s2.zero_grad()\n",
        "        output1 = s01(inputs)\n",
        "        output2 = s2(inputs)\n",
        "        loss1 = criterion(output1, targets[:,:256])\n",
        "        loss2 = criterion(output2, targets[:,256:])\n",
        "        loss1.backward()\n",
        "        loss2.backward()\n",
        "        optimizer1.step()\n",
        "        optimizer2.step()\n",
        "\n",
        "        train_loss1 += loss1.item()\n",
        "        train_loss2 += loss2.item()\n",
        "        if(batch_idx % 10 == 0):\n",
        "          print(\"Loss S01: \", train_loss1/(batch_idx+1))\n",
        "          print(\"Loss S2: \", train_loss2/(batch_idx+1))\n",
        "def test(epoch):\n",
        "    s01.eval()\n",
        "    test_loss1 = 0\n",
        "    test_loss2 = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            #code here\n",
        "            targets = DenseTest[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "            targets.to(device)\n",
        "            output1 = s01(inputs)\n",
        "            output2 = s2(inputs)\n",
        "            loss1 = criterion(output1, targets[:,:256])\n",
        "            loss2 = criterion(output2, targets[:,256:])\n",
        "            test_loss1 += loss1.item()\n",
        "            test_loss2 += loss2.item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\" Loss S01: \", test_loss1/(batch_idx+1))\n",
        "              print(\" Loss S2: \", test_loss2/(batch_idx+1))"
      ],
      "metadata": {
        "id": "kAcYq4NrnBBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+60):\n",
        "    train(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test(epoch)"
      ],
      "metadata": {
        "id": "RQJvw4e4nDwb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d84c7d9b-d9da-40cf-b33e-e8ed59046a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Loss S2:  0.09737641832036693\n",
            "Loss S01:  0.08519645580344981\n",
            "Loss S2:  0.09742397323003008\n",
            "Loss S01:  0.08523427786655603\n",
            "Loss S2:  0.09738614676377613\n",
            "Loss S01:  0.08524518095108928\n",
            "Loss S2:  0.09739330733646881\n",
            "Loss S01:  0.08524529589649843\n",
            "Loss S2:  0.09742966203427897\n",
            "Loss S01:  0.08527452904335071\n",
            "Loss S2:  0.09740996937754356\n",
            "Loss S01:  0.08524870929444671\n",
            "Loss S2:  0.09735804638032447\n",
            "Loss S01:  0.08525622309245587\n",
            "Loss S2:  0.09734532052724625\n",
            "Loss S01:  0.08520713257328313\n",
            "Loss S2:  0.09736156025085818\n",
            "Validation: \n",
            " Loss S01:  0.08557214587926865\n",
            " Loss S2:  0.13635456562042236\n",
            " Loss S01:  0.09896232116790045\n",
            " Loss S2:  0.15353174791449592\n",
            " Loss S01:  0.09769145581053525\n",
            " Loss S2:  0.15181874829094585\n",
            " Loss S01:  0.09643225518406415\n",
            " Loss S2:  0.14973108885718173\n",
            " Loss S01:  0.09578981811617628\n",
            " Loss S2:  0.14894451513702486\n",
            "\n",
            "Epoch: 17\n",
            "Loss S01:  0.07636817544698715\n",
            "Loss S2:  0.09023860096931458\n",
            "Loss S01:  0.08399835906245491\n",
            "Loss S2:  0.10091928866776553\n",
            "Loss S01:  0.08483476972296125\n",
            "Loss S2:  0.09913507068441027\n",
            "Loss S01:  0.08318139180060356\n",
            "Loss S2:  0.09731227664216872\n",
            "Loss S01:  0.08276670099031634\n",
            "Loss S2:  0.09641102737769848\n",
            "Loss S01:  0.0829745013924206\n",
            "Loss S2:  0.09556907167037328\n",
            "Loss S01:  0.0830836331502336\n",
            "Loss S2:  0.09580474481230876\n",
            "Loss S01:  0.0829618460905384\n",
            "Loss S2:  0.09560709688025461\n",
            "Loss S01:  0.08318620901784779\n",
            "Loss S2:  0.09568940673345401\n",
            "Loss S01:  0.08346517359489923\n",
            "Loss S2:  0.09576597924415882\n",
            "Loss S01:  0.08354108190477484\n",
            "Loss S2:  0.09552054608812427\n",
            "Loss S01:  0.08347797272978602\n",
            "Loss S2:  0.0952140279449858\n",
            "Loss S01:  0.08350280302861506\n",
            "Loss S2:  0.09521264441249784\n",
            "Loss S01:  0.08343369104252517\n",
            "Loss S2:  0.09549798645818507\n",
            "Loss S01:  0.08333348577961008\n",
            "Loss S2:  0.09555638761173749\n",
            "Loss S01:  0.08343097656373157\n",
            "Loss S2:  0.09564801290729977\n",
            "Loss S01:  0.08340501609426107\n",
            "Loss S2:  0.09547483754454192\n",
            "Loss S01:  0.08345007408432097\n",
            "Loss S2:  0.09548247439993753\n",
            "Loss S01:  0.08357066714631918\n",
            "Loss S2:  0.09551734040949227\n",
            "Loss S01:  0.08357865133210626\n",
            "Loss S2:  0.09549044603140566\n",
            "Loss S01:  0.08346103265214322\n",
            "Loss S2:  0.09551881980243607\n",
            "Loss S01:  0.0835299557560428\n",
            "Loss S2:  0.09559288688039327\n",
            "Loss S01:  0.08361241534959137\n",
            "Loss S2:  0.0957381695643809\n",
            "Loss S01:  0.08365502943853279\n",
            "Loss S2:  0.09586956871278358\n",
            "Loss S01:  0.08370724283313355\n",
            "Loss S2:  0.09601966727571369\n",
            "Loss S01:  0.08361764611357236\n",
            "Loss S2:  0.09597138196704873\n",
            "Loss S01:  0.08358228860344466\n",
            "Loss S2:  0.09591594455456826\n",
            "Loss S01:  0.08366873329745887\n",
            "Loss S2:  0.0958851598945491\n",
            "Loss S01:  0.08378631701142762\n",
            "Loss S2:  0.09595440890229046\n",
            "Loss S01:  0.0837929875580306\n",
            "Loss S2:  0.09585013125360627\n",
            "Loss S01:  0.083830196620816\n",
            "Loss S2:  0.09584305868592373\n",
            "Loss S01:  0.08366983745642413\n",
            "Loss S2:  0.09570639115725299\n",
            "Loss S01:  0.0837450842619685\n",
            "Loss S2:  0.09575604819155928\n",
            "Loss S01:  0.0836742681048788\n",
            "Loss S2:  0.0956934045358727\n",
            "Loss S01:  0.08368229107860246\n",
            "Loss S2:  0.09573717989967016\n",
            "Loss S01:  0.08364165001190625\n",
            "Loss S2:  0.09564760656353415\n",
            "Loss S01:  0.08360141722119085\n",
            "Loss S2:  0.0956204432447201\n",
            "Loss S01:  0.08352428688755575\n",
            "Loss S2:  0.09548431438095165\n",
            "Loss S01:  0.08344223111711462\n",
            "Loss S2:  0.09536275412310452\n",
            "Loss S01:  0.08334498326568042\n",
            "Loss S2:  0.0952188474557284\n",
            "Loss S01:  0.08334008894136125\n",
            "Loss S2:  0.09515027701854706\n",
            "Loss S01:  0.08325936685150848\n",
            "Loss S2:  0.09512392382552154\n",
            "Loss S01:  0.08334732268032155\n",
            "Loss S2:  0.09519261949985441\n",
            "Loss S01:  0.08330772931227275\n",
            "Loss S2:  0.09512066248190652\n",
            "Loss S01:  0.0833072274957114\n",
            "Loss S2:  0.0951654654748045\n",
            "Loss S01:  0.0832575465946927\n",
            "Loss S2:  0.09516642047732474\n",
            "Loss S01:  0.08328818292266117\n",
            "Loss S2:  0.09519474623712697\n",
            "Loss S01:  0.08325409112525847\n",
            "Loss S2:  0.0951358031347046\n",
            "Loss S01:  0.08325602849964789\n",
            "Loss S2:  0.09514490934766504\n",
            "Loss S01:  0.08319694120930071\n",
            "Loss S2:  0.0951134251570993\n",
            "Validation: \n",
            " Loss S01:  0.08337436616420746\n",
            " Loss S2:  0.13455362617969513\n",
            " Loss S01:  0.09560763906864893\n",
            " Loss S2:  0.14729789609000796\n",
            " Loss S01:  0.09383251681560423\n",
            " Loss S2:  0.14574341639512922\n",
            " Loss S01:  0.09228171652457753\n",
            " Loss S2:  0.14346001529302754\n",
            " Loss S01:  0.09153508201793388\n",
            " Loss S2:  0.1424766778026098\n",
            "\n",
            "Epoch: 18\n",
            "Loss S01:  0.08635689318180084\n",
            "Loss S2:  0.09736813604831696\n",
            "Loss S01:  0.08298230035738512\n",
            "Loss S2:  0.09684149040417238\n",
            "Loss S01:  0.08376908373265039\n",
            "Loss S2:  0.09511324656861168\n",
            "Loss S01:  0.08286663144826889\n",
            "Loss S2:  0.09357058617376512\n",
            "Loss S01:  0.08262267763294824\n",
            "Loss S2:  0.0930806420198301\n",
            "Loss S01:  0.08271743649361181\n",
            "Loss S2:  0.09313963660422493\n",
            "Loss S01:  0.0828055732074331\n",
            "Loss S2:  0.09373629484020296\n",
            "Loss S01:  0.08261226411436645\n",
            "Loss S2:  0.09352151808184637\n",
            "Loss S01:  0.08258507005226465\n",
            "Loss S2:  0.09354523854491151\n",
            "Loss S01:  0.0825000123663263\n",
            "Loss S2:  0.09369710775522086\n",
            "Loss S01:  0.0822340196902209\n",
            "Loss S2:  0.0931230543716119\n",
            "Loss S01:  0.08208179238948736\n",
            "Loss S2:  0.09293305612093694\n",
            "Loss S01:  0.08200391948469414\n",
            "Loss S2:  0.092832840609649\n",
            "Loss S01:  0.0820914720760957\n",
            "Loss S2:  0.09328500141624275\n",
            "Loss S01:  0.08210817021681062\n",
            "Loss S2:  0.09344394505023956\n",
            "Loss S01:  0.08207297399146668\n",
            "Loss S2:  0.09357604966653103\n",
            "Loss S01:  0.08181756327611318\n",
            "Loss S2:  0.09353986849881107\n",
            "Loss S01:  0.08176467959818087\n",
            "Loss S2:  0.09366192071758515\n",
            "Loss S01:  0.08185805273319476\n",
            "Loss S2:  0.0937864240773475\n",
            "Loss S01:  0.08183911910855958\n",
            "Loss S2:  0.093610084220689\n",
            "Loss S01:  0.08189905701733347\n",
            "Loss S2:  0.09375881529714337\n",
            "Loss S01:  0.0819077032013527\n",
            "Loss S2:  0.09377946220868007\n",
            "Loss S01:  0.08206189197922184\n",
            "Loss S2:  0.09387540436303454\n",
            "Loss S01:  0.08206734154776578\n",
            "Loss S2:  0.09395199156168735\n",
            "Loss S01:  0.08207780280424846\n",
            "Loss S2:  0.09404203156588978\n",
            "Loss S01:  0.08205287864958623\n",
            "Loss S2:  0.09398991261345457\n",
            "Loss S01:  0.08210189823218232\n",
            "Loss S2:  0.09392907530411906\n",
            "Loss S01:  0.08218887563781105\n",
            "Loss S2:  0.0938820807586297\n",
            "Loss S01:  0.08215469996912199\n",
            "Loss S2:  0.09392236904953723\n",
            "Loss S01:  0.08211638256139361\n",
            "Loss S2:  0.09388060132969696\n",
            "Loss S01:  0.08212912500993753\n",
            "Loss S2:  0.09384036086425433\n",
            "Loss S01:  0.08196498628502107\n",
            "Loss S2:  0.09377398988729121\n",
            "Loss S01:  0.08197312609428929\n",
            "Loss S2:  0.09382323995000477\n",
            "Loss S01:  0.0819221737026448\n",
            "Loss S2:  0.09368283479415399\n",
            "Loss S01:  0.08182083022209906\n",
            "Loss S2:  0.09366762738598407\n",
            "Loss S01:  0.08180095510095613\n",
            "Loss S2:  0.0935934053314717\n",
            "Loss S01:  0.08182414699541896\n",
            "Loss S2:  0.09351566852872721\n",
            "Loss S01:  0.08181003852674582\n",
            "Loss S2:  0.09341204146329926\n",
            "Loss S01:  0.08178697055171481\n",
            "Loss S2:  0.09334197268867743\n",
            "Loss S01:  0.08174495852511862\n",
            "Loss S2:  0.0931854236613759\n",
            "Loss S01:  0.08175947676944614\n",
            "Loss S2:  0.09316590469228359\n",
            "Loss S01:  0.08176265626130604\n",
            "Loss S2:  0.09317424382171492\n",
            "Loss S01:  0.08183061996930568\n",
            "Loss S2:  0.09323268408517656\n",
            "Loss S01:  0.08177024547849067\n",
            "Loss S2:  0.09317505141836861\n",
            "Loss S01:  0.08173931913784031\n",
            "Loss S2:  0.09318247489850807\n",
            "Loss S01:  0.08168338688540618\n",
            "Loss S2:  0.09313619771183462\n",
            "Loss S01:  0.08168671861969727\n",
            "Loss S2:  0.09314981067387504\n",
            "Loss S01:  0.08170990612666318\n",
            "Loss S2:  0.09318252469387277\n",
            "Loss S01:  0.0817021121292253\n",
            "Loss S2:  0.09314889564583555\n",
            "Loss S01:  0.08165959255151496\n",
            "Loss S2:  0.09316183315578644\n",
            "Validation: \n",
            " Loss S01:  0.0806315615773201\n",
            " Loss S2:  0.1247933954000473\n",
            " Loss S01:  0.0946094940106074\n",
            " Loss S2:  0.14202210236163365\n",
            " Loss S01:  0.09374657600391202\n",
            " Loss S2:  0.14103106045868338\n",
            " Loss S01:  0.09283606011847981\n",
            " Loss S2:  0.1390691827066609\n",
            " Loss S01:  0.09202130442416226\n",
            " Loss S2:  0.13845829610471372\n",
            "\n",
            "Epoch: 19\n",
            "Loss S01:  0.08138702809810638\n",
            "Loss S2:  0.08497370779514313\n",
            "Loss S01:  0.08331889388236133\n",
            "Loss S2:  0.09217959913340482\n",
            "Loss S01:  0.08334598087129139\n",
            "Loss S2:  0.09327915914001919\n",
            "Loss S01:  0.08106601334387256\n",
            "Loss S2:  0.09177915343353825\n",
            "Loss S01:  0.08065263999671471\n",
            "Loss S2:  0.09198367323090391\n",
            "Loss S01:  0.08053776855562247\n",
            "Loss S2:  0.09188809070517034\n",
            "Loss S01:  0.08094654769682494\n",
            "Loss S2:  0.09216786713385192\n",
            "Loss S01:  0.08094324916601181\n",
            "Loss S2:  0.09174016076074519\n",
            "Loss S01:  0.08087808381260177\n",
            "Loss S2:  0.09155640962683124\n",
            "Loss S01:  0.08054828062489792\n",
            "Loss S2:  0.09121732733079366\n",
            "Loss S01:  0.08021761366341373\n",
            "Loss S2:  0.090886222475236\n",
            "Loss S01:  0.08005565458589846\n",
            "Loss S2:  0.09066650519768397\n",
            "Loss S01:  0.08009528276348903\n",
            "Loss S2:  0.09094402797458585\n",
            "Loss S01:  0.08003751433077659\n",
            "Loss S2:  0.09123109110439097\n",
            "Loss S01:  0.07990735119327586\n",
            "Loss S2:  0.09125854930979141\n",
            "Loss S01:  0.07996097858378429\n",
            "Loss S2:  0.09119645333447993\n",
            "Loss S01:  0.0798682778240731\n",
            "Loss S2:  0.09089527089403283\n",
            "Loss S01:  0.07995095583256225\n",
            "Loss S2:  0.09089395486646229\n",
            "Loss S01:  0.08012250493903186\n",
            "Loss S2:  0.09093660976511339\n",
            "Loss S01:  0.08001501394035929\n",
            "Loss S2:  0.09094418152308589\n",
            "Loss S01:  0.07990800289075765\n",
            "Loss S2:  0.09100447194789772\n",
            "Loss S01:  0.0799465650136437\n",
            "Loss S2:  0.09104701484690346\n",
            "Loss S01:  0.08003548407986154\n",
            "Loss S2:  0.09113562825173814\n",
            "Loss S01:  0.08004661314028166\n",
            "Loss S2:  0.09114921854171919\n",
            "Loss S01:  0.08002712648686532\n",
            "Loss S2:  0.09126879930248893\n",
            "Loss S01:  0.08003874093412879\n",
            "Loss S2:  0.09138143994656217\n",
            "Loss S01:  0.08005435764789581\n",
            "Loss S2:  0.09134617533491945\n",
            "Loss S01:  0.08010656609306477\n",
            "Loss S2:  0.09140179768149703\n",
            "Loss S01:  0.08013270673378507\n",
            "Loss S2:  0.09144889597790945\n",
            "Loss S01:  0.08010726291494272\n",
            "Loss S2:  0.09140916550179937\n",
            "Loss S01:  0.08008502812381599\n",
            "Loss S2:  0.09143365429485359\n",
            "Loss S01:  0.07998333250882159\n",
            "Loss S2:  0.09135912250955006\n",
            "Loss S01:  0.0800427360651649\n",
            "Loss S2:  0.09146419559776597\n",
            "Loss S01:  0.08001566633266985\n",
            "Loss S2:  0.09140858033902695\n",
            "Loss S01:  0.07998363988315604\n",
            "Loss S2:  0.0914169270053637\n",
            "Loss S01:  0.0799584168399501\n",
            "Loss S2:  0.09137831469122161\n",
            "Loss S01:  0.07993691942946073\n",
            "Loss S2:  0.09136750165197657\n",
            "Loss S01:  0.0798572206071445\n",
            "Loss S2:  0.09125661892148684\n",
            "Loss S01:  0.07981058728350741\n",
            "Loss S2:  0.09112888439668446\n",
            "Loss S01:  0.07968227894943389\n",
            "Loss S2:  0.09094164038405698\n",
            "Loss S01:  0.07972524147898777\n",
            "Loss S2:  0.09091639554039796\n",
            "Loss S01:  0.0797232186961058\n",
            "Loss S2:  0.09097814708585576\n",
            "Loss S01:  0.07982650583260416\n",
            "Loss S2:  0.09108406970733135\n",
            "Loss S01:  0.07977035434229468\n",
            "Loss S2:  0.09107969697832785\n",
            "Loss S01:  0.07980876992078595\n",
            "Loss S2:  0.09115127431832744\n",
            "Loss S01:  0.07977840406799527\n",
            "Loss S2:  0.09113245468974907\n",
            "Loss S01:  0.07979226096203942\n",
            "Loss S2:  0.09114511427419046\n",
            "Loss S01:  0.07974769663848695\n",
            "Loss S2:  0.091108366151018\n",
            "Loss S01:  0.07973586228867827\n",
            "Loss S2:  0.09110593205616033\n",
            "Loss S01:  0.07971455640863255\n",
            "Loss S2:  0.0910336281459346\n",
            "Validation: \n",
            " Loss S01:  0.08442827314138412\n",
            " Loss S2:  0.12674687802791595\n",
            " Loss S01:  0.09554942732765562\n",
            " Loss S2:  0.14049886167049408\n",
            " Loss S01:  0.09471496449011128\n",
            " Loss S2:  0.13861623933402503\n",
            " Loss S01:  0.09369217934178524\n",
            " Loss S2:  0.1368017790258908\n",
            " Loss S01:  0.0928057242690781\n",
            " Loss S2:  0.1363390628018497\n",
            "\n",
            "Epoch: 20\n",
            "Loss S01:  0.08652421832084656\n",
            "Loss S2:  0.10198460519313812\n",
            "Loss S01:  0.08067765899679878\n",
            "Loss S2:  0.09499618614261801\n",
            "Loss S01:  0.079305880126499\n",
            "Loss S2:  0.09208407111111142\n",
            "Loss S01:  0.07909325486229311\n",
            "Loss S2:  0.09108303727642182\n",
            "Loss S01:  0.07867365875622122\n",
            "Loss S2:  0.09015669764541998\n",
            "Loss S01:  0.07966527269751418\n",
            "Loss S2:  0.0905826601619814\n",
            "Loss S01:  0.07991877247075566\n",
            "Loss S2:  0.09085985859397983\n",
            "Loss S01:  0.07969329999366277\n",
            "Loss S2:  0.0902981559785319\n",
            "Loss S01:  0.07959006395604876\n",
            "Loss S2:  0.09028532788341428\n",
            "Loss S01:  0.07935218480262128\n",
            "Loss S2:  0.09032874407021554\n",
            "Loss S01:  0.07896490144257498\n",
            "Loss S2:  0.08980179371515123\n",
            "Loss S01:  0.07881870771850552\n",
            "Loss S2:  0.08988128803871773\n",
            "Loss S01:  0.07860768022985498\n",
            "Loss S2:  0.0899023217849495\n",
            "Loss S01:  0.07871096976499521\n",
            "Loss S2:  0.09018312269494734\n",
            "Loss S01:  0.07882015293477275\n",
            "Loss S2:  0.09021263453343235\n",
            "Loss S01:  0.07912430559463848\n",
            "Loss S2:  0.09036096920635527\n",
            "Loss S01:  0.0790359411874543\n",
            "Loss S2:  0.0900736248918942\n",
            "Loss S01:  0.07903926181252936\n",
            "Loss S2:  0.08991569677117275\n",
            "Loss S01:  0.07924762757196611\n",
            "Loss S2:  0.08991010186586591\n",
            "Loss S01:  0.07916222982771734\n",
            "Loss S2:  0.0899060645496658\n",
            "Loss S01:  0.07912382767049235\n",
            "Loss S2:  0.08989042162302122\n",
            "Loss S01:  0.07920648508939133\n",
            "Loss S2:  0.08998066434080566\n",
            "Loss S01:  0.07927363043929117\n",
            "Loss S2:  0.0901849096919077\n",
            "Loss S01:  0.07929543026791507\n",
            "Loss S2:  0.09027507704444777\n",
            "Loss S01:  0.07927940586481351\n",
            "Loss S2:  0.09025507132789407\n",
            "Loss S01:  0.07934650729792526\n",
            "Loss S2:  0.09023814808681192\n",
            "Loss S01:  0.07932624185165227\n",
            "Loss S2:  0.09030724899179635\n",
            "Loss S01:  0.07937506683913104\n",
            "Loss S2:  0.0903052842155154\n",
            "Loss S01:  0.07945817349697347\n",
            "Loss S2:  0.09040546552460389\n",
            "Loss S01:  0.07953209118898381\n",
            "Loss S2:  0.09037026732238297\n",
            "Loss S01:  0.07952641107859802\n",
            "Loss S2:  0.0903521498968435\n",
            "Loss S01:  0.0794241628295164\n",
            "Loss S2:  0.09025799202765683\n",
            "Loss S01:  0.07950468341648764\n",
            "Loss S2:  0.09024882314268302\n",
            "Loss S01:  0.07936950328441905\n",
            "Loss S2:  0.09014172151999891\n",
            "Loss S01:  0.07935147325008496\n",
            "Loss S2:  0.09014819523106572\n",
            "Loss S01:  0.07929935774359947\n",
            "Loss S2:  0.0901054715329086\n",
            "Loss S01:  0.07920258784269361\n",
            "Loss S2:  0.09000829208920867\n",
            "Loss S01:  0.0791541810004377\n",
            "Loss S2:  0.08997527317136446\n",
            "Loss S01:  0.07906096026770712\n",
            "Loss S2:  0.08989403359689738\n",
            "Loss S01:  0.07894949261527842\n",
            "Loss S2:  0.08977909054597626\n",
            "Loss S01:  0.0789202454957433\n",
            "Loss S2:  0.08972431771327136\n",
            "Loss S01:  0.07890549631123125\n",
            "Loss S2:  0.08961646907811037\n",
            "Loss S01:  0.07893643999857178\n",
            "Loss S2:  0.08964907547852773\n",
            "Loss S01:  0.07884997465667204\n",
            "Loss S2:  0.08954855064988966\n",
            "Loss S01:  0.07882433855175432\n",
            "Loss S2:  0.08955187088643071\n",
            "Loss S01:  0.0787970611284551\n",
            "Loss S2:  0.08954334501879178\n",
            "Loss S01:  0.07878329536176555\n",
            "Loss S2:  0.08952900500656985\n",
            "Loss S01:  0.07877770926004508\n",
            "Loss S2:  0.08950949987028814\n",
            "Loss S01:  0.07877868070826709\n",
            "Loss S2:  0.08950519956756778\n",
            "Loss S01:  0.07872333199301708\n",
            "Loss S2:  0.0894552119637701\n",
            "Validation: \n",
            " Loss S01:  0.07446414977312088\n",
            " Loss S2:  0.12395896017551422\n",
            " Loss S01:  0.08716852430786405\n",
            " Loss S2:  0.14139688227857863\n",
            " Loss S01:  0.08567100236328637\n",
            " Loss S2:  0.13917575722060552\n",
            " Loss S01:  0.08458049546499721\n",
            " Loss S2:  0.13726916555009905\n",
            " Loss S01:  0.08394411610968319\n",
            " Loss S2:  0.1369381864313726\n",
            "\n",
            "Epoch: 21\n",
            "Loss S01:  0.07364737242460251\n",
            "Loss S2:  0.09381261467933655\n",
            "Loss S01:  0.08053518628532236\n",
            "Loss S2:  0.09180118549953807\n",
            "Loss S01:  0.07906786387874967\n",
            "Loss S2:  0.09016670499529157\n",
            "Loss S01:  0.07717553069514613\n",
            "Loss S2:  0.08853963546214565\n",
            "Loss S01:  0.07700467618500315\n",
            "Loss S2:  0.08866184840841991\n",
            "Loss S01:  0.07731797999026728\n",
            "Loss S2:  0.08879038501603931\n",
            "Loss S01:  0.07720586144533313\n",
            "Loss S2:  0.08916461834164917\n",
            "Loss S01:  0.0770631118769377\n",
            "Loss S2:  0.08856337391574617\n",
            "Loss S01:  0.07693270612278102\n",
            "Loss S2:  0.08818003489279452\n",
            "Loss S01:  0.07669555027406294\n",
            "Loss S2:  0.08829607642613925\n",
            "Loss S01:  0.07670554452308334\n",
            "Loss S2:  0.08825127964857782\n",
            "Loss S01:  0.07662660266096527\n",
            "Loss S2:  0.08825253836206488\n",
            "Loss S01:  0.07650439724449284\n",
            "Loss S2:  0.0881716439054032\n",
            "Loss S01:  0.07671971528129723\n",
            "Loss S2:  0.08820795546506198\n",
            "Loss S01:  0.0765812726730996\n",
            "Loss S2:  0.08808739324833484\n",
            "Loss S01:  0.07671709718885801\n",
            "Loss S2:  0.08803860418843908\n",
            "Loss S01:  0.07665670281333953\n",
            "Loss S2:  0.087978239662899\n",
            "Loss S01:  0.07659376841800952\n",
            "Loss S2:  0.08785198580974724\n",
            "Loss S01:  0.07659128705523291\n",
            "Loss S2:  0.08792495282974032\n",
            "Loss S01:  0.0766040671090181\n",
            "Loss S2:  0.08780669662808872\n",
            "Loss S01:  0.07654654275422072\n",
            "Loss S2:  0.08775930595931722\n",
            "Loss S01:  0.07670045654638119\n",
            "Loss S2:  0.087811132990071\n",
            "Loss S01:  0.07682834717348151\n",
            "Loss S2:  0.08808741273518601\n",
            "Loss S01:  0.07684711808766122\n",
            "Loss S2:  0.08805887920386864\n",
            "Loss S01:  0.07691270066618425\n",
            "Loss S2:  0.0881237060264433\n",
            "Loss S01:  0.07691891951508731\n",
            "Loss S2:  0.08809743429797579\n",
            "Loss S01:  0.07698276821680909\n",
            "Loss S2:  0.08815888814999226\n",
            "Loss S01:  0.07699821630967059\n",
            "Loss S2:  0.08811076626016645\n",
            "Loss S01:  0.07700125446205038\n",
            "Loss S2:  0.08817818872232878\n",
            "Loss S01:  0.07708012210237201\n",
            "Loss S2:  0.0881928549343368\n",
            "Loss S01:  0.07709087211131257\n",
            "Loss S2:  0.0882916260151768\n",
            "Loss S01:  0.07694887446533062\n",
            "Loss S2:  0.08813363179516562\n",
            "Loss S01:  0.07695551295518133\n",
            "Loss S2:  0.08809235531871563\n",
            "Loss S01:  0.0768732745706855\n",
            "Loss S2:  0.08798280583407583\n",
            "Loss S01:  0.0768445555634163\n",
            "Loss S2:  0.08794575575684523\n",
            "Loss S01:  0.07686212185526166\n",
            "Loss S2:  0.08795926322964182\n",
            "Loss S01:  0.07678617144885816\n",
            "Loss S2:  0.0879412032147854\n",
            "Loss S01:  0.076723306407183\n",
            "Loss S2:  0.08783241439701091\n",
            "Loss S01:  0.0766610798482194\n",
            "Loss S2:  0.08775492900312729\n",
            "Loss S01:  0.0766785411388063\n",
            "Loss S2:  0.08770119375966089\n",
            "Loss S01:  0.0767095696710589\n",
            "Loss S2:  0.08773971322988929\n",
            "Loss S01:  0.0766741207952627\n",
            "Loss S2:  0.08768377146726687\n",
            "Loss S01:  0.07673216973942419\n",
            "Loss S2:  0.0877498888240291\n",
            "Loss S01:  0.07672226109181092\n",
            "Loss S2:  0.08768127193099662\n",
            "Loss S01:  0.07667985855857261\n",
            "Loss S2:  0.08765141910909255\n",
            "Loss S01:  0.07663837965668702\n",
            "Loss S2:  0.08765888219067368\n",
            "Loss S01:  0.07667591138348921\n",
            "Loss S2:  0.0876579573478978\n",
            "Loss S01:  0.07671520688161729\n",
            "Loss S2:  0.08762292457930348\n",
            "Loss S01:  0.07673434953431825\n",
            "Loss S2:  0.08762556285037816\n",
            "Loss S01:  0.07671535087942834\n",
            "Loss S2:  0.08763669256705131\n",
            "Validation: \n",
            " Loss S01:  0.08145584166049957\n",
            " Loss S2:  0.13606970012187958\n",
            " Loss S01:  0.09169593843675795\n",
            " Loss S2:  0.14682429070983613\n",
            " Loss S01:  0.09025152082123407\n",
            " Loss S2:  0.1443692304012252\n",
            " Loss S01:  0.08912761655987286\n",
            " Loss S2:  0.1430222585064466\n",
            " Loss S01:  0.08827523169694124\n",
            " Loss S2:  0.14257705165648166\n",
            "\n",
            "Epoch: 22\n",
            "Loss S01:  0.07369846850633621\n",
            "Loss S2:  0.07938060909509659\n",
            "Loss S01:  0.07875806973739104\n",
            "Loss S2:  0.08765002272345802\n",
            "Loss S01:  0.07733149365300224\n",
            "Loss S2:  0.08689473143645696\n",
            "Loss S01:  0.07567030888411307\n",
            "Loss S2:  0.08619246151178114\n",
            "Loss S01:  0.07539720033727042\n",
            "Loss S2:  0.08614285889922119\n",
            "Loss S01:  0.07570986625026255\n",
            "Loss S2:  0.08644882385052886\n",
            "Loss S01:  0.07610431606652307\n",
            "Loss S2:  0.08672903588072198\n",
            "Loss S01:  0.07597867378466566\n",
            "Loss S2:  0.08629625573964186\n",
            "Loss S01:  0.07585410552996176\n",
            "Loss S2:  0.08620992045343658\n",
            "Loss S01:  0.07585170291937314\n",
            "Loss S2:  0.08618077504765856\n",
            "Loss S01:  0.07567667857845231\n",
            "Loss S2:  0.08600400602168376\n",
            "Loss S01:  0.07573710717596449\n",
            "Loss S2:  0.08594299785725705\n",
            "Loss S01:  0.07553055996367754\n",
            "Loss S2:  0.0857747274119992\n",
            "Loss S01:  0.07555595492474905\n",
            "Loss S2:  0.08574181362872815\n",
            "Loss S01:  0.07544262160329109\n",
            "Loss S2:  0.08583641628212962\n",
            "Loss S01:  0.0757935056277853\n",
            "Loss S2:  0.08610302195051647\n",
            "Loss S01:  0.07562307960220746\n",
            "Loss S2:  0.08599649055033737\n",
            "Loss S01:  0.07554151812753482\n",
            "Loss S2:  0.08597243350674534\n",
            "Loss S01:  0.07563552027825493\n",
            "Loss S2:  0.08598840335456047\n",
            "Loss S01:  0.07570186934155944\n",
            "Loss S2:  0.08595583326529457\n",
            "Loss S01:  0.0756369138053104\n",
            "Loss S2:  0.08604242178189814\n",
            "Loss S01:  0.07568356622522476\n",
            "Loss S2:  0.08603700977789847\n",
            "Loss S01:  0.07577188314693006\n",
            "Loss S2:  0.08602381349284186\n",
            "Loss S01:  0.07584472824678276\n",
            "Loss S2:  0.08610535177575561\n",
            "Loss S01:  0.07593756139711226\n",
            "Loss S2:  0.08625559363498728\n",
            "Loss S01:  0.07599184996044019\n",
            "Loss S2:  0.08623551333924213\n",
            "Loss S01:  0.0760196027028378\n",
            "Loss S2:  0.08626562476843253\n",
            "Loss S01:  0.07604392858503929\n",
            "Loss S2:  0.0862279218996143\n",
            "Loss S01:  0.07609748515529141\n",
            "Loss S2:  0.08631603788332583\n",
            "Loss S01:  0.07602141576073423\n",
            "Loss S2:  0.08635090550094127\n",
            "Loss S01:  0.07600308641593322\n",
            "Loss S2:  0.08636619838765293\n",
            "Loss S01:  0.07586018023623148\n",
            "Loss S2:  0.08626445447516978\n",
            "Loss S01:  0.0759117957146554\n",
            "Loss S2:  0.08639088403027377\n",
            "Loss S01:  0.0758931811369978\n",
            "Loss S2:  0.08635162310027644\n",
            "Loss S01:  0.07591175520656046\n",
            "Loss S2:  0.08640387551613217\n",
            "Loss S01:  0.07586808985582104\n",
            "Loss S2:  0.08639246121346102\n",
            "Loss S01:  0.07584100465323787\n",
            "Loss S2:  0.0863300604627073\n",
            "Loss S01:  0.07580434640422665\n",
            "Loss S2:  0.08627557146019049\n",
            "Loss S01:  0.0757884140813288\n",
            "Loss S2:  0.08619947884026474\n",
            "Loss S01:  0.07566234857186942\n",
            "Loss S2:  0.08606299024332514\n",
            "Loss S01:  0.0756942505421038\n",
            "Loss S2:  0.08606086466972371\n",
            "Loss S01:  0.07568108140878434\n",
            "Loss S2:  0.0860312273250009\n",
            "Loss S01:  0.07573141535465337\n",
            "Loss S2:  0.0861642305648525\n",
            "Loss S01:  0.07572361418065504\n",
            "Loss S2:  0.08611024643041695\n",
            "Loss S01:  0.07565477583346183\n",
            "Loss S2:  0.08605983534029552\n",
            "Loss S01:  0.07561064992273461\n",
            "Loss S2:  0.08602167670914444\n",
            "Loss S01:  0.0756484258136108\n",
            "Loss S2:  0.08610114624921773\n",
            "Loss S01:  0.07559936354431124\n",
            "Loss S2:  0.08606394790033879\n",
            "Loss S01:  0.07560033955284066\n",
            "Loss S2:  0.08604772774630426\n",
            "Loss S01:  0.07556946108273237\n",
            "Loss S2:  0.08603938004332017\n",
            "Validation: \n",
            " Loss S01:  0.07849053293466568\n",
            " Loss S2:  0.13112439215183258\n",
            " Loss S01:  0.08873505500100907\n",
            " Loss S2:  0.14257765454905375\n",
            " Loss S01:  0.08719792653147768\n",
            " Loss S2:  0.14047484281586436\n",
            " Loss S01:  0.08612460410985791\n",
            " Loss S2:  0.13856176653357802\n",
            " Loss S01:  0.0851893158238611\n",
            " Loss S2:  0.13799012470760463\n",
            "\n",
            "Epoch: 23\n",
            "Loss S01:  0.0758640244603157\n",
            "Loss S2:  0.09043022990226746\n",
            "Loss S01:  0.07671170207587155\n",
            "Loss S2:  0.08816896785389293\n",
            "Loss S01:  0.07564989974101384\n",
            "Loss S2:  0.08720271927969796\n",
            "Loss S01:  0.07465895373494394\n",
            "Loss S2:  0.08563158276580995\n",
            "Loss S01:  0.07406725380115392\n",
            "Loss S2:  0.08573077709936514\n",
            "Loss S01:  0.07443979565127223\n",
            "Loss S2:  0.08606608124340281\n",
            "Loss S01:  0.07496251342970817\n",
            "Loss S2:  0.08609654742186187\n",
            "Loss S01:  0.07457919308627156\n",
            "Loss S2:  0.08571195193159749\n",
            "Loss S01:  0.07461902586582267\n",
            "Loss S2:  0.08567372875081168\n",
            "Loss S01:  0.07442151980249437\n",
            "Loss S2:  0.08567030907987239\n",
            "Loss S01:  0.07439324846214587\n",
            "Loss S2:  0.08542012651958088\n",
            "Loss S01:  0.07408090982888196\n",
            "Loss S2:  0.08508225114227415\n",
            "Loss S01:  0.07409320503842733\n",
            "Loss S2:  0.08494267237087912\n",
            "Loss S01:  0.07415782196717408\n",
            "Loss S2:  0.08521128394676529\n",
            "Loss S01:  0.07418277857045755\n",
            "Loss S2:  0.08520333400220735\n",
            "Loss S01:  0.07434068109519434\n",
            "Loss S2:  0.08525643093104394\n",
            "Loss S01:  0.0742244324560121\n",
            "Loss S2:  0.08502006947253801\n",
            "Loss S01:  0.07420321922117507\n",
            "Loss S2:  0.08492865003863273\n",
            "Loss S01:  0.07426570242692752\n",
            "Loss S2:  0.08497422361406833\n",
            "Loss S01:  0.07424399964706437\n",
            "Loss S2:  0.08493296599200882\n",
            "Loss S01:  0.07430975226249861\n",
            "Loss S2:  0.08491170091267249\n",
            "Loss S01:  0.07436134134776784\n",
            "Loss S2:  0.08492158604035445\n",
            "Loss S01:  0.07457014319920971\n",
            "Loss S2:  0.08524508740567514\n",
            "Loss S01:  0.07456536760384386\n",
            "Loss S2:  0.08535650959520628\n",
            "Loss S01:  0.07466455974086686\n",
            "Loss S2:  0.08536603901519815\n",
            "Loss S01:  0.0746791564165596\n",
            "Loss S2:  0.08538829882543876\n",
            "Loss S01:  0.07472745062233845\n",
            "Loss S2:  0.08540456039809632\n",
            "Loss S01:  0.07473584650792318\n",
            "Loss S2:  0.08546906157063382\n",
            "Loss S01:  0.07477571819229482\n",
            "Loss S2:  0.08549040023012093\n",
            "Loss S01:  0.0748051744325669\n",
            "Loss S2:  0.08554974689926069\n",
            "Loss S01:  0.07481267676995046\n",
            "Loss S2:  0.08558362638237468\n",
            "Loss S01:  0.07469457761651067\n",
            "Loss S2:  0.08543359397212792\n",
            "Loss S01:  0.0747577203814857\n",
            "Loss S2:  0.08543968787910039\n",
            "Loss S01:  0.07475035236951448\n",
            "Loss S2:  0.08537920616184477\n",
            "Loss S01:  0.07473861100561807\n",
            "Loss S2:  0.08539555809539784\n",
            "Loss S01:  0.07466666723078812\n",
            "Loss S2:  0.08541344677196269\n",
            "Loss S01:  0.07461045626705703\n",
            "Loss S2:  0.08537984792132787\n",
            "Loss S01:  0.07458537780852974\n",
            "Loss S2:  0.08528519608582448\n",
            "Loss S01:  0.074556500149837\n",
            "Loss S2:  0.08529269646393658\n",
            "Loss S01:  0.07450045593787948\n",
            "Loss S2:  0.08513637500650742\n",
            "Loss S01:  0.07450330569559796\n",
            "Loss S2:  0.08512791220163764\n",
            "Loss S01:  0.0744908822750233\n",
            "Loss S2:  0.08514272820412967\n",
            "Loss S01:  0.07448067809864348\n",
            "Loss S2:  0.08512639034884559\n",
            "Loss S01:  0.07449265927283781\n",
            "Loss S2:  0.08509178442357587\n",
            "Loss S01:  0.07446192698278666\n",
            "Loss S2:  0.08510066266435615\n",
            "Loss S01:  0.07437946962114184\n",
            "Loss S2:  0.08505717922671142\n",
            "Loss S01:  0.07438637026123007\n",
            "Loss S2:  0.08510156250568894\n",
            "Loss S01:  0.07436148436332711\n",
            "Loss S2:  0.08507389927998724\n",
            "Loss S01:  0.07437590892250473\n",
            "Loss S2:  0.08505361567477922\n",
            "Loss S01:  0.0743686652414182\n",
            "Loss S2:  0.08505639367093865\n",
            "Validation: \n",
            " Loss S01:  0.08112431317567825\n",
            " Loss S2:  0.12747535109519958\n",
            " Loss S01:  0.0908456448288191\n",
            " Loss S2:  0.1420061659245264\n",
            " Loss S01:  0.08992476416070287\n",
            " Loss S2:  0.14010535107880104\n",
            " Loss S01:  0.08901323121590693\n",
            " Loss S2:  0.13818582501567778\n",
            " Loss S01:  0.088211431455465\n",
            " Loss S2:  0.1377217510232219\n",
            "\n",
            "Epoch: 24\n",
            "Loss S01:  0.07404279708862305\n",
            "Loss S2:  0.08598059415817261\n",
            "Loss S01:  0.07560171525586735\n",
            "Loss S2:  0.08676922050389377\n",
            "Loss S01:  0.07549162244512922\n",
            "Loss S2:  0.085267383427847\n",
            "Loss S01:  0.07451554532012632\n",
            "Loss S2:  0.08366711101224346\n",
            "Loss S01:  0.07417556807035353\n",
            "Loss S2:  0.08346803540863641\n",
            "Loss S01:  0.07477130171130686\n",
            "Loss S2:  0.08405634556330886\n",
            "Loss S01:  0.07468627845166159\n",
            "Loss S2:  0.08422114675650831\n",
            "Loss S01:  0.07459102717923446\n",
            "Loss S2:  0.0840180997907276\n",
            "Loss S01:  0.07442364823303105\n",
            "Loss S2:  0.0837859571164037\n",
            "Loss S01:  0.07437271976863945\n",
            "Loss S2:  0.08394923519629699\n",
            "Loss S01:  0.07408143661104806\n",
            "Loss S2:  0.08331687541881411\n",
            "Loss S01:  0.07380310983002723\n",
            "Loss S2:  0.0830869443781741\n",
            "Loss S01:  0.07362183996222237\n",
            "Loss S2:  0.08304125532384746\n",
            "Loss S01:  0.0736025110569619\n",
            "Loss S2:  0.08328304348783638\n",
            "Loss S01:  0.07349013325804514\n",
            "Loss S2:  0.08334939916294518\n",
            "Loss S01:  0.0735057308677806\n",
            "Loss S2:  0.08333304514553373\n",
            "Loss S01:  0.07349592070124164\n",
            "Loss S2:  0.08334666027785828\n",
            "Loss S01:  0.07338754094222136\n",
            "Loss S2:  0.08325672811932033\n",
            "Loss S01:  0.07334776416173956\n",
            "Loss S2:  0.08339334139343124\n",
            "Loss S01:  0.07321801832365116\n",
            "Loss S2:  0.08330790866263874\n",
            "Loss S01:  0.07326750160746313\n",
            "Loss S2:  0.08339566741120163\n",
            "Loss S01:  0.0733942006719056\n",
            "Loss S2:  0.08361996032332922\n",
            "Loss S01:  0.07355130443610756\n",
            "Loss S2:  0.08381576964218693\n",
            "Loss S01:  0.07361776633309079\n",
            "Loss S2:  0.08384261993102697\n",
            "Loss S01:  0.07364748708934704\n",
            "Loss S2:  0.08389056953157133\n",
            "Loss S01:  0.07362738159666973\n",
            "Loss S2:  0.08390568455020744\n",
            "Loss S01:  0.07371334497499284\n",
            "Loss S2:  0.08387528799502786\n",
            "Loss S01:  0.07375525537452135\n",
            "Loss S2:  0.08392155613076643\n",
            "Loss S01:  0.07377227793490759\n",
            "Loss S2:  0.08390627244613348\n",
            "Loss S01:  0.07377106819263439\n",
            "Loss S2:  0.0839705474905132\n",
            "Loss S01:  0.07383645849419987\n",
            "Loss S2:  0.08408572321813368\n",
            "Loss S01:  0.07367431809616626\n",
            "Loss S2:  0.0840402217971167\n",
            "Loss S01:  0.07373760305229006\n",
            "Loss S2:  0.08411417829563313\n",
            "Loss S01:  0.07368521517272082\n",
            "Loss S2:  0.08401579851020138\n",
            "Loss S01:  0.07367729586808562\n",
            "Loss S2:  0.08404102423323914\n",
            "Loss S01:  0.07365701287093325\n",
            "Loss S2:  0.0840215792301034\n",
            "Loss S01:  0.07363043246672094\n",
            "Loss S2:  0.08393193950613449\n",
            "Loss S01:  0.07358108766156066\n",
            "Loss S2:  0.08390024087499737\n",
            "Loss S01:  0.07351604043420532\n",
            "Loss S2:  0.08384773712067467\n",
            "Loss S01:  0.07342477196165363\n",
            "Loss S2:  0.08365393329002059\n",
            "Loss S01:  0.07343729538959161\n",
            "Loss S2:  0.0836080214626474\n",
            "Loss S01:  0.073402525571576\n",
            "Loss S2:  0.0835911121101565\n",
            "Loss S01:  0.07338420476879473\n",
            "Loss S2:  0.08364273066192228\n",
            "Loss S01:  0.07334495173322075\n",
            "Loss S2:  0.0835880896988435\n",
            "Loss S01:  0.07331453159012222\n",
            "Loss S2:  0.08358735322546797\n",
            "Loss S01:  0.07330354691344725\n",
            "Loss S2:  0.08360166487566913\n",
            "Loss S01:  0.07332747284153278\n",
            "Loss S2:  0.08357957579243985\n",
            "Loss S01:  0.07332497718101348\n",
            "Loss S2:  0.08357814837033582\n",
            "Loss S01:  0.07333050074981305\n",
            "Loss S2:  0.08360324161835866\n",
            "Loss S01:  0.07330000573530217\n",
            "Loss S2:  0.08359292859700936\n",
            "Validation: \n",
            " Loss S01:  0.07705047726631165\n",
            " Loss S2:  0.11781181395053864\n",
            " Loss S01:  0.08556801116182691\n",
            " Loss S2:  0.13655706566004527\n",
            " Loss S01:  0.08380515728054977\n",
            " Loss S2:  0.13558726830453408\n",
            " Loss S01:  0.08277868003141685\n",
            " Loss S2:  0.1340022335042719\n",
            " Loss S01:  0.08208561099973725\n",
            " Loss S2:  0.13344563083884156\n",
            "\n",
            "Epoch: 25\n",
            "Loss S01:  0.06462181359529495\n",
            "Loss S2:  0.08118534088134766\n",
            "Loss S01:  0.0759098929437724\n",
            "Loss S2:  0.08476334607059305\n",
            "Loss S01:  0.07529566224132266\n",
            "Loss S2:  0.0838759293158849\n",
            "Loss S01:  0.07399318451362272\n",
            "Loss S2:  0.08254264823852046\n",
            "Loss S01:  0.07356423439412582\n",
            "Loss S2:  0.08204432649583351\n",
            "Loss S01:  0.073435290113968\n",
            "Loss S2:  0.0821664456058951\n",
            "Loss S01:  0.07365773025839055\n",
            "Loss S2:  0.08277013714684815\n",
            "Loss S01:  0.07355657571428259\n",
            "Loss S2:  0.08268975183157853\n",
            "Loss S01:  0.07336684523357286\n",
            "Loss S2:  0.08276359221817535\n",
            "Loss S01:  0.07325887323899584\n",
            "Loss S2:  0.08273298538975664\n",
            "Loss S01:  0.07314664955333908\n",
            "Loss S2:  0.08239753219750848\n",
            "Loss S01:  0.07298743731535233\n",
            "Loss S2:  0.08228724637815545\n",
            "Loss S01:  0.07273105276394481\n",
            "Loss S2:  0.08208600068387907\n",
            "Loss S01:  0.07263510739644065\n",
            "Loss S2:  0.08223556471234969\n",
            "Loss S01:  0.07261975841424989\n",
            "Loss S2:  0.08222635032226008\n",
            "Loss S01:  0.07268574737653827\n",
            "Loss S2:  0.08254467394967742\n",
            "Loss S01:  0.0726342227610742\n",
            "Loss S2:  0.08241771675229813\n",
            "Loss S01:  0.07259071997383185\n",
            "Loss S2:  0.08232572862105063\n",
            "Loss S01:  0.07263133695441715\n",
            "Loss S2:  0.08241645110904841\n",
            "Loss S01:  0.07262623561692487\n",
            "Loss S2:  0.08231132387802863\n",
            "Loss S01:  0.0727169506844893\n",
            "Loss S2:  0.08246528194170093\n",
            "Loss S01:  0.0727785833321194\n",
            "Loss S2:  0.08243738259608147\n",
            "Loss S01:  0.0728601433539013\n",
            "Loss S2:  0.08254194475406974\n",
            "Loss S01:  0.07288744615076424\n",
            "Loss S2:  0.08260259370912205\n",
            "Loss S01:  0.07293940058820475\n",
            "Loss S2:  0.08263480351055312\n",
            "Loss S01:  0.07289131612060555\n",
            "Loss S2:  0.08264466321682075\n",
            "Loss S01:  0.07295962839861939\n",
            "Loss S2:  0.08274975504683352\n",
            "Loss S01:  0.07290005538186464\n",
            "Loss S2:  0.08274016471586544\n",
            "Loss S01:  0.07291259666990979\n",
            "Loss S2:  0.08275669248396816\n",
            "Loss S01:  0.07295905878044076\n",
            "Loss S2:  0.082739000954374\n",
            "Loss S01:  0.07295089977425198\n",
            "Loss S2:  0.08275240184460764\n",
            "Loss S01:  0.07286317657284032\n",
            "Loss S2:  0.08266499537938661\n",
            "Loss S01:  0.07286296950833077\n",
            "Loss S2:  0.08267220255927504\n",
            "Loss S01:  0.07285104661594705\n",
            "Loss S2:  0.08260315773710383\n",
            "Loss S01:  0.07281177848184213\n",
            "Loss S2:  0.08259707348437603\n",
            "Loss S01:  0.07282899628974433\n",
            "Loss S2:  0.0825861669079191\n",
            "Loss S01:  0.07279025871626559\n",
            "Loss S2:  0.08251471631744892\n",
            "Loss S01:  0.07281452303907943\n",
            "Loss S2:  0.08244692222930672\n",
            "Loss S01:  0.07277586763807795\n",
            "Loss S2:  0.08248133601479017\n",
            "Loss S01:  0.07270672062740606\n",
            "Loss S2:  0.08235937400775797\n",
            "Loss S01:  0.0726662029110434\n",
            "Loss S2:  0.08232162398590412\n",
            "Loss S01:  0.07259293721757666\n",
            "Loss S2:  0.0822927628047855\n",
            "Loss S01:  0.07261190871374058\n",
            "Loss S2:  0.08237841705882351\n",
            "Loss S01:  0.07257953422736126\n",
            "Loss S2:  0.08236100764949471\n",
            "Loss S01:  0.07254398725673455\n",
            "Loss S2:  0.08243224159930569\n",
            "Loss S01:  0.07253607314600384\n",
            "Loss S2:  0.0824137259514263\n",
            "Loss S01:  0.07250801730828275\n",
            "Loss S2:  0.08243203993862466\n",
            "Loss S01:  0.07247107456222715\n",
            "Loss S2:  0.0824208900237539\n",
            "Loss S01:  0.07245283808535945\n",
            "Loss S2:  0.0823987991299302\n",
            "Loss S01:  0.07245149599225117\n",
            "Loss S2:  0.08236836059333837\n",
            "Validation: \n",
            " Loss S01:  0.0747312530875206\n",
            " Loss S2:  0.1345931738615036\n",
            " Loss S01:  0.08463439097007115\n",
            " Loss S2:  0.15016756384145646\n",
            " Loss S01:  0.08377034507873582\n",
            " Loss S2:  0.14827848716479977\n",
            " Loss S01:  0.08284766825496173\n",
            " Loss S2:  0.14632228029067398\n",
            " Loss S01:  0.08205341391357375\n",
            " Loss S2:  0.14587498401050214\n",
            "\n",
            "Epoch: 26\n",
            "Loss S01:  0.07172902673482895\n",
            "Loss S2:  0.07937131077051163\n",
            "Loss S01:  0.07395514439452779\n",
            "Loss S2:  0.08563773875886743\n",
            "Loss S01:  0.0745528947029795\n",
            "Loss S2:  0.08506084481875102\n",
            "Loss S01:  0.07254044579402093\n",
            "Loss S2:  0.08271483187713931\n",
            "Loss S01:  0.07245604621201027\n",
            "Loss S2:  0.08212073710633487\n",
            "Loss S01:  0.07208229469902375\n",
            "Loss S2:  0.08208548073090759\n",
            "Loss S01:  0.07210846538426446\n",
            "Loss S2:  0.08222116016950763\n",
            "Loss S01:  0.07182667729720263\n",
            "Loss S2:  0.08211315402262648\n",
            "Loss S01:  0.07188574445468408\n",
            "Loss S2:  0.08203588876827264\n",
            "Loss S01:  0.0717661878155483\n",
            "Loss S2:  0.08189500524447514\n",
            "Loss S01:  0.07141673096483296\n",
            "Loss S2:  0.0815154651899149\n",
            "Loss S01:  0.07103357672154366\n",
            "Loss S2:  0.08127322497668567\n",
            "Loss S01:  0.07099350963619129\n",
            "Loss S2:  0.08117892660877922\n",
            "Loss S01:  0.0711158481319897\n",
            "Loss S2:  0.08157601093745413\n",
            "Loss S01:  0.07106716272995826\n",
            "Loss S2:  0.08162643792147332\n",
            "Loss S01:  0.07122987363216103\n",
            "Loss S2:  0.08173278852409085\n",
            "Loss S01:  0.0711246819599815\n",
            "Loss S2:  0.08161460432391729\n",
            "Loss S01:  0.07117148841682233\n",
            "Loss S2:  0.08140997863129566\n",
            "Loss S01:  0.0712184635973767\n",
            "Loss S2:  0.08135558166721249\n",
            "Loss S01:  0.07119763256367588\n",
            "Loss S2:  0.08136726670084199\n",
            "Loss S01:  0.07112573120576232\n",
            "Loss S2:  0.08143566007637859\n",
            "Loss S01:  0.07113979368413229\n",
            "Loss S2:  0.08140074415794481\n",
            "Loss S01:  0.0713384908316362\n",
            "Loss S2:  0.08155036916560178\n",
            "Loss S01:  0.07139542492437156\n",
            "Loss S2:  0.08160213790672682\n",
            "Loss S01:  0.07146597789654593\n",
            "Loss S2:  0.08167894855450793\n",
            "Loss S01:  0.07151097296718582\n",
            "Loss S2:  0.08172211243930566\n",
            "Loss S01:  0.07154206163695946\n",
            "Loss S2:  0.08175006982695554\n",
            "Loss S01:  0.07153204757019163\n",
            "Loss S2:  0.08169117817359657\n",
            "Loss S01:  0.07160278593995394\n",
            "Loss S2:  0.08169018601819714\n",
            "Loss S01:  0.07159117922903746\n",
            "Loss S2:  0.08173796777770281\n",
            "Loss S01:  0.07159160446387985\n",
            "Loss S2:  0.08168219856645578\n",
            "Loss S01:  0.07149986112999379\n",
            "Loss S2:  0.08163483783458973\n",
            "Loss S01:  0.07158343168991006\n",
            "Loss S2:  0.08164732622394681\n",
            "Loss S01:  0.07154074552105993\n",
            "Loss S2:  0.08160619311642431\n",
            "Loss S01:  0.07146859667168334\n",
            "Loss S2:  0.08152434063796773\n",
            "Loss S01:  0.07145145293484387\n",
            "Loss S2:  0.08146338818589506\n",
            "Loss S01:  0.07133230665012409\n",
            "Loss S2:  0.08140117165289427\n",
            "Loss S01:  0.07124686137725401\n",
            "Loss S2:  0.0813034197913026\n",
            "Loss S01:  0.07118012581089038\n",
            "Loss S2:  0.08121745515213864\n",
            "Loss S01:  0.07112028427860316\n",
            "Loss S2:  0.08117037317941865\n",
            "Loss S01:  0.07111144071430935\n",
            "Loss S2:  0.08119366435041452\n",
            "Loss S01:  0.07104805747269134\n",
            "Loss S2:  0.08114810980439476\n",
            "Loss S01:  0.071050569105389\n",
            "Loss S2:  0.0811840712740982\n",
            "Loss S01:  0.07100585386537343\n",
            "Loss S2:  0.08113490845446243\n",
            "Loss S01:  0.07097324176507742\n",
            "Loss S2:  0.08114728159239502\n",
            "Loss S01:  0.07095680550203355\n",
            "Loss S2:  0.08118542320117718\n",
            "Loss S01:  0.07093618400475725\n",
            "Loss S2:  0.08121376807868093\n",
            "Loss S01:  0.07090442729698625\n",
            "Loss S2:  0.08119408694548719\n",
            "Loss S01:  0.07086853437481948\n",
            "Loss S2:  0.08117184741519345\n",
            "Loss S01:  0.0708671801338492\n",
            "Loss S2:  0.08118158020458494\n",
            "Validation: \n",
            " Loss S01:  0.08062570542097092\n",
            " Loss S2:  0.12702487409114838\n",
            " Loss S01:  0.08618625182481039\n",
            " Loss S2:  0.1420293872555097\n",
            " Loss S01:  0.08482299745082855\n",
            " Loss S2:  0.14088082549775519\n",
            " Loss S01:  0.08393278021792897\n",
            " Loss S2:  0.13899100327589473\n",
            " Loss S01:  0.08329655818733168\n",
            " Loss S2:  0.13843515202587034\n",
            "\n",
            "Epoch: 27\n",
            "Loss S01:  0.06941533088684082\n",
            "Loss S2:  0.0817350372672081\n",
            "Loss S01:  0.07425499368797649\n",
            "Loss S2:  0.08476849577643654\n",
            "Loss S01:  0.07261201703832262\n",
            "Loss S2:  0.08191753675540288\n",
            "Loss S01:  0.0719947908674517\n",
            "Loss S2:  0.080210056516432\n",
            "Loss S01:  0.07203098004911004\n",
            "Loss S2:  0.08000049376633109\n",
            "Loss S01:  0.07198865507163253\n",
            "Loss S2:  0.0796212166839955\n",
            "Loss S01:  0.07185808740189818\n",
            "Loss S2:  0.07998364711882638\n",
            "Loss S01:  0.07144042461271018\n",
            "Loss S2:  0.08002229821933828\n",
            "Loss S01:  0.07127464415482533\n",
            "Loss S2:  0.08011207168484911\n",
            "Loss S01:  0.07098143089276093\n",
            "Loss S2:  0.08043446015198152\n",
            "Loss S01:  0.07088916899986786\n",
            "Loss S2:  0.08024392564698021\n",
            "Loss S01:  0.07071364694484719\n",
            "Loss S2:  0.08018065774225974\n",
            "Loss S01:  0.0705088037104646\n",
            "Loss S2:  0.08015982468019832\n",
            "Loss S01:  0.0705025271149992\n",
            "Loss S2:  0.08033533388649235\n",
            "Loss S01:  0.07034870802510715\n",
            "Loss S2:  0.08040715738179836\n",
            "Loss S01:  0.0705013018076783\n",
            "Loss S2:  0.08048657426573583\n",
            "Loss S01:  0.07044111837789138\n",
            "Loss S2:  0.08025374203365042\n",
            "Loss S01:  0.07023168934716119\n",
            "Loss S2:  0.08000888235387746\n",
            "Loss S01:  0.07030858626352489\n",
            "Loss S2:  0.08001663926885931\n",
            "Loss S01:  0.07032987492948926\n",
            "Loss S2:  0.07997756069087233\n",
            "Loss S01:  0.07033686990390962\n",
            "Loss S2:  0.07995481729803987\n",
            "Loss S01:  0.07043084149112068\n",
            "Loss S2:  0.07999413750041717\n",
            "Loss S01:  0.07064022079986684\n",
            "Loss S2:  0.08014957144923879\n",
            "Loss S01:  0.07061768947290135\n",
            "Loss S2:  0.08017637790281536\n",
            "Loss S01:  0.07064690597015298\n",
            "Loss S2:  0.08022454220229659\n",
            "Loss S01:  0.07062337801335343\n",
            "Loss S2:  0.08031070078036699\n",
            "Loss S01:  0.07055923018704428\n",
            "Loss S2:  0.08023412511380697\n",
            "Loss S01:  0.07066710597089736\n",
            "Loss S2:  0.08030238376442357\n",
            "Loss S01:  0.07067584603017335\n",
            "Loss S2:  0.0803313259552382\n",
            "Loss S01:  0.07075095218942337\n",
            "Loss S2:  0.08029733628956313\n",
            "Loss S01:  0.0706865358045727\n",
            "Loss S2:  0.08025171741397674\n",
            "Loss S01:  0.07061297858547168\n",
            "Loss S2:  0.08019663180281494\n",
            "Loss S01:  0.07065017224286575\n",
            "Loss S2:  0.08023666892189103\n",
            "Loss S01:  0.07063939309822469\n",
            "Loss S2:  0.08014687215993772\n",
            "Loss S01:  0.07061302170288528\n",
            "Loss S2:  0.08015252787434111\n",
            "Loss S01:  0.0706247540556977\n",
            "Loss S2:  0.08016429877603835\n",
            "Loss S01:  0.07055311960650613\n",
            "Loss S2:  0.08009485677503814\n",
            "Loss S01:  0.07053122419432167\n",
            "Loss S2:  0.08008886829540736\n",
            "Loss S01:  0.07051157324606665\n",
            "Loss S2:  0.0800539516401416\n",
            "Loss S01:  0.07045425366981865\n",
            "Loss S2:  0.07991515721201592\n",
            "Loss S01:  0.07052866706712882\n",
            "Loss S2:  0.07995908179708253\n",
            "Loss S01:  0.0705081174227153\n",
            "Loss S2:  0.0799431380274232\n",
            "Loss S01:  0.07054100605059689\n",
            "Loss S2:  0.07997495942968087\n",
            "Loss S01:  0.0704849585372168\n",
            "Loss S2:  0.07992633316328797\n",
            "Loss S01:  0.07045227884739434\n",
            "Loss S2:  0.07998058482024675\n",
            "Loss S01:  0.0704400083938089\n",
            "Loss S2:  0.08002239477277595\n",
            "Loss S01:  0.07043780139980761\n",
            "Loss S2:  0.0800367615422044\n",
            "Loss S01:  0.07043471257746094\n",
            "Loss S2:  0.0800215539121071\n",
            "Loss S01:  0.0704684772774608\n",
            "Loss S2:  0.08004703258092587\n",
            "Loss S01:  0.07048005676712621\n",
            "Loss S2:  0.0800836344843241\n",
            "Validation: \n",
            " Loss S01:  0.08117113262414932\n",
            " Loss S2:  0.12813778221607208\n",
            " Loss S01:  0.08458018764143899\n",
            " Loss S2:  0.1405418703243846\n",
            " Loss S01:  0.08271614026005675\n",
            " Loss S2:  0.13867469131946564\n",
            " Loss S01:  0.08145879221255661\n",
            " Loss S2:  0.1365021818973979\n",
            " Loss S01:  0.08082576785926465\n",
            " Loss S2:  0.13595154062833315\n",
            "\n",
            "Epoch: 28\n",
            "Loss S01:  0.06546039134263992\n",
            "Loss S2:  0.08189190924167633\n",
            "Loss S01:  0.07256902788173068\n",
            "Loss S2:  0.08414498310197484\n",
            "Loss S01:  0.07149758065740268\n",
            "Loss S2:  0.08373269367785681\n",
            "Loss S01:  0.07041092481343977\n",
            "Loss S2:  0.08169020544136724\n",
            "Loss S01:  0.06967633099454205\n",
            "Loss S2:  0.08122606975276296\n",
            "Loss S01:  0.06952724963718769\n",
            "Loss S2:  0.08049410464716893\n",
            "Loss S01:  0.06964510630388729\n",
            "Loss S2:  0.08021194353455403\n",
            "Loss S01:  0.06926212249927118\n",
            "Loss S2:  0.07962488603424019\n",
            "Loss S01:  0.069353665696618\n",
            "Loss S2:  0.07949228823920827\n",
            "Loss S01:  0.06929412074796445\n",
            "Loss S2:  0.07935547345614695\n",
            "Loss S01:  0.06922325067738495\n",
            "Loss S2:  0.07917957354595165\n",
            "Loss S01:  0.06905099816687472\n",
            "Loss S2:  0.0789829705346812\n",
            "Loss S01:  0.06888864666592977\n",
            "Loss S2:  0.07896723715234394\n",
            "Loss S01:  0.06910727110527853\n",
            "Loss S2:  0.07916138969532406\n",
            "Loss S01:  0.06917324260616979\n",
            "Loss S2:  0.07935773660527899\n",
            "Loss S01:  0.06933232337631137\n",
            "Loss S2:  0.07955868882651361\n",
            "Loss S01:  0.06941610778340641\n",
            "Loss S2:  0.07960259238755481\n",
            "Loss S01:  0.06933944227925518\n",
            "Loss S2:  0.07943613870799193\n",
            "Loss S01:  0.06930340896489212\n",
            "Loss S2:  0.07934727001091393\n",
            "Loss S01:  0.06923647185894831\n",
            "Loss S2:  0.07930871290374177\n",
            "Loss S01:  0.06918665038338348\n",
            "Loss S2:  0.07933419590713967\n",
            "Loss S01:  0.06922100207181338\n",
            "Loss S2:  0.07933174695166366\n",
            "Loss S01:  0.06928860151727276\n",
            "Loss S2:  0.07950084002444108\n",
            "Loss S01:  0.06935941816601919\n",
            "Loss S2:  0.07955357971000465\n",
            "Loss S01:  0.06939739469608826\n",
            "Loss S2:  0.07951960081999727\n",
            "Loss S01:  0.069409591431518\n",
            "Loss S2:  0.07952941428736386\n",
            "Loss S01:  0.06955027239133114\n",
            "Loss S2:  0.07953514781034769\n",
            "Loss S01:  0.06960668735123648\n",
            "Loss S2:  0.0796006413183529\n",
            "Loss S01:  0.06960914215053103\n",
            "Loss S2:  0.07959997839562834\n",
            "Loss S01:  0.0695740961280885\n",
            "Loss S2:  0.0794591887044333\n",
            "Loss S01:  0.06957493822687488\n",
            "Loss S2:  0.07946476758615519\n",
            "Loss S01:  0.06948799018259984\n",
            "Loss S2:  0.07943083738782399\n",
            "Loss S01:  0.06950799667064646\n",
            "Loss S2:  0.0794359133854462\n",
            "Loss S01:  0.06947160556102087\n",
            "Loss S2:  0.07926207413936309\n",
            "Loss S01:  0.06942454892650378\n",
            "Loss S2:  0.07921693179090002\n",
            "Loss S01:  0.06941783495056324\n",
            "Loss S2:  0.0792161536182773\n",
            "Loss S01:  0.0693680214233841\n",
            "Loss S2:  0.07914567402360181\n",
            "Loss S01:  0.06933306308207166\n",
            "Loss S2:  0.0791281549877555\n",
            "Loss S01:  0.06930904657509071\n",
            "Loss S2:  0.07908409429112757\n",
            "Loss S01:  0.06923292481990727\n",
            "Loss S2:  0.07897649854040512\n",
            "Loss S01:  0.06926382096145219\n",
            "Loss S2:  0.07897842452487445\n",
            "Loss S01:  0.06919550621959125\n",
            "Loss S2:  0.07893791299884337\n",
            "Loss S01:  0.06920492661466508\n",
            "Loss S2:  0.07896298908068845\n",
            "Loss S01:  0.06917008987504755\n",
            "Loss S2:  0.07893923706355616\n",
            "Loss S01:  0.06913147046486266\n",
            "Loss S2:  0.07895701660726076\n",
            "Loss S01:  0.06912943184739206\n",
            "Loss S2:  0.07894304903542124\n",
            "Loss S01:  0.06916801315487853\n",
            "Loss S2:  0.07896424005709088\n",
            "Loss S01:  0.06914232540757033\n",
            "Loss S2:  0.0789385297495848\n",
            "Loss S01:  0.0691024250817522\n",
            "Loss S2:  0.07892108632720186\n",
            "Loss S01:  0.06907484474802454\n",
            "Loss S2:  0.07890980397853735\n",
            "Validation: \n",
            " Loss S01:  0.07763028144836426\n",
            " Loss S2:  0.11838420480489731\n",
            " Loss S01:  0.08596461904900414\n",
            " Loss S2:  0.13150404287236078\n",
            " Loss S01:  0.08464871655877043\n",
            " Loss S2:  0.13065605065444622\n",
            " Loss S01:  0.08382933403624863\n",
            " Loss S2:  0.12843787047218103\n",
            " Loss S01:  0.08307450687811699\n",
            " Loss S2:  0.1279598453346594\n",
            "\n",
            "Epoch: 29\n",
            "Loss S01:  0.06853457540273666\n",
            "Loss S2:  0.07886862754821777\n",
            "Loss S01:  0.07179824262857437\n",
            "Loss S2:  0.08021761206063358\n",
            "Loss S01:  0.07080192633327984\n",
            "Loss S2:  0.07986001741318476\n",
            "Loss S01:  0.06978932167253186\n",
            "Loss S2:  0.07850279995510655\n",
            "Loss S01:  0.06920834848793542\n",
            "Loss S2:  0.07806758310009794\n",
            "Loss S01:  0.0689687102886976\n",
            "Loss S2:  0.07780317071021772\n",
            "Loss S01:  0.06917082512232124\n",
            "Loss S2:  0.07826560637989982\n",
            "Loss S01:  0.06921484557465768\n",
            "Loss S2:  0.07775231713140515\n",
            "Loss S01:  0.06905586810575591\n",
            "Loss S2:  0.07754202508999977\n",
            "Loss S01:  0.06897553130165561\n",
            "Loss S2:  0.0777669974735805\n",
            "Loss S01:  0.06883671481420497\n",
            "Loss S2:  0.07767341042509174\n",
            "Loss S01:  0.0686128027401529\n",
            "Loss S2:  0.07752830642569172\n",
            "Loss S01:  0.06857130547200353\n",
            "Loss S2:  0.0775815520035334\n",
            "Loss S01:  0.06874448379487481\n",
            "Loss S2:  0.07782239711466636\n",
            "Loss S01:  0.06874627709811461\n",
            "Loss S2:  0.0779006429914887\n",
            "Loss S01:  0.0688717228095263\n",
            "Loss S2:  0.07795991151538116\n",
            "Loss S01:  0.06896807223280764\n",
            "Loss S2:  0.0780076422954198\n",
            "Loss S01:  0.06896439155465678\n",
            "Loss S2:  0.07804817357781338\n",
            "Loss S01:  0.06897377451233443\n",
            "Loss S2:  0.07807997239558077\n",
            "Loss S01:  0.06893807386306568\n",
            "Loss S2:  0.07815428520918516\n",
            "Loss S01:  0.06887333588072317\n",
            "Loss S2:  0.07820896298360469\n",
            "Loss S01:  0.06892615623807455\n",
            "Loss S2:  0.0782998821287641\n",
            "Loss S01:  0.06895122552349557\n",
            "Loss S2:  0.07839169948281746\n",
            "Loss S01:  0.06909227700202496\n",
            "Loss S2:  0.07846970715886586\n",
            "Loss S01:  0.06913100186101628\n",
            "Loss S2:  0.07852380597554302\n",
            "Loss S01:  0.06908784408968284\n",
            "Loss S2:  0.07852203804123924\n",
            "Loss S01:  0.0691093111061045\n",
            "Loss S2:  0.07861740788680384\n",
            "Loss S01:  0.0691375207307154\n",
            "Loss S2:  0.07868130388240092\n",
            "Loss S01:  0.06919684074156225\n",
            "Loss S2:  0.07871931554000573\n",
            "Loss S01:  0.06914839431438659\n",
            "Loss S2:  0.07869688968119752\n",
            "Loss S01:  0.06909631393181526\n",
            "Loss S2:  0.07865947817894707\n",
            "Loss S01:  0.0689897693669681\n",
            "Loss S2:  0.07862279316188822\n",
            "Loss S01:  0.06905981242610287\n",
            "Loss S2:  0.07864634703113654\n",
            "Loss S01:  0.06899296312263725\n",
            "Loss S2:  0.07859116116274159\n",
            "Loss S01:  0.06894560596329376\n",
            "Loss S2:  0.07857938918156708\n",
            "Loss S01:  0.06892790796070697\n",
            "Loss S2:  0.07855243468259135\n",
            "Loss S01:  0.06889392229163416\n",
            "Loss S2:  0.07840138643856194\n",
            "Loss S01:  0.0688831710992155\n",
            "Loss S2:  0.07837710416100097\n",
            "Loss S01:  0.06882578926527594\n",
            "Loss S2:  0.07831486401556358\n",
            "Loss S01:  0.06874271193543054\n",
            "Loss S2:  0.07821493659673444\n",
            "Loss S01:  0.06876669238853336\n",
            "Loss S2:  0.07827499231086704\n",
            "Loss S01:  0.06874349436874518\n",
            "Loss S2:  0.07824641758906871\n",
            "Loss S01:  0.06880675972109736\n",
            "Loss S2:  0.07830906350546099\n",
            "Loss S01:  0.06883465335326239\n",
            "Loss S2:  0.07834109039048032\n",
            "Loss S01:  0.06877757093600946\n",
            "Loss S2:  0.07832570793846297\n",
            "Loss S01:  0.0687468687770372\n",
            "Loss S2:  0.07827086783036953\n",
            "Loss S01:  0.06880005609866337\n",
            "Loss S2:  0.07831663673909264\n",
            "Loss S01:  0.06874991768294839\n",
            "Loss S2:  0.07826985440942638\n",
            "Loss S01:  0.06876367846360573\n",
            "Loss S2:  0.07827746279522188\n",
            "Loss S01:  0.0687787554278029\n",
            "Loss S2:  0.07830199597748623\n",
            "Validation: \n",
            " Loss S01:  0.08064178377389908\n",
            " Loss S2:  0.12545707821846008\n",
            " Loss S01:  0.08506438668285098\n",
            " Loss S2:  0.13701912689776646\n",
            " Loss S01:  0.08419573561447423\n",
            " Loss S2:  0.1351828858619783\n",
            " Loss S01:  0.08314279369154914\n",
            " Loss S2:  0.1331822953996111\n",
            " Loss S01:  0.08231633598053897\n",
            " Loss S2:  0.13226617872714996\n",
            "\n",
            "Epoch: 30\n",
            "Loss S01:  0.061035700142383575\n",
            "Loss S2:  0.07765770703554153\n",
            "Loss S01:  0.06857186996124008\n",
            "Loss S2:  0.08166695995764299\n",
            "Loss S01:  0.06900453478807494\n",
            "Loss S2:  0.07981830480552855\n",
            "Loss S01:  0.06807788341276107\n",
            "Loss S2:  0.07793700070150437\n",
            "Loss S01:  0.06756762551461779\n",
            "Loss S2:  0.07740502223009016\n",
            "Loss S01:  0.06737829291937399\n",
            "Loss S2:  0.07753822733374204\n",
            "Loss S01:  0.06758175185713612\n",
            "Loss S2:  0.07774544250769694\n",
            "Loss S01:  0.06757562602279892\n",
            "Loss S2:  0.07756301099565667\n",
            "Loss S01:  0.06760776456859377\n",
            "Loss S2:  0.07738307725500178\n",
            "Loss S01:  0.06745609383170421\n",
            "Loss S2:  0.07720249915843481\n",
            "Loss S01:  0.06719093965274273\n",
            "Loss S2:  0.07689050924364883\n",
            "Loss S01:  0.0669503407972353\n",
            "Loss S2:  0.0766953609816663\n",
            "Loss S01:  0.06685295263844088\n",
            "Loss S2:  0.07661650964051239\n",
            "Loss S01:  0.06679241976323928\n",
            "Loss S2:  0.07669132117551701\n",
            "Loss S01:  0.06675493360516872\n",
            "Loss S2:  0.07677341263133583\n",
            "Loss S01:  0.06690504841062407\n",
            "Loss S2:  0.07676723249108586\n",
            "Loss S01:  0.06697012518781313\n",
            "Loss S2:  0.07672460749745369\n",
            "Loss S01:  0.067023289351784\n",
            "Loss S2:  0.07685039547538897\n",
            "Loss S01:  0.06711858562425355\n",
            "Loss S2:  0.07689002767534546\n",
            "Loss S01:  0.0672106406607553\n",
            "Loss S2:  0.0768989786622724\n",
            "Loss S01:  0.0671335896152762\n",
            "Loss S2:  0.07682915419265998\n",
            "Loss S01:  0.06733266373663717\n",
            "Loss S2:  0.07686768159694016\n",
            "Loss S01:  0.06744388420117926\n",
            "Loss S2:  0.07692796506021357\n",
            "Loss S01:  0.06757839411110073\n",
            "Loss S2:  0.07696337093200002\n",
            "Loss S01:  0.06768952922207686\n",
            "Loss S2:  0.07704976725071298\n",
            "Loss S01:  0.06766342573550593\n",
            "Loss S2:  0.07704250462561014\n",
            "Loss S01:  0.06770534279588539\n",
            "Loss S2:  0.07704310420315384\n",
            "Loss S01:  0.06776828710608376\n",
            "Loss S2:  0.07707994889014322\n",
            "Loss S01:  0.06787174437659067\n",
            "Loss S2:  0.0771923934729286\n",
            "Loss S01:  0.06783666026448876\n",
            "Loss S2:  0.07716768779039793\n",
            "Loss S01:  0.0678447116649032\n",
            "Loss S2:  0.07719912309434723\n",
            "Loss S01:  0.0677435436911905\n",
            "Loss S2:  0.07712637442628287\n",
            "Loss S01:  0.06780895340108425\n",
            "Loss S2:  0.07724061214896007\n",
            "Loss S01:  0.06779825529843299\n",
            "Loss S2:  0.0772280956300544\n",
            "Loss S01:  0.067810427080676\n",
            "Loss S2:  0.07728367819945134\n",
            "Loss S01:  0.06778316085155194\n",
            "Loss S2:  0.07723465728496554\n",
            "Loss S01:  0.0677856831834587\n",
            "Loss S2:  0.07718754463785243\n",
            "Loss S01:  0.06776631263026972\n",
            "Loss S2:  0.07718737854188022\n",
            "Loss S01:  0.06772943567533506\n",
            "Loss S2:  0.07717522387233931\n",
            "Loss S01:  0.06767400855298543\n",
            "Loss S2:  0.07705809218842355\n",
            "Loss S01:  0.06766587758710854\n",
            "Loss S2:  0.07704813816489424\n",
            "Loss S01:  0.06763408509810476\n",
            "Loss S2:  0.07700880945925295\n",
            "Loss S01:  0.06770245192877754\n",
            "Loss S2:  0.07709396027931124\n",
            "Loss S01:  0.06765650090028957\n",
            "Loss S2:  0.07704913563455894\n",
            "Loss S01:  0.06766658386980054\n",
            "Loss S2:  0.07707905503255981\n",
            "Loss S01:  0.06766213374233035\n",
            "Loss S2:  0.07709997202664151\n",
            "Loss S01:  0.06764932927915952\n",
            "Loss S2:  0.07712373007371731\n",
            "Loss S01:  0.06760662006344764\n",
            "Loss S2:  0.07707247988275409\n",
            "Loss S01:  0.06757722441842799\n",
            "Loss S2:  0.07706918125186046\n",
            "Loss S01:  0.0675775229384603\n",
            "Loss S2:  0.0770404407652114\n",
            "Validation: \n",
            " Loss S01:  0.07199771702289581\n",
            " Loss S2:  0.11566589027643204\n",
            " Loss S01:  0.07865274910415922\n",
            " Loss S2:  0.1297307564389138\n",
            " Loss S01:  0.07724561124313169\n",
            " Loss S2:  0.12841048618642295\n",
            " Loss S01:  0.07588060847559913\n",
            " Loss S2:  0.12677569650724285\n",
            " Loss S01:  0.07526194892915679\n",
            " Loss S2:  0.1263170032589524\n",
            "\n",
            "Epoch: 31\n",
            "Loss S01:  0.06520909070968628\n",
            "Loss S2:  0.0717051550745964\n",
            "Loss S01:  0.06921256299723279\n",
            "Loss S2:  0.07909307154742154\n",
            "Loss S01:  0.06832962305772872\n",
            "Loss S2:  0.07813098813806262\n",
            "Loss S01:  0.06737830429788559\n",
            "Loss S2:  0.07674087584018707\n",
            "Loss S01:  0.06727113102267428\n",
            "Loss S2:  0.07607521898135906\n",
            "Loss S01:  0.06741722885007952\n",
            "Loss S2:  0.07635942525139042\n",
            "Loss S01:  0.06733736206517844\n",
            "Loss S2:  0.07633086794712504\n",
            "Loss S01:  0.06710900927723294\n",
            "Loss S2:  0.07576353680080092\n",
            "Loss S01:  0.06715340935337691\n",
            "Loss S2:  0.07586002589007955\n",
            "Loss S01:  0.06701801017254264\n",
            "Loss S2:  0.07604631372205503\n",
            "Loss S01:  0.06701973715040943\n",
            "Loss S2:  0.07588746568354049\n",
            "Loss S01:  0.06676814104388426\n",
            "Loss S2:  0.0758129134774208\n",
            "Loss S01:  0.06665362072877648\n",
            "Loss S2:  0.07582552176861723\n",
            "Loss S01:  0.06664429468507985\n",
            "Loss S2:  0.07597560202347413\n",
            "Loss S01:  0.06667055004666038\n",
            "Loss S2:  0.07618861037788661\n",
            "Loss S01:  0.0668220708011002\n",
            "Loss S2:  0.07633620944638915\n",
            "Loss S01:  0.06676790000211379\n",
            "Loss S2:  0.0761774799653462\n",
            "Loss S01:  0.06676771330554583\n",
            "Loss S2:  0.07623352258526093\n",
            "Loss S01:  0.06675359771396573\n",
            "Loss S2:  0.07620190866085706\n",
            "Loss S01:  0.06683866673817185\n",
            "Loss S2:  0.07625018699943083\n",
            "Loss S01:  0.06681308806387347\n",
            "Loss S2:  0.07623212654792254\n",
            "Loss S01:  0.06685092333721888\n",
            "Loss S2:  0.07630039617348622\n",
            "Loss S01:  0.06691919171095434\n",
            "Loss S2:  0.07638918389292325\n",
            "Loss S01:  0.06694525178699266\n",
            "Loss S2:  0.07645815062574494\n",
            "Loss S01:  0.06706798538812958\n",
            "Loss S2:  0.07661548358894482\n",
            "Loss S01:  0.06718828367343936\n",
            "Loss S2:  0.07668962234638602\n",
            "Loss S01:  0.06724214623771409\n",
            "Loss S2:  0.07664765029346349\n",
            "Loss S01:  0.06726879473926836\n",
            "Loss S2:  0.07665082317436753\n",
            "Loss S01:  0.0672821170149961\n",
            "Loss S2:  0.07669865236587796\n",
            "Loss S01:  0.06734349771924444\n",
            "Loss S2:  0.07670313881588556\n",
            "Loss S01:  0.06739211655484878\n",
            "Loss S2:  0.07672957926246018\n",
            "Loss S01:  0.06735747173380621\n",
            "Loss S2:  0.07675965660973377\n",
            "Loss S01:  0.06743189325770857\n",
            "Loss S2:  0.07677830826502723\n",
            "Loss S01:  0.06736299808607\n",
            "Loss S2:  0.0766819029753842\n",
            "Loss S01:  0.06730714779008519\n",
            "Loss S2:  0.07666913980394165\n",
            "Loss S01:  0.06729654114470522\n",
            "Loss S2:  0.0766250350090683\n",
            "Loss S01:  0.0672555239535765\n",
            "Loss S2:  0.07657276817347204\n",
            "Loss S01:  0.06721638681271648\n",
            "Loss S2:  0.07655514759636632\n",
            "Loss S01:  0.06723416556562652\n",
            "Loss S2:  0.07652231662722397\n",
            "Loss S01:  0.06717990129195212\n",
            "Loss S2:  0.07643526536233895\n",
            "Loss S01:  0.06718074600038089\n",
            "Loss S2:  0.07638493798060013\n",
            "Loss S01:  0.06716800492864165\n",
            "Loss S2:  0.07645563115059895\n",
            "Loss S01:  0.06718828635303538\n",
            "Loss S2:  0.07648907363804106\n",
            "Loss S01:  0.06719604827757501\n",
            "Loss S2:  0.07648370246426689\n",
            "Loss S01:  0.0671313281114004\n",
            "Loss S2:  0.0764749490839689\n",
            "Loss S01:  0.06713674178640223\n",
            "Loss S2:  0.07646878456031933\n",
            "Loss S01:  0.06717771940628256\n",
            "Loss S2:  0.07650611366179656\n",
            "Loss S01:  0.06711459139286839\n",
            "Loss S2:  0.07643227397035135\n",
            "Loss S01:  0.06710982671987242\n",
            "Loss S2:  0.07639780926388415\n",
            "Loss S01:  0.06711045006202826\n",
            "Loss S2:  0.07635626255282317\n",
            "Validation: \n",
            " Loss S01:  0.08440127223730087\n",
            " Loss S2:  0.12727533280849457\n",
            " Loss S01:  0.08877606583493096\n",
            " Loss S2:  0.14128672012260982\n",
            " Loss S01:  0.08737941795006031\n",
            " Loss S2:  0.1394068611467757\n",
            " Loss S01:  0.08604399261415982\n",
            " Loss S2:  0.1374414669441395\n",
            " Loss S01:  0.0851419864245403\n",
            " Loss S2:  0.13668746804749524\n",
            "\n",
            "Epoch: 32\n",
            "Loss S01:  0.07237626612186432\n",
            "Loss S2:  0.07452412694692612\n",
            "Loss S01:  0.06942780282009732\n",
            "Loss S2:  0.0785604173486883\n",
            "Loss S01:  0.068493245080823\n",
            "Loss S2:  0.07584564103966668\n",
            "Loss S01:  0.06725410444121208\n",
            "Loss S2:  0.07527086258895936\n",
            "Loss S01:  0.0669872958849116\n",
            "Loss S2:  0.0753943409498145\n",
            "Loss S01:  0.0671019213018464\n",
            "Loss S2:  0.07546176469209147\n",
            "Loss S01:  0.06703867506785471\n",
            "Loss S2:  0.07578171374367886\n",
            "Loss S01:  0.06687293610942195\n",
            "Loss S2:  0.07558460733000662\n",
            "Loss S01:  0.06681058919172228\n",
            "Loss S2:  0.07557456508094881\n",
            "Loss S01:  0.06675141063201559\n",
            "Loss S2:  0.07580886347280753\n",
            "Loss S01:  0.06666504945790414\n",
            "Loss S2:  0.07555134137078087\n",
            "Loss S01:  0.06654697623070296\n",
            "Loss S2:  0.07548391751877896\n",
            "Loss S01:  0.06654277419255784\n",
            "Loss S2:  0.07549658245291592\n",
            "Loss S01:  0.06670214013971446\n",
            "Loss S2:  0.07554718783793558\n",
            "Loss S01:  0.06667401456663795\n",
            "Loss S2:  0.07558284816167034\n",
            "Loss S01:  0.06677195698713624\n",
            "Loss S2:  0.07558684559255247\n",
            "Loss S01:  0.06671274529758447\n",
            "Loss S2:  0.07557449494459614\n",
            "Loss S01:  0.06660210447963218\n",
            "Loss S2:  0.07561226881909788\n",
            "Loss S01:  0.06672720127425141\n",
            "Loss S2:  0.07575772558621939\n",
            "Loss S01:  0.06676030014665964\n",
            "Loss S2:  0.07575210601247417\n",
            "Loss S01:  0.06667973223462034\n",
            "Loss S2:  0.07580719447106271\n",
            "Loss S01:  0.06669649923158483\n",
            "Loss S2:  0.07581058829599083\n",
            "Loss S01:  0.0667850976278879\n",
            "Loss S2:  0.07593503769706278\n",
            "Loss S01:  0.06676905586089922\n",
            "Loss S2:  0.07594248792761332\n",
            "Loss S01:  0.06676438701226999\n",
            "Loss S2:  0.07593900999402109\n",
            "Loss S01:  0.06681895238232327\n",
            "Loss S2:  0.07593783244965562\n",
            "Loss S01:  0.06683531865991395\n",
            "Loss S2:  0.07603170118969062\n",
            "Loss S01:  0.06683632088103418\n",
            "Loss S2:  0.07604679865315832\n",
            "Loss S01:  0.06683040495126696\n",
            "Loss S2:  0.07609410842489517\n",
            "Loss S01:  0.0667722869241975\n",
            "Loss S2:  0.07598131120563373\n",
            "Loss S01:  0.0667749563745684\n",
            "Loss S2:  0.07596876919022034\n",
            "Loss S01:  0.06668150288403225\n",
            "Loss S2:  0.07586524481031673\n",
            "Loss S01:  0.06668549929266777\n",
            "Loss S2:  0.07591900655367292\n",
            "Loss S01:  0.06666933247690114\n",
            "Loss S2:  0.07589273817634655\n",
            "Loss S01:  0.06664334836243883\n",
            "Loss S2:  0.07588464328664139\n",
            "Loss S01:  0.06663852206237636\n",
            "Loss S2:  0.07586552382067398\n",
            "Loss S01:  0.06653024667972013\n",
            "Loss S2:  0.0757343995789907\n",
            "Loss S01:  0.06651960201862687\n",
            "Loss S2:  0.0757100977646212\n",
            "Loss S01:  0.06644531309096206\n",
            "Loss S2:  0.07563692589444439\n",
            "Loss S01:  0.06638689916533277\n",
            "Loss S2:  0.07554768498443887\n",
            "Loss S01:  0.0664384039095661\n",
            "Loss S2:  0.07558788567261208\n",
            "Loss S01:  0.06641562647410552\n",
            "Loss S2:  0.0756426430749197\n",
            "Loss S01:  0.06644793271558301\n",
            "Loss S2:  0.07578006675223557\n",
            "Loss S01:  0.06645825978913053\n",
            "Loss S2:  0.07574588591081637\n",
            "Loss S01:  0.06642408789792299\n",
            "Loss S2:  0.07577929581576734\n",
            "Loss S01:  0.06641901914699379\n",
            "Loss S2:  0.07579798208438107\n",
            "Loss S01:  0.06645602806432904\n",
            "Loss S2:  0.07582367974416294\n",
            "Loss S01:  0.06642898472694328\n",
            "Loss S2:  0.07583737094943943\n",
            "Loss S01:  0.06643989514133539\n",
            "Loss S2:  0.07583618472308974\n",
            "Loss S01:  0.06642190021768121\n",
            "Loss S2:  0.0758141872990884\n",
            "Validation: \n",
            " Loss S01:  0.07355619221925735\n",
            " Loss S2:  0.11874695867300034\n",
            " Loss S01:  0.08173441390196483\n",
            " Loss S2:  0.1310371078905605\n",
            " Loss S01:  0.08027297930746544\n",
            " Loss S2:  0.12947463734847744\n",
            " Loss S01:  0.07919978325972792\n",
            " Loss S2:  0.1278273766158057\n",
            " Loss S01:  0.07856085988474482\n",
            " Loss S2:  0.12706775797737968\n",
            "\n",
            "Epoch: 33\n",
            "Loss S01:  0.07049623131752014\n",
            "Loss S2:  0.07815130800008774\n",
            "Loss S01:  0.0660555548965931\n",
            "Loss S2:  0.0745905644514344\n",
            "Loss S01:  0.06642749887846765\n",
            "Loss S2:  0.07471495511985961\n",
            "Loss S01:  0.06557929792231129\n",
            "Loss S2:  0.07425617402599703\n",
            "Loss S01:  0.06544471632053213\n",
            "Loss S2:  0.0743132991398253\n",
            "Loss S01:  0.06555365237827394\n",
            "Loss S2:  0.07426307028999515\n",
            "Loss S01:  0.06563205455170303\n",
            "Loss S2:  0.07443577771792646\n",
            "Loss S01:  0.06527780872625365\n",
            "Loss S2:  0.07415462264292677\n",
            "Loss S01:  0.06525571694896545\n",
            "Loss S2:  0.0741111366653148\n",
            "Loss S01:  0.0653106203855394\n",
            "Loss S2:  0.07441442287885226\n",
            "Loss S01:  0.06520425284853076\n",
            "Loss S2:  0.07415766446012081\n",
            "Loss S01:  0.06512060013037545\n",
            "Loss S2:  0.07413694396749274\n",
            "Loss S01:  0.06489720404887002\n",
            "Loss S2:  0.07417435537685048\n",
            "Loss S01:  0.06492537936864008\n",
            "Loss S2:  0.07429982852617292\n",
            "Loss S01:  0.06485793929784855\n",
            "Loss S2:  0.07426786927361015\n",
            "Loss S01:  0.06524794825062846\n",
            "Loss S2:  0.07453703552188463\n",
            "Loss S01:  0.06534123915878141\n",
            "Loss S2:  0.07446138317818227\n",
            "Loss S01:  0.06540256470703242\n",
            "Loss S2:  0.07455853697413589\n",
            "Loss S01:  0.06559788038470468\n",
            "Loss S2:  0.07460018858642868\n",
            "Loss S01:  0.0656084843682057\n",
            "Loss S2:  0.07462111577703691\n",
            "Loss S01:  0.06556254784710965\n",
            "Loss S2:  0.0747124562920326\n",
            "Loss S01:  0.06557988084111169\n",
            "Loss S2:  0.07482650263402699\n",
            "Loss S01:  0.06562238115800452\n",
            "Loss S2:  0.07487179600208053\n",
            "Loss S01:  0.0656570273560363\n",
            "Loss S2:  0.07486254110674322\n",
            "Loss S01:  0.06576769679970267\n",
            "Loss S2:  0.07490349098792709\n",
            "Loss S01:  0.06573919386443389\n",
            "Loss S2:  0.07491944684272268\n",
            "Loss S01:  0.06577452112317542\n",
            "Loss S2:  0.07494999812309312\n",
            "Loss S01:  0.06584376889896129\n",
            "Loss S2:  0.07503420511789867\n",
            "Loss S01:  0.06584135177826966\n",
            "Loss S2:  0.0750168724734588\n",
            "Loss S01:  0.06584919913579099\n",
            "Loss S2:  0.07498904778478072\n",
            "Loss S01:  0.06589292071487421\n",
            "Loss S2:  0.0749861146524499\n",
            "Loss S01:  0.06581870449700923\n",
            "Loss S2:  0.07491709656079099\n",
            "Loss S01:  0.06586818395884608\n",
            "Loss S2:  0.07490823099917712\n",
            "Loss S01:  0.0657746194330047\n",
            "Loss S2:  0.07481145521125045\n",
            "Loss S01:  0.06581570471355642\n",
            "Loss S2:  0.07479967157249925\n",
            "Loss S01:  0.06581863749273482\n",
            "Loss S2:  0.07476827353663594\n",
            "Loss S01:  0.06582113279034886\n",
            "Loss S2:  0.07472382267096037\n",
            "Loss S01:  0.06577991674129532\n",
            "Loss S2:  0.07469118786147341\n",
            "Loss S01:  0.0657140024377918\n",
            "Loss S2:  0.07464641457780452\n",
            "Loss S01:  0.06563313037652493\n",
            "Loss S2:  0.07455940766598257\n",
            "Loss S01:  0.06568917895009987\n",
            "Loss S2:  0.07461200039508634\n",
            "Loss S01:  0.06566655235206413\n",
            "Loss S2:  0.07456042874976086\n",
            "Loss S01:  0.06568386418979412\n",
            "Loss S2:  0.07461902502738382\n",
            "Loss S01:  0.06566595510858395\n",
            "Loss S2:  0.07460506395231543\n",
            "Loss S01:  0.06561374771588244\n",
            "Loss S2:  0.07467177657245778\n",
            "Loss S01:  0.06559185758067869\n",
            "Loss S2:  0.07466494184639924\n",
            "Loss S01:  0.06565201988771008\n",
            "Loss S2:  0.07467033473245492\n",
            "Loss S01:  0.06562228032611231\n",
            "Loss S2:  0.07463838019252077\n",
            "Loss S01:  0.0656196188158404\n",
            "Loss S2:  0.07461211234021335\n",
            "Loss S01:  0.06562668120757868\n",
            "Loss S2:  0.07462399260211136\n",
            "Validation: \n",
            " Loss S01:  0.07889769226312637\n",
            " Loss S2:  0.1216292530298233\n",
            " Loss S01:  0.08463563592660994\n",
            " Loss S2:  0.13745660760572978\n",
            " Loss S01:  0.08331376168786025\n",
            " Loss S2:  0.13519766726871815\n",
            " Loss S01:  0.08213983024241495\n",
            " Loss S2:  0.13313485156805788\n",
            " Loss S01:  0.0814204642802109\n",
            " Loss S2:  0.13243811356800575\n",
            "\n",
            "Epoch: 34\n",
            "Loss S01:  0.06183949485421181\n",
            "Loss S2:  0.07340333610773087\n",
            "Loss S01:  0.06795644726265561\n",
            "Loss S2:  0.07552753253416582\n",
            "Loss S01:  0.06737601934444337\n",
            "Loss S2:  0.07545320015578043\n",
            "Loss S01:  0.06654599993940323\n",
            "Loss S2:  0.0748953432325394\n",
            "Loss S01:  0.0662462500537314\n",
            "Loss S2:  0.07431011946826446\n",
            "Loss S01:  0.06594302439514328\n",
            "Loss S2:  0.07403499713423205\n",
            "Loss S01:  0.06587313585838334\n",
            "Loss S2:  0.0741657548141284\n",
            "Loss S01:  0.065685422435193\n",
            "Loss S2:  0.07422762390383532\n",
            "Loss S01:  0.06555774987295822\n",
            "Loss S2:  0.07435113978054789\n",
            "Loss S01:  0.0654987572477414\n",
            "Loss S2:  0.07454378980692926\n",
            "Loss S01:  0.06528038845056354\n",
            "Loss S2:  0.07415767194758548\n",
            "Loss S01:  0.06509024998894683\n",
            "Loss S2:  0.07383459162067722\n",
            "Loss S01:  0.0649826349921463\n",
            "Loss S2:  0.07371735221837178\n",
            "Loss S01:  0.06505240935064455\n",
            "Loss S2:  0.07376470968468499\n",
            "Loss S01:  0.06504301008180524\n",
            "Loss S2:  0.0737579825288015\n",
            "Loss S01:  0.06520969845009165\n",
            "Loss S2:  0.07390182366631678\n",
            "Loss S01:  0.06518853201258996\n",
            "Loss S2:  0.07371441416025902\n",
            "Loss S01:  0.06516008789253513\n",
            "Loss S2:  0.07375346082780096\n",
            "Loss S01:  0.06524362799953361\n",
            "Loss S2:  0.07381885130349444\n",
            "Loss S01:  0.06512355644509431\n",
            "Loss S2:  0.07377305293348448\n",
            "Loss S01:  0.06516628955208247\n",
            "Loss S2:  0.07382808935212258\n",
            "Loss S01:  0.06516232661090755\n",
            "Loss S2:  0.07378454049122277\n",
            "Loss S01:  0.06528398030483884\n",
            "Loss S2:  0.07390539093592048\n",
            "Loss S01:  0.06525474151362588\n",
            "Loss S2:  0.07397873359389635\n",
            "Loss S01:  0.06535913976087115\n",
            "Loss S2:  0.07406572269515378\n",
            "Loss S01:  0.06538859631613907\n",
            "Loss S2:  0.07411354538275901\n",
            "Loss S01:  0.0653759637326573\n",
            "Loss S2:  0.07413276626061206\n",
            "Loss S01:  0.06539773825569786\n",
            "Loss S2:  0.07412302786848642\n",
            "Loss S01:  0.06543283998382897\n",
            "Loss S2:  0.07414880401564239\n",
            "Loss S01:  0.06546771553220208\n",
            "Loss S2:  0.07419552376231377\n",
            "Loss S01:  0.06544394372804617\n",
            "Loss S2:  0.07420785009761982\n",
            "Loss S01:  0.06535745292327029\n",
            "Loss S2:  0.07414693014031438\n",
            "Loss S01:  0.06539703809258722\n",
            "Loss S2:  0.07415689534951593\n",
            "Loss S01:  0.06537491242961221\n",
            "Loss S2:  0.074119403913302\n",
            "Loss S01:  0.06538162119358866\n",
            "Loss S2:  0.07406837775036038\n",
            "Loss S01:  0.06538867191584022\n",
            "Loss S2:  0.07403336004738795\n",
            "Loss S01:  0.06529743238829509\n",
            "Loss S2:  0.07397801697336735\n",
            "Loss S01:  0.0652302209579077\n",
            "Loss S2:  0.07390096817778127\n",
            "Loss S01:  0.06517405284866887\n",
            "Loss S2:  0.07380957375439447\n",
            "Loss S01:  0.0651369449084677\n",
            "Loss S2:  0.07372290242815871\n",
            "Loss S01:  0.0651386759041848\n",
            "Loss S2:  0.07370137198458883\n",
            "Loss S01:  0.06508869973267371\n",
            "Loss S2:  0.0736725040849688\n",
            "Loss S01:  0.06507611847228223\n",
            "Loss S2:  0.07367383479859653\n",
            "Loss S01:  0.0650973706106437\n",
            "Loss S2:  0.07365186482218468\n",
            "Loss S01:  0.06507499132499674\n",
            "Loss S2:  0.07366755274451779\n",
            "Loss S01:  0.06507251407546373\n",
            "Loss S2:  0.07369753322718942\n",
            "Loss S01:  0.06506142082448378\n",
            "Loss S2:  0.07369356126465922\n",
            "Loss S01:  0.06501182353009337\n",
            "Loss S2:  0.0736745712594495\n",
            "Loss S01:  0.0650271695966904\n",
            "Loss S2:  0.07367143328466187\n",
            "Loss S01:  0.06502645616011804\n",
            "Loss S2:  0.0736538181252125\n",
            "Validation: \n",
            " Loss S01:  0.07400504499673843\n",
            " Loss S2:  0.11984162777662277\n",
            " Loss S01:  0.08024420518250693\n",
            " Loss S2:  0.13187638421853384\n",
            " Loss S01:  0.07920915324513506\n",
            " Loss S2:  0.12998148126573097\n",
            " Loss S01:  0.0782887052561416\n",
            " Loss S2:  0.1283287973921807\n",
            " Loss S01:  0.07761234054226934\n",
            " Loss S2:  0.127799186165686\n",
            "\n",
            "Epoch: 35\n",
            "Loss S01:  0.05510229989886284\n",
            "Loss S2:  0.06858393549919128\n",
            "Loss S01:  0.06600345840508287\n",
            "Loss S2:  0.0755551586096937\n",
            "Loss S01:  0.06580977088638715\n",
            "Loss S2:  0.07449487135523841\n",
            "Loss S01:  0.06473073110945764\n",
            "Loss S2:  0.07305458091920422\n",
            "Loss S01:  0.06456736220819194\n",
            "Loss S2:  0.07323854561985993\n",
            "Loss S01:  0.06484196853696131\n",
            "Loss S2:  0.07306632370341058\n",
            "Loss S01:  0.06482537707588712\n",
            "Loss S2:  0.07343696412004408\n",
            "Loss S01:  0.06458758783172554\n",
            "Loss S2:  0.07309280766147963\n",
            "Loss S01:  0.06444918658630347\n",
            "Loss S2:  0.0732465492170534\n",
            "Loss S01:  0.06456045433878899\n",
            "Loss S2:  0.07352615671825934\n",
            "Loss S01:  0.06443932239371951\n",
            "Loss S2:  0.073084986224623\n",
            "Loss S01:  0.06416055274842021\n",
            "Loss S2:  0.07290318336438488\n",
            "Loss S01:  0.06412686770858843\n",
            "Loss S2:  0.07272212764571521\n",
            "Loss S01:  0.06407073245362471\n",
            "Loss S2:  0.07291954980437992\n",
            "Loss S01:  0.06403645149148102\n",
            "Loss S2:  0.0729895495935112\n",
            "Loss S01:  0.06425678263732928\n",
            "Loss S2:  0.07320144755260044\n",
            "Loss S01:  0.06421776081186643\n",
            "Loss S2:  0.07319998086331794\n",
            "Loss S01:  0.06420462147185677\n",
            "Loss S2:  0.07314793881007105\n",
            "Loss S01:  0.06430032007519712\n",
            "Loss S2:  0.07310887395265353\n",
            "Loss S01:  0.06433784561865617\n",
            "Loss S2:  0.07313145268025822\n",
            "Loss S01:  0.06435160122033376\n",
            "Loss S2:  0.07314433594841269\n",
            "Loss S01:  0.06438268622233405\n",
            "Loss S2:  0.0732153448236497\n",
            "Loss S01:  0.06451559599438404\n",
            "Loss S2:  0.07331634016193415\n",
            "Loss S01:  0.06452320670926726\n",
            "Loss S2:  0.0732671989055423\n",
            "Loss S01:  0.06465446432720082\n",
            "Loss S2:  0.07337099483769959\n",
            "Loss S01:  0.06470924342770976\n",
            "Loss S2:  0.07345525573093102\n",
            "Loss S01:  0.06477025146970804\n",
            "Loss S2:  0.07351273555180123\n",
            "Loss S01:  0.06477186692761759\n",
            "Loss S2:  0.07354993499623014\n",
            "Loss S01:  0.06479927905407665\n",
            "Loss S2:  0.07360731717530519\n",
            "Loss S01:  0.06477956663292299\n",
            "Loss S2:  0.0735402051807474\n",
            "Loss S01:  0.06475538287645954\n",
            "Loss S2:  0.07355208828599746\n",
            "Loss S01:  0.06469768887068297\n",
            "Loss S2:  0.07348478669330621\n",
            "Loss S01:  0.06470152972457564\n",
            "Loss S2:  0.07348995233343397\n",
            "Loss S01:  0.06466656909788483\n",
            "Loss S2:  0.0734804913716734\n",
            "Loss S01:  0.06466074541342223\n",
            "Loss S2:  0.07343269660631932\n",
            "Loss S01:  0.06469358152516208\n",
            "Loss S2:  0.07347333566033602\n",
            "Loss S01:  0.06466826172210173\n",
            "Loss S2:  0.07344150322378508\n",
            "Loss S01:  0.06465322575519349\n",
            "Loss S2:  0.0733694259850484\n",
            "Loss S01:  0.0645913777584479\n",
            "Loss S2:  0.07333408090777284\n",
            "Loss S01:  0.06456949083548982\n",
            "Loss S2:  0.07327126127565303\n",
            "Loss S01:  0.06459155309007054\n",
            "Loss S2:  0.07326161836336675\n",
            "Loss S01:  0.06458409436028949\n",
            "Loss S2:  0.07326297805510879\n",
            "Loss S01:  0.06458795070648193\n",
            "Loss S2:  0.07334216378823878\n",
            "Loss S01:  0.06455093335247925\n",
            "Loss S2:  0.07329234953649913\n",
            "Loss S01:  0.06446690622573537\n",
            "Loss S2:  0.0732771077123629\n",
            "Loss S01:  0.06446625839117626\n",
            "Loss S2:  0.07327339097758355\n",
            "Loss S01:  0.06450125385972787\n",
            "Loss S2:  0.07330541689264024\n",
            "Loss S01:  0.06444960721074396\n",
            "Loss S2:  0.07324577489919216\n",
            "Loss S01:  0.06445320970582119\n",
            "Loss S2:  0.0732691054253717\n",
            "Loss S01:  0.06444347716579613\n",
            "Loss S2:  0.07329741933680843\n",
            "Validation: \n",
            " Loss S01:  0.07374285906553268\n",
            " Loss S2:  0.1193222850561142\n",
            " Loss S01:  0.08293459032263074\n",
            " Loss S2:  0.13457256094330833\n",
            " Loss S01:  0.08166224037001772\n",
            " Loss S2:  0.13247687878405176\n",
            " Loss S01:  0.08049636004401035\n",
            " Loss S2:  0.13049952881258042\n",
            " Loss S01:  0.07989646006881455\n",
            " Loss S2:  0.12996347000201544\n",
            "\n",
            "Epoch: 36\n",
            "Loss S01:  0.06066315621137619\n",
            "Loss S2:  0.07200664281845093\n",
            "Loss S01:  0.06545900621197441\n",
            "Loss S2:  0.07511351596225392\n",
            "Loss S01:  0.06589425621288163\n",
            "Loss S2:  0.07416761985846929\n",
            "Loss S01:  0.06484554407577361\n",
            "Loss S2:  0.07276060576400449\n",
            "Loss S01:  0.06418630707918144\n",
            "Loss S2:  0.07277354961488305\n",
            "Loss S01:  0.06415481113043486\n",
            "Loss S2:  0.07245554132204429\n",
            "Loss S01:  0.064181742365243\n",
            "Loss S2:  0.07272022205298065\n",
            "Loss S01:  0.0639585684722578\n",
            "Loss S2:  0.07255916643730352\n",
            "Loss S01:  0.06389630733080852\n",
            "Loss S2:  0.07252360103122982\n",
            "Loss S01:  0.0639529022705424\n",
            "Loss S2:  0.0727526656464561\n",
            "Loss S01:  0.06394837674970674\n",
            "Loss S2:  0.0726108732863818\n",
            "Loss S01:  0.063767454612094\n",
            "Loss S2:  0.07241744890406325\n",
            "Loss S01:  0.06371621698264249\n",
            "Loss S2:  0.07217364933741979\n",
            "Loss S01:  0.06359802772298113\n",
            "Loss S2:  0.07219439076672074\n",
            "Loss S01:  0.06350659940999451\n",
            "Loss S2:  0.07235489487119601\n",
            "Loss S01:  0.06367639637249985\n",
            "Loss S2:  0.07249371601354997\n",
            "Loss S01:  0.06366889208832884\n",
            "Loss S2:  0.07236415450895055\n",
            "Loss S01:  0.06364714979515439\n",
            "Loss S2:  0.07242629021319033\n",
            "Loss S01:  0.0637215015102816\n",
            "Loss S2:  0.07248159273635617\n",
            "Loss S01:  0.06373738208834413\n",
            "Loss S2:  0.07253614729221579\n",
            "Loss S01:  0.06373267984064064\n",
            "Loss S2:  0.07262486647536505\n",
            "Loss S01:  0.06377140780407671\n",
            "Loss S2:  0.07262671539356923\n",
            "Loss S01:  0.06389353581076294\n",
            "Loss S2:  0.0727527999736335\n",
            "Loss S01:  0.06394122987663076\n",
            "Loss S2:  0.07276259338507404\n",
            "Loss S01:  0.06402709349000603\n",
            "Loss S2:  0.0729154205699432\n",
            "Loss S01:  0.06408453376525902\n",
            "Loss S2:  0.07291448408685833\n",
            "Loss S01:  0.06411630423361314\n",
            "Loss S2:  0.07287461882978107\n",
            "Loss S01:  0.06419746372917481\n",
            "Loss S2:  0.07294367323887305\n",
            "Loss S01:  0.06418319165812693\n",
            "Loss S2:  0.07300162290179857\n",
            "Loss S01:  0.06416813122857477\n",
            "Loss S2:  0.07299533766723171\n",
            "Loss S01:  0.06417950114043448\n",
            "Loss S2:  0.07297380612637118\n",
            "Loss S01:  0.06410910205921559\n",
            "Loss S2:  0.07296831266193911\n",
            "Loss S01:  0.06414143081951736\n",
            "Loss S2:  0.07298926811816164\n",
            "Loss S01:  0.06407588845248309\n",
            "Loss S2:  0.0729018715168054\n",
            "Loss S01:  0.06404086780294645\n",
            "Loss S2:  0.07285093435269296\n",
            "Loss S01:  0.06400048277444309\n",
            "Loss S2:  0.0728305124828958\n",
            "Loss S01:  0.06394816523319796\n",
            "Loss S2:  0.07273360212795292\n",
            "Loss S01:  0.06394320890066438\n",
            "Loss S2:  0.07269319503334655\n",
            "Loss S01:  0.0639062125206463\n",
            "Loss S2:  0.07263063254162395\n",
            "Loss S01:  0.06386447928445724\n",
            "Loss S2:  0.07252252145725138\n",
            "Loss S01:  0.06388047367260045\n",
            "Loss S2:  0.0725005104245985\n",
            "Loss S01:  0.06387422094234875\n",
            "Loss S2:  0.07244384790025198\n",
            "Loss S01:  0.06389348599130637\n",
            "Loss S2:  0.07244672674039764\n",
            "Loss S01:  0.06389309864344288\n",
            "Loss S2:  0.07239221648384413\n",
            "Loss S01:  0.06384243360824055\n",
            "Loss S2:  0.07243890737662781\n",
            "Loss S01:  0.06379608752127497\n",
            "Loss S2:  0.07239579210293266\n",
            "Loss S01:  0.0638078490106506\n",
            "Loss S2:  0.07243248000094533\n",
            "Loss S01:  0.06380733349330865\n",
            "Loss S2:  0.07244064000033776\n",
            "Loss S01:  0.06381205401989377\n",
            "Loss S2:  0.07239164133980219\n",
            "Loss S01:  0.0637994275149527\n",
            "Loss S2:  0.07243458903157542\n",
            "Validation: \n",
            " Loss S01:  0.0755867213010788\n",
            " Loss S2:  0.12149936705827713\n",
            " Loss S01:  0.08175871485755556\n",
            " Loss S2:  0.1357051833044915\n",
            " Loss S01:  0.08019169547208925\n",
            " Loss S2:  0.13392467342498826\n",
            " Loss S01:  0.07912435174965468\n",
            " Loss S2:  0.13201586181511643\n",
            " Loss S01:  0.0784181057487005\n",
            " Loss S2:  0.13128242825652348\n",
            "\n",
            "Epoch: 37\n",
            "Loss S01:  0.07069048285484314\n",
            "Loss S2:  0.07510372251272202\n",
            "Loss S01:  0.06589793583208864\n",
            "Loss S2:  0.07402267916636034\n",
            "Loss S01:  0.06467292422340029\n",
            "Loss S2:  0.07301404149759383\n",
            "Loss S01:  0.06386106833815575\n",
            "Loss S2:  0.07250307211952825\n",
            "Loss S01:  0.06359955096026747\n",
            "Loss S2:  0.07220200903531981\n",
            "Loss S01:  0.06345404158620273\n",
            "Loss S2:  0.07233310622327468\n",
            "Loss S01:  0.0637654087460432\n",
            "Loss S2:  0.07273807321659854\n",
            "Loss S01:  0.06362415356955058\n",
            "Loss S2:  0.07232345241895864\n",
            "Loss S01:  0.0633919908216706\n",
            "Loss S2:  0.0722718244349515\n",
            "Loss S01:  0.06308851882324114\n",
            "Loss S2:  0.07229124828354343\n",
            "Loss S01:  0.06298450240404299\n",
            "Loss S2:  0.07206313732531991\n",
            "Loss S01:  0.06281817795054333\n",
            "Loss S2:  0.07195148029708648\n",
            "Loss S01:  0.06273703769711424\n",
            "Loss S2:  0.07180065005894534\n",
            "Loss S01:  0.06276262486140237\n",
            "Loss S2:  0.07180347279396676\n",
            "Loss S01:  0.06271202693171535\n",
            "Loss S2:  0.0718137910655627\n",
            "Loss S01:  0.06291392038496124\n",
            "Loss S2:  0.07212569581850475\n",
            "Loss S01:  0.06292405363565647\n",
            "Loss S2:  0.07214466176973366\n",
            "Loss S01:  0.06277509497707351\n",
            "Loss S2:  0.07218177154747366\n",
            "Loss S01:  0.06288692997305433\n",
            "Loss S2:  0.07218172261234146\n",
            "Loss S01:  0.0629395142353642\n",
            "Loss S2:  0.07225037568994842\n",
            "Loss S01:  0.06295870667073264\n",
            "Loss S2:  0.07232905538817544\n",
            "Loss S01:  0.06294595340699381\n",
            "Loss S2:  0.07229271951303663\n",
            "Loss S01:  0.06304326463001886\n",
            "Loss S2:  0.07239242400384058\n",
            "Loss S01:  0.06305659878434557\n",
            "Loss S2:  0.07237400185494196\n",
            "Loss S01:  0.06318084564817397\n",
            "Loss S2:  0.0724865470498924\n",
            "Loss S01:  0.06332192452720913\n",
            "Loss S2:  0.07254411238598159\n",
            "Loss S01:  0.0633287463636919\n",
            "Loss S2:  0.07258003910154219\n",
            "Loss S01:  0.06336071339480552\n",
            "Loss S2:  0.07257633609525392\n",
            "Loss S01:  0.0633517386013927\n",
            "Loss S2:  0.07258031778435266\n",
            "Loss S01:  0.06333839977832184\n",
            "Loss S2:  0.07254193084397677\n",
            "Loss S01:  0.06324837213239796\n",
            "Loss S2:  0.0724664835315012\n",
            "Loss S01:  0.06314505725332395\n",
            "Loss S2:  0.07233450578531651\n",
            "Loss S01:  0.0632254764019886\n",
            "Loss S2:  0.07234957661027107\n",
            "Loss S01:  0.063192444816907\n",
            "Loss S2:  0.07226175347798901\n",
            "Loss S01:  0.0631662975352181\n",
            "Loss S2:  0.07227724829901698\n",
            "Loss S01:  0.06316623773075576\n",
            "Loss S2:  0.07225693143543355\n",
            "Loss S01:  0.06311658376678206\n",
            "Loss S2:  0.07220494992762722\n",
            "Loss S01:  0.06308650089844539\n",
            "Loss S2:  0.07217100003578591\n",
            "Loss S01:  0.0630503628706056\n",
            "Loss S2:  0.0721141550680158\n",
            "Loss S01:  0.06299034399373452\n",
            "Loss S2:  0.07200335311081708\n",
            "Loss S01:  0.06306118179958063\n",
            "Loss S2:  0.07197452533497775\n",
            "Loss S01:  0.06301948673787489\n",
            "Loss S2:  0.07196896048285376\n",
            "Loss S01:  0.06300976829970534\n",
            "Loss S2:  0.07199274904300368\n",
            "Loss S01:  0.0629874884373489\n",
            "Loss S2:  0.0719780976216367\n",
            "Loss S01:  0.06297837886241288\n",
            "Loss S2:  0.07199537649311447\n",
            "Loss S01:  0.06296779422663532\n",
            "Loss S2:  0.07197645217378494\n",
            "Loss S01:  0.06301738255517861\n",
            "Loss S2:  0.07198729896170457\n",
            "Loss S01:  0.06301178343420069\n",
            "Loss S2:  0.07199018252440066\n",
            "Loss S01:  0.06303453697917863\n",
            "Loss S2:  0.07198379628685557\n",
            "Loss S01:  0.06303486329656997\n",
            "Loss S2:  0.07201640578005804\n",
            "Validation: \n",
            " Loss S01:  0.07239461690187454\n",
            " Loss S2:  0.11518881469964981\n",
            " Loss S01:  0.07746040377588499\n",
            " Loss S2:  0.12699787567059198\n",
            " Loss S01:  0.07625835670567141\n",
            " Loss S2:  0.12583723773316638\n",
            " Loss S01:  0.07528775344129468\n",
            " Loss S2:  0.12426969189135754\n",
            " Loss S01:  0.0747419242505674\n",
            " Loss S2:  0.12381141540812857\n",
            "\n",
            "Epoch: 38\n",
            "Loss S01:  0.06078510731458664\n",
            "Loss S2:  0.07044760882854462\n",
            "Loss S01:  0.06419406018473885\n",
            "Loss S2:  0.07217248800125989\n",
            "Loss S01:  0.06383464698280607\n",
            "Loss S2:  0.07166299809302602\n",
            "Loss S01:  0.06273663392470728\n",
            "Loss S2:  0.07081947932320257\n",
            "Loss S01:  0.06278535133091415\n",
            "Loss S2:  0.07086890099979029\n",
            "Loss S01:  0.06292098380771338\n",
            "Loss S2:  0.07052753164487727\n",
            "Loss S01:  0.06298445452187883\n",
            "Loss S2:  0.07097639412176414\n",
            "Loss S01:  0.06279299460666281\n",
            "Loss S2:  0.07045052898391871\n",
            "Loss S01:  0.06277996743166889\n",
            "Loss S2:  0.07063077431586054\n",
            "Loss S01:  0.06260516531356089\n",
            "Loss S2:  0.07078027401815404\n",
            "Loss S01:  0.062462663148889444\n",
            "Loss S2:  0.07074937670685277\n",
            "Loss S01:  0.062313025934739155\n",
            "Loss S2:  0.07056051163791537\n",
            "Loss S01:  0.06211977276550837\n",
            "Loss S2:  0.07043631451804776\n",
            "Loss S01:  0.06216721114425259\n",
            "Loss S2:  0.07048041655020859\n",
            "Loss S01:  0.06219547548403977\n",
            "Loss S2:  0.0705032191430846\n",
            "Loss S01:  0.062278972062843524\n",
            "Loss S2:  0.07063699629626527\n",
            "Loss S01:  0.06222790294552442\n",
            "Loss S2:  0.07051971237759412\n",
            "Loss S01:  0.0622673822207409\n",
            "Loss S2:  0.0705989841262848\n",
            "Loss S01:  0.06231950719465208\n",
            "Loss S2:  0.07062914673218411\n",
            "Loss S01:  0.06235030822026792\n",
            "Loss S2:  0.07064614727781081\n",
            "Loss S01:  0.062313262651215735\n",
            "Loss S2:  0.07066153455062292\n",
            "Loss S01:  0.06237316369939754\n",
            "Loss S2:  0.07071581120951481\n",
            "Loss S01:  0.06244987335342627\n",
            "Loss S2:  0.07086783497514229\n",
            "Loss S01:  0.06250986918097451\n",
            "Loss S2:  0.07090336674606645\n",
            "Loss S01:  0.0626835694142397\n",
            "Loss S2:  0.07101682913624895\n",
            "Loss S01:  0.06273537238993018\n",
            "Loss S2:  0.07103938833472738\n",
            "Loss S01:  0.06278227034142647\n",
            "Loss S2:  0.07107020356474708\n",
            "Loss S01:  0.0628034470541011\n",
            "Loss S2:  0.07104814941977662\n",
            "Loss S01:  0.06284198711139027\n",
            "Loss S2:  0.07104928090213881\n",
            "Loss S01:  0.0629121724042491\n",
            "Loss S2:  0.07105013670534202\n",
            "Loss S01:  0.06292265257566078\n",
            "Loss S2:  0.07109767478268804\n",
            "Loss S01:  0.06284005448845038\n",
            "Loss S2:  0.07106704056454624\n",
            "Loss S01:  0.06287569363494157\n",
            "Loss S2:  0.07110000749893278\n",
            "Loss S01:  0.06279683029813708\n",
            "Loss S2:  0.07107291810687215\n",
            "Loss S01:  0.06278484844523441\n",
            "Loss S2:  0.07114097967612779\n",
            "Loss S01:  0.06279236167414576\n",
            "Loss S2:  0.07115260411871124\n",
            "Loss S01:  0.06278210599749372\n",
            "Loss S2:  0.07109401263606185\n",
            "Loss S01:  0.06279640665875291\n",
            "Loss S2:  0.07107274137899239\n",
            "Loss S01:  0.06272325635425688\n",
            "Loss S2:  0.071026762776331\n",
            "Loss S01:  0.06264379732977704\n",
            "Loss S2:  0.0709759254494439\n",
            "Loss S01:  0.06263961061426529\n",
            "Loss S2:  0.07095839540262769\n",
            "Loss S01:  0.06262613890959978\n",
            "Loss S2:  0.07095356801311756\n",
            "Loss S01:  0.0626742943873598\n",
            "Loss S2:  0.07100051345918637\n",
            "Loss S01:  0.06264521860951891\n",
            "Loss S2:  0.0709860333502984\n",
            "Loss S01:  0.06260559811782675\n",
            "Loss S2:  0.07098959067038127\n",
            "Loss S01:  0.06259586339085435\n",
            "Loss S2:  0.0710023720552546\n",
            "Loss S01:  0.06259521626243106\n",
            "Loss S2:  0.07099859351837713\n",
            "Loss S01:  0.06259402412043255\n",
            "Loss S2:  0.07095683538483459\n",
            "Loss S01:  0.06259937511598246\n",
            "Loss S2:  0.07097301765377953\n",
            "Loss S01:  0.06261799555810552\n",
            "Loss S2:  0.07097669152159798\n",
            "Validation: \n",
            " Loss S01:  0.07256544381380081\n",
            " Loss S2:  0.11446384340524673\n",
            " Loss S01:  0.07754152339129221\n",
            " Loss S2:  0.12848376092456637\n",
            " Loss S01:  0.07642159810880335\n",
            " Loss S2:  0.12738070764192722\n",
            " Loss S01:  0.07541658820920304\n",
            " Loss S2:  0.12551552582471098\n",
            " Loss S01:  0.07486282854720398\n",
            " Loss S2:  0.12465433270475011\n",
            "\n",
            "Epoch: 39\n",
            "Loss S01:  0.06302862614393234\n",
            "Loss S2:  0.07176760584115982\n",
            "Loss S01:  0.06346237286925316\n",
            "Loss S2:  0.070852438157255\n",
            "Loss S01:  0.0630883083102249\n",
            "Loss S2:  0.07171732683976491\n",
            "Loss S01:  0.06258013988694837\n",
            "Loss S2:  0.0710286254123334\n",
            "Loss S01:  0.062346862674486345\n",
            "Loss S2:  0.0709366013364094\n",
            "Loss S01:  0.06234343773594089\n",
            "Loss S2:  0.07059434472638018\n",
            "Loss S01:  0.06251548029115943\n",
            "Loss S2:  0.07105454075776163\n",
            "Loss S01:  0.0624936456516595\n",
            "Loss S2:  0.07061560425749967\n",
            "Loss S01:  0.0622767080311422\n",
            "Loss S2:  0.0706234776219468\n",
            "Loss S01:  0.06213521740430004\n",
            "Loss S2:  0.07054493154634486\n",
            "Loss S01:  0.06220011431539413\n",
            "Loss S2:  0.07050131818298067\n",
            "Loss S01:  0.061794267715634524\n",
            "Loss S2:  0.0703421346507631\n",
            "Loss S01:  0.06171240177282617\n",
            "Loss S2:  0.07022088895405619\n",
            "Loss S01:  0.06182918037849528\n",
            "Loss S2:  0.07033486032986459\n",
            "Loss S01:  0.061706378184100415\n",
            "Loss S2:  0.07029918615593977\n",
            "Loss S01:  0.06191361498161657\n",
            "Loss S2:  0.07058538642051994\n",
            "Loss S01:  0.06185844200460807\n",
            "Loss S2:  0.0704976543552757\n",
            "Loss S01:  0.06178454202953835\n",
            "Loss S2:  0.07050769817497995\n",
            "Loss S01:  0.061826263919719675\n",
            "Loss S2:  0.07059877174417617\n",
            "Loss S01:  0.0618688016544774\n",
            "Loss S2:  0.07061373411204802\n",
            "Loss S01:  0.06182404320260779\n",
            "Loss S2:  0.07068006190197978\n",
            "Loss S01:  0.06179859416815342\n",
            "Loss S2:  0.070697300471542\n",
            "Loss S01:  0.061806989420727904\n",
            "Loss S2:  0.07074369957072163\n",
            "Loss S01:  0.061755811577880536\n",
            "Loss S2:  0.07079549396644423\n",
            "Loss S01:  0.06184670154422645\n",
            "Loss S2:  0.0708156817465155\n",
            "Loss S01:  0.06187165855055311\n",
            "Loss S2:  0.07087815121053699\n",
            "Loss S01:  0.061882509168423\n",
            "Loss S2:  0.0709131875314475\n",
            "Loss S01:  0.06188093502473127\n",
            "Loss S2:  0.07088401618094022\n",
            "Loss S01:  0.06191441648742482\n",
            "Loss S2:  0.07094709483701139\n",
            "Loss S01:  0.06190651214819184\n",
            "Loss S2:  0.07087814302225293\n",
            "Loss S01:  0.06194456513240884\n",
            "Loss S2:  0.07089048882705033\n",
            "Loss S01:  0.06187143483441742\n",
            "Loss S2:  0.07087793166470681\n",
            "Loss S01:  0.061939022274589244\n",
            "Loss S2:  0.07085243462819919\n",
            "Loss S01:  0.06192955630198346\n",
            "Loss S2:  0.0708023932427618\n",
            "Loss S01:  0.06201004641146954\n",
            "Loss S2:  0.0708292537529972\n",
            "Loss S01:  0.06199332461440325\n",
            "Loss S2:  0.07080687577186147\n",
            "Loss S01:  0.061946111155926686\n",
            "Loss S2:  0.0707332610650571\n",
            "Loss S01:  0.06189876423290155\n",
            "Loss S2:  0.07069753617490077\n",
            "Loss S01:  0.061867203416786795\n",
            "Loss S2:  0.07061700714541859\n",
            "Loss S01:  0.06181980412253334\n",
            "Loss S2:  0.07054741877843351\n",
            "Loss S01:  0.061860636173935606\n",
            "Loss S2:  0.07056828174842267\n",
            "Loss S01:  0.06182520040775447\n",
            "Loss S2:  0.07050977998986442\n",
            "Loss S01:  0.06185629270139047\n",
            "Loss S2:  0.07058966726135471\n",
            "Loss S01:  0.06188288393264027\n",
            "Loss S2:  0.0705910391172096\n",
            "Loss S01:  0.06187003858539523\n",
            "Loss S2:  0.07063307500784359\n",
            "Loss S01:  0.061830250996509836\n",
            "Loss S2:  0.07061224994136066\n",
            "Loss S01:  0.06181002965847479\n",
            "Loss S2:  0.07061454239190271\n",
            "Loss S01:  0.06179786325608343\n",
            "Loss S2:  0.07057208915076944\n",
            "Loss S01:  0.061820110643046315\n",
            "Loss S2:  0.07059341798422242\n",
            "Loss S01:  0.06182545240657393\n",
            "Loss S2:  0.07061629524632772\n",
            "Validation: \n",
            " Loss S01:  0.07227282971143723\n",
            " Loss S2:  0.11475820094347\n",
            " Loss S01:  0.07722137584572747\n",
            " Loss S2:  0.12881556720960707\n",
            " Loss S01:  0.07612423308011962\n",
            " Loss S2:  0.1276982761010891\n",
            " Loss S01:  0.07529581118313992\n",
            " Loss S2:  0.12605894760030215\n",
            " Loss S01:  0.07464998675349319\n",
            " Loss S2:  0.12572213253121317\n",
            "\n",
            "Epoch: 40\n",
            "Loss S01:  0.059475935995578766\n",
            "Loss S2:  0.07523810118436813\n",
            "Loss S01:  0.06354198570955884\n",
            "Loss S2:  0.07351069457151672\n",
            "Loss S01:  0.0629078256232398\n",
            "Loss S2:  0.0734745388229688\n",
            "Loss S01:  0.0617871240021721\n",
            "Loss S2:  0.07185874927428461\n",
            "Loss S01:  0.06155568770155674\n",
            "Loss S2:  0.07163502294115903\n",
            "Loss S01:  0.06141040810182983\n",
            "Loss S2:  0.0712829921029362\n",
            "Loss S01:  0.06134237550565454\n",
            "Loss S2:  0.07148044006746324\n",
            "Loss S01:  0.06122170362464139\n",
            "Loss S2:  0.07107781115132318\n",
            "Loss S01:  0.06123858368323173\n",
            "Loss S2:  0.07109190827166592\n",
            "Loss S01:  0.061171990379199875\n",
            "Loss S2:  0.07097984756250958\n",
            "Loss S01:  0.06100747498250244\n",
            "Loss S2:  0.07046712049753359\n",
            "Loss S01:  0.06087303003883577\n",
            "Loss S2:  0.07029933727405092\n",
            "Loss S01:  0.06076571022060292\n",
            "Loss S2:  0.07008080190617191\n",
            "Loss S01:  0.060896940699970446\n",
            "Loss S2:  0.07026532952112095\n",
            "Loss S01:  0.060915850552049934\n",
            "Loss S2:  0.07022950713727491\n",
            "Loss S01:  0.061116829169112326\n",
            "Loss S2:  0.07036853820973674\n",
            "Loss S01:  0.061072569849513335\n",
            "Loss S2:  0.07029622807080702\n",
            "Loss S01:  0.06116057572919026\n",
            "Loss S2:  0.07028191209885112\n",
            "Loss S01:  0.06127829810614744\n",
            "Loss S2:  0.0704178371523296\n",
            "Loss S01:  0.06129712985916287\n",
            "Loss S2:  0.07038177212417437\n",
            "Loss S01:  0.0613474850705014\n",
            "Loss S2:  0.07053243433732298\n",
            "Loss S01:  0.06136338622860999\n",
            "Loss S2:  0.07049512042182882\n",
            "Loss S01:  0.06141963113959019\n",
            "Loss S2:  0.0707000010726948\n",
            "Loss S01:  0.06142166671363306\n",
            "Loss S2:  0.07059575680207897\n",
            "Loss S01:  0.06153043878894624\n",
            "Loss S2:  0.07067615376530346\n",
            "Loss S01:  0.061630114245106025\n",
            "Loss S2:  0.0708322659045814\n",
            "Loss S01:  0.06162084220869331\n",
            "Loss S2:  0.07082888415490074\n",
            "Loss S01:  0.061642163890204306\n",
            "Loss S2:  0.07078507534362294\n",
            "Loss S01:  0.06167965008482814\n",
            "Loss S2:  0.07074933397016915\n",
            "Loss S01:  0.06173266231040774\n",
            "Loss S2:  0.07071083073609884\n",
            "Loss S01:  0.06168283963965815\n",
            "Loss S2:  0.0706686430372471\n",
            "Loss S01:  0.0616210438790237\n",
            "Loss S2:  0.07054016192913821\n",
            "Loss S01:  0.06168435403723212\n",
            "Loss S2:  0.07057867338445699\n",
            "Loss S01:  0.06167477497611521\n",
            "Loss S2:  0.07046467397823435\n",
            "Loss S01:  0.06172348941263915\n",
            "Loss S2:  0.07045423386951695\n",
            "Loss S01:  0.06176793630476351\n",
            "Loss S2:  0.07046246721788689\n",
            "Loss S01:  0.06175307164850988\n",
            "Loss S2:  0.07040085201861125\n",
            "Loss S01:  0.061783999616325386\n",
            "Loss S2:  0.07036274438598407\n",
            "Loss S01:  0.06175404427245533\n",
            "Loss S2:  0.07025413918174471\n",
            "Loss S01:  0.061717701670916184\n",
            "Loss S2:  0.07020443643602874\n",
            "Loss S01:  0.06175322456290003\n",
            "Loss S2:  0.07018245869035138\n",
            "Loss S01:  0.06176726172005173\n",
            "Loss S2:  0.07019558158270343\n",
            "Loss S01:  0.061795594431725366\n",
            "Loss S2:  0.07020555456963118\n",
            "Loss S01:  0.061804423389606296\n",
            "Loss S2:  0.07018566764286112\n",
            "Loss S01:  0.06176117222223963\n",
            "Loss S2:  0.07017483082420431\n",
            "Loss S01:  0.06173316202644762\n",
            "Loss S2:  0.07019221495233988\n",
            "Loss S01:  0.06172472219146513\n",
            "Loss S2:  0.07021263992482565\n",
            "Loss S01:  0.06165512586348629\n",
            "Loss S2:  0.07020439013775494\n",
            "Loss S01:  0.06167409007520785\n",
            "Loss S2:  0.07018699476073288\n",
            "Loss S01:  0.061668549089592004\n",
            "Loss S2:  0.07018382664458815\n",
            "Validation: \n",
            " Loss S01:  0.07359796017408371\n",
            " Loss S2:  0.1132597103714943\n",
            " Loss S01:  0.08130693435668945\n",
            " Loss S2:  0.12982836685010365\n",
            " Loss S01:  0.08001199219284988\n",
            " Loss S2:  0.12850023460824314\n",
            " Loss S01:  0.07890498137376348\n",
            " Loss S2:  0.12691394984722137\n",
            " Loss S01:  0.078219905955556\n",
            " Loss S2:  0.12607639650871724\n",
            "\n",
            "Epoch: 41\n",
            "Loss S01:  0.06450426578521729\n",
            "Loss S2:  0.07029393315315247\n",
            "Loss S01:  0.06226626986807043\n",
            "Loss S2:  0.07190285623073578\n",
            "Loss S01:  0.06181112499464126\n",
            "Loss S2:  0.07161797157355718\n",
            "Loss S01:  0.06163421453487489\n",
            "Loss S2:  0.07053970557547384\n",
            "Loss S01:  0.061335905023464345\n",
            "Loss S2:  0.0700065292781446\n",
            "Loss S01:  0.061155990058300545\n",
            "Loss S2:  0.06998015333916627\n",
            "Loss S01:  0.061369250482711635\n",
            "Loss S2:  0.06987860627838823\n",
            "Loss S01:  0.06126353302052323\n",
            "Loss S2:  0.06960453166508339\n",
            "Loss S01:  0.06127619081073337\n",
            "Loss S2:  0.06948632489383956\n",
            "Loss S01:  0.06118302906935032\n",
            "Loss S2:  0.06959222396323969\n",
            "Loss S01:  0.0610362236009966\n",
            "Loss S2:  0.06939451934972612\n",
            "Loss S01:  0.06091593518047719\n",
            "Loss S2:  0.06931512510857067\n",
            "Loss S01:  0.06085769524259015\n",
            "Loss S2:  0.06906929572135949\n",
            "Loss S01:  0.060917768513655844\n",
            "Loss S2:  0.06919070444148005\n",
            "Loss S01:  0.060961824121838766\n",
            "Loss S2:  0.06921847418267676\n",
            "Loss S01:  0.061155624268268115\n",
            "Loss S2:  0.06928151071268991\n",
            "Loss S01:  0.06119824284167023\n",
            "Loss S2:  0.06937667051827685\n",
            "Loss S01:  0.06109584626137165\n",
            "Loss S2:  0.06941226651968314\n",
            "Loss S01:  0.06104349826580912\n",
            "Loss S2:  0.06940062599287507\n",
            "Loss S01:  0.06102594339254639\n",
            "Loss S2:  0.06945465263271831\n",
            "Loss S01:  0.06101868931778628\n",
            "Loss S2:  0.0695636868106192\n",
            "Loss S01:  0.06107876433933516\n",
            "Loss S2:  0.06961750532213545\n",
            "Loss S01:  0.06116162542236876\n",
            "Loss S2:  0.06975212973032602\n",
            "Loss S01:  0.061187442617885994\n",
            "Loss S2:  0.0697337222563756\n",
            "Loss S01:  0.06121691045857564\n",
            "Loss S2:  0.06974778602603066\n",
            "Loss S01:  0.061243441564390856\n",
            "Loss S2:  0.06980114817559957\n",
            "Loss S01:  0.06127546913208176\n",
            "Loss S2:  0.0698343067933088\n",
            "Loss S01:  0.06131099427043292\n",
            "Loss S2:  0.0699125389083945\n",
            "Loss S01:  0.06125829652535109\n",
            "Loss S2:  0.0699131756596718\n",
            "Loss S01:  0.06123843474179199\n",
            "Loss S2:  0.06992705869152374\n",
            "Loss S01:  0.06122675524449032\n",
            "Loss S2:  0.06993178252217382\n",
            "Loss S01:  0.06119390581869236\n",
            "Loss S2:  0.06991060964547553\n",
            "Loss S01:  0.061241715496574235\n",
            "Loss S2:  0.0699308071273881\n",
            "Loss S01:  0.06127095583747521\n",
            "Loss S2:  0.06996092904820543\n",
            "Loss S01:  0.06129294813028878\n",
            "Loss S2:  0.06993256697207252\n",
            "Loss S01:  0.06131803072415865\n",
            "Loss S2:  0.069951248177436\n",
            "Loss S01:  0.06129299874038247\n",
            "Loss S2:  0.06990154046266032\n",
            "Loss S01:  0.06126774436821835\n",
            "Loss S2:  0.06989105684416634\n",
            "Loss S01:  0.061234582264435886\n",
            "Loss S2:  0.0697997952970307\n",
            "Loss S01:  0.061204217021803724\n",
            "Loss S2:  0.0697444554923288\n",
            "Loss S01:  0.0612118174589037\n",
            "Loss S2:  0.06975356438324933\n",
            "Loss S01:  0.06119931836379126\n",
            "Loss S2:  0.06973520551248479\n",
            "Loss S01:  0.06125052367069376\n",
            "Loss S2:  0.06980112818846226\n",
            "Loss S01:  0.06124817096820564\n",
            "Loss S2:  0.06975776055641905\n",
            "Loss S01:  0.061220365291347306\n",
            "Loss S2:  0.06978114875130642\n",
            "Loss S01:  0.0611796436083819\n",
            "Loss S2:  0.06976124127199275\n",
            "Loss S01:  0.061198424297403096\n",
            "Loss S2:  0.06973033706271727\n",
            "Loss S01:  0.06120428461146456\n",
            "Loss S2:  0.06970751183522735\n",
            "Loss S01:  0.061219757039432963\n",
            "Loss S2:  0.0696699185811928\n",
            "Loss S01:  0.061237010907556275\n",
            "Loss S2:  0.06965797396790228\n",
            "Validation: \n",
            " Loss S01:  0.07472767680883408\n",
            " Loss S2:  0.1173774003982544\n",
            " Loss S01:  0.07864927997191747\n",
            " Loss S2:  0.1314687029946418\n",
            " Loss S01:  0.07734129087227147\n",
            " Loss S2:  0.1303317986610459\n",
            " Loss S01:  0.0763847300263702\n",
            " Loss S2:  0.12840434261521355\n",
            " Loss S01:  0.07561812191097825\n",
            " Loss S2:  0.12762668305709038\n",
            "\n",
            "Epoch: 42\n",
            "Loss S01:  0.058583926409482956\n",
            "Loss S2:  0.06672300398349762\n",
            "Loss S01:  0.06195181743665175\n",
            "Loss S2:  0.06993033906275575\n",
            "Loss S01:  0.06161142388979594\n",
            "Loss S2:  0.06927775023948579\n",
            "Loss S01:  0.06104578618561068\n",
            "Loss S2:  0.06899644097974224\n",
            "Loss S01:  0.061159907681186024\n",
            "Loss S2:  0.0689994813647212\n",
            "Loss S01:  0.06111137773476395\n",
            "Loss S2:  0.06848312972807417\n",
            "Loss S01:  0.06105814489429114\n",
            "Loss S2:  0.06919243217247431\n",
            "Loss S01:  0.060882906573759\n",
            "Loss S2:  0.06893759845218188\n",
            "Loss S01:  0.06088353182982515\n",
            "Loss S2:  0.06901264103290475\n",
            "Loss S01:  0.06096019243801033\n",
            "Loss S2:  0.06913561189731399\n",
            "Loss S01:  0.060872717548419934\n",
            "Loss S2:  0.06899368290853973\n",
            "Loss S01:  0.06046532188449894\n",
            "Loss S2:  0.06862424814083555\n",
            "Loss S01:  0.06033210271646169\n",
            "Loss S2:  0.06836102702770351\n",
            "Loss S01:  0.06034487175918717\n",
            "Loss S2:  0.06842812392434092\n",
            "Loss S01:  0.06039613408716858\n",
            "Loss S2:  0.06853204630051099\n",
            "Loss S01:  0.060544463824357415\n",
            "Loss S2:  0.06869141986926659\n",
            "Loss S01:  0.06059084513357708\n",
            "Loss S2:  0.06866919857837399\n",
            "Loss S01:  0.060587674379348755\n",
            "Loss S2:  0.06872166181255503\n",
            "Loss S01:  0.06063242419236931\n",
            "Loss S2:  0.06879902340842216\n",
            "Loss S01:  0.06064145071968358\n",
            "Loss S2:  0.06886521945293037\n",
            "Loss S01:  0.06065103743429208\n",
            "Loss S2:  0.06906562773001135\n",
            "Loss S01:  0.06067530759632305\n",
            "Loss S2:  0.06909165105901623\n",
            "Loss S01:  0.060714409210428394\n",
            "Loss S2:  0.0692031152748684\n",
            "Loss S01:  0.060800882193433256\n",
            "Loss S2:  0.06929541459847323\n",
            "Loss S01:  0.0608631729523176\n",
            "Loss S2:  0.06939411398286147\n",
            "Loss S01:  0.060854760121064354\n",
            "Loss S2:  0.0694084219129912\n",
            "Loss S01:  0.06086345557908446\n",
            "Loss S2:  0.06941859147215255\n",
            "Loss S01:  0.06093267703298273\n",
            "Loss S2:  0.06949323157322802\n",
            "Loss S01:  0.0609489884824091\n",
            "Loss S2:  0.06951485950751661\n",
            "Loss S01:  0.06093765299805661\n",
            "Loss S2:  0.0695228599703189\n",
            "Loss S01:  0.06096139907886419\n",
            "Loss S2:  0.06956628677852922\n",
            "Loss S01:  0.06090847080591407\n",
            "Loss S2:  0.06949226130578678\n",
            "Loss S01:  0.060966517207592816\n",
            "Loss S2:  0.06950633350840982\n",
            "Loss S01:  0.06095148395645655\n",
            "Loss S2:  0.06943598851066342\n",
            "Loss S01:  0.06093645271711336\n",
            "Loss S2:  0.06940743706224602\n",
            "Loss S01:  0.06094685299826144\n",
            "Loss S2:  0.06939899913987882\n",
            "Loss S01:  0.06090014200874313\n",
            "Loss S2:  0.06936583487404681\n",
            "Loss S01:  0.06086847596774204\n",
            "Loss S2:  0.06932234001970998\n",
            "Loss S01:  0.06086055974517594\n",
            "Loss S2:  0.069269179730747\n",
            "Loss S01:  0.06081725849443689\n",
            "Loss S2:  0.06919801402884676\n",
            "Loss S01:  0.060782764264918916\n",
            "Loss S2:  0.06917255267760998\n",
            "Loss S01:  0.060771003329260796\n",
            "Loss S2:  0.06915951788497958\n",
            "Loss S01:  0.0608274445663297\n",
            "Loss S2:  0.0692132450197768\n",
            "Loss S01:  0.06079527015717843\n",
            "Loss S2:  0.06917086420818437\n",
            "Loss S01:  0.060791407228191964\n",
            "Loss S2:  0.06917689592828827\n",
            "Loss S01:  0.06080501625714968\n",
            "Loss S2:  0.06916234473761856\n",
            "Loss S01:  0.06083153546212035\n",
            "Loss S2:  0.06916936108773805\n",
            "Loss S01:  0.06080616441913218\n",
            "Loss S2:  0.06910988997723393\n",
            "Loss S01:  0.06079510945571188\n",
            "Loss S2:  0.069113639196896\n",
            "Loss S01:  0.06076304851262972\n",
            "Loss S2:  0.06910623924886379\n",
            "Validation: \n",
            " Loss S01:  0.07320093363523483\n",
            " Loss S2:  0.1152237132191658\n",
            " Loss S01:  0.07871649875527337\n",
            " Loss S2:  0.12977223914294017\n",
            " Loss S01:  0.07788745822702967\n",
            " Loss S2:  0.1280735582113266\n",
            " Loss S01:  0.0767603115346588\n",
            " Loss S2:  0.12606101971669276\n",
            " Loss S01:  0.07580964538602182\n",
            " Loss S2:  0.12545754263798395\n",
            "\n",
            "Epoch: 43\n",
            "Loss S01:  0.06306812167167664\n",
            "Loss S2:  0.06972804665565491\n",
            "Loss S01:  0.06299053674394434\n",
            "Loss S2:  0.06928079710765318\n",
            "Loss S01:  0.06219463422894478\n",
            "Loss S2:  0.06902027910663969\n",
            "Loss S01:  0.06096305986565928\n",
            "Loss S2:  0.0678417066172246\n",
            "Loss S01:  0.06059730861608575\n",
            "Loss S2:  0.06793684912164037\n",
            "Loss S01:  0.06069275977856973\n",
            "Loss S2:  0.06797027690153495\n",
            "Loss S01:  0.06088603959708917\n",
            "Loss S2:  0.06827241506000034\n",
            "Loss S01:  0.060515853982995936\n",
            "Loss S2:  0.0681311707471458\n",
            "Loss S01:  0.060508693671888776\n",
            "Loss S2:  0.06825258325279494\n",
            "Loss S01:  0.060693755049954404\n",
            "Loss S2:  0.0683714883340584\n",
            "Loss S01:  0.06058602042422436\n",
            "Loss S2:  0.06831123370050203\n",
            "Loss S01:  0.06024778369176495\n",
            "Loss S2:  0.06808344462701867\n",
            "Loss S01:  0.06019908836311545\n",
            "Loss S2:  0.06801482339289562\n",
            "Loss S01:  0.06008733271528746\n",
            "Loss S2:  0.06810148651818283\n",
            "Loss S01:  0.060086680668677\n",
            "Loss S2:  0.06813798194870035\n",
            "Loss S01:  0.060036700761673466\n",
            "Loss S2:  0.06822879701260699\n",
            "Loss S01:  0.06007946472360481\n",
            "Loss S2:  0.06813891581404283\n",
            "Loss S01:  0.05999157124618341\n",
            "Loss S2:  0.0681392781610726\n",
            "Loss S01:  0.06002350355297821\n",
            "Loss S2:  0.06813111300297205\n",
            "Loss S01:  0.060084411416066255\n",
            "Loss S2:  0.06812166676159305\n",
            "Loss S01:  0.060144247033109714\n",
            "Loss S2:  0.06817037176655892\n",
            "Loss S01:  0.060236289797094764\n",
            "Loss S2:  0.0682052882538305\n",
            "Loss S01:  0.060295784146402756\n",
            "Loss S2:  0.06830475666471736\n",
            "Loss S01:  0.060362796972453336\n",
            "Loss S2:  0.06831150897073024\n",
            "Loss S01:  0.060467211908324624\n",
            "Loss S2:  0.0684222314213074\n",
            "Loss S01:  0.060479831855848015\n",
            "Loss S2:  0.06849315202984202\n",
            "Loss S01:  0.06052177898956898\n",
            "Loss S2:  0.06856709647338509\n",
            "Loss S01:  0.060532526061543676\n",
            "Loss S2:  0.06856283364040826\n",
            "Loss S01:  0.06049672923849571\n",
            "Loss S2:  0.06857711779371276\n",
            "Loss S01:  0.06046522345995575\n",
            "Loss S2:  0.06853066275978006\n",
            "Loss S01:  0.06043104327398281\n",
            "Loss S2:  0.06856154238316307\n",
            "Loss S01:  0.06035424703043373\n",
            "Loss S2:  0.0685333575801834\n",
            "Loss S01:  0.060357719497331576\n",
            "Loss S2:  0.0685115038327339\n",
            "Loss S01:  0.060370214463540794\n",
            "Loss S2:  0.0684867108471444\n",
            "Loss S01:  0.06037356683716746\n",
            "Loss S2:  0.06848286444053622\n",
            "Loss S01:  0.060386232278200974\n",
            "Loss S2:  0.06852556148443127\n",
            "Loss S01:  0.06031202691042192\n",
            "Loss S2:  0.06845065188746373\n",
            "Loss S01:  0.060282541825523915\n",
            "Loss S2:  0.06850239137473775\n",
            "Loss S01:  0.060282801216825097\n",
            "Loss S2:  0.0684817948933505\n",
            "Loss S01:  0.06023020096256605\n",
            "Loss S2:  0.06841598812233457\n",
            "Loss S01:  0.060216801552255256\n",
            "Loss S2:  0.0683776768402863\n",
            "Loss S01:  0.06021035505218517\n",
            "Loss S2:  0.06837448867495623\n",
            "Loss S01:  0.06026231129421474\n",
            "Loss S2:  0.06843658169452764\n",
            "Loss S01:  0.060251163668809525\n",
            "Loss S2:  0.0684270830500845\n",
            "Loss S01:  0.06025244522831337\n",
            "Loss S2:  0.06845832215685423\n",
            "Loss S01:  0.06026733970628874\n",
            "Loss S2:  0.06848272805963282\n",
            "Loss S01:  0.06028111026078915\n",
            "Loss S2:  0.06843574651377579\n",
            "Loss S01:  0.06026539295880152\n",
            "Loss S2:  0.06843567478239157\n",
            "Loss S01:  0.06025533378898726\n",
            "Loss S2:  0.0684085021070408\n",
            "Loss S01:  0.06024781862641789\n",
            "Loss S2:  0.06839677875068668\n",
            "Validation: \n",
            " Loss S01:  0.07216119021177292\n",
            " Loss S2:  0.11431840807199478\n",
            " Loss S01:  0.07485087960958481\n",
            " Loss S2:  0.12968398133913675\n",
            " Loss S01:  0.07411496268539894\n",
            " Loss S2:  0.12838486236769978\n",
            " Loss S01:  0.07310180201149377\n",
            " Loss S2:  0.12646410125689428\n",
            " Loss S01:  0.07260776380145992\n",
            " Loss S2:  0.12574410539718323\n",
            "\n",
            "Epoch: 44\n",
            "Loss S01:  0.05877649039030075\n",
            "Loss S2:  0.07438652217388153\n",
            "Loss S01:  0.06059525263580409\n",
            "Loss S2:  0.06993587890809233\n",
            "Loss S01:  0.06076850742101669\n",
            "Loss S2:  0.06976890794578053\n",
            "Loss S01:  0.06007727716238268\n",
            "Loss S2:  0.06838717792303331\n",
            "Loss S01:  0.060092647173782675\n",
            "Loss S2:  0.0681379138514763\n",
            "Loss S01:  0.06001501859110944\n",
            "Loss S2:  0.06826440960753198\n",
            "Loss S01:  0.06026784037468863\n",
            "Loss S2:  0.06859126851939765\n",
            "Loss S01:  0.059880806264323246\n",
            "Loss S2:  0.06800551256033736\n",
            "Loss S01:  0.05986898958131119\n",
            "Loss S2:  0.06811060457501883\n",
            "Loss S01:  0.059750447602389935\n",
            "Loss S2:  0.06798021548560687\n",
            "Loss S01:  0.05970345540802077\n",
            "Loss S2:  0.06791953019576498\n",
            "Loss S01:  0.05955334621909503\n",
            "Loss S2:  0.06766464463896579\n",
            "Loss S01:  0.059484776786782524\n",
            "Loss S2:  0.06758896036704709\n",
            "Loss S01:  0.05946192810781129\n",
            "Loss S2:  0.06771945671839569\n",
            "Loss S01:  0.05940229313593384\n",
            "Loss S2:  0.06772890726619578\n",
            "Loss S01:  0.0594651700052994\n",
            "Loss S2:  0.06788667054563169\n",
            "Loss S01:  0.05945472995409314\n",
            "Loss S2:  0.06784989145694312\n",
            "Loss S01:  0.05950117165669363\n",
            "Loss S2:  0.0679271030539309\n",
            "Loss S01:  0.05954243137773888\n",
            "Loss S2:  0.06797375046334214\n",
            "Loss S01:  0.059587882587890974\n",
            "Loss S2:  0.06808685523053114\n",
            "Loss S01:  0.059639598988923265\n",
            "Loss S2:  0.0681012998247028\n",
            "Loss S01:  0.05973486411628\n",
            "Loss S2:  0.06810615155227942\n",
            "Loss S01:  0.05981281920593249\n",
            "Loss S2:  0.06817278744678153\n",
            "Loss S01:  0.059820732287256234\n",
            "Loss S2:  0.06818608863851724\n",
            "Loss S01:  0.059920889785304604\n",
            "Loss S2:  0.06829319815479869\n",
            "Loss S01:  0.05988761744057515\n",
            "Loss S2:  0.06827295386043203\n",
            "Loss S01:  0.05992122304222593\n",
            "Loss S2:  0.06832800972564466\n",
            "Loss S01:  0.059885275553623246\n",
            "Loss S2:  0.06834095880059299\n",
            "Loss S01:  0.05988763521998802\n",
            "Loss S2:  0.06828277051660939\n",
            "Loss S01:  0.05991087460896813\n",
            "Loss S2:  0.06825725290015391\n",
            "Loss S01:  0.05987715619644057\n",
            "Loss S2:  0.06826545720134067\n",
            "Loss S01:  0.05974991606410677\n",
            "Loss S2:  0.06813480047407258\n",
            "Loss S01:  0.05978551208740825\n",
            "Loss S2:  0.06814763733298979\n",
            "Loss S01:  0.05975827692903781\n",
            "Loss S2:  0.06809128986339555\n",
            "Loss S01:  0.059763790072758526\n",
            "Loss S2:  0.06809775418768531\n",
            "Loss S01:  0.05974960581869142\n",
            "Loss S2:  0.06806895168673278\n",
            "Loss S01:  0.05973850905771401\n",
            "Loss S2:  0.0680332167533296\n",
            "Loss S01:  0.05968131228397799\n",
            "Loss S2:  0.06794337700359583\n",
            "Loss S01:  0.05962823137758285\n",
            "Loss S2:  0.06792808955735735\n",
            "Loss S01:  0.05956990293720189\n",
            "Loss S2:  0.06783711237599478\n",
            "Loss S01:  0.05961588185810091\n",
            "Loss S2:  0.06781971970215403\n",
            "Loss S01:  0.05959486816341279\n",
            "Loss S2:  0.06782923656948582\n",
            "Loss S01:  0.059603640805230854\n",
            "Loss S2:  0.06782197584165814\n",
            "Loss S01:  0.05957766400170437\n",
            "Loss S2:  0.06781412690463033\n",
            "Loss S01:  0.05951815841892679\n",
            "Loss S2:  0.06782534547031872\n",
            "Loss S01:  0.05951132582389066\n",
            "Loss S2:  0.06781741149079244\n",
            "Loss S01:  0.05952978732949224\n",
            "Loss S2:  0.06783160501828421\n",
            "Loss S01:  0.059522125181878445\n",
            "Loss S2:  0.06784488006065859\n",
            "Loss S01:  0.059526388546966964\n",
            "Loss S2:  0.06784373720848387\n",
            "Loss S01:  0.059553617433229435\n",
            "Loss S2:  0.06779965298542423\n",
            "Validation: \n",
            " Loss S01:  0.07494549453258514\n",
            " Loss S2:  0.12205932289361954\n",
            " Loss S01:  0.07769018979299636\n",
            " Loss S2:  0.13792766701607478\n",
            " Loss S01:  0.07682878160622061\n",
            " Loss S2:  0.13632052028324546\n",
            " Loss S01:  0.0757114934261705\n",
            " Loss S2:  0.13456243459807068\n",
            " Loss S01:  0.07498852379712058\n",
            " Loss S2:  0.13367469443215263\n",
            "\n",
            "Epoch: 45\n",
            "Loss S01:  0.05716563016176224\n",
            "Loss S2:  0.06736598908901215\n",
            "Loss S01:  0.06122156638990749\n",
            "Loss S2:  0.06959620118141174\n",
            "Loss S01:  0.06091883920487903\n",
            "Loss S2:  0.06842131291826566\n",
            "Loss S01:  0.05974512655408152\n",
            "Loss S2:  0.06761553078409165\n",
            "Loss S01:  0.05955193500693252\n",
            "Loss S2:  0.06720969580659052\n",
            "Loss S01:  0.059530434216938766\n",
            "Loss S2:  0.06708974902536355\n",
            "Loss S01:  0.059841898560035425\n",
            "Loss S2:  0.06732198406682639\n",
            "Loss S01:  0.059661676608760594\n",
            "Loss S2:  0.06705789530361203\n",
            "Loss S01:  0.059423429822480237\n",
            "Loss S2:  0.06700302784641583\n",
            "Loss S01:  0.05922348019513455\n",
            "Loss S2:  0.06711399608424731\n",
            "Loss S01:  0.059143964754472866\n",
            "Loss S2:  0.06694132657629429\n",
            "Loss S01:  0.05888602624202634\n",
            "Loss S2:  0.06681587355765137\n",
            "Loss S01:  0.058897433109766195\n",
            "Loss S2:  0.06690257362836649\n",
            "Loss S01:  0.058962097604766145\n",
            "Loss S2:  0.06701884239570785\n",
            "Loss S01:  0.058822309822900924\n",
            "Loss S2:  0.06705969669823106\n",
            "Loss S01:  0.05895368690719668\n",
            "Loss S2:  0.06736516254331103\n",
            "Loss S01:  0.05889584242094378\n",
            "Loss S2:  0.06715725607568433\n",
            "Loss S01:  0.058826874549451624\n",
            "Loss S2:  0.06720405684141388\n",
            "Loss S01:  0.05892337906014854\n",
            "Loss S2:  0.06721837271148987\n",
            "Loss S01:  0.05898652432476663\n",
            "Loss S2:  0.06723813683340686\n",
            "Loss S01:  0.058958370442414164\n",
            "Loss S2:  0.06727582996535064\n",
            "Loss S01:  0.058996288185294773\n",
            "Loss S2:  0.06731726192114477\n",
            "Loss S01:  0.05903230710334368\n",
            "Loss S2:  0.06737196708426756\n",
            "Loss S01:  0.059015351956411874\n",
            "Loss S2:  0.06743940269147163\n",
            "Loss S01:  0.05908739951698117\n",
            "Loss S2:  0.067560362710745\n",
            "Loss S01:  0.05907937574137254\n",
            "Loss S2:  0.06763040197204309\n",
            "Loss S01:  0.0591403863239334\n",
            "Loss S2:  0.0676291934946031\n",
            "Loss S01:  0.05914353375835173\n",
            "Loss S2:  0.06764346446388322\n",
            "Loss S01:  0.05919858024478807\n",
            "Loss S2:  0.06770402883772748\n",
            "Loss S01:  0.05923993721655554\n",
            "Loss S2:  0.06773703263694887\n",
            "Loss S01:  0.05925700281347547\n",
            "Loss S2:  0.06773146766661806\n",
            "Loss S01:  0.059225792310820514\n",
            "Loss S2:  0.0676635881568458\n",
            "Loss S01:  0.059299431369869135\n",
            "Loss S2:  0.0677281743455156\n",
            "Loss S01:  0.059332757490459166\n",
            "Loss S2:  0.06767245860393314\n",
            "Loss S01:  0.059357414071248775\n",
            "Loss S2:  0.06769750596097837\n",
            "Loss S01:  0.059330007714084074\n",
            "Loss S2:  0.06767005014878053\n",
            "Loss S01:  0.05931423811907583\n",
            "Loss S2:  0.06757503208237342\n",
            "Loss S01:  0.05930029457269332\n",
            "Loss S2:  0.06751867502204813\n",
            "Loss S01:  0.059291553309583286\n",
            "Loss S2:  0.06748001030071826\n",
            "Loss S01:  0.059223409294319886\n",
            "Loss S2:  0.0673907872413278\n",
            "Loss S01:  0.059276408090853036\n",
            "Loss S2:  0.0673788696193041\n",
            "Loss S01:  0.059261239122445276\n",
            "Loss S2:  0.06738438834311608\n",
            "Loss S01:  0.059276141677573856\n",
            "Loss S2:  0.06738808632534077\n",
            "Loss S01:  0.05927210559597425\n",
            "Loss S2:  0.06735057909292301\n",
            "Loss S01:  0.05925645145361656\n",
            "Loss S2:  0.06732458486105579\n",
            "Loss S01:  0.059277792000717706\n",
            "Loss S2:  0.06730939463573919\n",
            "Loss S01:  0.0592797344102001\n",
            "Loss S2:  0.06730885541070568\n",
            "Loss S01:  0.059247211222942214\n",
            "Loss S2:  0.06726216864149281\n",
            "Loss S01:  0.05925722115067088\n",
            "Loss S2:  0.0672827330330801\n",
            "Loss S01:  0.05925266847560702\n",
            "Loss S2:  0.06731912291128864\n",
            "Validation: \n",
            " Loss S01:  0.0716540589928627\n",
            " Loss S2:  0.11152772605419159\n",
            " Loss S01:  0.07487977863777251\n",
            " Loss S2:  0.12928174613487153\n",
            " Loss S01:  0.07368375651720094\n",
            " Loss S2:  0.12819063427244745\n",
            " Loss S01:  0.07280038046787997\n",
            " Loss S2:  0.12644343810980438\n",
            " Loss S01:  0.07199556808228846\n",
            " Loss S2:  0.12568255808250403\n",
            "\n",
            "Epoch: 46\n",
            "Loss S01:  0.0568048469722271\n",
            "Loss S2:  0.06637075543403625\n",
            "Loss S01:  0.0602792813019319\n",
            "Loss S2:  0.06958211348815398\n",
            "Loss S01:  0.06006665935828572\n",
            "Loss S2:  0.06907102784940175\n",
            "Loss S01:  0.0598406660700998\n",
            "Loss S2:  0.0687839301122773\n",
            "Loss S01:  0.0595284178853035\n",
            "Loss S2:  0.0684765868070649\n",
            "Loss S01:  0.05957285599673495\n",
            "Loss S2:  0.06834473921095624\n",
            "Loss S01:  0.05959737355836102\n",
            "Loss S2:  0.06845361165335921\n",
            "Loss S01:  0.059451938462509235\n",
            "Loss S2:  0.06811840207853788\n",
            "Loss S01:  0.059435561023376604\n",
            "Loss S2:  0.06793439779200672\n",
            "Loss S01:  0.059383450710511466\n",
            "Loss S2:  0.06792369251559069\n",
            "Loss S01:  0.05925778889715081\n",
            "Loss S2:  0.06784476929962045\n",
            "Loss S01:  0.05897136979006432\n",
            "Loss S2:  0.06760661968508282\n",
            "Loss S01:  0.05892882425307242\n",
            "Loss S2:  0.06739374927499077\n",
            "Loss S01:  0.058827795002979175\n",
            "Loss S2:  0.06755645308672016\n",
            "Loss S01:  0.05876578874093421\n",
            "Loss S2:  0.06761934304385321\n",
            "Loss S01:  0.0588224431783553\n",
            "Loss S2:  0.06769505014877446\n",
            "Loss S01:  0.05876601040826081\n",
            "Loss S2:  0.06750314282426922\n",
            "Loss S01:  0.05873117517483862\n",
            "Loss S2:  0.06737264694526182\n",
            "Loss S01:  0.05880328514793301\n",
            "Loss S2:  0.06737885646810189\n",
            "Loss S01:  0.058782400943689944\n",
            "Loss S2:  0.06737739417253365\n",
            "Loss S01:  0.058818862768844585\n",
            "Loss S2:  0.06747815498516928\n",
            "Loss S01:  0.05887422454653758\n",
            "Loss S2:  0.06753750129545469\n",
            "Loss S01:  0.058966049730642886\n",
            "Loss S2:  0.0675840466978593\n",
            "Loss S01:  0.0589045351814656\n",
            "Loss S2:  0.06753320817823534\n",
            "Loss S01:  0.05901638446259795\n",
            "Loss S2:  0.06760840182853438\n",
            "Loss S01:  0.05905323080807568\n",
            "Loss S2:  0.06759070095966062\n",
            "Loss S01:  0.05909422730805773\n",
            "Loss S2:  0.06764002965042418\n",
            "Loss S01:  0.05909765066365914\n",
            "Loss S2:  0.06759173707603529\n",
            "Loss S01:  0.05909575809628514\n",
            "Loss S2:  0.0675890475007775\n",
            "Loss S01:  0.05902901935771978\n",
            "Loss S2:  0.06755718916552174\n",
            "Loss S01:  0.059080422543410054\n",
            "Loss S2:  0.06761558861332478\n",
            "Loss S01:  0.05901902395381421\n",
            "Loss S2:  0.06753198027467038\n",
            "Loss S01:  0.05909637771430788\n",
            "Loss S2:  0.06755957991534675\n",
            "Loss S01:  0.0591001591203436\n",
            "Loss S2:  0.0674980707083944\n",
            "Loss S01:  0.05908308068940367\n",
            "Loss S2:  0.06750826430547971\n",
            "Loss S01:  0.059105680698258244\n",
            "Loss S2:  0.06755002972237405\n",
            "Loss S01:  0.05909057016229035\n",
            "Loss S2:  0.06744157127685164\n",
            "Loss S01:  0.05905217291772205\n",
            "Loss S2:  0.06741834406783638\n",
            "Loss S01:  0.059013370189845094\n",
            "Loss S2:  0.06740598430431734\n",
            "Loss S01:  0.05894559754244507\n",
            "Loss S2:  0.0673280510279681\n",
            "Loss S01:  0.05896665473605629\n",
            "Loss S2:  0.06733216113989193\n",
            "Loss S01:  0.05895931731429123\n",
            "Loss S2:  0.06734301053767076\n",
            "Loss S01:  0.05896275858627079\n",
            "Loss S2:  0.0673163976086715\n",
            "Loss S01:  0.05896608624823409\n",
            "Loss S2:  0.06725470645660038\n",
            "Loss S01:  0.05900270725830612\n",
            "Loss S2:  0.0672442719948535\n",
            "Loss S01:  0.05899890252861903\n",
            "Loss S2:  0.06724091826580844\n",
            "Loss S01:  0.05901011958750623\n",
            "Loss S2:  0.06725774666621731\n",
            "Loss S01:  0.05896537562717551\n",
            "Loss S2:  0.06725711282144939\n",
            "Loss S01:  0.058956553257242805\n",
            "Loss S2:  0.06725261023504323\n",
            "Loss S01:  0.058959108635085175\n",
            "Loss S2:  0.06725362814249672\n",
            "Validation: \n",
            " Loss S01:  0.07069575041532516\n",
            " Loss S2:  0.11622945219278336\n",
            " Loss S01:  0.07491887857516606\n",
            " Loss S2:  0.13083756608622416\n",
            " Loss S01:  0.07365697004446169\n",
            " Loss S2:  0.12970086768632982\n",
            " Loss S01:  0.07288897800885263\n",
            " Loss S2:  0.12795458868390225\n",
            " Loss S01:  0.07248054973689126\n",
            " Loss S2:  0.12734719025500027\n",
            "\n",
            "Epoch: 47\n",
            "Loss S01:  0.05912812054157257\n",
            "Loss S2:  0.0773356482386589\n",
            "Loss S01:  0.059540803798220375\n",
            "Loss S2:  0.06875432621348988\n",
            "Loss S01:  0.0586965163903577\n",
            "Loss S2:  0.06795176757233483\n",
            "Loss S01:  0.05841200089743061\n",
            "Loss S2:  0.06652855452510618\n",
            "Loss S01:  0.05835578790525111\n",
            "Loss S2:  0.06632801481499905\n",
            "Loss S01:  0.05885811697910814\n",
            "Loss S2:  0.06655445548833586\n",
            "Loss S01:  0.058717025779798385\n",
            "Loss S2:  0.06687542838884182\n",
            "Loss S01:  0.058622943001313946\n",
            "Loss S2:  0.06662000327462882\n",
            "Loss S01:  0.058426427436463625\n",
            "Loss S2:  0.06646417849409728\n",
            "Loss S01:  0.05839417031505606\n",
            "Loss S2:  0.06653782422398473\n",
            "Loss S01:  0.05829835321643565\n",
            "Loss S2:  0.06649861661809506\n",
            "Loss S01:  0.058164915657258245\n",
            "Loss S2:  0.06641259980765549\n",
            "Loss S01:  0.05810013037821478\n",
            "Loss S2:  0.06634658248710239\n",
            "Loss S01:  0.05817860654974712\n",
            "Loss S2:  0.06639447731484893\n",
            "Loss S01:  0.05827906302420806\n",
            "Loss S2:  0.06645882108532791\n",
            "Loss S01:  0.05850521635436854\n",
            "Loss S2:  0.0665588579233119\n",
            "Loss S01:  0.0585763034243021\n",
            "Loss S2:  0.06658365600597785\n",
            "Loss S01:  0.058564427521145136\n",
            "Loss S2:  0.06666079015411132\n",
            "Loss S01:  0.0586432581613077\n",
            "Loss S2:  0.06669006523424091\n",
            "Loss S01:  0.05864578664926958\n",
            "Loss S2:  0.06676778372825752\n",
            "Loss S01:  0.058564994298848344\n",
            "Loss S2:  0.0667113275892699\n",
            "Loss S01:  0.058640345330769415\n",
            "Loss S2:  0.06670917644743671\n",
            "Loss S01:  0.058721427666655494\n",
            "Loss S2:  0.06672895770536828\n",
            "Loss S01:  0.05879889843074274\n",
            "Loss S2:  0.06676476269766882\n",
            "Loss S01:  0.05881996854706919\n",
            "Loss S2:  0.06674154163087058\n",
            "Loss S01:  0.05882494264389889\n",
            "Loss S2:  0.06677105208020286\n",
            "Loss S01:  0.05888692644992094\n",
            "Loss S2:  0.06689942788004419\n",
            "Loss S01:  0.0589027034730489\n",
            "Loss S2:  0.06684975908911096\n",
            "Loss S01:  0.05889104284160502\n",
            "Loss S2:  0.06686019577960951\n",
            "Loss S01:  0.058895332962786616\n",
            "Loss S2:  0.06691962613682567\n",
            "Loss S01:  0.058853576337281256\n",
            "Loss S2:  0.0669013535931855\n",
            "Loss S01:  0.05872635195972069\n",
            "Loss S2:  0.06685814122486727\n",
            "Loss S01:  0.0587806762703854\n",
            "Loss S2:  0.06683639939468226\n",
            "Loss S01:  0.05870698026614967\n",
            "Loss S2:  0.06679393669584727\n",
            "Loss S01:  0.05869589836672604\n",
            "Loss S2:  0.0667956317297\n",
            "Loss S01:  0.05874389026876528\n",
            "Loss S2:  0.06680931972387509\n",
            "Loss S01:  0.05873591604531637\n",
            "Loss S2:  0.0668249755912045\n",
            "Loss S01:  0.0587099652406019\n",
            "Loss S2:  0.06677100898803405\n",
            "Loss S01:  0.0586822499029749\n",
            "Loss S2:  0.06672299777468045\n",
            "Loss S01:  0.0586488011109707\n",
            "Loss S2:  0.06666371474028243\n",
            "Loss S01:  0.05870331188389785\n",
            "Loss S2:  0.06668556227053787\n",
            "Loss S01:  0.05868352750212027\n",
            "Loss S2:  0.06670099206794497\n",
            "Loss S01:  0.05870984509728867\n",
            "Loss S2:  0.06678652983698596\n",
            "Loss S01:  0.058680980248061246\n",
            "Loss S2:  0.06674355591740243\n",
            "Loss S01:  0.05868443037444502\n",
            "Loss S2:  0.06673403042885992\n",
            "Loss S01:  0.058698925980575335\n",
            "Loss S2:  0.06674916288937274\n",
            "Loss S01:  0.05870827363395898\n",
            "Loss S2:  0.06672387689739918\n",
            "Loss S01:  0.05866995919693554\n",
            "Loss S2:  0.06667772402903836\n",
            "Loss S01:  0.05867362755840633\n",
            "Loss S2:  0.06664494613259102\n",
            "Loss S01:  0.058653172120725065\n",
            "Loss S2:  0.06663893081262981\n",
            "Validation: \n",
            " Loss S01:  0.07297257333993912\n",
            " Loss S2:  0.11691873520612717\n",
            " Loss S01:  0.07796049898579008\n",
            " Loss S2:  0.1324669055285908\n",
            " Loss S01:  0.07745361600707217\n",
            " Loss S2:  0.1310282967439512\n",
            " Loss S01:  0.07651300502360844\n",
            " Loss S2:  0.12943027131870144\n",
            " Loss S01:  0.07578076307236413\n",
            " Loss S2:  0.12868982590275047\n",
            "\n",
            "Epoch: 48\n",
            "Loss S01:  0.05753815546631813\n",
            "Loss S2:  0.06995056569576263\n",
            "Loss S01:  0.06019375202330676\n",
            "Loss S2:  0.06993490592999892\n",
            "Loss S01:  0.06034549077351888\n",
            "Loss S2:  0.06985015457584745\n",
            "Loss S01:  0.05926418953364895\n",
            "Loss S2:  0.06787682036238332\n",
            "Loss S01:  0.05939400505001952\n",
            "Loss S2:  0.06754311737490863\n",
            "Loss S01:  0.058910226851117374\n",
            "Loss S2:  0.06690225735598919\n",
            "Loss S01:  0.05897051370779022\n",
            "Loss S2:  0.06687747351214533\n",
            "Loss S01:  0.05880537246104697\n",
            "Loss S2:  0.06653631562498254\n",
            "Loss S01:  0.05866415274363977\n",
            "Loss S2:  0.0664243346747057\n",
            "Loss S01:  0.05843900942376682\n",
            "Loss S2:  0.0665538463291231\n",
            "Loss S01:  0.05831196162812781\n",
            "Loss S2:  0.06638495019166776\n",
            "Loss S01:  0.05813273941879874\n",
            "Loss S2:  0.06612097774003003\n",
            "Loss S01:  0.057994918266603766\n",
            "Loss S2:  0.06586641688977392\n",
            "Loss S01:  0.05806685088358762\n",
            "Loss S2:  0.06599382737665686\n",
            "Loss S01:  0.057989736450902114\n",
            "Loss S2:  0.06605735822772303\n",
            "Loss S01:  0.05816129521028885\n",
            "Loss S2:  0.066177940822595\n",
            "Loss S01:  0.05811190308991426\n",
            "Loss S2:  0.0660817246907246\n",
            "Loss S01:  0.057988686236547446\n",
            "Loss S2:  0.06606369698692484\n",
            "Loss S01:  0.05814609853602246\n",
            "Loss S2:  0.06599397208150579\n",
            "Loss S01:  0.05822434256838254\n",
            "Loss S2:  0.06598932892864287\n",
            "Loss S01:  0.05822979736683974\n",
            "Loss S2:  0.06608652442100629\n",
            "Loss S01:  0.05831269346071646\n",
            "Loss S2:  0.06600222076284942\n",
            "Loss S01:  0.05837068206112309\n",
            "Loss S2:  0.06608062340325899\n",
            "Loss S01:  0.058427678783992666\n",
            "Loss S2:  0.06606332385243276\n",
            "Loss S01:  0.058483275295045864\n",
            "Loss S2:  0.06611040535989639\n",
            "Loss S01:  0.0585395715448011\n",
            "Loss S2:  0.06618353969369277\n",
            "Loss S01:  0.05856460232958483\n",
            "Loss S2:  0.06621367942499018\n",
            "Loss S01:  0.058584097662865015\n",
            "Loss S2:  0.06623875657680729\n",
            "Loss S01:  0.0585763069048683\n",
            "Loss S2:  0.06626471250029646\n",
            "Loss S01:  0.058519201986875734\n",
            "Loss S2:  0.06622879563021086\n",
            "Loss S01:  0.05849278619222071\n",
            "Loss S2:  0.06625672075835573\n",
            "Loss S01:  0.05841549041522277\n",
            "Loss S2:  0.06626467391177772\n",
            "Loss S01:  0.05848021632367948\n",
            "Loss S2:  0.06629753286871955\n",
            "Loss S01:  0.05842742779688893\n",
            "Loss S2:  0.06628310827825365\n",
            "Loss S01:  0.058406481492029955\n",
            "Loss S2:  0.0662998308555996\n",
            "Loss S01:  0.05844616010105848\n",
            "Loss S2:  0.06634136139709726\n",
            "Loss S01:  0.058389374888256976\n",
            "Loss S2:  0.06629070369242962\n",
            "Loss S01:  0.05837066546885794\n",
            "Loss S2:  0.06624206540117045\n",
            "Loss S01:  0.05834296957792572\n",
            "Loss S2:  0.06622710695925348\n",
            "Loss S01:  0.05833345200018505\n",
            "Loss S2:  0.0661872056172327\n",
            "Loss S01:  0.058342804588804816\n",
            "Loss S2:  0.06619147160218243\n",
            "Loss S01:  0.05830157577193856\n",
            "Loss S2:  0.06613771189593341\n",
            "Loss S01:  0.05828973327955837\n",
            "Loss S2:  0.06614172357773837\n",
            "Loss S01:  0.058282093916373574\n",
            "Loss S2:  0.06610969224136556\n",
            "Loss S01:  0.05832265521866394\n",
            "Loss S2:  0.06616998618455971\n",
            "Loss S01:  0.05833973947028628\n",
            "Loss S2:  0.06614290527222955\n",
            "Loss S01:  0.058330830833076656\n",
            "Loss S2:  0.06613265427229462\n",
            "Loss S01:  0.058297198155060445\n",
            "Loss S2:  0.0661136440521466\n",
            "Loss S01:  0.058317718667262805\n",
            "Loss S2:  0.06614513794374566\n",
            "Loss S01:  0.05832409332914168\n",
            "Loss S2:  0.06615786291583006\n",
            "Validation: \n",
            " Loss S01:  0.07178319990634918\n",
            " Loss S2:  0.11561986804008484\n",
            " Loss S01:  0.07427977167424701\n",
            " Loss S2:  0.13299251205864407\n",
            " Loss S01:  0.07432382208545034\n",
            " Loss S2:  0.1330236440024725\n",
            " Loss S01:  0.07351237388907886\n",
            " Loss S2:  0.1313544245528393\n",
            " Loss S01:  0.07279297824443122\n",
            " Loss S2:  0.13068442699717886\n",
            "\n",
            "Epoch: 49\n",
            "Loss S01:  0.05334968864917755\n",
            "Loss S2:  0.060549452900886536\n",
            "Loss S01:  0.05829867991534146\n",
            "Loss S2:  0.06945023143833334\n",
            "Loss S01:  0.059543075128680185\n",
            "Loss S2:  0.0678034857625053\n",
            "Loss S01:  0.058475462539542104\n",
            "Loss S2:  0.06646699686684916\n",
            "Loss S01:  0.05812432271678273\n",
            "Loss S2:  0.06612941804455548\n",
            "Loss S01:  0.05852934535519749\n",
            "Loss S2:  0.06622616172421213\n",
            "Loss S01:  0.05875644387036073\n",
            "Loss S2:  0.06632273882383206\n",
            "Loss S01:  0.058371966206271886\n",
            "Loss S2:  0.06568203488705864\n",
            "Loss S01:  0.05814558005443326\n",
            "Loss S2:  0.065576599565921\n",
            "Loss S01:  0.0582195520401001\n",
            "Loss S2:  0.06551968805737547\n",
            "Loss S01:  0.05816063465605868\n",
            "Loss S2:  0.06539452267755376\n",
            "Loss S01:  0.05800246454036988\n",
            "Loss S2:  0.0652661014448952\n",
            "Loss S01:  0.057845099559746496\n",
            "Loss S2:  0.06511594552146502\n",
            "Loss S01:  0.057871901437298943\n",
            "Loss S2:  0.06532785629156891\n",
            "Loss S01:  0.05785057561617371\n",
            "Loss S2:  0.06543651548154811\n",
            "Loss S01:  0.05804854310703594\n",
            "Loss S2:  0.06565430089337937\n",
            "Loss S01:  0.05804121225887204\n",
            "Loss S2:  0.06550999501754778\n",
            "Loss S01:  0.05804569733857411\n",
            "Loss S2:  0.06552417543635033\n",
            "Loss S01:  0.05804863533015409\n",
            "Loss S2:  0.06554206353689425\n",
            "Loss S01:  0.058016392065904525\n",
            "Loss S2:  0.06547607337149026\n",
            "Loss S01:  0.05801829451055669\n",
            "Loss S2:  0.06548924293758264\n",
            "Loss S01:  0.057986484672785936\n",
            "Loss S2:  0.06551016860098635\n",
            "Loss S01:  0.058066178062652574\n",
            "Loss S2:  0.06557041480800145\n",
            "Loss S01:  0.058033681680242734\n",
            "Loss S2:  0.06558501172117341\n",
            "Loss S01:  0.058055660165691775\n",
            "Loss S2:  0.06558607463025454\n",
            "Loss S01:  0.05803998354896606\n",
            "Loss S2:  0.06565401395894142\n",
            "Loss S01:  0.05807043627703783\n",
            "Loss S2:  0.06562624549157775\n",
            "Loss S01:  0.058067733263837455\n",
            "Loss S2:  0.06564719530984045\n",
            "Loss S01:  0.05806771672440169\n",
            "Loss S2:  0.06565907118954692\n",
            "Loss S01:  0.05803026530341184\n",
            "Loss S2:  0.06564762416420523\n",
            "Loss S01:  0.058011737822298196\n",
            "Loss S2:  0.06568883713396681\n",
            "Loss S01:  0.05796326533966126\n",
            "Loss S2:  0.06564142419642191\n",
            "Loss S01:  0.05799918511528464\n",
            "Loss S2:  0.06566434680858504\n",
            "Loss S01:  0.05797928442272535\n",
            "Loss S2:  0.0656448020135528\n",
            "Loss S01:  0.057947177483626475\n",
            "Loss S2:  0.06567458672956987\n",
            "Loss S01:  0.05795451433018402\n",
            "Loss S2:  0.06566174372903302\n",
            "Loss S01:  0.057990326961486954\n",
            "Loss S2:  0.065703582421069\n",
            "Loss S01:  0.05797964844581573\n",
            "Loss S2:  0.06567506621669567\n",
            "Loss S01:  0.057935025630973455\n",
            "Loss S2:  0.06564452791503408\n",
            "Loss S01:  0.0579106776934603\n",
            "Loss S2:  0.06558279842709946\n",
            "Loss S01:  0.05794584081320097\n",
            "Loss S2:  0.0655832651937543\n",
            "Loss S01:  0.05793167239195529\n",
            "Loss S2:  0.06557712036835306\n",
            "Loss S01:  0.05794673180318218\n",
            "Loss S2:  0.06563457841332339\n",
            "Loss S01:  0.05794721999985713\n",
            "Loss S2:  0.06561464608026768\n",
            "Loss S01:  0.057919569718999926\n",
            "Loss S2:  0.0656349678732911\n",
            "Loss S01:  0.05792212461486888\n",
            "Loss S2:  0.06562589387308203\n",
            "Loss S01:  0.05793818191437297\n",
            "Loss S2:  0.0656448392734869\n",
            "Loss S01:  0.05789897676296295\n",
            "Loss S2:  0.06560402628501003\n",
            "Loss S01:  0.05786934465541661\n",
            "Loss S2:  0.06558225894823144\n",
            "Loss S01:  0.05783455357592849\n",
            "Loss S2:  0.06559788567713953\n",
            "Validation: \n",
            " Loss S01:  0.0726407989859581\n",
            " Loss S2:  0.11189506202936172\n",
            " Loss S01:  0.074933532448042\n",
            " Loss S2:  0.12853939831256866\n",
            " Loss S01:  0.07438717982391031\n",
            " Loss S2:  0.1277917659864193\n",
            " Loss S01:  0.07358921746738621\n",
            " Loss S2:  0.12652983211103033\n",
            " Loss S01:  0.0730417929129836\n",
            " Loss S2:  0.12573552471987995\n",
            "\n",
            "Epoch: 50\n",
            "Loss S01:  0.05516216158866882\n",
            "Loss S2:  0.06752456724643707\n",
            "Loss S01:  0.058471245860511604\n",
            "Loss S2:  0.06717301701957529\n",
            "Loss S01:  0.058443887247925715\n",
            "Loss S2:  0.06641969705621402\n",
            "Loss S01:  0.05756587042443214\n",
            "Loss S2:  0.06511205879430618\n",
            "Loss S01:  0.05712777839564696\n",
            "Loss S2:  0.06490792143272191\n",
            "Loss S01:  0.056966188929828944\n",
            "Loss S2:  0.06458938501629174\n",
            "Loss S01:  0.057191237746203535\n",
            "Loss S2:  0.06498518433482921\n",
            "Loss S01:  0.05703351956228135\n",
            "Loss S2:  0.0646693081083432\n",
            "Loss S01:  0.05703233527364554\n",
            "Loss S2:  0.0645673386292693\n",
            "Loss S01:  0.05706991779280233\n",
            "Loss S2:  0.06489492150453421\n",
            "Loss S01:  0.05699612897368941\n",
            "Loss S2:  0.06505174791016201\n",
            "Loss S01:  0.056827479453237205\n",
            "Loss S2:  0.0648389365020636\n",
            "Loss S01:  0.05677121317337367\n",
            "Loss S2:  0.06488328079177329\n",
            "Loss S01:  0.05679717278890027\n",
            "Loss S2:  0.06493349006499043\n",
            "Loss S01:  0.056786372649965555\n",
            "Loss S2:  0.06485407257862125\n",
            "Loss S01:  0.05706325839489501\n",
            "Loss S2:  0.06513932635945989\n",
            "Loss S01:  0.05712421150496287\n",
            "Loss S2:  0.06501607582991167\n",
            "Loss S01:  0.05716164105119761\n",
            "Loss S2:  0.06497187413580237\n",
            "Loss S01:  0.057217410091537135\n",
            "Loss S2:  0.06492324424547385\n",
            "Loss S01:  0.057274695992937885\n",
            "Loss S2:  0.0649542352721017\n",
            "Loss S01:  0.05728499687726225\n",
            "Loss S2:  0.06503987086204747\n",
            "Loss S01:  0.0573402281091394\n",
            "Loss S2:  0.06509937993934935\n",
            "Loss S01:  0.057365517911598156\n",
            "Loss S2:  0.06518913148080602\n",
            "Loss S01:  0.05737357562432041\n",
            "Loss S2:  0.06527426248634016\n",
            "Loss S01:  0.05746948092990396\n",
            "Loss S2:  0.06536854452673825\n",
            "Loss S01:  0.05746819136805269\n",
            "Loss S2:  0.06538257335286692\n",
            "Loss S01:  0.057547906263806355\n",
            "Loss S2:  0.06543211187896143\n",
            "Loss S01:  0.057571635901158144\n",
            "Loss S2:  0.06541515541747488\n",
            "Loss S01:  0.05757361006164042\n",
            "Loss S2:  0.06544033392892613\n",
            "Loss S01:  0.05754530385034191\n",
            "Loss S2:  0.0654134305891712\n",
            "Loss S01:  0.057557081921255075\n",
            "Loss S2:  0.06547620167268867\n",
            "Loss S01:  0.05746352399468805\n",
            "Loss S2:  0.06535192974319029\n",
            "Loss S01:  0.057582207859676576\n",
            "Loss S2:  0.06542827667105607\n",
            "Loss S01:  0.057536000909221854\n",
            "Loss S2:  0.06535521613327037\n",
            "Loss S01:  0.05755340713253818\n",
            "Loss S2:  0.0653786418680921\n",
            "Loss S01:  0.05757664689565996\n",
            "Loss S2:  0.06536729684752277\n",
            "Loss S01:  0.05754394801410942\n",
            "Loss S2:  0.06532077020720432\n",
            "Loss S01:  0.05754597467995397\n",
            "Loss S2:  0.06533330749228315\n",
            "Loss S01:  0.05752412122222069\n",
            "Loss S2:  0.06526562874633184\n",
            "Loss S01:  0.0574899328601025\n",
            "Loss S2:  0.06519696447055054\n",
            "Loss S01:  0.05753454078276853\n",
            "Loss S2:  0.06523521643995941\n",
            "Loss S01:  0.057494668811197115\n",
            "Loss S2:  0.06517326612469633\n",
            "Loss S01:  0.057515539326251544\n",
            "Loss S2:  0.06521136829554326\n",
            "Loss S01:  0.05751515625870145\n",
            "Loss S2:  0.06520388215745684\n",
            "Loss S01:  0.05748627217414698\n",
            "Loss S2:  0.06522007797614517\n",
            "Loss S01:  0.057479690116088826\n",
            "Loss S2:  0.06516386797350156\n",
            "Loss S01:  0.057506636835618546\n",
            "Loss S2:  0.0652047958136122\n",
            "Loss S01:  0.05747138254834841\n",
            "Loss S2:  0.06514585170016926\n",
            "Loss S01:  0.05746873434857618\n",
            "Loss S2:  0.06512539730002627\n",
            "Loss S01:  0.05749382296398798\n",
            "Loss S2:  0.06510599692733371\n",
            "Validation: \n",
            " Loss S01:  0.07677440345287323\n",
            " Loss S2:  0.11414238065481186\n",
            " Loss S01:  0.07947138909782682\n",
            " Loss S2:  0.13033928580227352\n",
            " Loss S01:  0.07856996975293974\n",
            " Loss S2:  0.12965559159837117\n",
            " Loss S01:  0.07777286687346756\n",
            " Loss S2:  0.1281248056253449\n",
            " Loss S01:  0.07700084287811208\n",
            " Loss S2:  0.12728199178789867\n",
            "\n",
            "Epoch: 51\n",
            "Loss S01:  0.057053882628679276\n",
            "Loss S2:  0.06457912921905518\n",
            "Loss S01:  0.058252514763311905\n",
            "Loss S2:  0.06589873439886353\n",
            "Loss S01:  0.05790685064026287\n",
            "Loss S2:  0.06583720463372413\n",
            "Loss S01:  0.05663269229473606\n",
            "Loss S2:  0.06484934435256066\n",
            "Loss S01:  0.056919847319765786\n",
            "Loss S2:  0.06487137669833695\n",
            "Loss S01:  0.05699489652818324\n",
            "Loss S2:  0.06478802501863125\n",
            "Loss S01:  0.05716027288896139\n",
            "Loss S2:  0.06496588959068549\n",
            "Loss S01:  0.05689957958291954\n",
            "Loss S2:  0.06473417165623584\n",
            "Loss S01:  0.0567998286842564\n",
            "Loss S2:  0.06473066519807887\n",
            "Loss S01:  0.05694116463700494\n",
            "Loss S2:  0.06494062659995896\n",
            "Loss S01:  0.057063915794438654\n",
            "Loss S2:  0.06522196136636309\n",
            "Loss S01:  0.05695909251635139\n",
            "Loss S2:  0.06520012088187106\n",
            "Loss S01:  0.05671094173242238\n",
            "Loss S2:  0.06506917333184195\n",
            "Loss S01:  0.05669154698839624\n",
            "Loss S2:  0.06498997527679415\n",
            "Loss S01:  0.056733201898581595\n",
            "Loss S2:  0.06502277567876992\n",
            "Loss S01:  0.05692339769953134\n",
            "Loss S2:  0.0651184366090803\n",
            "Loss S01:  0.056964461732169853\n",
            "Loss S2:  0.06499498814159299\n",
            "Loss S01:  0.05691746543896826\n",
            "Loss S2:  0.06499030770004144\n",
            "Loss S01:  0.05689585017482879\n",
            "Loss S2:  0.06493085449901075\n",
            "Loss S01:  0.05689023171809955\n",
            "Loss S2:  0.06488102928038043\n",
            "Loss S01:  0.05694284985092149\n",
            "Loss S2:  0.06492252449565265\n",
            "Loss S01:  0.05693241793185614\n",
            "Loss S2:  0.06491480764195817\n",
            "Loss S01:  0.05693389367463902\n",
            "Loss S2:  0.06503942495894648\n",
            "Loss S01:  0.056937421361605324\n",
            "Loss S2:  0.06512118611500893\n",
            "Loss S01:  0.05698469907230856\n",
            "Loss S2:  0.06520469505262573\n",
            "Loss S01:  0.056960704999853416\n",
            "Loss S2:  0.06518810941462973\n",
            "Loss S01:  0.05695828106040242\n",
            "Loss S2:  0.0650864990563685\n",
            "Loss S01:  0.05693958343669937\n",
            "Loss S2:  0.06500846359232695\n",
            "Loss S01:  0.05688121380595974\n",
            "Loss S2:  0.06496169898009385\n",
            "Loss S01:  0.05688446496462904\n",
            "Loss S2:  0.06491490343387184\n",
            "Loss S01:  0.0568888746820811\n",
            "Loss S2:  0.06484639718881081\n",
            "Loss S01:  0.0568133652138365\n",
            "Loss S2:  0.06479842013341054\n",
            "Loss S01:  0.05691348680083254\n",
            "Loss S2:  0.06479638457483963\n",
            "Loss S01:  0.05686055422054317\n",
            "Loss S2:  0.06477673751304877\n",
            "Loss S01:  0.05684879056213538\n",
            "Loss S2:  0.06476347281698608\n",
            "Loss S01:  0.05682313026186408\n",
            "Loss S2:  0.06478559879580793\n",
            "Loss S01:  0.05681010806246808\n",
            "Loss S2:  0.0647248856083508\n",
            "Loss S01:  0.056777791233075595\n",
            "Loss S2:  0.06473501195322792\n",
            "Loss S01:  0.05676100337560096\n",
            "Loss S2:  0.06471255492037675\n",
            "Loss S01:  0.05674730812001716\n",
            "Loss S2:  0.06468103183885975\n",
            "Loss S01:  0.056787954202093684\n",
            "Loss S2:  0.06468267793630127\n",
            "Loss S01:  0.056755212600141255\n",
            "Loss S2:  0.06465400210661029\n",
            "Loss S01:  0.05672923508244002\n",
            "Loss S2:  0.06468186654911472\n",
            "Loss S01:  0.05673975488840842\n",
            "Loss S2:  0.06467821188876081\n",
            "Loss S01:  0.056768223322302844\n",
            "Loss S2:  0.06468868190570483\n",
            "Loss S01:  0.05677054938911863\n",
            "Loss S2:  0.06470765720251659\n",
            "Loss S01:  0.05680074226798049\n",
            "Loss S2:  0.06475304231985013\n",
            "Loss S01:  0.05673351036799941\n",
            "Loss S2:  0.06471032042633702\n",
            "Loss S01:  0.056731339420139665\n",
            "Loss S2:  0.06465365655580826\n",
            "Loss S01:  0.05674062560780956\n",
            "Loss S2:  0.06465367981196664\n",
            "Validation: \n",
            " Loss S01:  0.0705251693725586\n",
            " Loss S2:  0.11388496309518814\n",
            " Loss S01:  0.07487720871965091\n",
            " Loss S2:  0.1323457055148624\n",
            " Loss S01:  0.07442749119022997\n",
            " Loss S2:  0.1321370830623115\n",
            " Loss S01:  0.07377555325138764\n",
            " Loss S2:  0.13044247422061983\n",
            " Loss S01:  0.07323275690461382\n",
            " Loss S2:  0.12972386521689686\n",
            "\n",
            "Epoch: 52\n",
            "Loss S01:  0.051736585795879364\n",
            "Loss S2:  0.06289840489625931\n",
            "Loss S01:  0.0588406619023193\n",
            "Loss S2:  0.06762882999398491\n",
            "Loss S01:  0.0587547765601249\n",
            "Loss S2:  0.06636821620521091\n",
            "Loss S01:  0.05741297417590695\n",
            "Loss S2:  0.06545578664348971\n",
            "Loss S01:  0.057250334994822016\n",
            "Loss S2:  0.06490224563493961\n",
            "Loss S01:  0.05716020672344694\n",
            "Loss S2:  0.06471353265292504\n",
            "Loss S01:  0.05736090200113469\n",
            "Loss S2:  0.06540488622716216\n",
            "Loss S01:  0.057109152662082455\n",
            "Loss S2:  0.06499329141118157\n",
            "Loss S01:  0.056986159564536294\n",
            "Loss S2:  0.0649174442169843\n",
            "Loss S01:  0.05718618209709178\n",
            "Loss S2:  0.06491627773413292\n",
            "Loss S01:  0.057102104519853494\n",
            "Loss S2:  0.06483438839711765\n",
            "Loss S01:  0.0569257512353025\n",
            "Loss S2:  0.06476883200911789\n",
            "Loss S01:  0.056672109952889196\n",
            "Loss S2:  0.0645932414807564\n",
            "Loss S01:  0.056617383880697134\n",
            "Loss S2:  0.06455947922272537\n",
            "Loss S01:  0.056585050907963556\n",
            "Loss S2:  0.06462967673197706\n",
            "Loss S01:  0.05671336524040494\n",
            "Loss S2:  0.0648082426644319\n",
            "Loss S01:  0.05674294224992302\n",
            "Loss S2:  0.06471901132453302\n",
            "Loss S01:  0.0566821245814276\n",
            "Loss S2:  0.06465256416745353\n",
            "Loss S01:  0.056712104161964594\n",
            "Loss S2:  0.06464826332106775\n",
            "Loss S01:  0.056735374247996594\n",
            "Loss S2:  0.06461830770704134\n",
            "Loss S01:  0.056670626634685556\n",
            "Loss S2:  0.06462067178455158\n",
            "Loss S01:  0.056653134189369556\n",
            "Loss S2:  0.064646857433127\n",
            "Loss S01:  0.05672877533066327\n",
            "Loss S2:  0.06465712214959153\n",
            "Loss S01:  0.056720165124574264\n",
            "Loss S2:  0.0646489876528065\n",
            "Loss S01:  0.05684567947729\n",
            "Loss S2:  0.06476560969261214\n",
            "Loss S01:  0.05684624505470474\n",
            "Loss S2:  0.0648136770434351\n",
            "Loss S01:  0.056934843432172505\n",
            "Loss S2:  0.06486345684848069\n",
            "Loss S01:  0.05699402764714512\n",
            "Loss S2:  0.06484974248547835\n",
            "Loss S01:  0.057020303060787855\n",
            "Loss S2:  0.06484723726148282\n",
            "Loss S01:  0.05702259582291354\n",
            "Loss S2:  0.06479091955112018\n",
            "Loss S01:  0.05700311892699958\n",
            "Loss S2:  0.06478406703006785\n",
            "Loss S01:  0.0569131439042628\n",
            "Loss S2:  0.06470078137670299\n",
            "Loss S01:  0.056985754159937764\n",
            "Loss S2:  0.06468596905934106\n",
            "Loss S01:  0.0569170516918433\n",
            "Loss S2:  0.0646194334836885\n",
            "Loss S01:  0.05687635748092729\n",
            "Loss S2:  0.06456171490710852\n",
            "Loss S01:  0.056933050927443385\n",
            "Loss S2:  0.0646029399798127\n",
            "Loss S01:  0.05689570700899386\n",
            "Loss S2:  0.06453802341652048\n",
            "Loss S01:  0.05686711625389333\n",
            "Loss S2:  0.06455487553080458\n",
            "Loss S01:  0.05684273970604256\n",
            "Loss S2:  0.06454455918018899\n",
            "Loss S01:  0.05680115615277339\n",
            "Loss S2:  0.06443060466738613\n",
            "Loss S01:  0.05682694963980791\n",
            "Loss S2:  0.06443322366312555\n",
            "Loss S01:  0.05682491300822465\n",
            "Loss S2:  0.06443326976467513\n",
            "Loss S01:  0.05679691522747207\n",
            "Loss S2:  0.06445247550758217\n",
            "Loss S01:  0.056789899759184734\n",
            "Loss S2:  0.06446258872401686\n",
            "Loss S01:  0.0567641790255127\n",
            "Loss S2:  0.06451351541915988\n",
            "Loss S01:  0.05677332022865437\n",
            "Loss S2:  0.06447937152866778\n",
            "Loss S01:  0.056792211417844136\n",
            "Loss S2:  0.06445126822157692\n",
            "Loss S01:  0.0567837355227354\n",
            "Loss S2:  0.06444233178997495\n",
            "Loss S01:  0.056797534692064396\n",
            "Loss S2:  0.06444690824174583\n",
            "Loss S01:  0.05679376593311061\n",
            "Loss S2:  0.0644367142388874\n",
            "Validation: \n",
            " Loss S01:  0.07330058515071869\n",
            " Loss S2:  0.1155872717499733\n",
            " Loss S01:  0.07974077938568025\n",
            " Loss S2:  0.1305788251615706\n",
            " Loss S01:  0.0784532796682381\n",
            " Loss S2:  0.12861446382069006\n",
            " Loss S01:  0.07761059774727118\n",
            " Loss S2:  0.12722743339225895\n",
            " Loss S01:  0.07680404324222494\n",
            " Loss S2:  0.12648417837457893\n",
            "\n",
            "Epoch: 53\n",
            "Loss S01:  0.056705038994550705\n",
            "Loss S2:  0.0691494569182396\n",
            "Loss S01:  0.05691901763731783\n",
            "Loss S2:  0.06604190441695126\n",
            "Loss S01:  0.05673474907165482\n",
            "Loss S2:  0.06569932862406686\n",
            "Loss S01:  0.0563665829118221\n",
            "Loss S2:  0.06454036180530826\n",
            "Loss S01:  0.05655188195225669\n",
            "Loss S2:  0.0646492302781198\n",
            "Loss S01:  0.056501923968978955\n",
            "Loss S2:  0.06430569349550734\n",
            "Loss S01:  0.05702001406032531\n",
            "Loss S2:  0.06461112537100668\n",
            "Loss S01:  0.05658767477307521\n",
            "Loss S2:  0.06431642925025711\n",
            "Loss S01:  0.05652335106774613\n",
            "Loss S2:  0.06402564310917148\n",
            "Loss S01:  0.05653184611391235\n",
            "Loss S2:  0.06402538282858146\n",
            "Loss S01:  0.056515739511440295\n",
            "Loss S2:  0.06392715461920984\n",
            "Loss S01:  0.056253328449554274\n",
            "Loss S2:  0.06367160000645362\n",
            "Loss S01:  0.05598962571749017\n",
            "Loss S2:  0.06345459058388206\n",
            "Loss S01:  0.05596905366610025\n",
            "Loss S2:  0.06353264873605648\n",
            "Loss S01:  0.05597760174291354\n",
            "Loss S2:  0.06368936386936945\n",
            "Loss S01:  0.05625088517831651\n",
            "Loss S2:  0.06387281644818009\n",
            "Loss S01:  0.05614328213174891\n",
            "Loss S2:  0.06375496603132035\n",
            "Loss S01:  0.056208694364592346\n",
            "Loss S2:  0.06378980963463672\n",
            "Loss S01:  0.05627627858058524\n",
            "Loss S2:  0.06376472111400319\n",
            "Loss S01:  0.05629680524165718\n",
            "Loss S2:  0.06382067975495498\n",
            "Loss S01:  0.056347384733791965\n",
            "Loss S2:  0.0637661752230789\n",
            "Loss S01:  0.056386215091457866\n",
            "Loss S2:  0.06377727623093185\n",
            "Loss S01:  0.05643289536237717\n",
            "Loss S2:  0.06394283320097362\n",
            "Loss S01:  0.05641788150712009\n",
            "Loss S2:  0.06404070025592139\n",
            "Loss S01:  0.05646755515416133\n",
            "Loss S2:  0.06410780777822392\n",
            "Loss S01:  0.056492579707586435\n",
            "Loss S2:  0.06414851679863683\n",
            "Loss S01:  0.056525190571372994\n",
            "Loss S2:  0.06419910567587819\n",
            "Loss S01:  0.05656563651462763\n",
            "Loss S2:  0.06412517847115264\n",
            "Loss S01:  0.0565602607972045\n",
            "Loss S2:  0.06415564541661867\n",
            "Loss S01:  0.056541767752252496\n",
            "Loss S2:  0.06408627272369116\n",
            "Loss S01:  0.05652195110025992\n",
            "Loss S2:  0.06406129619409871\n",
            "Loss S01:  0.05650842041256343\n",
            "Loss S2:  0.06404713706571573\n",
            "Loss S01:  0.05659801567501368\n",
            "Loss S2:  0.06409991241271994\n",
            "Loss S01:  0.05655275667902206\n",
            "Loss S2:  0.06408126686004714\n",
            "Loss S01:  0.056562754524942714\n",
            "Loss S2:  0.06411455242820849\n",
            "Loss S01:  0.05659163056572958\n",
            "Loss S2:  0.06408870035511816\n",
            "Loss S01:  0.05653296063274888\n",
            "Loss S2:  0.0639915143209316\n",
            "Loss S01:  0.056534028770226355\n",
            "Loss S2:  0.06398331154027397\n",
            "Loss S01:  0.05650742859469624\n",
            "Loss S2:  0.0639427265157224\n",
            "Loss S01:  0.05648976365280578\n",
            "Loss S2:  0.06388707166475713\n",
            "Loss S01:  0.056500548854507414\n",
            "Loss S2:  0.06390611356370467\n",
            "Loss S01:  0.05644480935739775\n",
            "Loss S2:  0.06386601274103434\n",
            "Loss S01:  0.05646961741591859\n",
            "Loss S2:  0.06389567082539874\n",
            "Loss S01:  0.05644906695992653\n",
            "Loss S2:  0.06384619641746557\n",
            "Loss S01:  0.056434068077005226\n",
            "Loss S2:  0.0638588544260077\n",
            "Loss S01:  0.0564248156124631\n",
            "Loss S2:  0.06385441217777998\n",
            "Loss S01:  0.05647216534440016\n",
            "Loss S2:  0.06387286860326107\n",
            "Loss S01:  0.05642716497214483\n",
            "Loss S2:  0.06386226397442717\n",
            "Loss S01:  0.05641587677251029\n",
            "Loss S2:  0.06385063686700472\n",
            "Loss S01:  0.0564072092429805\n",
            "Loss S2:  0.06383339377867471\n",
            "Validation: \n",
            " Loss S01:  0.07332856208086014\n",
            " Loss S2:  0.1091153621673584\n",
            " Loss S01:  0.076434842000405\n",
            " Loss S2:  0.12606970007930482\n",
            " Loss S01:  0.0759747825199511\n",
            " Loss S2:  0.1252972151084644\n",
            " Loss S01:  0.07514600681721187\n",
            " Loss S2:  0.12341643601167397\n",
            " Loss S01:  0.07443663864224045\n",
            " Loss S2:  0.12277211239676417\n",
            "\n",
            "Epoch: 54\n",
            "Loss S01:  0.055901408195495605\n",
            "Loss S2:  0.06606438755989075\n",
            "Loss S01:  0.05659224634820765\n",
            "Loss S2:  0.06594516811045734\n",
            "Loss S01:  0.056913902362187706\n",
            "Loss S2:  0.06468912125343368\n",
            "Loss S01:  0.05648320132205563\n",
            "Loss S2:  0.06407761657910963\n",
            "Loss S01:  0.056461263266278476\n",
            "Loss S2:  0.06376940175527479\n",
            "Loss S01:  0.05615917790461989\n",
            "Loss S2:  0.06386988485852878\n",
            "Loss S01:  0.05627693455727374\n",
            "Loss S2:  0.06410870652218334\n",
            "Loss S01:  0.05621946503368901\n",
            "Loss S2:  0.0638967343199421\n",
            "Loss S01:  0.05609183611325276\n",
            "Loss S2:  0.06367162523078329\n",
            "Loss S01:  0.05613186202206454\n",
            "Loss S2:  0.06379580415867186\n",
            "Loss S01:  0.05604006656178153\n",
            "Loss S2:  0.06371107147914348\n",
            "Loss S01:  0.05577173865995966\n",
            "Loss S2:  0.0635002190331081\n",
            "Loss S01:  0.05558988085586177\n",
            "Loss S2:  0.06350676170434834\n",
            "Loss S01:  0.05551233950234551\n",
            "Loss S2:  0.06348666725040392\n",
            "Loss S01:  0.05552465056485318\n",
            "Loss S2:  0.06361264917761722\n",
            "Loss S01:  0.05575701892869362\n",
            "Loss S2:  0.06391191208698102\n",
            "Loss S01:  0.05577333866161589\n",
            "Loss S2:  0.06380257413068914\n",
            "Loss S01:  0.055673896939608086\n",
            "Loss S2:  0.0637809626583816\n",
            "Loss S01:  0.055676167285409424\n",
            "Loss S2:  0.06373952203610325\n",
            "Loss S01:  0.055747593143535536\n",
            "Loss S2:  0.06375467055599103\n",
            "Loss S01:  0.05575349026206714\n",
            "Loss S2:  0.06377258587313529\n",
            "Loss S01:  0.055796170704313926\n",
            "Loss S2:  0.06373810482109893\n",
            "Loss S01:  0.05585603685670309\n",
            "Loss S2:  0.06378231656092864\n",
            "Loss S01:  0.055920661398858734\n",
            "Loss S2:  0.06380702390686258\n",
            "Loss S01:  0.056034296642818886\n",
            "Loss S2:  0.06384702131955951\n",
            "Loss S01:  0.056087247834381355\n",
            "Loss S2:  0.06391756379034415\n",
            "Loss S01:  0.05606726766831573\n",
            "Loss S2:  0.06383570718982211\n",
            "Loss S01:  0.056077966308461784\n",
            "Loss S2:  0.06379734364602839\n",
            "Loss S01:  0.05606748951181398\n",
            "Loss S2:  0.06380733561261269\n",
            "Loss S01:  0.05603776704455979\n",
            "Loss S2:  0.06377679717653396\n",
            "Loss S01:  0.05603651098436691\n",
            "Loss S2:  0.0638072047865272\n",
            "Loss S01:  0.05598453719803758\n",
            "Loss S2:  0.06374522850779858\n",
            "Loss S01:  0.055975202018412475\n",
            "Loss S2:  0.06376251180270379\n",
            "Loss S01:  0.055966567090540854\n",
            "Loss S2:  0.0637046607982176\n",
            "Loss S01:  0.05601826217298633\n",
            "Loss S2:  0.0637657986817297\n",
            "Loss S01:  0.056073480511238095\n",
            "Loss S2:  0.06385589489697391\n",
            "Loss S01:  0.05604399097799594\n",
            "Loss S2:  0.06379884390619653\n",
            "Loss S01:  0.05601023421735776\n",
            "Loss S2:  0.06379066284173582\n",
            "Loss S01:  0.05599102956729298\n",
            "Loss S2:  0.06371893182279557\n",
            "Loss S01:  0.055967599353598206\n",
            "Loss S2:  0.06367262024098955\n",
            "Loss S01:  0.05602173913186625\n",
            "Loss S2:  0.06368194698841495\n",
            "Loss S01:  0.05599105768721469\n",
            "Loss S2:  0.06368516875009467\n",
            "Loss S01:  0.056024448180708356\n",
            "Loss S2:  0.06370411258447482\n",
            "Loss S01:  0.056007919534867986\n",
            "Loss S2:  0.06365392352789295\n",
            "Loss S01:  0.05600660903349755\n",
            "Loss S2:  0.06367248987732561\n",
            "Loss S01:  0.05604799873218304\n",
            "Loss S2:  0.06366227121978271\n",
            "Loss S01:  0.056078104338651104\n",
            "Loss S2:  0.06368308314758629\n",
            "Loss S01:  0.056051164992042914\n",
            "Loss S2:  0.0636043144971322\n",
            "Loss S01:  0.05605853680451546\n",
            "Loss S2:  0.06357561442187819\n",
            "Loss S01:  0.05606791666896668\n",
            "Loss S2:  0.06355038772350416\n",
            "Validation: \n",
            " Loss S01:  0.06829668581485748\n",
            " Loss S2:  0.10932644456624985\n",
            " Loss S01:  0.0759582594037056\n",
            " Loss S2:  0.12944821481193816\n",
            " Loss S01:  0.07532027400121456\n",
            " Loss S2:  0.1283161716490257\n",
            " Loss S01:  0.07470306815182576\n",
            " Loss S2:  0.12663588269812162\n",
            " Loss S01:  0.07404670822951528\n",
            " Loss S2:  0.1258111105840883\n",
            "\n",
            "Epoch: 55\n",
            "Loss S01:  0.054932601749897\n",
            "Loss S2:  0.05987163260579109\n",
            "Loss S01:  0.056142526255412536\n",
            "Loss S2:  0.06232945756478743\n",
            "Loss S01:  0.05613420495674724\n",
            "Loss S2:  0.06272687106615021\n",
            "Loss S01:  0.05585123694712116\n",
            "Loss S2:  0.06302462842675947\n",
            "Loss S01:  0.05593655185728538\n",
            "Loss S2:  0.06339421150524442\n",
            "Loss S01:  0.05580205148925968\n",
            "Loss S2:  0.06323719514059085\n",
            "Loss S01:  0.05572607441515219\n",
            "Loss S2:  0.06328210155250596\n",
            "Loss S01:  0.05528044606178579\n",
            "Loss S2:  0.06275784943095396\n",
            "Loss S01:  0.05545702167315247\n",
            "Loss S2:  0.06282658870389432\n",
            "Loss S01:  0.05537660722876643\n",
            "Loss S2:  0.06287155083411343\n",
            "Loss S01:  0.05535840298427214\n",
            "Loss S2:  0.06287645489567577\n",
            "Loss S01:  0.05518593823721817\n",
            "Loss S2:  0.06274421774857752\n",
            "Loss S01:  0.05504671721295877\n",
            "Loss S2:  0.0626995415726969\n",
            "Loss S01:  0.055090563528182854\n",
            "Loss S2:  0.0628866205233654\n",
            "Loss S01:  0.05514349323426578\n",
            "Loss S2:  0.06311089698727249\n",
            "Loss S01:  0.05528255559454691\n",
            "Loss S2:  0.06317179346617484\n",
            "Loss S01:  0.05531289178577269\n",
            "Loss S2:  0.06309614679920747\n",
            "Loss S01:  0.05529308109952692\n",
            "Loss S2:  0.06303374817845417\n",
            "Loss S01:  0.055299112295412886\n",
            "Loss S2:  0.06297794375465719\n",
            "Loss S01:  0.055333978569632424\n",
            "Loss S2:  0.06304620116637015\n",
            "Loss S01:  0.055409076294643964\n",
            "Loss S2:  0.06313955933967633\n",
            "Loss S01:  0.05549339568727954\n",
            "Loss S2:  0.06320214741179163\n",
            "Loss S01:  0.05556023517008281\n",
            "Loss S2:  0.06329326063596824\n",
            "Loss S01:  0.055633015446848684\n",
            "Loss S2:  0.06342675025651465\n",
            "Loss S01:  0.05573359199826648\n",
            "Loss S2:  0.06348592406177422\n",
            "Loss S01:  0.055764599595649306\n",
            "Loss S2:  0.06356181615436694\n",
            "Loss S01:  0.05588444660621128\n",
            "Loss S2:  0.06364760432263901\n",
            "Loss S01:  0.05590108255563627\n",
            "Loss S2:  0.06362885266982321\n",
            "Loss S01:  0.05596725669195644\n",
            "Loss S2:  0.06366293964861126\n",
            "Loss S01:  0.05596618886870617\n",
            "Loss S2:  0.06369418077350072\n",
            "Loss S01:  0.0560057344800016\n",
            "Loss S2:  0.06377277319068925\n",
            "Loss S01:  0.05595180151498969\n",
            "Loss S2:  0.0636539974420592\n",
            "Loss S01:  0.055969367436252276\n",
            "Loss S2:  0.06366152457916105\n",
            "Loss S01:  0.05593428791154548\n",
            "Loss S2:  0.06359442276176729\n",
            "Loss S01:  0.05594337573193035\n",
            "Loss S2:  0.06360427839708818\n",
            "Loss S01:  0.0559783286582201\n",
            "Loss S2:  0.06364367557344613\n",
            "Loss S01:  0.05591152561957486\n",
            "Loss S2:  0.0635893780872267\n",
            "Loss S01:  0.05588545356036197\n",
            "Loss S2:  0.0635551111396111\n",
            "Loss S01:  0.055874897768412986\n",
            "Loss S2:  0.06349097516905917\n",
            "Loss S01:  0.05583690515602641\n",
            "Loss S2:  0.06349187839747694\n",
            "Loss S01:  0.05586791801222245\n",
            "Loss S2:  0.06347194569478011\n",
            "Loss S01:  0.055820725876339454\n",
            "Loss S2:  0.0634669508068956\n",
            "Loss S01:  0.05580592336854289\n",
            "Loss S2:  0.06351538887358052\n",
            "Loss S01:  0.055800122591362196\n",
            "Loss S2:  0.0634491074648783\n",
            "Loss S01:  0.05581135550290008\n",
            "Loss S2:  0.06351790183525778\n",
            "Loss S01:  0.055855239128194206\n",
            "Loss S2:  0.06349225872570025\n",
            "Loss S01:  0.055833928666831095\n",
            "Loss S2:  0.06344313540550478\n",
            "Loss S01:  0.05583416167528007\n",
            "Loss S2:  0.06337664981081986\n",
            "Loss S01:  0.05583541129334305\n",
            "Loss S2:  0.06339339481322037\n",
            "Loss S01:  0.05581797333908421\n",
            "Loss S2:  0.06336590376077504\n",
            "Validation: \n",
            " Loss S01:  0.06683684885501862\n",
            " Loss S2:  0.10810195654630661\n",
            " Loss S01:  0.07327265647195634\n",
            " Loss S2:  0.1297691652462596\n",
            " Loss S01:  0.07267147030045347\n",
            " Loss S2:  0.1284984786336015\n",
            " Loss S01:  0.07197378734584714\n",
            " Loss S2:  0.12727456258945777\n",
            " Loss S01:  0.07122199816836251\n",
            " Loss S2:  0.12649202622749187\n",
            "\n",
            "Epoch: 56\n",
            "Loss S01:  0.048941683024168015\n",
            "Loss S2:  0.0661613717675209\n",
            "Loss S01:  0.05515821108763868\n",
            "Loss S2:  0.06512128697200255\n",
            "Loss S01:  0.05521177713360105\n",
            "Loss S2:  0.06501497381499835\n",
            "Loss S01:  0.05495242125565006\n",
            "Loss S2:  0.06389568493731561\n",
            "Loss S01:  0.05488453878135216\n",
            "Loss S2:  0.06351436474701254\n",
            "Loss S01:  0.054544960386028477\n",
            "Loss S2:  0.06315800540295302\n",
            "Loss S01:  0.05499646605038252\n",
            "Loss S2:  0.06344843350472998\n",
            "Loss S01:  0.05478895077822914\n",
            "Loss S2:  0.06290697915033555\n",
            "Loss S01:  0.054891356301528436\n",
            "Loss S2:  0.06294873186651571\n",
            "Loss S01:  0.054980943368358924\n",
            "Loss S2:  0.06295878321423636\n",
            "Loss S01:  0.05493093514353922\n",
            "Loss S2:  0.06280158902748977\n",
            "Loss S01:  0.054725333746220614\n",
            "Loss S2:  0.06261862552649267\n",
            "Loss S01:  0.05458674586016284\n",
            "Loss S2:  0.06246465503061113\n",
            "Loss S01:  0.05455685378485964\n",
            "Loss S2:  0.062428664074826785\n",
            "Loss S01:  0.05453924533534557\n",
            "Loss S2:  0.06245296379775866\n",
            "Loss S01:  0.05479272089849245\n",
            "Loss S2:  0.06257447847072652\n",
            "Loss S01:  0.05483204126358032\n",
            "Loss S2:  0.0625522674657173\n",
            "Loss S01:  0.054846733612449544\n",
            "Loss S2:  0.06257690426114706\n",
            "Loss S01:  0.054876951881535144\n",
            "Loss S2:  0.0625889556719124\n",
            "Loss S01:  0.054845990193763954\n",
            "Loss S2:  0.06268081601455573\n",
            "Loss S01:  0.05487845100425369\n",
            "Loss S2:  0.06273045909790258\n",
            "Loss S01:  0.05498252046334235\n",
            "Loss S2:  0.06266976613992763\n",
            "Loss S01:  0.05499538910739562\n",
            "Loss S2:  0.06268436134670655\n",
            "Loss S01:  0.05502826001220967\n",
            "Loss S2:  0.06274482490115868\n",
            "Loss S01:  0.05517249503385477\n",
            "Loss S2:  0.06286895493068635\n",
            "Loss S01:  0.05523947673846051\n",
            "Loss S2:  0.06293237797055111\n",
            "Loss S01:  0.05528763174953589\n",
            "Loss S2:  0.06289790829079818\n",
            "Loss S01:  0.055284336690423236\n",
            "Loss S2:  0.0628991838118467\n",
            "Loss S01:  0.055273543473240316\n",
            "Loss S2:  0.06300701829356231\n",
            "Loss S01:  0.055260741590327005\n",
            "Loss S2:  0.06306004338606526\n",
            "Loss S01:  0.05530423770018194\n",
            "Loss S2:  0.06310992054269955\n",
            "Loss S01:  0.055218656191009415\n",
            "Loss S2:  0.06304924515857574\n",
            "Loss S01:  0.055240916767131505\n",
            "Loss S2:  0.0630576354785129\n",
            "Loss S01:  0.05521807785840913\n",
            "Loss S2:  0.06305968798584448\n",
            "Loss S01:  0.05519900290716079\n",
            "Loss S2:  0.06308142760020197\n",
            "Loss S01:  0.05524317836362412\n",
            "Loss S2:  0.06305804158802386\n",
            "Loss S01:  0.05521848752392956\n",
            "Loss S2:  0.06297843186107369\n",
            "Loss S01:  0.05521319257321383\n",
            "Loss S2:  0.0629244030546146\n",
            "Loss S01:  0.0551967659862492\n",
            "Loss S2:  0.06288007220492901\n",
            "Loss S01:  0.05514925569676987\n",
            "Loss S2:  0.06281317347455817\n",
            "Loss S01:  0.05518745059644492\n",
            "Loss S2:  0.06279219075702967\n",
            "Loss S01:  0.05518204157768665\n",
            "Loss S2:  0.06278100793777881\n",
            "Loss S01:  0.05521236203451621\n",
            "Loss S2:  0.06278648415045048\n",
            "Loss S01:  0.055221834256726464\n",
            "Loss S2:  0.06276975065608036\n",
            "Loss S01:  0.055199355866903624\n",
            "Loss S2:  0.06278226366625622\n",
            "Loss S01:  0.0552368854934519\n",
            "Loss S2:  0.06275409764864227\n",
            "Loss S01:  0.055275700897495554\n",
            "Loss S2:  0.06272740355832199\n",
            "Loss S01:  0.05524709104162872\n",
            "Loss S2:  0.06268711064619877\n",
            "Loss S01:  0.0552606351611644\n",
            "Loss S2:  0.06268737804369223\n",
            "Loss S01:  0.055259339564504545\n",
            "Loss S2:  0.06268225315874802\n",
            "Validation: \n",
            " Loss S01:  0.07066676020622253\n",
            " Loss S2:  0.11308057606220245\n",
            " Loss S01:  0.07577608348358245\n",
            " Loss S2:  0.1287788071093105\n",
            " Loss S01:  0.07523886150703198\n",
            " Loss S2:  0.12843809167786344\n",
            " Loss S01:  0.0745287971784834\n",
            " Loss S2:  0.12664597432632915\n",
            " Loss S01:  0.07402383255553835\n",
            " Loss S2:  0.12606010190498682\n",
            "\n",
            "Epoch: 57\n",
            "Loss S01:  0.05473945662379265\n",
            "Loss S2:  0.06369239836931229\n",
            "Loss S01:  0.05567094886844808\n",
            "Loss S2:  0.06377220018343492\n",
            "Loss S01:  0.055599555195797054\n",
            "Loss S2:  0.06277524467025485\n",
            "Loss S01:  0.055447971268046285\n",
            "Loss S2:  0.06224297023107929\n",
            "Loss S01:  0.05517469046682846\n",
            "Loss S2:  0.06207375691794768\n",
            "Loss S01:  0.05527136021969365\n",
            "Loss S2:  0.061893134213545746\n",
            "Loss S01:  0.05537058669524115\n",
            "Loss S2:  0.06248621239525373\n",
            "Loss S01:  0.05532920050998809\n",
            "Loss S2:  0.06238619385051056\n",
            "Loss S01:  0.05523283871603601\n",
            "Loss S2:  0.06242827114499645\n",
            "Loss S01:  0.05524641054344701\n",
            "Loss S2:  0.06245379935909104\n",
            "Loss S01:  0.05516068176320284\n",
            "Loss S2:  0.06227008718075139\n",
            "Loss S01:  0.05500830391103083\n",
            "Loss S2:  0.062076013468138805\n",
            "Loss S01:  0.054919611754988835\n",
            "Loss S2:  0.062078240273658894\n",
            "Loss S01:  0.05489172249008681\n",
            "Loss S2:  0.062125355574010895\n",
            "Loss S01:  0.0547972135510005\n",
            "Loss S2:  0.06210069984514663\n",
            "Loss S01:  0.05491948922146235\n",
            "Loss S2:  0.06217294161682887\n",
            "Loss S01:  0.05499732369406623\n",
            "Loss S2:  0.06217551786707055\n",
            "Loss S01:  0.05494049006299666\n",
            "Loss S2:  0.062131295013323165\n",
            "Loss S01:  0.05499325867598228\n",
            "Loss S2:  0.06216113806511816\n",
            "Loss S01:  0.05503097517874228\n",
            "Loss S2:  0.062158299795308035\n",
            "Loss S01:  0.0550117064174728\n",
            "Loss S2:  0.062286416949027805\n",
            "Loss S01:  0.055104983573276284\n",
            "Loss S2:  0.062316158583378904\n",
            "Loss S01:  0.05505574985124946\n",
            "Loss S2:  0.062276767929215236\n",
            "Loss S01:  0.055033867928759875\n",
            "Loss S2:  0.062288227664443835\n",
            "Loss S01:  0.05520275399027029\n",
            "Loss S2:  0.062483961728722226\n",
            "Loss S01:  0.0552396898814644\n",
            "Loss S2:  0.06253256351943035\n",
            "Loss S01:  0.05535091359839129\n",
            "Loss S2:  0.06257692934281525\n",
            "Loss S01:  0.05536312740502322\n",
            "Loss S2:  0.06265819815032157\n",
            "Loss S01:  0.05538454331006868\n",
            "Loss S2:  0.06269216192786804\n",
            "Loss S01:  0.05533158027685385\n",
            "Loss S2:  0.06267489183092445\n",
            "Loss S01:  0.055366607252941576\n",
            "Loss S2:  0.06268802219599584\n",
            "Loss S01:  0.055246188589805956\n",
            "Loss S2:  0.06261330967547427\n",
            "Loss S01:  0.05531324811767195\n",
            "Loss S2:  0.0626822825973836\n",
            "Loss S01:  0.05526049444142425\n",
            "Loss S2:  0.06265463269234063\n",
            "Loss S01:  0.05527817673129071\n",
            "Loss S2:  0.06266096570799427\n",
            "Loss S01:  0.055309750362593904\n",
            "Loss S2:  0.06271277300143174\n",
            "Loss S01:  0.05531712872210962\n",
            "Loss S2:  0.06264703114029443\n",
            "Loss S01:  0.0552814559273\n",
            "Loss S2:  0.06265865696087038\n",
            "Loss S01:  0.05526683152973495\n",
            "Loss S2:  0.0626795881982707\n",
            "Loss S01:  0.055226105015219934\n",
            "Loss S2:  0.06261316522994005\n",
            "Loss S01:  0.05524568185396028\n",
            "Loss S2:  0.06262392312882845\n",
            "Loss S01:  0.05522644054824418\n",
            "Loss S2:  0.06263470479316666\n",
            "Loss S01:  0.05528204849171242\n",
            "Loss S2:  0.0627201654707168\n",
            "Loss S01:  0.05527984745921502\n",
            "Loss S2:  0.0627051777878383\n",
            "Loss S01:  0.05527281300787753\n",
            "Loss S2:  0.06270480815684444\n",
            "Loss S01:  0.055328464147917714\n",
            "Loss S2:  0.06271105515718989\n",
            "Loss S01:  0.05533563341630255\n",
            "Loss S2:  0.06271683587137117\n",
            "Loss S01:  0.055302920464560736\n",
            "Loss S2:  0.06268712428977788\n",
            "Loss S01:  0.055305326384590965\n",
            "Loss S2:  0.06268348506111612\n",
            "Loss S01:  0.055277449111103286\n",
            "Loss S2:  0.0627415677468063\n",
            "Validation: \n",
            " Loss S01:  0.06969216465950012\n",
            " Loss S2:  0.11135531961917877\n",
            " Loss S01:  0.07478353186022668\n",
            " Loss S2:  0.1253518969530151\n",
            " Loss S01:  0.07398854168813403\n",
            " Loss S2:  0.12477653709853567\n",
            " Loss S01:  0.07328467095484499\n",
            " Loss S2:  0.12318693822036024\n",
            " Loss S01:  0.0727834720854406\n",
            " Loss S2:  0.12242107957969477\n",
            "\n",
            "Epoch: 58\n",
            "Loss S01:  0.054841913282871246\n",
            "Loss S2:  0.06344980001449585\n",
            "Loss S01:  0.055572372607209465\n",
            "Loss S2:  0.06368427384983409\n",
            "Loss S01:  0.05547204046022324\n",
            "Loss S2:  0.06333006918430328\n",
            "Loss S01:  0.05507377499053555\n",
            "Loss S2:  0.06232850277616132\n",
            "Loss S01:  0.05474458416787589\n",
            "Loss S2:  0.061991542305161317\n",
            "Loss S01:  0.05477026718504289\n",
            "Loss S2:  0.06194492926200231\n",
            "Loss S01:  0.0547523634111295\n",
            "Loss S2:  0.062369267898993414\n",
            "Loss S01:  0.05454657305504235\n",
            "Loss S2:  0.062130528016829153\n",
            "Loss S01:  0.05443843385135686\n",
            "Loss S2:  0.06203799041700952\n",
            "Loss S01:  0.05456915853933974\n",
            "Loss S2:  0.062138920301919455\n",
            "Loss S01:  0.05458200741374847\n",
            "Loss S2:  0.06216877248912755\n",
            "Loss S01:  0.054392766845118894\n",
            "Loss S2:  0.06192857470061328\n",
            "Loss S01:  0.05444962829967176\n",
            "Loss S2:  0.06195583203730504\n",
            "Loss S01:  0.05439221005858356\n",
            "Loss S2:  0.06189137501002268\n",
            "Loss S01:  0.054372317666280354\n",
            "Loss S2:  0.06193376525391078\n",
            "Loss S01:  0.05458109438518025\n",
            "Loss S2:  0.06226819371249502\n",
            "Loss S01:  0.05462004200364492\n",
            "Loss S2:  0.062296994146167864\n",
            "Loss S01:  0.05460602700797438\n",
            "Loss S2:  0.06234307199368003\n",
            "Loss S01:  0.05467654624696595\n",
            "Loss S2:  0.06239962859789311\n",
            "Loss S01:  0.05475397745430158\n",
            "Loss S2:  0.062398036054447686\n",
            "Loss S01:  0.05479839216194936\n",
            "Loss S2:  0.06241500915134724\n",
            "Loss S01:  0.054866673133525806\n",
            "Loss S2:  0.06246483988035911\n",
            "Loss S01:  0.05491143112740905\n",
            "Loss S2:  0.06251124619628509\n",
            "Loss S01:  0.05493579550093903\n",
            "Loss S2:  0.0624877442142148\n",
            "Loss S01:  0.0550503220362782\n",
            "Loss S2:  0.06256505602746584\n",
            "Loss S01:  0.05507078426945732\n",
            "Loss S2:  0.06252433031619307\n",
            "Loss S01:  0.05509880577638688\n",
            "Loss S2:  0.06258141878863861\n",
            "Loss S01:  0.05512699384475986\n",
            "Loss S2:  0.06262454423517319\n",
            "Loss S01:  0.05514573838787147\n",
            "Loss S2:  0.06262032040709703\n",
            "Loss S01:  0.055173373163463324\n",
            "Loss S2:  0.06264109545966604\n",
            "Loss S01:  0.055195649275153975\n",
            "Loss S2:  0.06259603730319346\n",
            "Loss S01:  0.05510726396198058\n",
            "Loss S2:  0.0625781340350867\n",
            "Loss S01:  0.05514972890231097\n",
            "Loss S2:  0.06261864179205671\n",
            "Loss S01:  0.05508979341279704\n",
            "Loss S2:  0.0625770045078593\n",
            "Loss S01:  0.055083388107613034\n",
            "Loss S2:  0.0625806146047332\n",
            "Loss S01:  0.05507561795816802\n",
            "Loss S2:  0.06256822647362353\n",
            "Loss S01:  0.05502089600071022\n",
            "Loss S2:  0.06252522661332609\n",
            "Loss S01:  0.054993877152227005\n",
            "Loss S2:  0.06249135392253611\n",
            "Loss S01:  0.054952544920400685\n",
            "Loss S2:  0.06241396187836417\n",
            "Loss S01:  0.054905014288852284\n",
            "Loss S2:  0.062362520133747774\n",
            "Loss S01:  0.054898784651497654\n",
            "Loss S2:  0.06236884883254246\n",
            "Loss S01:  0.0548887684886908\n",
            "Loss S2:  0.06232148543942874\n",
            "Loss S01:  0.054870712935853744\n",
            "Loss S2:  0.06232193756556568\n",
            "Loss S01:  0.054800849840079546\n",
            "Loss S2:  0.06227080463464188\n",
            "Loss S01:  0.05476378791389011\n",
            "Loss S2:  0.06228740467708938\n",
            "Loss S01:  0.054771933060329396\n",
            "Loss S2:  0.06226713603523512\n",
            "Loss S01:  0.05479654302889251\n",
            "Loss S2:  0.06229464015610306\n",
            "Loss S01:  0.054767315088120744\n",
            "Loss S2:  0.06223814702748999\n",
            "Loss S01:  0.05476404818669426\n",
            "Loss S2:  0.06223054727434864\n",
            "Loss S01:  0.05477897989774187\n",
            "Loss S2:  0.06220959146675899\n",
            "Validation: \n",
            " Loss S01:  0.0700422152876854\n",
            " Loss S2:  0.11367200314998627\n",
            " Loss S01:  0.07529447476069133\n",
            " Loss S2:  0.1319218221164885\n",
            " Loss S01:  0.0746386509116103\n",
            " Loss S2:  0.13110387325286865\n",
            " Loss S01:  0.0739523312229602\n",
            " Loss S2:  0.12903354341377976\n",
            " Loss S01:  0.07360260390941008\n",
            " Loss S2:  0.12815285133726803\n",
            "\n",
            "Epoch: 59\n",
            "Loss S01:  0.05060286447405815\n",
            "Loss S2:  0.060464370995759964\n",
            "Loss S01:  0.055393471297892655\n",
            "Loss S2:  0.06296911158344963\n",
            "Loss S01:  0.05603307875848952\n",
            "Loss S2:  0.062311838780130656\n",
            "Loss S01:  0.05544686245341455\n",
            "Loss S2:  0.06196537181254356\n",
            "Loss S01:  0.05528002277743525\n",
            "Loss S2:  0.0622792293930926\n",
            "Loss S01:  0.054934496680895485\n",
            "Loss S2:  0.06190028194995487\n",
            "Loss S01:  0.05507864186265429\n",
            "Loss S2:  0.06184361068928828\n",
            "Loss S01:  0.05478951056868258\n",
            "Loss S2:  0.06165629808961506\n",
            "Loss S01:  0.054766392864194914\n",
            "Loss S2:  0.061484518104497295\n",
            "Loss S01:  0.05480994206372198\n",
            "Loss S2:  0.061612211290624115\n",
            "Loss S01:  0.05482788107330256\n",
            "Loss S2:  0.06159887868579071\n",
            "Loss S01:  0.05458424700138805\n",
            "Loss S2:  0.061401238748887636\n",
            "Loss S01:  0.05452532077993243\n",
            "Loss S2:  0.061301382273928194\n",
            "Loss S01:  0.054524810003642815\n",
            "Loss S2:  0.06136579131920829\n",
            "Loss S01:  0.0546044833588262\n",
            "Loss S2:  0.061427805942635166\n",
            "Loss S01:  0.05469089507168492\n",
            "Loss S2:  0.06152148586728715\n",
            "Loss S01:  0.05470790871366951\n",
            "Loss S2:  0.06144593514918541\n",
            "Loss S01:  0.054637314849778226\n",
            "Loss S2:  0.06144715579803924\n",
            "Loss S01:  0.05473153828257355\n",
            "Loss S2:  0.06139125444447797\n",
            "Loss S01:  0.0547642782330513\n",
            "Loss S2:  0.061476609235658695\n",
            "Loss S01:  0.05485650446655145\n",
            "Loss S2:  0.061474730515509696\n",
            "Loss S01:  0.05489980367617019\n",
            "Loss S2:  0.06148388457453646\n",
            "Loss S01:  0.054939197755642065\n",
            "Loss S2:  0.06144733853402181\n",
            "Loss S01:  0.054952508782024505\n",
            "Loss S2:  0.06151479556834027\n",
            "Loss S01:  0.055037394088704554\n",
            "Loss S2:  0.061641160043196065\n",
            "Loss S01:  0.05509133893714483\n",
            "Loss S2:  0.061752453253444924\n",
            "Loss S01:  0.05509322444940436\n",
            "Loss S2:  0.06180198841738975\n",
            "Loss S01:  0.055121215287169845\n",
            "Loss S2:  0.06179287729997916\n",
            "Loss S01:  0.05515236941785999\n",
            "Loss S2:  0.06181864160712928\n",
            "Loss S01:  0.05514148351355517\n",
            "Loss S2:  0.06182993689865591\n",
            "Loss S01:  0.055159614777248164\n",
            "Loss S2:  0.061841933731224846\n",
            "Loss S01:  0.055080007881501095\n",
            "Loss S2:  0.06179768110586516\n",
            "Loss S01:  0.055101255982000136\n",
            "Loss S2:  0.06187877566132961\n",
            "Loss S01:  0.05504529567553558\n",
            "Loss S2:  0.061869182311606194\n",
            "Loss S01:  0.05506362315933725\n",
            "Loss S2:  0.061849304745274204\n",
            "Loss S01:  0.055052425701733666\n",
            "Loss S2:  0.06187561285971237\n",
            "Loss S01:  0.054997115099281485\n",
            "Loss S2:  0.061884606819486356\n",
            "Loss S01:  0.05495446226547028\n",
            "Loss S2:  0.061874352686729715\n",
            "Loss S01:  0.05494778217097593\n",
            "Loss S2:  0.061819630550352604\n",
            "Loss S01:  0.05491505136422794\n",
            "Loss S2:  0.06172660237078167\n",
            "Loss S01:  0.05493341586053223\n",
            "Loss S2:  0.06175319470483763\n",
            "Loss S01:  0.05486714584331443\n",
            "Loss S2:  0.06174921744713818\n",
            "Loss S01:  0.054934578180100745\n",
            "Loss S2:  0.06178484061586885\n",
            "Loss S01:  0.05492761370719447\n",
            "Loss S2:  0.06176077099970764\n",
            "Loss S01:  0.05489335029202254\n",
            "Loss S2:  0.06176669425975168\n",
            "Loss S01:  0.05494464738108631\n",
            "Loss S2:  0.061781302053159196\n",
            "Loss S01:  0.05495540155279662\n",
            "Loss S2:  0.061782919306817126\n",
            "Loss S01:  0.05494989771910028\n",
            "Loss S2:  0.061752642700626595\n",
            "Loss S01:  0.05494123243430548\n",
            "Loss S2:  0.061757937445462124\n",
            "Loss S01:  0.05493373187095716\n",
            "Loss S2:  0.06172471673298028\n",
            "Validation: \n",
            " Loss S01:  0.07283209264278412\n",
            " Loss S2:  0.11566726118326187\n",
            " Loss S01:  0.0756809532287575\n",
            " Loss S2:  0.133539732723009\n",
            " Loss S01:  0.07499148142410487\n",
            " Loss S2:  0.13217462780998973\n",
            " Loss S01:  0.07428988612821845\n",
            " Loss S2:  0.1305318971882101\n",
            " Loss S01:  0.07369216848854665\n",
            " Loss S2:  0.1296657465490294\n",
            "\n",
            "Epoch: 60\n",
            "Loss S01:  0.05420606583356857\n",
            "Loss S2:  0.0559966154396534\n",
            "Loss S01:  0.05505858260122212\n",
            "Loss S2:  0.060586860234087166\n",
            "Loss S01:  0.055466290208555404\n",
            "Loss S2:  0.06104474514722824\n",
            "Loss S01:  0.05466431150993993\n",
            "Loss S2:  0.060905851664081696\n",
            "Loss S01:  0.05426066341560062\n",
            "Loss S2:  0.06076723182710206\n",
            "Loss S01:  0.05399971177764967\n",
            "Loss S2:  0.060400072631298325\n",
            "Loss S01:  0.05423042874355785\n",
            "Loss S2:  0.060772867659564876\n",
            "Loss S01:  0.05401262740643931\n",
            "Loss S2:  0.06051294138314019\n",
            "Loss S01:  0.053918711795115176\n",
            "Loss S2:  0.06042067757175292\n",
            "Loss S01:  0.053918326375903665\n",
            "Loss S2:  0.06073940840068754\n",
            "Loss S01:  0.054002550716447356\n",
            "Loss S2:  0.06080331826711645\n",
            "Loss S01:  0.05380005218289994\n",
            "Loss S2:  0.06062057513642956\n",
            "Loss S01:  0.05375055885635131\n",
            "Loss S2:  0.06076831207418245\n",
            "Loss S01:  0.053793986278180855\n",
            "Loss S2:  0.06081496034073466\n",
            "Loss S01:  0.05384383458934777\n",
            "Loss S2:  0.06087960606665476\n",
            "Loss S01:  0.05407066677777183\n",
            "Loss S2:  0.061133371974458756\n",
            "Loss S01:  0.05407996258195143\n",
            "Loss S2:  0.0611101778814141\n",
            "Loss S01:  0.05408698976127028\n",
            "Loss S2:  0.06111461943701694\n",
            "Loss S01:  0.0541486763575459\n",
            "Loss S2:  0.061191937191545635\n",
            "Loss S01:  0.054220217587280024\n",
            "Loss S2:  0.06124010576783675\n",
            "Loss S01:  0.05425127412178623\n",
            "Loss S2:  0.061440328719900614\n",
            "Loss S01:  0.054280227023702096\n",
            "Loss S2:  0.06145113300514447\n",
            "Loss S01:  0.05431104163171479\n",
            "Loss S2:  0.06158775502470284\n",
            "Loss S01:  0.05429781334740775\n",
            "Loss S2:  0.06156132663741256\n",
            "Loss S01:  0.05436928548629848\n",
            "Loss S2:  0.06157443289069219\n",
            "Loss S01:  0.05434288182462829\n",
            "Loss S2:  0.06160920668408215\n",
            "Loss S01:  0.05440626473262392\n",
            "Loss S2:  0.061650699753870906\n",
            "Loss S01:  0.05442308039297917\n",
            "Loss S2:  0.061671440699223666\n",
            "Loss S01:  0.05440891715714516\n",
            "Loss S2:  0.06169033807526704\n",
            "Loss S01:  0.05435445273608686\n",
            "Loss S2:  0.061677188922961555\n",
            "Loss S01:  0.05436549619137251\n",
            "Loss S2:  0.06172030818868317\n",
            "Loss S01:  0.05429437294171171\n",
            "Loss S2:  0.06165340292444183\n",
            "Loss S01:  0.054326322892930275\n",
            "Loss S2:  0.06166199210601803\n",
            "Loss S01:  0.05428583269483013\n",
            "Loss S2:  0.06161484256206683\n",
            "Loss S01:  0.0542882208513025\n",
            "Loss S2:  0.06161022768560742\n",
            "Loss S01:  0.05433640496386082\n",
            "Loss S2:  0.06163754296141472\n",
            "Loss S01:  0.054312304931581845\n",
            "Loss S2:  0.06160301397299172\n",
            "Loss S01:  0.05425078818459074\n",
            "Loss S2:  0.06158932615724214\n",
            "Loss S01:  0.054235885579754985\n",
            "Loss S2:  0.06157231879398579\n",
            "Loss S01:  0.05420838189704339\n",
            "Loss S2:  0.0615354241979549\n",
            "Loss S01:  0.054221893672336666\n",
            "Loss S2:  0.06150524377674236\n",
            "Loss S01:  0.05419448253737169\n",
            "Loss S2:  0.061459570896524\n",
            "Loss S01:  0.05417317180339061\n",
            "Loss S2:  0.06142563889612778\n",
            "Loss S01:  0.05416518536216975\n",
            "Loss S2:  0.061415062218973644\n",
            "Loss S01:  0.05415840414004261\n",
            "Loss S2:  0.06147544320816356\n",
            "Loss S01:  0.0541278522486433\n",
            "Loss S2:  0.061460998132329296\n",
            "Loss S01:  0.05414464660069689\n",
            "Loss S2:  0.06144670050373047\n",
            "Loss S01:  0.054142685777126096\n",
            "Loss S2:  0.06143474667441819\n",
            "Loss S01:  0.054140413377488227\n",
            "Loss S2:  0.06142857649098315\n",
            "Loss S01:  0.05412797256545477\n",
            "Loss S2:  0.06139411372104148\n",
            "Validation: \n",
            " Loss S01:  0.06371431052684784\n",
            " Loss S2:  0.11108814179897308\n",
            " Loss S01:  0.07014895674018633\n",
            " Loss S2:  0.13084957535777772\n",
            " Loss S01:  0.06964292513524614\n",
            " Loss S2:  0.13038480681617085\n",
            " Loss S01:  0.0690612507770296\n",
            " Loss S2:  0.128766802002172\n",
            " Loss S01:  0.06866115897342011\n",
            " Loss S2:  0.12835629155606398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create ensamble model\n",
        "\n",
        "1.8 In this step you will create a new network class that takes s1, and s2 as perimeters. This class should initiate a new network that ensembles both s1 and s2, and have a classifier for cross-entropy. In the forward method pass the input x from both s1 and s2 and then concatenate there outputs along axis 1. Then pass this concatinated output through classifier of appropriate shape. "
      ],
      "metadata": {
        "id": "25tcDBBIu2H2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, s1,s2):\n",
        "        super(Net, self).__init__()\n",
        "        self.s1 = s1\n",
        "        self.s2 = s2\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.s1(x)\n",
        "        out2 = self.s2(x)\n",
        "        out = torch.cat((out1,out2),1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "net = Net(s01,s2)\n",
        "net.to(device)\n",
        "summary(net, (3, 32, 32))"
      ],
      "metadata": {
        "id": "EpzuTHRovW1T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "069fe745-3c54-456c-cf6b-9832eb9ffdc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 256, 1, 1]               0\n",
            "           Linear-31                  [-1, 256]          65,792\n",
            "              VGG-32                  [-1, 256]               0\n",
            "           Conv2d-33           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-34           [-1, 32, 32, 32]              64\n",
            "             ReLU-35           [-1, 32, 32, 32]               0\n",
            "           Conv2d-36           [-1, 32, 32, 32]           9,248\n",
            "      BatchNorm2d-37           [-1, 32, 32, 32]              64\n",
            "             ReLU-38           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-39           [-1, 32, 16, 16]               0\n",
            "           Conv2d-40           [-1, 64, 16, 16]          18,496\n",
            "      BatchNorm2d-41           [-1, 64, 16, 16]             128\n",
            "             ReLU-42           [-1, 64, 16, 16]               0\n",
            "           Conv2d-43           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-44           [-1, 64, 16, 16]             128\n",
            "             ReLU-45           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-46             [-1, 64, 8, 8]               0\n",
            "           Conv2d-47            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-48            [-1, 128, 8, 8]             256\n",
            "             ReLU-49            [-1, 128, 8, 8]               0\n",
            "           Conv2d-50            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-51            [-1, 128, 8, 8]             256\n",
            "             ReLU-52            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-53            [-1, 128, 4, 4]               0\n",
            "           Conv2d-54            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-55            [-1, 256, 4, 4]             512\n",
            "             ReLU-56            [-1, 256, 4, 4]               0\n",
            "           Conv2d-57            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-58            [-1, 256, 4, 4]             512\n",
            "             ReLU-59            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-60            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-61            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-62            [-1, 256, 1, 1]               0\n",
            "           Linear-63                  [-1, 256]          65,792\n",
            "              VGG-64                  [-1, 256]               0\n",
            "           Linear-65                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 2,485,066\n",
            "Trainable params: 2,485,066\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 5.88\n",
            "Params size (MB): 9.48\n",
            "Estimated Total Size (MB): 15.37\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train The Ensambled network\n",
        "1.9 In this step you will freez all the conv layers in the ensambled network and then finetune it on orignal dataset. "
      ],
      "metadata": {
        "id": "qNFSWlD5wvZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in net.modules():\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        m.weight.requires_grad = False\n",
        "        if m.bias is not None:\n",
        "            m.bias.requires_grad = False\n",
        "net = net.to(device)\n",
        "summary(net, (3, 32, 32))"
      ],
      "metadata": {
        "id": "tbZ9_90YxCwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dececac-adb6-4404-ee18-ab11c3a7f475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "           Conv2d-11           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
            "             ReLU-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-26            [-1, 256, 4, 4]             512\n",
            "             ReLU-27            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-28            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 256, 1, 1]               0\n",
            "           Linear-31                  [-1, 256]          65,792\n",
            "              VGG-32                  [-1, 256]               0\n",
            "           Conv2d-33           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-34           [-1, 32, 32, 32]              64\n",
            "             ReLU-35           [-1, 32, 32, 32]               0\n",
            "           Conv2d-36           [-1, 32, 32, 32]           9,248\n",
            "      BatchNorm2d-37           [-1, 32, 32, 32]              64\n",
            "             ReLU-38           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-39           [-1, 32, 16, 16]               0\n",
            "           Conv2d-40           [-1, 64, 16, 16]          18,496\n",
            "      BatchNorm2d-41           [-1, 64, 16, 16]             128\n",
            "             ReLU-42           [-1, 64, 16, 16]               0\n",
            "           Conv2d-43           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-44           [-1, 64, 16, 16]             128\n",
            "             ReLU-45           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-46             [-1, 64, 8, 8]               0\n",
            "           Conv2d-47            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-48            [-1, 128, 8, 8]             256\n",
            "             ReLU-49            [-1, 128, 8, 8]               0\n",
            "           Conv2d-50            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-51            [-1, 128, 8, 8]             256\n",
            "             ReLU-52            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-53            [-1, 128, 4, 4]               0\n",
            "           Conv2d-54            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-55            [-1, 256, 4, 4]             512\n",
            "             ReLU-56            [-1, 256, 4, 4]               0\n",
            "           Conv2d-57            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-58            [-1, 256, 4, 4]             512\n",
            "             ReLU-59            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-60            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-61            [-1, 256, 1, 1]               0\n",
            "        AvgPool2d-62            [-1, 256, 1, 1]               0\n",
            "           Linear-63                  [-1, 256]          65,792\n",
            "              VGG-64                  [-1, 256]               0\n",
            "           Linear-65                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 2,485,066\n",
            "Trainable params: 140,554\n",
            "Non-trainable params: 2,344,512\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 5.88\n",
            "Params size (MB): 9.48\n",
            "Estimated Total Size (MB): 15.37\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        net.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if(batch_idx % 200 == 0):\n",
        "          print(\"Accuracy : \",100.*correct/total,\" Loss : \", train_loss/(batch_idx+1))\n",
        "def test(epoch):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\"Accuracy : \",100.*correct/total,\" Loss : \", test_loss/(batch_idx+1))"
      ],
      "metadata": {
        "id": "2SzjCW-6xHIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "    train(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test(epoch)"
      ],
      "metadata": {
        "id": "gxz8dPNXxMKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5e02216-6102-48ec-960c-228d54141da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Accuracy :  3.0  Loss :  2.7756900787353516\n",
            "Accuracy :  76.46268656716418  Loss :  0.9236625142358429\n",
            "Accuracy :  81.69576059850374  Loss :  0.6757286233349037\n",
            "Validation: \n",
            "Accuracy :  88.0  Loss :  0.41089946031570435\n",
            "Accuracy :  85.57142857142857  Loss :  0.4320966714904422\n",
            "Accuracy :  85.29268292682927  Loss :  0.4398074920584516\n",
            "Accuracy :  85.47540983606558  Loss :  0.43419554927310006\n",
            "Accuracy :  85.5679012345679  Loss :  0.4320388576130808\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy :  88.0  Loss :  0.3620695471763611\n",
            "Accuracy :  87.1592039800995  Loss :  0.3779773723278473\n",
            "Accuracy :  87.41147132169576  Loss :  0.3703098126926327\n",
            "Validation: \n",
            "Accuracy :  88.0  Loss :  0.3846610188484192\n",
            "Accuracy :  85.52380952380952  Loss :  0.4216852848018919\n",
            "Accuracy :  85.4390243902439  Loss :  0.43068354267899583\n",
            "Accuracy :  85.80327868852459  Loss :  0.4236821461407865\n",
            "Accuracy :  85.90123456790124  Loss :  0.42114091250631547\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy :  88.0  Loss :  0.32414543628692627\n",
            "Accuracy :  87.34328358208955  Loss :  0.3643357510590435\n",
            "Accuracy :  87.54862842892769  Loss :  0.3593078283151784\n",
            "Validation: \n",
            "Accuracy :  89.0  Loss :  0.373589426279068\n",
            "Accuracy :  85.61904761904762  Loss :  0.42010817357472013\n",
            "Accuracy :  85.60975609756098  Loss :  0.42905809094266195\n",
            "Accuracy :  85.90163934426229  Loss :  0.4216647553639334\n",
            "Accuracy :  85.98765432098766  Loss :  0.4186022697407522\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy :  87.0  Loss :  0.3618098199367523\n",
            "Accuracy :  87.77114427860697  Loss :  0.3556503264316872\n",
            "Accuracy :  87.80299251870325  Loss :  0.35383304704603113\n",
            "Validation: \n",
            "Accuracy :  89.0  Loss :  0.36259734630584717\n",
            "Accuracy :  85.85714285714286  Loss :  0.4184972352924801\n",
            "Accuracy :  85.6829268292683  Loss :  0.4280198176459568\n",
            "Accuracy :  86.0327868852459  Loss :  0.4203831384905049\n",
            "Accuracy :  86.08641975308642  Loss :  0.4166967239644792\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy :  89.0  Loss :  0.35093265771865845\n",
            "Accuracy :  87.76616915422886  Loss :  0.35416572003518765\n",
            "Accuracy :  87.87032418952619  Loss :  0.35186337562570547\n",
            "Validation: \n",
            "Accuracy :  89.0  Loss :  0.3633570373058319\n",
            "Accuracy :  85.80952380952381  Loss :  0.41618414719899494\n",
            "Accuracy :  85.73170731707317  Loss :  0.42525940988121963\n",
            "Accuracy :  86.19672131147541  Loss :  0.4171576468182392\n",
            "Accuracy :  86.34567901234568  Loss :  0.41390687392817604\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy :  85.0  Loss :  0.36696675419807434\n",
            "Accuracy :  87.92039800995025  Loss :  0.3491462283021775\n",
            "Accuracy :  88.0498753117207  Loss :  0.3463751680758826\n",
            "Validation: \n",
            "Accuracy :  89.0  Loss :  0.3562230169773102\n",
            "Accuracy :  86.04761904761905  Loss :  0.4151407600868316\n",
            "Accuracy :  85.85365853658537  Loss :  0.42424865721202476\n",
            "Accuracy :  86.24590163934427  Loss :  0.4159699392611863\n",
            "Accuracy :  86.33333333333333  Loss :  0.41272827522990146\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy :  88.0  Loss :  0.34210407733917236\n",
            "Accuracy :  87.87064676616916  Loss :  0.3486340641975403\n",
            "Accuracy :  87.9426433915212  Loss :  0.3467060509242322\n",
            "Validation: \n",
            "Accuracy :  89.0  Loss :  0.350302517414093\n",
            "Accuracy :  86.33333333333333  Loss :  0.41261246303717297\n",
            "Accuracy :  86.09756097560975  Loss :  0.4211451778324639\n",
            "Accuracy :  86.45901639344262  Loss :  0.4133005809099948\n",
            "Accuracy :  86.50617283950618  Loss :  0.40964892046687046\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy :  91.0  Loss :  0.3316323459148407\n",
            "Accuracy :  88.11442786069652  Loss :  0.34492503091767057\n",
            "Accuracy :  88.24937655860349  Loss :  0.34212399896540846\n",
            "Validation: \n",
            "Accuracy :  89.0  Loss :  0.3539385497570038\n",
            "Accuracy :  86.33333333333333  Loss :  0.4116812135492052\n",
            "Accuracy :  86.07317073170732  Loss :  0.41990983340798355\n",
            "Accuracy :  86.45901639344262  Loss :  0.41147462681668706\n",
            "Accuracy :  86.58024691358025  Loss :  0.4078119805565587\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy :  88.0  Loss :  0.29628467559814453\n",
            "Accuracy :  88.1044776119403  Loss :  0.3419098576799554\n",
            "Accuracy :  88.19451371571073  Loss :  0.34048280929984\n",
            "Validation: \n",
            "Accuracy :  89.0  Loss :  0.3497781455516815\n",
            "Accuracy :  86.28571428571429  Loss :  0.41135939884753453\n",
            "Accuracy :  86.02439024390245  Loss :  0.4201301310847445\n",
            "Accuracy :  86.40983606557377  Loss :  0.4115898215868434\n",
            "Accuracy :  86.50617283950618  Loss :  0.40841398176587657\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy :  88.0  Loss :  0.3072596490383148\n",
            "Accuracy :  88.11442786069652  Loss :  0.3412435687151714\n",
            "Accuracy :  88.19950124688279  Loss :  0.33953213561651413\n",
            "Validation: \n",
            "Accuracy :  89.0  Loss :  0.34982651472091675\n",
            "Accuracy :  86.33333333333333  Loss :  0.4092894253276643\n",
            "Accuracy :  86.1219512195122  Loss :  0.41769621503062365\n",
            "Accuracy :  86.45901639344262  Loss :  0.4092940353467816\n",
            "Accuracy :  86.5679012345679  Loss :  0.4053891558706025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_name = 'ss1student.pt'\n",
        "path = F\"/content/gdrive/MyDrive/{model_save_name}\" \n",
        "torch.save(s01.state_dict(), path)\n",
        "# #s01.load_state_dict(torch.load(path))\n",
        "model_save_name = 'ss2student.pt'\n",
        "path = F\"/content/gdrive/MyDrive/{model_save_name}\" \n",
        "torch.save(s2.state_dict(), path)\n",
        "# #s2.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "_TF6UDYswobN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Create** **4** **More Students.**\n",
        "\n"
      ],
      "metadata": {
        "id": "42aMmjCt8EOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "    'VGGS1': [32, 32, 'M', 64, 64, 'M', 128, 128, 'M', 256, 256, 'M','M'],\n",
        "    'VGGS33': [32, 32, 'M', 64, 64, 'M', 128, 'M','M','M'],\n",
        "    'VGGS2':[32,'M', 32, 'M', 64, 64, 'M', 128,128,'M',128,128, 'M'],\n",
        "    \n",
        "\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(128, 128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "s11 = VGG('VGGS2')\n",
        "s11 = s11.to(device)\n",
        "summary(s11, (3, 32, 32))\n",
        "s22 = VGG('VGGS2')\n",
        "s22 = s22.to(device)\n",
        "summary(s22, (3,32,32))\n",
        "s33 = VGG('VGGS2')\n",
        "s33 = s33.to(device)\n",
        "summary(s33, (3, 32, 32))\n",
        "s44 = VGG('VGGS2')\n",
        "s44 = s44.to(device)\n",
        "summary(s44, (3, 32, 32))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrL4hr--x0Hg",
        "outputId": "a5d1f183-8c68-4959-ecf6-1c9c06467f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "================================================================\n",
            "Total params: 600,096\n",
            "Trainable params: 600,096\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.33\n",
            "Params size (MB): 2.29\n",
            "Estimated Total Size (MB): 3.64\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "================================================================\n",
            "Total params: 600,096\n",
            "Trainable params: 600,096\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.33\n",
            "Params size (MB): 2.29\n",
            "Estimated Total Size (MB): 3.64\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "================================================================\n",
            "Total params: 600,096\n",
            "Trainable params: 600,096\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.33\n",
            "Params size (MB): 2.29\n",
            "Estimated Total Size (MB): 3.64\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "================================================================\n",
            "Total params: 600,096\n",
            "Trainable params: 600,096\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.33\n",
            "Params size (MB): 2.29\n",
            "Estimated Total Size (MB): 3.64\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TA_WOH = nn.Sequential(*list(s01.children())[:-1],nn.Flatten())\n",
        "# #TA_WOH = nn.Sequential(*list(s1.children())[:],nn.Flatten())\n",
        "# summary(s01, (3, 32, 32))\n",
        "# summary(TA_WOH, (3, 32, 32))"
      ],
      "metadata": {
        "id": "Jq3XMEdH0kYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TA2_WOH = nn.Sequential(*list(s2.children())[:-1],nn.Flatten())\n",
        "# summary(s2, (3, 32, 32))\n",
        "# summary(TA2_WOH, (3, 32, 32))"
      ],
      "metadata": {
        "id": "buiyd5tGEY4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # TA_WOH.eval()\n",
        "# # TA2_WOH.eval()\n",
        "# # s1.eval()\n",
        "# # s2.eval()\n",
        "# TADenseTrain = None\n",
        "# TADenseTest = None\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "#         inputs, targets = inputs.to(device), targets.to(device)\n",
        "#         outputs1 = TA_WOH(inputs)\n",
        "#         outputs2 = TA2_WOH(inputs)\n",
        "#         if(TADenseTrain == None):\n",
        "#             TADenseTrain = torch.cat((outputs1,outputs2),1) \n",
        "#         else:\n",
        "#             totalOUTPUT = torch.cat((outputs1,outputs2),1)         \n",
        "#             TADenseTrain = torch.cat((TADenseTrain,totalOUTPUT))\n",
        "           \n",
        "# with torch.no_grad():\n",
        "#     for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "#         inputs, targets = inputs.to(device), targets.to(device)\n",
        "#         outputs1 = TA_WOH(inputs)\n",
        "#         outputs2 = TA2_WOH(inputs)\n",
        "#         if(TADenseTest == None):\n",
        "#             TADenseTest = torch.cat((outputs1,outputs2),1)\n",
        "#         else:\n",
        "#             totalOUTPUT = torch.cat((outputs1,outputs2),1)         \n",
        "#             TADenseTest = torch.cat((TADenseTest,totalOUTPUT))\n",
        "           "
      ],
      "metadata": {
        "id": "0CQM3bJZFE70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(TADenseTrain.shape)  "
      ],
      "metadata": {
        "id": "vFUqixLL0tYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer1 = optim.Adam(s11.parameters(), lr=0.0001)\n",
        "optimizer2 = optim.Adam(s22.parameters(), lr=0.0001)\n",
        "optimizer3 = optim.Adam(s33.parameters(), lr=0.0001)\n",
        "optimizer4 = optim.Adam(s44.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def train4(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    s11.train()\n",
        "    train_loss1 = 0\n",
        "    train_loss2 = 0\n",
        "    train_loss3 = 0\n",
        "    train_loss4= 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        #code here\n",
        "        targets = DenseTrain[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "        targets.to(device)\n",
        "        s11.zero_grad()\n",
        "        s22.zero_grad()\n",
        "        s33.zero_grad()\n",
        "        s44.zero_grad()\n",
        "        output1 = s11(inputs)\n",
        "        output2 = s22(inputs)\n",
        "        output3 = s33(inputs)\n",
        "        output4 = s44(inputs)\n",
        "        \n",
        "        loss1 = criterion(output1, targets[:,:128])\n",
        "        loss2 = criterion(output2, targets[:,128:256])\n",
        "        loss3 = criterion(output3, targets[:,256:384])\n",
        "        loss4 = criterion(output4, targets[:,384:512])\n",
        "        loss1.backward()\n",
        "        loss2.backward()\n",
        "        loss3.backward()\n",
        "        loss4.backward()\n",
        "        optimizer1.step()\n",
        "        optimizer2.step()\n",
        "        optimizer3.step()\n",
        "        optimizer4.step()\n",
        "      \n",
        "\n",
        "        train_loss1 += loss1.item()\n",
        "        train_loss2 += loss2.item()\n",
        "        train_loss3 += loss3.item()\n",
        "        train_loss4 += loss4.item()\n",
        "        if(batch_idx % 10 == 0):\n",
        "          print(\"Loss S11: \", train_loss1/(batch_idx+1))\n",
        "          print(\"Loss S22: \", train_loss2/(batch_idx+1))\n",
        "          print(\"Loss S33: \", train_loss3/(batch_idx+1))\n",
        "          print(\"Loss S44: \", train_loss4/(batch_idx+1))\n",
        "def test4(epoch):\n",
        "    s11.eval()\n",
        "    \n",
        "    test_loss1 = 0\n",
        "    test_loss2 = 0\n",
        "    test_loss3 = 0\n",
        "    test_loss4= 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            #code here\n",
        "            targets = DenseTest[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "            targets.to(device)\n",
        "            output1 = s11(inputs)\n",
        "            output2 = s22(inputs)\n",
        "            output3 = s33(inputs)\n",
        "            output4 = s44(inputs)\n",
        "            loss1 = criterion(output1, targets[:,:128])\n",
        "            loss2 = criterion(output2, targets[:,128:256])\n",
        "            loss3 = criterion(output3, targets[:,256:384])\n",
        "            loss4 = criterion(output4, targets[:,384:512])\n",
        "            test_loss1 += loss1.item()\n",
        "            test_loss2 += loss2.item()\n",
        "            test_loss3 += loss3.item()\n",
        "            test_loss4 += loss4.item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\" Loss S11: \", test_loss1/(batch_idx+1))\n",
        "              print(\" Loss S22: \", test_loss2/(batch_idx+1))\n",
        "              print(\" Loss S33: \", test_loss3/(batch_idx+1))\n",
        "              print(\" Loss S44: \", test_loss4/(batch_idx+1))"
      ],
      "metadata": {
        "id": "e_zhZ-fv0vse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+60):\n",
        "    train4(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test4(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5b7RheK0x0Y",
        "outputId": "c66553e0-29ef-460a-ddf5-b2522be56358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Loss S44:  0.09872736834540628\n",
            "Loss S11:  0.08327140232020078\n",
            "Loss S22:  0.08650474554374582\n",
            "Loss S33:  0.09420220198753838\n",
            "Loss S44:  0.09863210318615875\n",
            "Loss S11:  0.08315601483482608\n",
            "Loss S22:  0.0863831467318751\n",
            "Loss S33:  0.09411942607686599\n",
            "Loss S44:  0.09852926975560693\n",
            "Loss S11:  0.08312696535304145\n",
            "Loss S22:  0.08648001603629232\n",
            "Loss S33:  0.09419581481962959\n",
            "Loss S44:  0.09860280977665854\n",
            "Loss S11:  0.08309912161524818\n",
            "Loss S22:  0.08644381933911913\n",
            "Loss S33:  0.09417948485524566\n",
            "Loss S44:  0.0986110211437584\n",
            "Loss S11:  0.08312344404551461\n",
            "Loss S22:  0.08639878404024895\n",
            "Loss S33:  0.09421892049322499\n",
            "Loss S44:  0.09853602574307503\n",
            "Loss S11:  0.08313511567054734\n",
            "Loss S22:  0.08643486001340527\n",
            "Loss S33:  0.09413647282155697\n",
            "Loss S44:  0.09848206199083046\n",
            "Loss S11:  0.08308299589814164\n",
            "Loss S22:  0.08635803358996008\n",
            "Loss S33:  0.09404984740369277\n",
            "Loss S44:  0.09839742454722172\n",
            "Loss S11:  0.08298111748893547\n",
            "Loss S22:  0.08628319584957474\n",
            "Loss S33:  0.09397917515252863\n",
            "Loss S44:  0.09830918675645843\n",
            "Loss S11:  0.08305845314873722\n",
            "Loss S22:  0.08628715492989357\n",
            "Loss S33:  0.09403009926366093\n",
            "Loss S44:  0.0983904742280444\n",
            "Loss S11:  0.08306109988631413\n",
            "Loss S22:  0.08632693490480274\n",
            "Loss S33:  0.09407266193135232\n",
            "Loss S44:  0.09833871243281377\n",
            "Loss S11:  0.08303658266036358\n",
            "Loss S22:  0.08638121364660331\n",
            "Loss S33:  0.09420089988201644\n",
            "Loss S44:  0.09837887710460291\n",
            "Loss S11:  0.0830079153906165\n",
            "Loss S22:  0.08636378325746673\n",
            "Loss S33:  0.09419263223415185\n",
            "Loss S44:  0.09830580855743515\n",
            "Loss S11:  0.083032822527853\n",
            "Loss S22:  0.08636279380510724\n",
            "Loss S33:  0.09427474393528334\n",
            "Loss S44:  0.09834820773798862\n",
            "Loss S11:  0.08309222752860275\n",
            "Loss S22:  0.0864142982037263\n",
            "Loss S33:  0.09434178134554505\n",
            "Loss S44:  0.09841507266868245\n",
            "Loss S11:  0.08312836693450323\n",
            "Loss S22:  0.08639069714476384\n",
            "Loss S33:  0.09435885971707016\n",
            "Loss S44:  0.09846536774762023\n",
            "Loss S11:  0.08308951080541449\n",
            "Loss S22:  0.08639534559662428\n",
            "Loss S33:  0.09433842202027638\n",
            "Loss S44:  0.0984141172845146\n",
            "Loss S11:  0.08309710887018708\n",
            "Loss S22:  0.08637210128832756\n",
            "Loss S33:  0.09432823601296934\n",
            "Loss S44:  0.09839673690389447\n",
            "Loss S11:  0.08305372634633253\n",
            "Loss S22:  0.08634848672661422\n",
            "Loss S33:  0.09427386725628692\n",
            "Loss S44:  0.09832777639816832\n",
            "Validation: \n",
            " Loss S11:  0.09996721893548965\n",
            " Loss S22:  0.1158662959933281\n",
            " Loss S33:  0.1205541267991066\n",
            " Loss S44:  0.13021503388881683\n",
            " Loss S11:  0.09731832217602503\n",
            " Loss S22:  0.12416979067382358\n",
            " Loss S33:  0.12852527378570466\n",
            " Loss S44:  0.137763255408832\n",
            " Loss S11:  0.09515876359328991\n",
            " Loss S22:  0.1236446020079822\n",
            " Loss S33:  0.12850833302590906\n",
            " Loss S44:  0.13711167126893997\n",
            " Loss S11:  0.09454490403171445\n",
            " Loss S22:  0.12238224925564938\n",
            " Loss S33:  0.12643731226686572\n",
            " Loss S44:  0.13589444109162346\n",
            " Loss S11:  0.0939318013411981\n",
            " Loss S22:  0.12205337806616301\n",
            " Loss S33:  0.12534530543618733\n",
            " Loss S44:  0.13489981952272814\n",
            "\n",
            "Epoch: 39\n",
            "Loss S11:  0.08374828100204468\n",
            "Loss S22:  0.08575395494699478\n",
            "Loss S33:  0.08895615488290787\n",
            "Loss S44:  0.09859953075647354\n",
            "Loss S11:  0.0856878777796572\n",
            "Loss S22:  0.08863888545469804\n",
            "Loss S33:  0.0983743125742132\n",
            "Loss S44:  0.10052207044579765\n",
            "Loss S11:  0.08469462323756445\n",
            "Loss S22:  0.08624815444151561\n",
            "Loss S33:  0.0944574290797824\n",
            "Loss S44:  0.10007585797991071\n",
            "Loss S11:  0.08294678815910893\n",
            "Loss S22:  0.08579090982675552\n",
            "Loss S33:  0.09305224880095452\n",
            "Loss S44:  0.0981053416767428\n",
            "Loss S11:  0.08223787513447971\n",
            "Loss S22:  0.08562596105947727\n",
            "Loss S33:  0.0932142312933759\n",
            "Loss S44:  0.09789956597293295\n",
            "Loss S11:  0.08260413960498922\n",
            "Loss S22:  0.08581596335359648\n",
            "Loss S33:  0.09324188384355284\n",
            "Loss S44:  0.09704282499995887\n",
            "Loss S11:  0.08250839128846028\n",
            "Loss S22:  0.08552664640496989\n",
            "Loss S33:  0.09372399490876276\n",
            "Loss S44:  0.09718089282024102\n",
            "Loss S11:  0.08204771483867941\n",
            "Loss S22:  0.08519011557521955\n",
            "Loss S33:  0.09351013758232896\n",
            "Loss S44:  0.09723297013363368\n",
            "Loss S11:  0.08213312160453679\n",
            "Loss S22:  0.08520432929565877\n",
            "Loss S33:  0.09335473014248742\n",
            "Loss S44:  0.09730891616624078\n",
            "Loss S11:  0.08215945770779809\n",
            "Loss S22:  0.08542342418497735\n",
            "Loss S33:  0.09373054325908095\n",
            "Loss S44:  0.0976234685097422\n",
            "Loss S11:  0.08174282489436688\n",
            "Loss S22:  0.08522377008258707\n",
            "Loss S33:  0.09330839848164285\n",
            "Loss S44:  0.09716248733572441\n",
            "Loss S11:  0.0815596847652315\n",
            "Loss S22:  0.08526966016034822\n",
            "Loss S33:  0.09315270547931259\n",
            "Loss S44:  0.09733106578524048\n",
            "Loss S11:  0.08145158633220294\n",
            "Loss S22:  0.0852175957161533\n",
            "Loss S33:  0.09322664431057685\n",
            "Loss S44:  0.09711649158030503\n",
            "Loss S11:  0.08179661881832676\n",
            "Loss S22:  0.08568523534154164\n",
            "Loss S33:  0.09356026038641238\n",
            "Loss S44:  0.0972858972790587\n",
            "Loss S11:  0.08167332659165065\n",
            "Loss S22:  0.0857224887144481\n",
            "Loss S33:  0.09353436001226412\n",
            "Loss S44:  0.09728124024386102\n",
            "Loss S11:  0.08195986722084071\n",
            "Loss S22:  0.08594879626438318\n",
            "Loss S33:  0.09377877996457333\n",
            "Loss S44:  0.09752321332101001\n",
            "Loss S11:  0.08184937114671151\n",
            "Loss S22:  0.08554841545612915\n",
            "Loss S33:  0.0936314647334703\n",
            "Loss S44:  0.09748731049136346\n",
            "Loss S11:  0.08171522965905262\n",
            "Loss S22:  0.08547505339858127\n",
            "Loss S33:  0.09365301823232607\n",
            "Loss S44:  0.09745494947273131\n",
            "Loss S11:  0.08175475722518415\n",
            "Loss S22:  0.08544591069221497\n",
            "Loss S33:  0.09369828992456362\n",
            "Loss S44:  0.09750431730766981\n",
            "Loss S11:  0.08169650500981596\n",
            "Loss S22:  0.08527419091987361\n",
            "Loss S33:  0.09347686128622575\n",
            "Loss S44:  0.09730952888415122\n",
            "Loss S11:  0.0814873255529807\n",
            "Loss S22:  0.08532694675287797\n",
            "Loss S33:  0.09340383620256215\n",
            "Loss S44:  0.0973148238080651\n",
            "Loss S11:  0.08151531261855392\n",
            "Loss S22:  0.08513268941386616\n",
            "Loss S33:  0.0932457269085527\n",
            "Loss S44:  0.09735778535570579\n",
            "Loss S11:  0.08178949190885233\n",
            "Loss S22:  0.08513058904069581\n",
            "Loss S33:  0.09333892658824834\n",
            "Loss S44:  0.09753604880555183\n",
            "Loss S11:  0.08184326278827923\n",
            "Loss S22:  0.0853077271929035\n",
            "Loss S33:  0.09332358950015271\n",
            "Loss S44:  0.09753242331794847\n",
            "Loss S11:  0.08194656806120734\n",
            "Loss S22:  0.08548214880139006\n",
            "Loss S33:  0.09354676847264974\n",
            "Loss S44:  0.09760744436895204\n",
            "Loss S11:  0.08188957551798498\n",
            "Loss S22:  0.08525049330228829\n",
            "Loss S33:  0.09357323737970862\n",
            "Loss S44:  0.09755985518969387\n",
            "Loss S11:  0.08183715986337699\n",
            "Loss S22:  0.08525095925705643\n",
            "Loss S33:  0.0934719012843238\n",
            "Loss S44:  0.09766294073556119\n",
            "Loss S11:  0.08192348087075892\n",
            "Loss S22:  0.08532103651454088\n",
            "Loss S33:  0.0935633081351699\n",
            "Loss S44:  0.09778766622732486\n",
            "Loss S11:  0.08203299123515438\n",
            "Loss S22:  0.08537526190068798\n",
            "Loss S33:  0.09366290329614144\n",
            "Loss S44:  0.09776086630664262\n",
            "Loss S11:  0.08204090590124688\n",
            "Loss S22:  0.08540729930003484\n",
            "Loss S33:  0.09370050218301951\n",
            "Loss S44:  0.09778641540672361\n",
            "Loss S11:  0.08204464173792209\n",
            "Loss S22:  0.08542571783659862\n",
            "Loss S33:  0.09366784249726325\n",
            "Loss S44:  0.09783618533334067\n",
            "Loss S11:  0.08196128732330163\n",
            "Loss S22:  0.08536816211087911\n",
            "Loss S33:  0.09364125228388133\n",
            "Loss S44:  0.09785373168742925\n",
            "Loss S11:  0.08195836389454726\n",
            "Loss S22:  0.08533340929267562\n",
            "Loss S33:  0.09370172186246914\n",
            "Loss S44:  0.09782268798611246\n",
            "Loss S11:  0.08191770321923081\n",
            "Loss S22:  0.08519545294276898\n",
            "Loss S33:  0.09361417704118342\n",
            "Loss S44:  0.09776111142963081\n",
            "Loss S11:  0.08196382255148678\n",
            "Loss S22:  0.08530246588753815\n",
            "Loss S33:  0.09358020153038663\n",
            "Loss S44:  0.09787931607877753\n",
            "Loss S11:  0.08195406992381114\n",
            "Loss S22:  0.08532367275226489\n",
            "Loss S33:  0.09357452148470784\n",
            "Loss S44:  0.09789891827564973\n",
            "Loss S11:  0.08195108773800805\n",
            "Loss S22:  0.08532431808369972\n",
            "Loss S33:  0.09362510160395973\n",
            "Loss S44:  0.09787302340414386\n",
            "Loss S11:  0.08185994102948438\n",
            "Loss S22:  0.08528858493521528\n",
            "Loss S33:  0.09346844808190981\n",
            "Loss S44:  0.09779840089240165\n",
            "Loss S11:  0.08178126254654307\n",
            "Loss S22:  0.08524907120256599\n",
            "Loss S33:  0.09338217774244743\n",
            "Loss S44:  0.09768270243575254\n",
            "Loss S11:  0.08170539626608724\n",
            "Loss S22:  0.08512082097627928\n",
            "Loss S33:  0.0933183231736388\n",
            "Loss S44:  0.09758506673376274\n",
            "Loss S11:  0.08173947340978352\n",
            "Loss S22:  0.0851268621528535\n",
            "Loss S33:  0.09336862436256504\n",
            "Loss S44:  0.0976365689847832\n",
            "Loss S11:  0.0817915492321743\n",
            "Loss S22:  0.08513553975344865\n",
            "Loss S33:  0.0933800192499973\n",
            "Loss S44:  0.09764417942495532\n",
            "Loss S11:  0.08182582434381838\n",
            "Loss S22:  0.08523794472429928\n",
            "Loss S33:  0.09339293241146908\n",
            "Loss S44:  0.09768908285046983\n",
            "Loss S11:  0.08189973306005903\n",
            "Loss S22:  0.08524635527499037\n",
            "Loss S33:  0.09346333683712577\n",
            "Loss S44:  0.09764060728995264\n",
            "Loss S11:  0.08188308025298476\n",
            "Loss S22:  0.08527471971552388\n",
            "Loss S33:  0.09348060999935717\n",
            "Loss S44:  0.09760973050075324\n",
            "Loss S11:  0.0819740136651665\n",
            "Loss S22:  0.08538133104201695\n",
            "Loss S33:  0.0935019373794617\n",
            "Loss S44:  0.09769796383486619\n",
            "Loss S11:  0.08194856032115001\n",
            "Loss S22:  0.08540259120650509\n",
            "Loss S33:  0.09346746818168561\n",
            "Loss S44:  0.09772705456439947\n",
            "Loss S11:  0.08191537594466199\n",
            "Loss S22:  0.08530081330844044\n",
            "Loss S33:  0.09339526529777835\n",
            "Loss S44:  0.09766464313440769\n",
            "Loss S11:  0.08191386466214662\n",
            "Loss S22:  0.0852722493222994\n",
            "Loss S33:  0.09332074715974673\n",
            "Loss S44:  0.09770760933041821\n",
            "Loss S11:  0.08192974228844381\n",
            "Loss S22:  0.08530636356343077\n",
            "Loss S33:  0.09338864108999004\n",
            "Loss S44:  0.09771425257268117\n",
            "Validation: \n",
            " Loss S11:  0.0975097268819809\n",
            " Loss S22:  0.11459515988826752\n",
            " Loss S33:  0.1276855170726776\n",
            " Loss S44:  0.13083001971244812\n",
            " Loss S11:  0.09381341153667086\n",
            " Loss S22:  0.12527100316115788\n",
            " Loss S33:  0.13080302519457682\n",
            " Loss S44:  0.13855221832082384\n",
            " Loss S11:  0.09216935187578201\n",
            " Loss S22:  0.12395944246431677\n",
            " Loss S33:  0.1306224005614839\n",
            " Loss S44:  0.1371363236046419\n",
            " Loss S11:  0.09130324081319277\n",
            " Loss S22:  0.12302033908543039\n",
            " Loss S33:  0.12836582494563745\n",
            " Loss S44:  0.13550523863952668\n",
            " Loss S11:  0.09086743429487135\n",
            " Loss S22:  0.12288459969891442\n",
            " Loss S33:  0.12734015755079411\n",
            " Loss S44:  0.1344464922576775\n",
            "\n",
            "Epoch: 40\n",
            "Loss S11:  0.0839787945151329\n",
            "Loss S22:  0.0737830176949501\n",
            "Loss S33:  0.0911407545208931\n",
            "Loss S44:  0.08874166011810303\n",
            "Loss S11:  0.08254630931399086\n",
            "Loss S22:  0.08672678402879021\n",
            "Loss S33:  0.09627934138883244\n",
            "Loss S44:  0.09473568065599962\n",
            "Loss S11:  0.08358891521181379\n",
            "Loss S22:  0.0867396021882693\n",
            "Loss S33:  0.09478658118418284\n",
            "Loss S44:  0.09649228411061424\n",
            "Loss S11:  0.08315381720181435\n",
            "Loss S22:  0.08523707308115498\n",
            "Loss S33:  0.09384705582934041\n",
            "Loss S44:  0.09547359977037675\n",
            "Loss S11:  0.08239949912559695\n",
            "Loss S22:  0.08564246518582833\n",
            "Loss S33:  0.09395162488629179\n",
            "Loss S44:  0.09551138630727442\n",
            "Loss S11:  0.08244919543172799\n",
            "Loss S22:  0.08519513656695683\n",
            "Loss S33:  0.09321604449959363\n",
            "Loss S44:  0.09500151039922938\n",
            "Loss S11:  0.08261765796141546\n",
            "Loss S22:  0.08539074903628865\n",
            "Loss S33:  0.09271549323543174\n",
            "Loss S44:  0.09548009272481574\n",
            "Loss S11:  0.08210598121226674\n",
            "Loss S22:  0.0850496359274421\n",
            "Loss S33:  0.09244721981001572\n",
            "Loss S44:  0.09552576949059123\n",
            "Loss S11:  0.08198750764131546\n",
            "Loss S22:  0.0849393234576708\n",
            "Loss S33:  0.09245638567724346\n",
            "Loss S44:  0.0958078251576718\n",
            "Loss S11:  0.08201150375080633\n",
            "Loss S22:  0.08479557972360444\n",
            "Loss S33:  0.0928152770950244\n",
            "Loss S44:  0.09582147321530751\n",
            "Loss S11:  0.08154877083431376\n",
            "Loss S22:  0.08436663087346766\n",
            "Loss S33:  0.09217925059913408\n",
            "Loss S44:  0.09529758871781945\n",
            "Loss S11:  0.08131523249117104\n",
            "Loss S22:  0.08396916883485811\n",
            "Loss S33:  0.09191898410921698\n",
            "Loss S44:  0.09510281198733561\n",
            "Loss S11:  0.08121342624514556\n",
            "Loss S22:  0.08381276392985966\n",
            "Loss S33:  0.09186564952381386\n",
            "Loss S44:  0.09510474202554088\n",
            "Loss S11:  0.08128110404687984\n",
            "Loss S22:  0.08401037394317962\n",
            "Loss S33:  0.09213361405689298\n",
            "Loss S44:  0.09529035606684576\n",
            "Loss S11:  0.0812618319235795\n",
            "Loss S22:  0.08442649609865026\n",
            "Loss S33:  0.09227465410181816\n",
            "Loss S44:  0.09549382064782136\n",
            "Loss S11:  0.08136297071611645\n",
            "Loss S22:  0.08441162681737482\n",
            "Loss S33:  0.0924581168897894\n",
            "Loss S44:  0.0957503905458166\n",
            "Loss S11:  0.08116994145679178\n",
            "Loss S22:  0.08418946079215649\n",
            "Loss S33:  0.09242055722460243\n",
            "Loss S44:  0.09562731233442792\n",
            "Loss S11:  0.08092895280896571\n",
            "Loss S22:  0.08394000470115427\n",
            "Loss S33:  0.09233214200762978\n",
            "Loss S44:  0.09555574083885951\n",
            "Loss S11:  0.08096231252613648\n",
            "Loss S22:  0.08400731978330823\n",
            "Loss S33:  0.09238651197259598\n",
            "Loss S44:  0.09559104134693988\n",
            "Loss S11:  0.0810687565756718\n",
            "Loss S22:  0.083925292832065\n",
            "Loss S33:  0.0923146428394068\n",
            "Loss S44:  0.09567662987721529\n",
            "Loss S11:  0.08098186908373192\n",
            "Loss S22:  0.08393433849461636\n",
            "Loss S33:  0.09214858206646953\n",
            "Loss S44:  0.09569665367033944\n",
            "Loss S11:  0.08100668189084925\n",
            "Loss S22:  0.08395721170150838\n",
            "Loss S33:  0.09224846002191164\n",
            "Loss S44:  0.09579837170398631\n",
            "Loss S11:  0.08118943189064302\n",
            "Loss S22:  0.08414563007349342\n",
            "Loss S33:  0.0924291100302433\n",
            "Loss S44:  0.09596688404881576\n",
            "Loss S11:  0.08135389207632511\n",
            "Loss S22:  0.0842160177114722\n",
            "Loss S33:  0.09257642405502724\n",
            "Loss S44:  0.09619063190567545\n",
            "Loss S11:  0.08148712896707147\n",
            "Loss S22:  0.0843521197059837\n",
            "Loss S33:  0.09266801286658806\n",
            "Loss S44:  0.09650077324561558\n",
            "Loss S11:  0.08150543812736571\n",
            "Loss S22:  0.08443970446923814\n",
            "Loss S33:  0.09268919657544786\n",
            "Loss S44:  0.09653292498384339\n",
            "Loss S11:  0.08149894050026306\n",
            "Loss S22:  0.08448338345892129\n",
            "Loss S33:  0.09265959593984816\n",
            "Loss S44:  0.0965443029999733\n",
            "Loss S11:  0.08163458535042196\n",
            "Loss S22:  0.08472323676096997\n",
            "Loss S33:  0.09271870534248458\n",
            "Loss S44:  0.096665689316183\n",
            "Loss S11:  0.08177817037521308\n",
            "Loss S22:  0.08493038041523768\n",
            "Loss S33:  0.09296678356640704\n",
            "Loss S44:  0.09667803514152234\n",
            "Loss S11:  0.0818121006449883\n",
            "Loss S22:  0.08509677190243993\n",
            "Loss S33:  0.09307916936083757\n",
            "Loss S44:  0.0967191196338008\n",
            "Loss S11:  0.08183430315945235\n",
            "Loss S22:  0.08511883978332792\n",
            "Loss S33:  0.093090477724408\n",
            "Loss S44:  0.09676968994429737\n",
            "Loss S11:  0.08177070022587607\n",
            "Loss S22:  0.08500115508722722\n",
            "Loss S33:  0.09303131601338985\n",
            "Loss S44:  0.09676775652495995\n",
            "Loss S11:  0.08174692022373371\n",
            "Loss S22:  0.08496601276122892\n",
            "Loss S33:  0.09304577741203278\n",
            "Loss S44:  0.09664163267315362\n",
            "Loss S11:  0.08172622314032471\n",
            "Loss S22:  0.08481530295938164\n",
            "Loss S33:  0.09286449453441759\n",
            "Loss S44:  0.09653149862573947\n",
            "Loss S11:  0.0818150879656814\n",
            "Loss S22:  0.0848763825868931\n",
            "Loss S33:  0.09294771407444107\n",
            "Loss S44:  0.09659660799849418\n",
            "Loss S11:  0.08188066561507364\n",
            "Loss S22:  0.08485006672619415\n",
            "Loss S33:  0.09303912652014328\n",
            "Loss S44:  0.09673483480839988\n",
            "Loss S11:  0.08184232114919995\n",
            "Loss S22:  0.08478327658945833\n",
            "Loss S33:  0.09299871595860187\n",
            "Loss S44:  0.09670444934982342\n",
            "Loss S11:  0.08181142760817253\n",
            "Loss S22:  0.08477307048769331\n",
            "Loss S33:  0.09296821714491857\n",
            "Loss S44:  0.0967093254276363\n",
            "Loss S11:  0.0816880944050516\n",
            "Loss S22:  0.08470762282453496\n",
            "Loss S33:  0.09291523683258868\n",
            "Loss S44:  0.096633777511245\n",
            "Loss S11:  0.08166213097322322\n",
            "Loss S22:  0.08462598256747741\n",
            "Loss S33:  0.09289460612075104\n",
            "Loss S44:  0.09656959882630106\n",
            "Loss S11:  0.08178987731214177\n",
            "Loss S22:  0.0846473778275183\n",
            "Loss S33:  0.09288803984102169\n",
            "Loss S44:  0.09664799622318096\n",
            "Loss S11:  0.08176218362982836\n",
            "Loss S22:  0.08461572726567586\n",
            "Loss S33:  0.09287412428362816\n",
            "Loss S44:  0.09667200410235537\n",
            "Loss S11:  0.08180713588084292\n",
            "Loss S22:  0.08461674173625801\n",
            "Loss S33:  0.09288557172883434\n",
            "Loss S44:  0.09678010446088048\n",
            "Loss S11:  0.08177513660314188\n",
            "Loss S22:  0.0846314096084327\n",
            "Loss S33:  0.0928829419847984\n",
            "Loss S44:  0.09670678460473528\n",
            "Loss S11:  0.08176721865628041\n",
            "Loss S22:  0.08463296336449193\n",
            "Loss S33:  0.0928516501134215\n",
            "Loss S44:  0.09676097230373326\n",
            "Loss S11:  0.0817366894623657\n",
            "Loss S22:  0.08471507063660019\n",
            "Loss S33:  0.0927846751561979\n",
            "Loss S44:  0.09677506086236887\n",
            "Loss S11:  0.08179776431940124\n",
            "Loss S22:  0.08477901958268531\n",
            "Loss S33:  0.0928592040423716\n",
            "Loss S44:  0.09682008798624067\n",
            "Loss S11:  0.08181213684016225\n",
            "Loss S22:  0.08477575482203449\n",
            "Loss S33:  0.09282600316525517\n",
            "Loss S44:  0.09679366746272758\n",
            "Loss S11:  0.08179050693269083\n",
            "Loss S22:  0.0847487929364252\n",
            "Loss S33:  0.09281424118550552\n",
            "Loss S44:  0.09680831195349009\n",
            "Loss S11:  0.08171106142024159\n",
            "Loss S22:  0.0847283985670618\n",
            "Loss S33:  0.09277405887235206\n",
            "Loss S44:  0.09681964218009513\n",
            "Validation: \n",
            " Loss S11:  0.09348049014806747\n",
            " Loss S22:  0.11592895537614822\n",
            " Loss S33:  0.12097669392824173\n",
            " Loss S44:  0.12868010997772217\n",
            " Loss S11:  0.09128349593707494\n",
            " Loss S22:  0.12617410648436772\n",
            " Loss S33:  0.1266257560678891\n",
            " Loss S44:  0.13612638662258783\n",
            " Loss S11:  0.08938066334259219\n",
            " Loss S22:  0.12537124916547682\n",
            " Loss S33:  0.1268850679441196\n",
            " Loss S44:  0.13444366197033628\n",
            " Loss S11:  0.08853206294970434\n",
            " Loss S22:  0.12455903506669842\n",
            " Loss S33:  0.12464312757136392\n",
            " Loss S44:  0.1331726153854464\n",
            " Loss S11:  0.08785419506423267\n",
            " Loss S22:  0.12440988715784049\n",
            " Loss S33:  0.12359537340608644\n",
            " Loss S44:  0.1321133808405311\n",
            "\n",
            "Epoch: 41\n",
            "Loss S11:  0.08040742576122284\n",
            "Loss S22:  0.08836407214403152\n",
            "Loss S33:  0.08928579092025757\n",
            "Loss S44:  0.10012544691562653\n",
            "Loss S11:  0.08367086811499162\n",
            "Loss S22:  0.08596553450280969\n",
            "Loss S33:  0.09558365358547731\n",
            "Loss S44:  0.09860291061076251\n",
            "Loss S11:  0.08318266769250234\n",
            "Loss S22:  0.08589322581177666\n",
            "Loss S33:  0.0943183618642035\n",
            "Loss S44:  0.09656009078025818\n",
            "Loss S11:  0.08205855373413332\n",
            "Loss S22:  0.08431380698757787\n",
            "Loss S33:  0.09331193350015148\n",
            "Loss S44:  0.09569994768788738\n",
            "Loss S11:  0.08160243328751587\n",
            "Loss S22:  0.0848497014220168\n",
            "Loss S33:  0.09342975950822598\n",
            "Loss S44:  0.09568533792001445\n",
            "Loss S11:  0.08203759541114171\n",
            "Loss S22:  0.08511705053787605\n",
            "Loss S33:  0.09265154074220096\n",
            "Loss S44:  0.09560530238291796\n",
            "Loss S11:  0.08215471467033761\n",
            "Loss S22:  0.08474841494052136\n",
            "Loss S33:  0.09267830054779522\n",
            "Loss S44:  0.09626591181168791\n",
            "Loss S11:  0.08174027250686162\n",
            "Loss S22:  0.08446450898764839\n",
            "Loss S33:  0.0922231934439968\n",
            "Loss S44:  0.09632679347840833\n",
            "Loss S11:  0.08192043190385088\n",
            "Loss S22:  0.08443787096091258\n",
            "Loss S33:  0.09238262152598228\n",
            "Loss S44:  0.09642912226694601\n",
            "Loss S11:  0.08186713687993669\n",
            "Loss S22:  0.08448618863310133\n",
            "Loss S33:  0.09270580710618051\n",
            "Loss S44:  0.09653029762781583\n",
            "Loss S11:  0.08144605344179834\n",
            "Loss S22:  0.0844196475083285\n",
            "Loss S33:  0.09214669067670803\n",
            "Loss S44:  0.09605644274466109\n",
            "Loss S11:  0.08132061449525592\n",
            "Loss S22:  0.08405344973544816\n",
            "Loss S33:  0.09219111864631241\n",
            "Loss S44:  0.09596781674269084\n",
            "Loss S11:  0.08085798053455746\n",
            "Loss S22:  0.0837595177706608\n",
            "Loss S33:  0.0917077371773641\n",
            "Loss S44:  0.09545350234863187\n",
            "Loss S11:  0.08087535623375697\n",
            "Loss S22:  0.08408426908591321\n",
            "Loss S33:  0.0920151143356134\n",
            "Loss S44:  0.09555786279321626\n",
            "Loss S11:  0.08087571810746023\n",
            "Loss S22:  0.08425857544156677\n",
            "Loss S33:  0.09195532084356808\n",
            "Loss S44:  0.09542865052502206\n",
            "Loss S11:  0.08106426287763166\n",
            "Loss S22:  0.0844945130462678\n",
            "Loss S33:  0.0921323432630261\n",
            "Loss S44:  0.09575614870976139\n",
            "Loss S11:  0.0808315436292139\n",
            "Loss S22:  0.08423102767386051\n",
            "Loss S33:  0.09193304176471248\n",
            "Loss S44:  0.09551637231007866\n",
            "Loss S11:  0.08060482022357963\n",
            "Loss S22:  0.08403451270178745\n",
            "Loss S33:  0.09190534116231908\n",
            "Loss S44:  0.09555570049244061\n",
            "Loss S11:  0.08057717394433628\n",
            "Loss S22:  0.08399697286154025\n",
            "Loss S33:  0.0918214159775834\n",
            "Loss S44:  0.09561817903709675\n",
            "Loss S11:  0.08069353542870876\n",
            "Loss S22:  0.08395294259980086\n",
            "Loss S33:  0.09182300601954235\n",
            "Loss S44:  0.09552088040487929\n",
            "Loss S11:  0.08057488860627313\n",
            "Loss S22:  0.08392950777538974\n",
            "Loss S33:  0.09184892238372594\n",
            "Loss S44:  0.09544729611915143\n",
            "Loss S11:  0.08069934713614496\n",
            "Loss S22:  0.08405724019518396\n",
            "Loss S33:  0.09185970722922782\n",
            "Loss S44:  0.09543763574265755\n",
            "Loss S11:  0.08086614128691039\n",
            "Loss S22:  0.0841309388096516\n",
            "Loss S33:  0.09194945254072345\n",
            "Loss S44:  0.09564823775269866\n",
            "Loss S11:  0.08096928323631163\n",
            "Loss S22:  0.08418012129796015\n",
            "Loss S33:  0.09195214755091316\n",
            "Loss S44:  0.09573945148295654\n",
            "Loss S11:  0.08102930040404015\n",
            "Loss S22:  0.08425699938011368\n",
            "Loss S33:  0.09213075885757865\n",
            "Loss S44:  0.09593807831715746\n",
            "Loss S11:  0.08113078520711199\n",
            "Loss S22:  0.08420949821096968\n",
            "Loss S33:  0.09209181736427474\n",
            "Loss S44:  0.09582160110492631\n",
            "Loss S11:  0.08121392821671862\n",
            "Loss S22:  0.08432513549638435\n",
            "Loss S33:  0.09215698750882313\n",
            "Loss S44:  0.09590431822328276\n",
            "Loss S11:  0.08122361333167862\n",
            "Loss S22:  0.08449302707211118\n",
            "Loss S33:  0.09220030513416796\n",
            "Loss S44:  0.09590772784064176\n",
            "Loss S11:  0.08126809177555648\n",
            "Loss S22:  0.08459948297603274\n",
            "Loss S33:  0.09219493266208316\n",
            "Loss S44:  0.09599935451648413\n",
            "Loss S11:  0.0812300413181282\n",
            "Loss S22:  0.08462543629391496\n",
            "Loss S33:  0.09219562410796221\n",
            "Loss S44:  0.09605675802607716\n",
            "Loss S11:  0.08118880973305813\n",
            "Loss S22:  0.08460264707423523\n",
            "Loss S33:  0.0922474990849479\n",
            "Loss S44:  0.09618652587218142\n",
            "Loss S11:  0.08108199713601943\n",
            "Loss S22:  0.08455636512811544\n",
            "Loss S33:  0.0922459638003751\n",
            "Loss S44:  0.0962173842205112\n",
            "Loss S11:  0.08112391913048575\n",
            "Loss S22:  0.08458525808150895\n",
            "Loss S33:  0.09240177345702953\n",
            "Loss S44:  0.09626304339675518\n",
            "Loss S11:  0.08104407260605216\n",
            "Loss S22:  0.08453206152984383\n",
            "Loss S33:  0.09228194207853421\n",
            "Loss S44:  0.09617747339417207\n",
            "Loss S11:  0.08102051898443803\n",
            "Loss S22:  0.08455958728653944\n",
            "Loss S33:  0.09236943492354535\n",
            "Loss S44:  0.09618632965161304\n",
            "Loss S11:  0.08098639991463419\n",
            "Loss S22:  0.08455764684836749\n",
            "Loss S33:  0.09246107944396147\n",
            "Loss S44:  0.09617382984215718\n",
            "Loss S11:  0.08093068697115721\n",
            "Loss S22:  0.0845473373613199\n",
            "Loss S33:  0.09247103023892295\n",
            "Loss S44:  0.09624517657419981\n",
            "Loss S11:  0.08089079513061401\n",
            "Loss S22:  0.08449063873837258\n",
            "Loss S33:  0.09238862083607285\n",
            "Loss S44:  0.09618700727458913\n",
            "Loss S11:  0.08085491250115116\n",
            "Loss S22:  0.08441972752099275\n",
            "Loss S33:  0.09231447094068752\n",
            "Loss S44:  0.09614460389288705\n",
            "Loss S11:  0.0807850862593602\n",
            "Loss S22:  0.08431304465321933\n",
            "Loss S33:  0.09225103504898603\n",
            "Loss S44:  0.09605520230043879\n",
            "Loss S11:  0.08081898678493618\n",
            "Loss S22:  0.08433828268785429\n",
            "Loss S33:  0.09225788463529506\n",
            "Loss S44:  0.09602655533543251\n",
            "Loss S11:  0.08077544249467317\n",
            "Loss S22:  0.08438002577373291\n",
            "Loss S33:  0.09220965705141244\n",
            "Loss S44:  0.09593683773076157\n",
            "Loss S11:  0.08079260108094317\n",
            "Loss S22:  0.08439716549851266\n",
            "Loss S33:  0.09210448648400091\n",
            "Loss S44:  0.09602135421615315\n",
            "Loss S11:  0.08079351434644026\n",
            "Loss S22:  0.08433136378529453\n",
            "Loss S33:  0.09213685373972298\n",
            "Loss S44:  0.09590362696940827\n",
            "Loss S11:  0.08078424522733473\n",
            "Loss S22:  0.08431480091715617\n",
            "Loss S33:  0.09211484817555703\n",
            "Loss S44:  0.09590479875165048\n",
            "Loss S11:  0.08084905886002496\n",
            "Loss S22:  0.08434758019817377\n",
            "Loss S33:  0.09212263379485539\n",
            "Loss S44:  0.09589604323031897\n",
            "Loss S11:  0.08086078360256559\n",
            "Loss S22:  0.08439871827201058\n",
            "Loss S33:  0.09213911884195634\n",
            "Loss S44:  0.09595893488271634\n",
            "Loss S11:  0.08079535436604939\n",
            "Loss S22:  0.08439925509147077\n",
            "Loss S33:  0.09212159835229254\n",
            "Loss S44:  0.09585225147110135\n",
            "Loss S11:  0.08087339608498274\n",
            "Loss S22:  0.08442868682673964\n",
            "Loss S33:  0.0921315952144145\n",
            "Loss S44:  0.09588155313056125\n",
            "Loss S11:  0.08083735940534083\n",
            "Loss S22:  0.08438816631702202\n",
            "Loss S33:  0.09215355034758263\n",
            "Loss S44:  0.09583919883806205\n",
            "Validation: \n",
            " Loss S11:  0.09262029081583023\n",
            " Loss S22:  0.1133907213807106\n",
            " Loss S33:  0.12024389952421188\n",
            " Loss S44:  0.130904421210289\n",
            " Loss S11:  0.09095443714232672\n",
            " Loss S22:  0.12119198377643313\n",
            " Loss S33:  0.12933900420154845\n",
            " Loss S44:  0.13730739234458833\n",
            " Loss S11:  0.08918388278746023\n",
            " Loss S22:  0.12115120724207018\n",
            " Loss S33:  0.12936791305134937\n",
            " Loss S44:  0.13550874536357274\n",
            " Loss S11:  0.08813564980127772\n",
            " Loss S22:  0.12036970709679556\n",
            " Loss S33:  0.12698313190800245\n",
            " Loss S44:  0.13386048681911875\n",
            " Loss S11:  0.08734336798941647\n",
            " Loss S22:  0.11989560962459188\n",
            " Loss S33:  0.12587604809690406\n",
            " Loss S44:  0.13298198212811976\n",
            "\n",
            "Epoch: 42\n",
            "Loss S11:  0.0836455300450325\n",
            "Loss S22:  0.08567937463521957\n",
            "Loss S33:  0.09155110269784927\n",
            "Loss S44:  0.09969794750213623\n",
            "Loss S11:  0.08089912547306581\n",
            "Loss S22:  0.08415312455459074\n",
            "Loss S33:  0.09416504881598732\n",
            "Loss S44:  0.09673477844758467\n",
            "Loss S11:  0.08067083607117335\n",
            "Loss S22:  0.08518624625035695\n",
            "Loss S33:  0.09203041273923147\n",
            "Loss S44:  0.09634979459501448\n",
            "Loss S11:  0.08031169012669594\n",
            "Loss S22:  0.08377345411046859\n",
            "Loss S33:  0.09062524308120051\n",
            "Loss S44:  0.09504968024069263\n",
            "Loss S11:  0.08071812660228915\n",
            "Loss S22:  0.08362472620679111\n",
            "Loss S33:  0.09077862668328168\n",
            "Loss S44:  0.09483207998479284\n",
            "Loss S11:  0.08038643455388499\n",
            "Loss S22:  0.0839904239949058\n",
            "Loss S33:  0.09092461785265044\n",
            "Loss S44:  0.09426028252232309\n",
            "Loss S11:  0.08078557069672913\n",
            "Loss S22:  0.08402257253889178\n",
            "Loss S33:  0.0911926445169527\n",
            "Loss S44:  0.09494733700498206\n",
            "Loss S11:  0.08040782483950468\n",
            "Loss S22:  0.08390905445730182\n",
            "Loss S33:  0.0911031703294163\n",
            "Loss S44:  0.09518184533841173\n",
            "Loss S11:  0.08041873059155029\n",
            "Loss S22:  0.08381426371174094\n",
            "Loss S33:  0.09098150056821329\n",
            "Loss S44:  0.09541483094662796\n",
            "Loss S11:  0.0805547588637897\n",
            "Loss S22:  0.08395537996030115\n",
            "Loss S33:  0.09141851838800934\n",
            "Loss S44:  0.09592045147667874\n",
            "Loss S11:  0.08002624557454988\n",
            "Loss S22:  0.08398856480817983\n",
            "Loss S33:  0.09068736183171225\n",
            "Loss S44:  0.09554887651511938\n",
            "Loss S11:  0.07975057552795152\n",
            "Loss S22:  0.08358528008600613\n",
            "Loss S33:  0.0906465557647181\n",
            "Loss S44:  0.09564587171818759\n",
            "Loss S11:  0.07985086061737755\n",
            "Loss S22:  0.08366124553621308\n",
            "Loss S33:  0.0908082348625522\n",
            "Loss S44:  0.09554533931342038\n",
            "Loss S11:  0.07999524673205295\n",
            "Loss S22:  0.08379904920135746\n",
            "Loss S33:  0.09100502774915623\n",
            "Loss S44:  0.09540488326367531\n",
            "Loss S11:  0.07992753729963979\n",
            "Loss S22:  0.08381468469792223\n",
            "Loss S33:  0.09115454047284227\n",
            "Loss S44:  0.09534711026130839\n",
            "Loss S11:  0.08008946882967917\n",
            "Loss S22:  0.08413088963126504\n",
            "Loss S33:  0.09110119396092876\n",
            "Loss S44:  0.09539858903119106\n",
            "Loss S11:  0.07989965448653476\n",
            "Loss S22:  0.08382241256673884\n",
            "Loss S33:  0.09076482705447984\n",
            "Loss S44:  0.09514113020452654\n",
            "Loss S11:  0.07971668918753227\n",
            "Loss S22:  0.08363790757823408\n",
            "Loss S33:  0.09053210726781198\n",
            "Loss S44:  0.09487370157625243\n",
            "Loss S11:  0.07968880199101748\n",
            "Loss S22:  0.0835729078363977\n",
            "Loss S33:  0.090567447245121\n",
            "Loss S44:  0.0949731264160483\n",
            "Loss S11:  0.07980596164914326\n",
            "Loss S22:  0.08356278280469136\n",
            "Loss S33:  0.09067101152467479\n",
            "Loss S44:  0.09497370235426887\n",
            "Loss S11:  0.07987796429970964\n",
            "Loss S22:  0.08366429053284043\n",
            "Loss S33:  0.09068648368861545\n",
            "Loss S44:  0.09517544570995208\n",
            "Loss S11:  0.07999740205528612\n",
            "Loss S22:  0.08378697359731412\n",
            "Loss S33:  0.09077299884145294\n",
            "Loss S44:  0.09522022691806911\n",
            "Loss S11:  0.08021993968821219\n",
            "Loss S22:  0.08386071939948457\n",
            "Loss S33:  0.09090734579983879\n",
            "Loss S44:  0.09545098713755068\n",
            "Loss S11:  0.08023226625604547\n",
            "Loss S22:  0.08384619930605867\n",
            "Loss S33:  0.0909709527895048\n",
            "Loss S44:  0.09546196789710552\n",
            "Loss S11:  0.08029257190796349\n",
            "Loss S22:  0.08398580684083132\n",
            "Loss S33:  0.09123082647556091\n",
            "Loss S44:  0.0956502164795191\n",
            "Loss S11:  0.08019399200658874\n",
            "Loss S22:  0.08397491620593812\n",
            "Loss S33:  0.09125684869598108\n",
            "Loss S44:  0.09561903255275521\n",
            "Loss S11:  0.08020561550997227\n",
            "Loss S22:  0.08390585527223646\n",
            "Loss S33:  0.09134593417589691\n",
            "Loss S44:  0.0955220357554169\n",
            "Loss S11:  0.08019549000967033\n",
            "Loss S22:  0.08391507484046296\n",
            "Loss S33:  0.09141620012117048\n",
            "Loss S44:  0.09554526897154171\n",
            "Loss S11:  0.08020399266926843\n",
            "Loss S22:  0.08386196432058497\n",
            "Loss S33:  0.09152318407314104\n",
            "Loss S44:  0.09547846626450583\n",
            "Loss S11:  0.08032316271586926\n",
            "Loss S22:  0.08399815516885613\n",
            "Loss S33:  0.0915971423444879\n",
            "Loss S44:  0.09549965768335611\n",
            "Loss S11:  0.08037465813251032\n",
            "Loss S22:  0.08402410340170528\n",
            "Loss S33:  0.09177991646369826\n",
            "Loss S44:  0.09568030018347046\n",
            "Loss S11:  0.0803124034280179\n",
            "Loss S22:  0.08390214481058611\n",
            "Loss S33:  0.09168159837124815\n",
            "Loss S44:  0.09557390380710651\n",
            "Loss S11:  0.08040904583310785\n",
            "Loss S22:  0.0839619461296132\n",
            "Loss S33:  0.09176466286739457\n",
            "Loss S44:  0.09548256736585284\n",
            "Loss S11:  0.0803289249809487\n",
            "Loss S22:  0.08387764248422985\n",
            "Loss S33:  0.09156685928023474\n",
            "Loss S44:  0.09537761705699645\n",
            "Loss S11:  0.08031754846796611\n",
            "Loss S22:  0.08389162350872045\n",
            "Loss S33:  0.09156992799900145\n",
            "Loss S44:  0.09540873774685119\n",
            "Loss S11:  0.08033084759005794\n",
            "Loss S22:  0.08384125925961383\n",
            "Loss S33:  0.09156146089405755\n",
            "Loss S44:  0.09542492751637076\n",
            "Loss S11:  0.08033003249409457\n",
            "Loss S22:  0.0837909766263909\n",
            "Loss S33:  0.09160991679680974\n",
            "Loss S44:  0.09546277393429563\n",
            "Loss S11:  0.08031378244614665\n",
            "Loss S22:  0.08376657143436031\n",
            "Loss S33:  0.09157086894759592\n",
            "Loss S44:  0.09542207100561687\n",
            "Loss S11:  0.08023633759009244\n",
            "Loss S22:  0.08370454904679551\n",
            "Loss S33:  0.09146531465954667\n",
            "Loss S44:  0.09536198205913458\n",
            "Loss S11:  0.08020022385717963\n",
            "Loss S22:  0.08362612080619768\n",
            "Loss S33:  0.0913439381991506\n",
            "Loss S44:  0.09526175517788933\n",
            "Loss S11:  0.08028405014787826\n",
            "Loss S22:  0.08366373920827137\n",
            "Loss S33:  0.09139800430310337\n",
            "Loss S44:  0.09526232421918403\n",
            "Loss S11:  0.08027540225254648\n",
            "Loss S22:  0.08365839540305799\n",
            "Loss S33:  0.09140535567291172\n",
            "Loss S44:  0.09517527654440734\n",
            "Loss S11:  0.0803200097626292\n",
            "Loss S22:  0.08370447311010609\n",
            "Loss S33:  0.0915459935462673\n",
            "Loss S44:  0.09517164149646805\n",
            "Loss S11:  0.08031209985230749\n",
            "Loss S22:  0.08370977290960033\n",
            "Loss S33:  0.09155130737273157\n",
            "Loss S44:  0.09512533470469672\n",
            "Loss S11:  0.08025666351626519\n",
            "Loss S22:  0.08370152225637652\n",
            "Loss S33:  0.09150278595299408\n",
            "Loss S44:  0.09506786124708971\n",
            "Loss S11:  0.08034024171184277\n",
            "Loss S22:  0.08365677492052911\n",
            "Loss S33:  0.09142848834428449\n",
            "Loss S44:  0.09502899532770107\n",
            "Loss S11:  0.08035119398297043\n",
            "Loss S22:  0.0836211610221811\n",
            "Loss S33:  0.0915088222206544\n",
            "Loss S44:  0.09499226420018782\n",
            "Loss S11:  0.0803410290342987\n",
            "Loss S22:  0.08355340609947841\n",
            "Loss S33:  0.09147232074899532\n",
            "Loss S44:  0.09497660986936776\n",
            "Loss S11:  0.08036041823593346\n",
            "Loss S22:  0.08349530999848848\n",
            "Loss S33:  0.09151873619851353\n",
            "Loss S44:  0.09501915744151494\n",
            "Loss S11:  0.08030735045034143\n",
            "Loss S22:  0.08344478929479836\n",
            "Loss S33:  0.09158235930983744\n",
            "Loss S44:  0.09504647901975939\n",
            "Validation: \n",
            " Loss S11:  0.09150000661611557\n",
            " Loss S22:  0.12073387950658798\n",
            " Loss S33:  0.1241273283958435\n",
            " Loss S44:  0.13116136193275452\n",
            " Loss S11:  0.08855574365173068\n",
            " Loss S22:  0.12756156673034033\n",
            " Loss S33:  0.13267832036529267\n",
            " Loss S44:  0.13630296573752448\n",
            " Loss S11:  0.08683430639708914\n",
            " Loss S22:  0.12745307585815105\n",
            " Loss S33:  0.13214480913266902\n",
            " Loss S44:  0.13451101703614723\n",
            " Loss S11:  0.08579219205946219\n",
            " Loss S22:  0.12634942338603442\n",
            " Loss S33:  0.12943245067459638\n",
            " Loss S44:  0.13287716212331271\n",
            " Loss S11:  0.08548898764966446\n",
            " Loss S22:  0.1259958193074038\n",
            " Loss S33:  0.12805347972446018\n",
            " Loss S44:  0.1320587134471646\n",
            "\n",
            "Epoch: 43\n",
            "Loss S11:  0.09259830415248871\n",
            "Loss S22:  0.08232492208480835\n",
            "Loss S33:  0.09455998986959457\n",
            "Loss S44:  0.09739139676094055\n",
            "Loss S11:  0.07993332499807532\n",
            "Loss S22:  0.08638766340234062\n",
            "Loss S33:  0.09416850520805879\n",
            "Loss S44:  0.09846979040991176\n",
            "Loss S11:  0.08088570336500804\n",
            "Loss S22:  0.08618597437938054\n",
            "Loss S33:  0.09220085789759953\n",
            "Loss S44:  0.09612509395395007\n",
            "Loss S11:  0.08092928894104497\n",
            "Loss S22:  0.08489993911597037\n",
            "Loss S33:  0.09134724567974767\n",
            "Loss S44:  0.0950020936227614\n",
            "Loss S11:  0.08008318376250384\n",
            "Loss S22:  0.08435087342087816\n",
            "Loss S33:  0.09137391644280131\n",
            "Loss S44:  0.09473702765819503\n",
            "Loss S11:  0.08052715454615798\n",
            "Loss S22:  0.08461319070820715\n",
            "Loss S33:  0.09117654521091312\n",
            "Loss S44:  0.0942823653127633\n",
            "Loss S11:  0.08067751150639331\n",
            "Loss S22:  0.08437643041376208\n",
            "Loss S33:  0.09104629080803668\n",
            "Loss S44:  0.09529012331708533\n",
            "Loss S11:  0.08013372679411525\n",
            "Loss S22:  0.0838144106764189\n",
            "Loss S33:  0.09063483040097733\n",
            "Loss S44:  0.09527694370964883\n",
            "Loss S11:  0.08005322598748738\n",
            "Loss S22:  0.08399502638681436\n",
            "Loss S33:  0.0905043598678377\n",
            "Loss S44:  0.09514413718824033\n",
            "Loss S11:  0.08036940905091526\n",
            "Loss S22:  0.08386831665104562\n",
            "Loss S33:  0.09094191891151471\n",
            "Loss S44:  0.09556080793941414\n",
            "Loss S11:  0.08005296188120795\n",
            "Loss S22:  0.08353129707940735\n",
            "Loss S33:  0.09031858479622568\n",
            "Loss S44:  0.0950409018314711\n",
            "Loss S11:  0.07971754663430893\n",
            "Loss S22:  0.08309071762739001\n",
            "Loss S33:  0.09013940850356678\n",
            "Loss S44:  0.09482697786780091\n",
            "Loss S11:  0.07952779445273817\n",
            "Loss S22:  0.08298085780798896\n",
            "Loss S33:  0.09009378717457953\n",
            "Loss S44:  0.09439196388337238\n",
            "Loss S11:  0.07984921501110528\n",
            "Loss S22:  0.0831232980633055\n",
            "Loss S33:  0.09044974365534673\n",
            "Loss S44:  0.09449848999048917\n",
            "Loss S11:  0.07972958108000722\n",
            "Loss S22:  0.08306953979405106\n",
            "Loss S33:  0.09055955713310986\n",
            "Loss S44:  0.09440401740742067\n",
            "Loss S11:  0.07981183093707293\n",
            "Loss S22:  0.08308944616787481\n",
            "Loss S33:  0.09068475140641068\n",
            "Loss S44:  0.09435549792864464\n",
            "Loss S11:  0.07961841593988193\n",
            "Loss S22:  0.08290702435448304\n",
            "Loss S33:  0.09055034769988209\n",
            "Loss S44:  0.09405429392867948\n",
            "Loss S11:  0.07959237712168554\n",
            "Loss S22:  0.08266845543743574\n",
            "Loss S33:  0.09037203964782738\n",
            "Loss S44:  0.09400687933142422\n",
            "Loss S11:  0.07951688050235832\n",
            "Loss S22:  0.08273234961358882\n",
            "Loss S33:  0.09048594748446954\n",
            "Loss S44:  0.09410937422711546\n",
            "Loss S11:  0.07955237712067459\n",
            "Loss S22:  0.0828416166310223\n",
            "Loss S33:  0.09063236310063856\n",
            "Loss S44:  0.09421515086402443\n",
            "Loss S11:  0.07940954362871636\n",
            "Loss S22:  0.08287140880873547\n",
            "Loss S33:  0.0907379841033499\n",
            "Loss S44:  0.09420151396918652\n",
            "Loss S11:  0.07949912290296283\n",
            "Loss S22:  0.08292584359575222\n",
            "Loss S33:  0.09075833249713572\n",
            "Loss S44:  0.09422247531995954\n",
            "Loss S11:  0.07967799928932708\n",
            "Loss S22:  0.08298978629222822\n",
            "Loss S33:  0.09076839628127906\n",
            "Loss S44:  0.09426553992394408\n",
            "Loss S11:  0.07965419634983137\n",
            "Loss S22:  0.08303443714976311\n",
            "Loss S33:  0.09075035928905785\n",
            "Loss S44:  0.09422116713735448\n",
            "Loss S11:  0.07967003750232246\n",
            "Loss S22:  0.08316013750260796\n",
            "Loss S33:  0.09094656391757158\n",
            "Loss S44:  0.09428972695254687\n",
            "Loss S11:  0.07961113365047956\n",
            "Loss S22:  0.08323091356106013\n",
            "Loss S33:  0.09082966950903851\n",
            "Loss S44:  0.0941747862148095\n",
            "Loss S11:  0.0795636372890509\n",
            "Loss S22:  0.08325527735425595\n",
            "Loss S33:  0.09086667609283294\n",
            "Loss S44:  0.09414893243162112\n",
            "Loss S11:  0.07964962149018291\n",
            "Loss S22:  0.08334023308874935\n",
            "Loss S33:  0.09099355315260342\n",
            "Loss S44:  0.09432531670560695\n",
            "Loss S11:  0.07970937537659105\n",
            "Loss S22:  0.08345379443090158\n",
            "Loss S33:  0.09106533205487974\n",
            "Loss S44:  0.09441210074993216\n",
            "Loss S11:  0.07973813534397439\n",
            "Loss S22:  0.08350367870988305\n",
            "Loss S33:  0.0911067719363265\n",
            "Loss S44:  0.09445867057620864\n",
            "Loss S11:  0.07973740451607197\n",
            "Loss S22:  0.08348713898133994\n",
            "Loss S33:  0.0911604911336471\n",
            "Loss S44:  0.09450426999714684\n",
            "Loss S11:  0.07964043298861988\n",
            "Loss S22:  0.08337045181842095\n",
            "Loss S33:  0.09109029236138826\n",
            "Loss S44:  0.0944222860110151\n",
            "Loss S11:  0.07964618198828906\n",
            "Loss S22:  0.08336442570866454\n",
            "Loss S33:  0.09111866668284496\n",
            "Loss S44:  0.09450927926836727\n",
            "Loss S11:  0.07957190111999786\n",
            "Loss S22:  0.08329115354447567\n",
            "Loss S33:  0.09099121418784392\n",
            "Loss S44:  0.0944617100712756\n",
            "Loss S11:  0.07958066748602649\n",
            "Loss S22:  0.08335959674282507\n",
            "Loss S33:  0.09113682872039482\n",
            "Loss S44:  0.0944674859517131\n",
            "Loss S11:  0.0795985330055412\n",
            "Loss S22:  0.08337017701689334\n",
            "Loss S33:  0.0911611250875003\n",
            "Loss S44:  0.0944926564043064\n",
            "Loss S11:  0.07959862273196765\n",
            "Loss S22:  0.08337275381522495\n",
            "Loss S33:  0.09120008689957643\n",
            "Loss S44:  0.0945816905420903\n",
            "Loss S11:  0.07960840568669401\n",
            "Loss S22:  0.08333355795863183\n",
            "Loss S33:  0.09110277144092756\n",
            "Loss S44:  0.09457814763739424\n",
            "Loss S11:  0.07950750934913403\n",
            "Loss S22:  0.08325873830617256\n",
            "Loss S33:  0.09095933366557118\n",
            "Loss S44:  0.0945351881969945\n",
            "Loss S11:  0.07938906861959821\n",
            "Loss S22:  0.0831905084059519\n",
            "Loss S33:  0.09085976745923767\n",
            "Loss S44:  0.09441646798263731\n",
            "Loss S11:  0.0794873470881782\n",
            "Loss S22:  0.08326747947537394\n",
            "Loss S33:  0.09094177058287095\n",
            "Loss S44:  0.09447309640503286\n",
            "Loss S11:  0.07949579087450846\n",
            "Loss S22:  0.08329692366023134\n",
            "Loss S33:  0.09084858893985585\n",
            "Loss S44:  0.09445399134771088\n",
            "Loss S11:  0.07950960583719392\n",
            "Loss S22:  0.08330893593523961\n",
            "Loss S33:  0.09079234112682365\n",
            "Loss S44:  0.09451270890986269\n",
            "Loss S11:  0.07951548148439268\n",
            "Loss S22:  0.08326156633601388\n",
            "Loss S33:  0.09076089367028456\n",
            "Loss S44:  0.09448910848585469\n",
            "Loss S11:  0.07951633629648863\n",
            "Loss S22:  0.08327889661230738\n",
            "Loss S33:  0.09074350226831003\n",
            "Loss S44:  0.09453888522997465\n",
            "Loss S11:  0.07952782576965652\n",
            "Loss S22:  0.08329107086205165\n",
            "Loss S33:  0.09072438493668372\n",
            "Loss S44:  0.09449274036744217\n",
            "Loss S11:  0.07952175846165793\n",
            "Loss S22:  0.08322953748761186\n",
            "Loss S33:  0.0907579069702589\n",
            "Loss S44:  0.09449695482235927\n",
            "Loss S11:  0.07951101601408546\n",
            "Loss S22:  0.08320493927332247\n",
            "Loss S33:  0.09076288682576704\n",
            "Loss S44:  0.0944569173353999\n",
            "Loss S11:  0.07951834456309707\n",
            "Loss S22:  0.08321965740617993\n",
            "Loss S33:  0.09076164164067306\n",
            "Loss S44:  0.09445353176080759\n",
            "Loss S11:  0.07945619481795917\n",
            "Loss S22:  0.08313589250998925\n",
            "Loss S33:  0.09072861336399242\n",
            "Loss S44:  0.09442910301648914\n",
            "Validation: \n",
            " Loss S11:  0.09263792634010315\n",
            " Loss S22:  0.1116359755396843\n",
            " Loss S33:  0.11510308086872101\n",
            " Loss S44:  0.12910760939121246\n",
            " Loss S11:  0.0895513083253588\n",
            " Loss S22:  0.12053283091102328\n",
            " Loss S33:  0.12386173506577809\n",
            " Loss S44:  0.13471328609046482\n",
            " Loss S11:  0.08760976137184515\n",
            " Loss S22:  0.11991374049244857\n",
            " Loss S33:  0.12403874826140521\n",
            " Loss S44:  0.13330098659527012\n",
            " Loss S11:  0.08652896275285815\n",
            " Loss S22:  0.11868718816120116\n",
            " Loss S33:  0.12160484614919444\n",
            " Loss S44:  0.13191238326615976\n",
            " Loss S11:  0.08628651416964\n",
            " Loss S22:  0.11846852762463653\n",
            " Loss S33:  0.12049546920590931\n",
            " Loss S44:  0.1311176284963702\n",
            "\n",
            "Epoch: 44\n",
            "Loss S11:  0.08903402090072632\n",
            "Loss S22:  0.0849459245800972\n",
            "Loss S33:  0.08668487519025803\n",
            "Loss S44:  0.09519220143556595\n",
            "Loss S11:  0.08381511000069705\n",
            "Loss S22:  0.08335226706483147\n",
            "Loss S33:  0.0946352021260695\n",
            "Loss S44:  0.09526050023057243\n",
            "Loss S11:  0.08220346804176058\n",
            "Loss S22:  0.08450128528333846\n",
            "Loss S33:  0.09147166105962935\n",
            "Loss S44:  0.0938411078282765\n",
            "Loss S11:  0.08167019101881212\n",
            "Loss S22:  0.08335031040253178\n",
            "Loss S33:  0.09021185122189983\n",
            "Loss S44:  0.09263626486063004\n",
            "Loss S11:  0.08136816504524975\n",
            "Loss S22:  0.08377464133791807\n",
            "Loss S33:  0.09006491466993238\n",
            "Loss S44:  0.09352526504818987\n",
            "Loss S11:  0.0809172378743396\n",
            "Loss S22:  0.08405453507222381\n",
            "Loss S33:  0.08976419664481107\n",
            "Loss S44:  0.09299523617122687\n",
            "Loss S11:  0.0807656859521006\n",
            "Loss S22:  0.08399907511765839\n",
            "Loss S33:  0.09001241490000585\n",
            "Loss S44:  0.09370727189740197\n",
            "Loss S11:  0.08003240897202156\n",
            "Loss S22:  0.08345806105455882\n",
            "Loss S33:  0.08988184954079104\n",
            "Loss S44:  0.09383963090433202\n",
            "Loss S11:  0.07984219785825705\n",
            "Loss S22:  0.08341954519719254\n",
            "Loss S33:  0.08988185806406869\n",
            "Loss S44:  0.09337475831493919\n",
            "Loss S11:  0.07972669658752587\n",
            "Loss S22:  0.08308466008076301\n",
            "Loss S33:  0.08980646582095178\n",
            "Loss S44:  0.09320692447843132\n",
            "Loss S11:  0.07945080654750956\n",
            "Loss S22:  0.08268022485593758\n",
            "Loss S33:  0.08979821345298597\n",
            "Loss S44:  0.09311748561587664\n",
            "Loss S11:  0.07923250786356024\n",
            "Loss S22:  0.08217367328502037\n",
            "Loss S33:  0.08962136627854528\n",
            "Loss S44:  0.09302817097118309\n",
            "Loss S11:  0.07925917183564714\n",
            "Loss S22:  0.08237261318963421\n",
            "Loss S33:  0.08960306508974596\n",
            "Loss S44:  0.0929471982781552\n",
            "Loss S11:  0.07939147039224173\n",
            "Loss S22:  0.08237492941490566\n",
            "Loss S33:  0.08964112719506707\n",
            "Loss S44:  0.09299082536506288\n",
            "Loss S11:  0.07920163172356626\n",
            "Loss S22:  0.08240400508363196\n",
            "Loss S33:  0.08949275092875704\n",
            "Loss S44:  0.0928476188622468\n",
            "Loss S11:  0.07930670407236806\n",
            "Loss S22:  0.08250714241471512\n",
            "Loss S33:  0.08957054623902239\n",
            "Loss S44:  0.09296646472436702\n",
            "Loss S11:  0.07911094130011079\n",
            "Loss S22:  0.08230993586667576\n",
            "Loss S33:  0.08928950649240742\n",
            "Loss S44:  0.09285831664289747\n",
            "Loss S11:  0.07889339458524135\n",
            "Loss S22:  0.08223888709356911\n",
            "Loss S33:  0.0891975429758691\n",
            "Loss S44:  0.09288092937908675\n",
            "Loss S11:  0.07892913046133453\n",
            "Loss S22:  0.08219595558076932\n",
            "Loss S33:  0.08931334699550386\n",
            "Loss S44:  0.09289568670876118\n",
            "Loss S11:  0.07902360953273574\n",
            "Loss S22:  0.08209407583113117\n",
            "Loss S33:  0.08930550667783976\n",
            "Loss S44:  0.09300722392441714\n",
            "Loss S11:  0.0790608538249832\n",
            "Loss S22:  0.08208922244867875\n",
            "Loss S33:  0.08943384159263687\n",
            "Loss S44:  0.09304265577846499\n",
            "Loss S11:  0.07902362875605082\n",
            "Loss S22:  0.08203870736027216\n",
            "Loss S33:  0.08961153736611678\n",
            "Loss S44:  0.09303742294062935\n",
            "Loss S11:  0.07919944920556038\n",
            "Loss S22:  0.0822953951979115\n",
            "Loss S33:  0.08984194247566198\n",
            "Loss S44:  0.09317560675996461\n",
            "Loss S11:  0.07931125428511467\n",
            "Loss S22:  0.08242995240342565\n",
            "Loss S33:  0.08999683940178388\n",
            "Loss S44:  0.09335961412170748\n",
            "Loss S11:  0.07939169160186997\n",
            "Loss S22:  0.0824762347822862\n",
            "Loss S33:  0.09012108246691494\n",
            "Loss S44:  0.09354012258567256\n",
            "Loss S11:  0.07933487087131971\n",
            "Loss S22:  0.08241935457247662\n",
            "Loss S33:  0.09002066023677469\n",
            "Loss S44:  0.09359028549545789\n",
            "Loss S11:  0.07941117437406518\n",
            "Loss S22:  0.08247193722660971\n",
            "Loss S33:  0.09001464787799279\n",
            "Loss S44:  0.09363079784702068\n",
            "Loss S11:  0.07941301296227972\n",
            "Loss S22:  0.08254673279739394\n",
            "Loss S33:  0.09005242712603284\n",
            "Loss S44:  0.09361294228102449\n",
            "Loss S11:  0.07943542301654816\n",
            "Loss S22:  0.08273685856115776\n",
            "Loss S33:  0.09009790831507312\n",
            "Loss S44:  0.09366396072178125\n",
            "Loss S11:  0.07946916941002882\n",
            "Loss S22:  0.08276648181922656\n",
            "Loss S33:  0.09015195381498009\n",
            "Loss S44:  0.09367347419057105\n",
            "Loss S11:  0.07942013315039616\n",
            "Loss S22:  0.08273853505172603\n",
            "Loss S33:  0.090154261981134\n",
            "Loss S44:  0.09378074703620518\n",
            "Loss S11:  0.07932739123893703\n",
            "Loss S22:  0.08270282784650564\n",
            "Loss S33:  0.08999178544213918\n",
            "Loss S44:  0.09376646894542351\n",
            "Loss S11:  0.07938139417971778\n",
            "Loss S22:  0.0826757458790069\n",
            "Loss S33:  0.09002633118926551\n",
            "Loss S44:  0.0937745310250101\n",
            "Loss S11:  0.07938650409125489\n",
            "Loss S22:  0.08258259575532642\n",
            "Loss S33:  0.08986035254966816\n",
            "Loss S44:  0.09361736213062465\n",
            "Loss S11:  0.07933599376215264\n",
            "Loss S22:  0.08259833953684725\n",
            "Loss S33:  0.0898962302728832\n",
            "Loss S44:  0.09358409119229164\n",
            "Loss S11:  0.07930968227422136\n",
            "Loss S22:  0.08263757958626136\n",
            "Loss S33:  0.08993211723970213\n",
            "Loss S44:  0.09371778740417584\n",
            "Loss S11:  0.079307356813195\n",
            "Loss S22:  0.08269855530720999\n",
            "Loss S33:  0.08992980232777027\n",
            "Loss S44:  0.09375171835980588\n",
            "Loss S11:  0.07932579566566449\n",
            "Loss S22:  0.08269287623726133\n",
            "Loss S33:  0.08985979575914835\n",
            "Loss S44:  0.09375906082979753\n",
            "Loss S11:  0.07917133604330341\n",
            "Loss S22:  0.08261373967636289\n",
            "Loss S33:  0.08976053874793015\n",
            "Loss S44:  0.09371288480564678\n",
            "Loss S11:  0.0790649020229764\n",
            "Loss S22:  0.0825293807460524\n",
            "Loss S33:  0.08967234363869937\n",
            "Loss S44:  0.09367493012219744\n",
            "Loss S11:  0.07913747857187751\n",
            "Loss S22:  0.08256458812520985\n",
            "Loss S33:  0.08975071891064655\n",
            "Loss S44:  0.09366328289681242\n",
            "Loss S11:  0.07908785406617933\n",
            "Loss S22:  0.08259022041901475\n",
            "Loss S33:  0.08981336323304188\n",
            "Loss S44:  0.0937142330980939\n",
            "Loss S11:  0.07906840159391847\n",
            "Loss S22:  0.08258436436183096\n",
            "Loss S33:  0.08986074346473834\n",
            "Loss S44:  0.0937642197427727\n",
            "Loss S11:  0.07909440819666723\n",
            "Loss S22:  0.082593229177933\n",
            "Loss S33:  0.08990346470938482\n",
            "Loss S44:  0.09376459833225906\n",
            "Loss S11:  0.07916930025599711\n",
            "Loss S22:  0.08258043467795768\n",
            "Loss S33:  0.08987722378604267\n",
            "Loss S44:  0.0937680997410599\n",
            "Loss S11:  0.07921868471837626\n",
            "Loss S22:  0.08260445666815383\n",
            "Loss S33:  0.08987087290527551\n",
            "Loss S44:  0.09377439840471666\n",
            "Loss S11:  0.07925244795720127\n",
            "Loss S22:  0.08258824116689781\n",
            "Loss S33:  0.08992368525900189\n",
            "Loss S44:  0.09381490822986512\n",
            "Loss S11:  0.0791910012942717\n",
            "Loss S22:  0.08257508891656394\n",
            "Loss S33:  0.08991725552993216\n",
            "Loss S44:  0.09378740715056721\n",
            "Loss S11:  0.07921774911533522\n",
            "Loss S22:  0.08254186177080237\n",
            "Loss S33:  0.08989381974489426\n",
            "Loss S44:  0.09380229150493032\n",
            "Loss S11:  0.07919944956322066\n",
            "Loss S22:  0.08250092202133656\n",
            "Loss S33:  0.08987017230807885\n",
            "Loss S44:  0.09377830357209242\n",
            "Validation: \n",
            " Loss S11:  0.08422093838453293\n",
            " Loss S22:  0.11866569519042969\n",
            " Loss S33:  0.11564909666776657\n",
            " Loss S44:  0.12615618109703064\n",
            " Loss S11:  0.08434312010095232\n",
            " Loss S22:  0.12561291881969996\n",
            " Loss S33:  0.1252966168380919\n",
            " Loss S44:  0.1329017270888601\n",
            " Loss S11:  0.08262785015309729\n",
            " Loss S22:  0.1256079441163598\n",
            " Loss S33:  0.12507654999087497\n",
            " Loss S44:  0.13143721259221797\n",
            " Loss S11:  0.08167437354072196\n",
            " Loss S22:  0.12441831985946561\n",
            " Loss S33:  0.12276026252351824\n",
            " Loss S44:  0.12993929400795795\n",
            " Loss S11:  0.0815398492562918\n",
            " Loss S22:  0.1242945146413497\n",
            " Loss S33:  0.1218701899971491\n",
            " Loss S44:  0.12936849551804272\n",
            "\n",
            "Epoch: 45\n",
            "Loss S11:  0.07586304098367691\n",
            "Loss S22:  0.07929039746522903\n",
            "Loss S33:  0.09068598598241806\n",
            "Loss S44:  0.09736068546772003\n",
            "Loss S11:  0.07948109372095628\n",
            "Loss S22:  0.08327499235218222\n",
            "Loss S33:  0.09280982749028639\n",
            "Loss S44:  0.094538076357408\n",
            "Loss S11:  0.07893903695401691\n",
            "Loss S22:  0.08317787235691435\n",
            "Loss S33:  0.09036162708486829\n",
            "Loss S44:  0.09430638913597379\n",
            "Loss S11:  0.07830675426990755\n",
            "Loss S22:  0.08178041034167813\n",
            "Loss S33:  0.09034295692559212\n",
            "Loss S44:  0.09312174469232559\n",
            "Loss S11:  0.07813400466267656\n",
            "Loss S22:  0.08222849121907862\n",
            "Loss S33:  0.09065815715528117\n",
            "Loss S44:  0.09290145228548748\n",
            "Loss S11:  0.0777010382974849\n",
            "Loss S22:  0.08298456800334594\n",
            "Loss S33:  0.09080100322470945\n",
            "Loss S44:  0.09267921716559167\n",
            "Loss S11:  0.07802973394511176\n",
            "Loss S22:  0.0832744790393798\n",
            "Loss S33:  0.09061015946943252\n",
            "Loss S44:  0.09323809576816246\n",
            "Loss S11:  0.07797390107117907\n",
            "Loss S22:  0.08316511157113061\n",
            "Loss S33:  0.0902798356304706\n",
            "Loss S44:  0.09370003493738846\n",
            "Loss S11:  0.07802763792835636\n",
            "Loss S22:  0.08304477427844648\n",
            "Loss S33:  0.08998012597914096\n",
            "Loss S44:  0.09360513229061056\n",
            "Loss S11:  0.0780795584995668\n",
            "Loss S22:  0.08311358741024039\n",
            "Loss S33:  0.09016685277878583\n",
            "Loss S44:  0.0938349595436683\n",
            "Loss S11:  0.07797556237714125\n",
            "Loss S22:  0.08288060084427937\n",
            "Loss S33:  0.08990019375439917\n",
            "Loss S44:  0.09359059940175254\n",
            "Loss S11:  0.07790357283912264\n",
            "Loss S22:  0.08259839423604913\n",
            "Loss S33:  0.08956857515616459\n",
            "Loss S44:  0.09335271061003746\n",
            "Loss S11:  0.07797805669386525\n",
            "Loss S22:  0.08250495218787311\n",
            "Loss S33:  0.08952966751145923\n",
            "Loss S44:  0.09291706886912181\n",
            "Loss S11:  0.07823854218457492\n",
            "Loss S22:  0.0826973502758805\n",
            "Loss S33:  0.08971107745443592\n",
            "Loss S44:  0.09300556272723293\n",
            "Loss S11:  0.07836534489765234\n",
            "Loss S22:  0.08271214804539444\n",
            "Loss S33:  0.08961404114961624\n",
            "Loss S44:  0.09297094351433693\n",
            "Loss S11:  0.07857559473309296\n",
            "Loss S22:  0.08264589916593981\n",
            "Loss S33:  0.08967927567019368\n",
            "Loss S44:  0.09302099828688515\n",
            "Loss S11:  0.07826703856265323\n",
            "Loss S22:  0.08228177524038724\n",
            "Loss S33:  0.08951240828873948\n",
            "Loss S44:  0.09279481728809962\n",
            "Loss S11:  0.07804784498978079\n",
            "Loss S22:  0.0821244047617006\n",
            "Loss S33:  0.0892416810867382\n",
            "Loss S44:  0.09286271514948348\n",
            "Loss S11:  0.07808988238813469\n",
            "Loss S22:  0.08202159587931897\n",
            "Loss S33:  0.08919996348369187\n",
            "Loss S44:  0.09305276635272727\n",
            "Loss S11:  0.0781152588729771\n",
            "Loss S22:  0.0820220318179168\n",
            "Loss S33:  0.08923200826058213\n",
            "Loss S44:  0.09319177605406777\n",
            "Loss S11:  0.0782162975056551\n",
            "Loss S22:  0.08208171406121396\n",
            "Loss S33:  0.0892297526571288\n",
            "Loss S44:  0.09324065740428754\n",
            "Loss S11:  0.07825364650920104\n",
            "Loss S22:  0.08220193776968532\n",
            "Loss S33:  0.08943966257063699\n",
            "Loss S44:  0.09328982696572752\n",
            "Loss S11:  0.07833739786463625\n",
            "Loss S22:  0.08226060316101458\n",
            "Loss S33:  0.0896000028398242\n",
            "Loss S44:  0.0933630133376402\n",
            "Loss S11:  0.07853959092781657\n",
            "Loss S22:  0.08236904801937925\n",
            "Loss S33:  0.08965749474185886\n",
            "Loss S44:  0.0933315557834906\n",
            "Loss S11:  0.07862301104358123\n",
            "Loss S22:  0.08249298966957326\n",
            "Loss S33:  0.08981026548320327\n",
            "Loss S44:  0.09342891994971952\n",
            "Loss S11:  0.07865704633147118\n",
            "Loss S22:  0.08244894461268448\n",
            "Loss S33:  0.08971472609921756\n",
            "Loss S44:  0.09352994018579384\n",
            "Loss S11:  0.07869885903265741\n",
            "Loss S22:  0.08247500468202477\n",
            "Loss S33:  0.08975989308736333\n",
            "Loss S44:  0.09358298784243192\n",
            "Loss S11:  0.07860109637529208\n",
            "Loss S22:  0.08257040459786394\n",
            "Loss S33:  0.0897845714129645\n",
            "Loss S44:  0.09353086445824246\n",
            "Loss S11:  0.07871079931426728\n",
            "Loss S22:  0.08270918512948891\n",
            "Loss S33:  0.089802611208259\n",
            "Loss S44:  0.093631306762585\n",
            "Loss S11:  0.07879774811392798\n",
            "Loss S22:  0.08270425711761635\n",
            "Loss S33:  0.08985289793039106\n",
            "Loss S44:  0.09364045232124754\n",
            "Loss S11:  0.07875043521341295\n",
            "Loss S22:  0.08261662859011726\n",
            "Loss S33:  0.08983545752854838\n",
            "Loss S44:  0.09370341436609478\n",
            "Loss S11:  0.07866640222465493\n",
            "Loss S22:  0.0825618714740997\n",
            "Loss S33:  0.08980972262344943\n",
            "Loss S44:  0.09375067117513185\n",
            "Loss S11:  0.0787386945405296\n",
            "Loss S22:  0.08255425556287216\n",
            "Loss S33:  0.08997726640040259\n",
            "Loss S44:  0.09382620152097625\n",
            "Loss S11:  0.07864985188463663\n",
            "Loss S22:  0.08239847033360573\n",
            "Loss S33:  0.08987353523782372\n",
            "Loss S44:  0.09373885098270779\n",
            "Loss S11:  0.07861202875496355\n",
            "Loss S22:  0.08243313002402831\n",
            "Loss S33:  0.08989341753668799\n",
            "Loss S44:  0.09376707493909293\n",
            "Loss S11:  0.07868737112889942\n",
            "Loss S22:  0.08245138402635556\n",
            "Loss S33:  0.08992777047333894\n",
            "Loss S44:  0.09390942616510255\n",
            "Loss S11:  0.07859900270761545\n",
            "Loss S22:  0.08244105433616943\n",
            "Loss S33:  0.08984344173996732\n",
            "Loss S44:  0.09391038237433684\n",
            "Loss S11:  0.07855912148350011\n",
            "Loss S22:  0.08245410568349446\n",
            "Loss S33:  0.08976923694427444\n",
            "Loss S44:  0.09387270823080907\n",
            "Loss S11:  0.07845590276316082\n",
            "Loss S22:  0.08230045540478285\n",
            "Loss S33:  0.08966563604869868\n",
            "Loss S44:  0.09376058541727192\n",
            "Loss S11:  0.07831800737611168\n",
            "Loss S22:  0.08221013096096876\n",
            "Loss S33:  0.0895706909087003\n",
            "Loss S44:  0.09364423653125153\n",
            "Loss S11:  0.07834541659216929\n",
            "Loss S22:  0.08224989287090717\n",
            "Loss S33:  0.08960078729135437\n",
            "Loss S44:  0.0936519343991232\n",
            "Loss S11:  0.07831821419353033\n",
            "Loss S22:  0.08221236153204366\n",
            "Loss S33:  0.08956174710153664\n",
            "Loss S44:  0.09368698898965715\n",
            "Loss S11:  0.07836910830080651\n",
            "Loss S22:  0.08214442000996472\n",
            "Loss S33:  0.08959069674626666\n",
            "Loss S44:  0.09373650795419539\n",
            "Loss S11:  0.07842806577163894\n",
            "Loss S22:  0.08215211592009615\n",
            "Loss S33:  0.08955079109652136\n",
            "Loss S44:  0.09370828312746057\n",
            "Loss S11:  0.07849093948125568\n",
            "Loss S22:  0.0821216212767187\n",
            "Loss S33:  0.08955654463057075\n",
            "Loss S44:  0.0936728088763836\n",
            "Loss S11:  0.07860219434828822\n",
            "Loss S22:  0.08214105853187272\n",
            "Loss S33:  0.08953096416929608\n",
            "Loss S44:  0.09374303836581976\n",
            "Loss S11:  0.0786128946844642\n",
            "Loss S22:  0.08217051312521326\n",
            "Loss S33:  0.08955764560404672\n",
            "Loss S44:  0.09371064451407972\n",
            "Loss S11:  0.07854497784673535\n",
            "Loss S22:  0.082119458064342\n",
            "Loss S33:  0.08949812704147553\n",
            "Loss S44:  0.09369882071220698\n",
            "Loss S11:  0.07853613551435004\n",
            "Loss S22:  0.08211008900013635\n",
            "Loss S33:  0.0894879704949266\n",
            "Loss S44:  0.09368959817408028\n",
            "Loss S11:  0.07849991735063357\n",
            "Loss S22:  0.08204757274259375\n",
            "Loss S33:  0.08945297252561313\n",
            "Loss S44:  0.09364972623205477\n",
            "Validation: \n",
            " Loss S11:  0.08919121325016022\n",
            " Loss S22:  0.1210629791021347\n",
            " Loss S33:  0.11687742173671722\n",
            " Loss S44:  0.12402912229299545\n",
            " Loss S11:  0.08800110469261806\n",
            " Loss S22:  0.12381347694567271\n",
            " Loss S33:  0.12474494604837327\n",
            " Loss S44:  0.13401465508199872\n",
            " Loss S11:  0.08650851049801199\n",
            " Loss S22:  0.12326739637590037\n",
            " Loss S33:  0.12494440213209246\n",
            " Loss S44:  0.13215821472609915\n",
            " Loss S11:  0.08582719671921651\n",
            " Loss S22:  0.12190437732172794\n",
            " Loss S33:  0.12251522636315862\n",
            " Loss S44:  0.13090153722489467\n",
            " Loss S11:  0.08550451475161093\n",
            " Loss S22:  0.12188590207585583\n",
            " Loss S33:  0.12156626582145691\n",
            " Loss S44:  0.13016948609808346\n",
            "\n",
            "Epoch: 46\n",
            "Loss S11:  0.07942075282335281\n",
            "Loss S22:  0.07757491618394852\n",
            "Loss S33:  0.08065932244062424\n",
            "Loss S44:  0.08144273608922958\n",
            "Loss S11:  0.07838285849852995\n",
            "Loss S22:  0.08273138783194801\n",
            "Loss S33:  0.08873086761344563\n",
            "Loss S44:  0.08999998867511749\n",
            "Loss S11:  0.07890502718232927\n",
            "Loss S22:  0.08366754012448448\n",
            "Loss S33:  0.08695451063769204\n",
            "Loss S44:  0.09101939272312891\n",
            "Loss S11:  0.07795199367307848\n",
            "Loss S22:  0.08186131980149977\n",
            "Loss S33:  0.08645816099259161\n",
            "Loss S44:  0.09133229500824405\n",
            "Loss S11:  0.07752325076882433\n",
            "Loss S22:  0.08197998709794951\n",
            "Loss S33:  0.08731438928261036\n",
            "Loss S44:  0.09216016730884226\n",
            "Loss S11:  0.07752575126348757\n",
            "Loss S22:  0.08249489729310952\n",
            "Loss S33:  0.08727245062005286\n",
            "Loss S44:  0.09181979430072448\n",
            "Loss S11:  0.07780037478345339\n",
            "Loss S22:  0.08227027464108388\n",
            "Loss S33:  0.08734854188610296\n",
            "Loss S44:  0.09230421227021296\n",
            "Loss S11:  0.07779321043004453\n",
            "Loss S22:  0.08245388355473397\n",
            "Loss S33:  0.08778978485456655\n",
            "Loss S44:  0.09217386084123397\n",
            "Loss S11:  0.07773147137076766\n",
            "Loss S22:  0.0824361284389908\n",
            "Loss S33:  0.08784509312223505\n",
            "Loss S44:  0.09216893786265526\n",
            "Loss S11:  0.07767156249546743\n",
            "Loss S22:  0.0824043407708734\n",
            "Loss S33:  0.08831528852601628\n",
            "Loss S44:  0.09256567298383503\n",
            "Loss S11:  0.07732303359425895\n",
            "Loss S22:  0.08196123873833383\n",
            "Loss S33:  0.08793141833036253\n",
            "Loss S44:  0.0920330685849237\n",
            "Loss S11:  0.07712185604346765\n",
            "Loss S22:  0.08166666751777804\n",
            "Loss S33:  0.08772578922746417\n",
            "Loss S44:  0.09192542261905498\n",
            "Loss S11:  0.0769265241736223\n",
            "Loss S22:  0.08168024865310054\n",
            "Loss S33:  0.0878514741077896\n",
            "Loss S44:  0.0916564387600284\n",
            "Loss S11:  0.07698483054192011\n",
            "Loss S22:  0.08159378989962221\n",
            "Loss S33:  0.08805570410180638\n",
            "Loss S44:  0.09161689871822605\n",
            "Loss S11:  0.07691431330873612\n",
            "Loss S22:  0.08163618058600325\n",
            "Loss S33:  0.08817621447304462\n",
            "Loss S44:  0.09178796685333793\n",
            "Loss S11:  0.07721476029876052\n",
            "Loss S22:  0.08151416051269367\n",
            "Loss S33:  0.08828846735275345\n",
            "Loss S44:  0.09196785891687634\n",
            "Loss S11:  0.07712373651147629\n",
            "Loss S22:  0.08131120758860008\n",
            "Loss S33:  0.08822060297734989\n",
            "Loss S44:  0.0919179226856054\n",
            "Loss S11:  0.07718098433742747\n",
            "Loss S22:  0.08121507429675749\n",
            "Loss S33:  0.08826054314598006\n",
            "Loss S44:  0.09196134334244924\n",
            "Loss S11:  0.07716584386746528\n",
            "Loss S22:  0.08120614852118228\n",
            "Loss S33:  0.08840724241338382\n",
            "Loss S44:  0.09205165147287411\n",
            "Loss S11:  0.07724001656496088\n",
            "Loss S22:  0.08117544758304251\n",
            "Loss S33:  0.08844148621197147\n",
            "Loss S44:  0.09205724872375658\n",
            "Loss S11:  0.07741129709713494\n",
            "Loss S22:  0.08108703037770233\n",
            "Loss S33:  0.08840620577038817\n",
            "Loss S44:  0.09195694508985501\n",
            "Loss S11:  0.07747505375700539\n",
            "Loss S22:  0.08104133982017142\n",
            "Loss S33:  0.08844064236110985\n",
            "Loss S44:  0.09205186081017364\n",
            "Loss S11:  0.07768523858278585\n",
            "Loss S22:  0.08120650641316741\n",
            "Loss S33:  0.0885019026595543\n",
            "Loss S44:  0.0922069403583108\n",
            "Loss S11:  0.0777707535660628\n",
            "Loss S22:  0.08122675628824667\n",
            "Loss S33:  0.08861148438025347\n",
            "Loss S44:  0.09214374590745736\n",
            "Loss S11:  0.07785898657260594\n",
            "Loss S22:  0.08124402015473833\n",
            "Loss S33:  0.08878584907880957\n",
            "Loss S44:  0.0924571785627559\n",
            "Loss S11:  0.07779541671038624\n",
            "Loss S22:  0.08114732945879617\n",
            "Loss S33:  0.08879302848738978\n",
            "Loss S44:  0.09245495141977333\n",
            "Loss S11:  0.07779840145873841\n",
            "Loss S22:  0.08125369720360785\n",
            "Loss S33:  0.08887545125009456\n",
            "Loss S44:  0.09247106679097902\n",
            "Loss S11:  0.07778643695740682\n",
            "Loss S22:  0.08133706594800157\n",
            "Loss S33:  0.08903741091489792\n",
            "Loss S44:  0.0925350918321152\n",
            "Loss S11:  0.07784122560053958\n",
            "Loss S22:  0.08152057481034795\n",
            "Loss S33:  0.08901195670892335\n",
            "Loss S44:  0.09250202716669578\n",
            "Loss S11:  0.077801084088296\n",
            "Loss S22:  0.08155941073478702\n",
            "Loss S33:  0.08898600544204417\n",
            "Loss S44:  0.0926068736515504\n",
            "Loss S11:  0.07781349748769076\n",
            "Loss S22:  0.08161952137402521\n",
            "Loss S33:  0.08905435719858372\n",
            "Loss S44:  0.0925560970589568\n",
            "Loss S11:  0.0777851772701242\n",
            "Loss S22:  0.08153310186945358\n",
            "Loss S33:  0.08888165130492576\n",
            "Loss S44:  0.09241495051472133\n",
            "Loss S11:  0.07783558361255491\n",
            "Loss S22:  0.08165640194792985\n",
            "Loss S33:  0.08905052823069683\n",
            "Loss S44:  0.09257908172418024\n",
            "Loss S11:  0.07773535376769178\n",
            "Loss S22:  0.08153780706204317\n",
            "Loss S33:  0.08891340108796549\n",
            "Loss S44:  0.0923864557168635\n",
            "Loss S11:  0.07774368736925713\n",
            "Loss S22:  0.08160743533559908\n",
            "Loss S33:  0.08901020694425728\n",
            "Loss S44:  0.09242289666294003\n",
            "Loss S11:  0.07776029919542139\n",
            "Loss S22:  0.08166687524853608\n",
            "Loss S33:  0.08903331237386095\n",
            "Loss S44:  0.09247348584935196\n",
            "Loss S11:  0.07770871141940933\n",
            "Loss S22:  0.08172837065899141\n",
            "Loss S33:  0.08903542266245364\n",
            "Loss S44:  0.09255329732089161\n",
            "Loss S11:  0.07765081115167417\n",
            "Loss S22:  0.08173588686434406\n",
            "Loss S33:  0.08896877088476063\n",
            "Loss S44:  0.09246530130786716\n",
            "Loss S11:  0.07757366241354328\n",
            "Loss S22:  0.08164831445481521\n",
            "Loss S33:  0.08887673149819136\n",
            "Loss S44:  0.09240181712810136\n",
            "Loss S11:  0.07742131004095687\n",
            "Loss S22:  0.0815960437612003\n",
            "Loss S33:  0.088817712843723\n",
            "Loss S44:  0.09231877382225392\n",
            "Loss S11:  0.07754492861894598\n",
            "Loss S22:  0.08163548482958218\n",
            "Loss S33:  0.08892457198323751\n",
            "Loss S44:  0.09243271698678224\n",
            "Loss S11:  0.07753390340256865\n",
            "Loss S22:  0.08165942102561902\n",
            "Loss S33:  0.08888968261561545\n",
            "Loss S44:  0.092406908181607\n",
            "Loss S11:  0.07759348288921732\n",
            "Loss S22:  0.08173846277339725\n",
            "Loss S33:  0.0889186181853616\n",
            "Loss S44:  0.09246254708803747\n",
            "Loss S11:  0.07753646133463234\n",
            "Loss S22:  0.08164718066906154\n",
            "Loss S33:  0.08886572922603714\n",
            "Loss S44:  0.09242750425972131\n",
            "Loss S11:  0.07747971504723945\n",
            "Loss S22:  0.08166561830712825\n",
            "Loss S33:  0.08886734264440277\n",
            "Loss S44:  0.0923933432418473\n",
            "Loss S11:  0.0775195409744119\n",
            "Loss S22:  0.08168094744802579\n",
            "Loss S33:  0.08878900203234341\n",
            "Loss S44:  0.09243855265252078\n",
            "Loss S11:  0.07756526021533312\n",
            "Loss S22:  0.08167018242778333\n",
            "Loss S33:  0.08881211343853695\n",
            "Loss S44:  0.0925152343970576\n",
            "Loss S11:  0.07753208308083237\n",
            "Loss S22:  0.08167846288112825\n",
            "Loss S33:  0.08879206190380068\n",
            "Loss S44:  0.09245570886666608\n",
            "Loss S11:  0.07756210089101613\n",
            "Loss S22:  0.08158137247338117\n",
            "Loss S33:  0.08882716885165712\n",
            "Loss S44:  0.09245736576353437\n",
            "Loss S11:  0.07756138729527137\n",
            "Loss S22:  0.08155433190967547\n",
            "Loss S33:  0.08882209044063649\n",
            "Loss S44:  0.09235972865596566\n",
            "Validation: \n",
            " Loss S11:  0.08909934014081955\n",
            " Loss S22:  0.11557011306285858\n",
            " Loss S33:  0.11520591378211975\n",
            " Loss S44:  0.12643714249134064\n",
            " Loss S11:  0.08811234150614057\n",
            " Loss S22:  0.1215302124619484\n",
            " Loss S33:  0.12444973417690822\n",
            " Loss S44:  0.13266546598502568\n",
            " Loss S11:  0.08645846731052166\n",
            " Loss S22:  0.12142839846087665\n",
            " Loss S33:  0.12442236121107893\n",
            " Loss S44:  0.1316651843669938\n",
            " Loss S11:  0.0855658293747511\n",
            " Loss S22:  0.12006867310551346\n",
            " Loss S33:  0.12228969374641044\n",
            " Loss S44:  0.13037901176292388\n",
            " Loss S11:  0.08506127538872355\n",
            " Loss S22:  0.12015601284342048\n",
            " Loss S33:  0.12157061769638533\n",
            " Loss S44:  0.12978957915379677\n",
            "\n",
            "Epoch: 47\n",
            "Loss S11:  0.07306008785963058\n",
            "Loss S22:  0.0772632285952568\n",
            "Loss S33:  0.08518669009208679\n",
            "Loss S44:  0.0890786424279213\n",
            "Loss S11:  0.07725575024431403\n",
            "Loss S22:  0.08369626240296797\n",
            "Loss S33:  0.09343322298743507\n",
            "Loss S44:  0.09084795144471255\n",
            "Loss S11:  0.07849360328345072\n",
            "Loss S22:  0.0830806912410827\n",
            "Loss S33:  0.09104037639640626\n",
            "Loss S44:  0.08955507228771846\n",
            "Loss S11:  0.07731975855365876\n",
            "Loss S22:  0.08143141240842881\n",
            "Loss S33:  0.08902957723025352\n",
            "Loss S44:  0.08922183249265916\n",
            "Loss S11:  0.07715656299416612\n",
            "Loss S22:  0.0813112949452749\n",
            "Loss S33:  0.0890327183211722\n",
            "Loss S44:  0.08963691861164279\n",
            "Loss S11:  0.07750667631626129\n",
            "Loss S22:  0.08140386977032119\n",
            "Loss S33:  0.08856297430454516\n",
            "Loss S44:  0.0897428023756719\n",
            "Loss S11:  0.07750659228348342\n",
            "Loss S22:  0.08115601612896216\n",
            "Loss S33:  0.08855815769219008\n",
            "Loss S44:  0.09075025969841441\n",
            "Loss S11:  0.07701954696799668\n",
            "Loss S22:  0.0808718934865065\n",
            "Loss S33:  0.08838394798443351\n",
            "Loss S44:  0.0909231843453058\n",
            "Loss S11:  0.07701027549711274\n",
            "Loss S22:  0.08077928810207932\n",
            "Loss S33:  0.0886347003189134\n",
            "Loss S44:  0.09104888693050102\n",
            "Loss S11:  0.07709315018011974\n",
            "Loss S22:  0.08064692467451096\n",
            "Loss S33:  0.08881184254046325\n",
            "Loss S44:  0.09143546035329064\n",
            "Loss S11:  0.07691334977303402\n",
            "Loss S22:  0.08053131873654847\n",
            "Loss S33:  0.08854276008240067\n",
            "Loss S44:  0.09131441759591055\n",
            "Loss S11:  0.0767956482129054\n",
            "Loss S22:  0.08036432348124616\n",
            "Loss S33:  0.0885948256866352\n",
            "Loss S44:  0.09126764361385827\n",
            "Loss S11:  0.07679828207108601\n",
            "Loss S22:  0.0803051231936975\n",
            "Loss S33:  0.08875448989474084\n",
            "Loss S44:  0.09134739988352641\n",
            "Loss S11:  0.0770467466866697\n",
            "Loss S22:  0.08039860330692684\n",
            "Loss S33:  0.08892227288420873\n",
            "Loss S44:  0.0918611188540022\n",
            "Loss S11:  0.0770529492738399\n",
            "Loss S22:  0.08040483966998174\n",
            "Loss S33:  0.08900525304653965\n",
            "Loss S44:  0.09205523714528861\n",
            "Loss S11:  0.07724759308312902\n",
            "Loss S22:  0.08044692691390877\n",
            "Loss S33:  0.08906865253156385\n",
            "Loss S44:  0.09207414243592332\n",
            "Loss S11:  0.07704595916019463\n",
            "Loss S22:  0.08006776857727803\n",
            "Loss S33:  0.0887951439952258\n",
            "Loss S44:  0.09184279302077264\n",
            "Loss S11:  0.07686928045331386\n",
            "Loss S22:  0.08001610577890747\n",
            "Loss S33:  0.08841358155709261\n",
            "Loss S44:  0.09162852647360305\n",
            "Loss S11:  0.076937402480215\n",
            "Loss S22:  0.08013562152151904\n",
            "Loss S33:  0.08841393896229359\n",
            "Loss S44:  0.09168970658963556\n",
            "Loss S11:  0.07700609283141441\n",
            "Loss S22:  0.08028555238200108\n",
            "Loss S33:  0.08839893333262798\n",
            "Loss S44:  0.09172034314321598\n",
            "Loss S11:  0.07711431762175773\n",
            "Loss S22:  0.08029578168371423\n",
            "Loss S33:  0.0884166513361148\n",
            "Loss S44:  0.09188016074065546\n",
            "Loss S11:  0.07708948772947935\n",
            "Loss S22:  0.08031752099993669\n",
            "Loss S33:  0.08840932478130711\n",
            "Loss S44:  0.09178623045084036\n",
            "Loss S11:  0.07721937882684475\n",
            "Loss S22:  0.08048475407299953\n",
            "Loss S33:  0.08859870235574731\n",
            "Loss S44:  0.09208169775041519\n",
            "Loss S11:  0.07726784308493395\n",
            "Loss S22:  0.08067548428198476\n",
            "Loss S33:  0.08871957288805024\n",
            "Loss S44:  0.0920884647281655\n",
            "Loss S11:  0.07732083645351695\n",
            "Loss S22:  0.08081089298692977\n",
            "Loss S33:  0.08893894738057836\n",
            "Loss S44:  0.09223659766289208\n",
            "Loss S11:  0.0773108153496368\n",
            "Loss S22:  0.08072603302469766\n",
            "Loss S33:  0.08893068268360844\n",
            "Loss S44:  0.09212493047533757\n",
            "Loss S11:  0.07735989238510187\n",
            "Loss S22:  0.08085175602200845\n",
            "Loss S33:  0.08885186122751784\n",
            "Loss S44:  0.09209714755701379\n",
            "Loss S11:  0.07737202817278595\n",
            "Loss S22:  0.08102736793925841\n",
            "Loss S33:  0.08893346253136428\n",
            "Loss S44:  0.09215189318595338\n",
            "Loss S11:  0.07739580959558912\n",
            "Loss S22:  0.08106539760354999\n",
            "Loss S33:  0.08892923002247284\n",
            "Loss S44:  0.09219859203728069\n",
            "Loss S11:  0.07741934986612231\n",
            "Loss S22:  0.08108447519006189\n",
            "Loss S33:  0.0889006633682759\n",
            "Loss S44:  0.09218033016547304\n",
            "Loss S11:  0.0774631813242784\n",
            "Loss S22:  0.08114935921820692\n",
            "Loss S33:  0.08884239285887278\n",
            "Loss S44:  0.0921877518890308\n",
            "Loss S11:  0.07743905737421138\n",
            "Loss S22:  0.081160416832595\n",
            "Loss S33:  0.08881602901066998\n",
            "Loss S44:  0.09223017625965872\n",
            "Loss S11:  0.07739332718687637\n",
            "Loss S22:  0.08118052914088761\n",
            "Loss S33:  0.0887592428384169\n",
            "Loss S44:  0.0922213216947618\n",
            "Loss S11:  0.07728316748565417\n",
            "Loss S22:  0.0810337225228458\n",
            "Loss S33:  0.08862626797933233\n",
            "Loss S44:  0.09208964599223296\n",
            "Loss S11:  0.07733866464226477\n",
            "Loss S22:  0.08110015975895865\n",
            "Loss S33:  0.08859154171258474\n",
            "Loss S44:  0.0921110785252188\n",
            "Loss S11:  0.07736885741862476\n",
            "Loss S22:  0.08107972318078378\n",
            "Loss S33:  0.0886040710528352\n",
            "Loss S44:  0.0920659529729786\n",
            "Loss S11:  0.07739844015306713\n",
            "Loss S22:  0.08113611176857657\n",
            "Loss S33:  0.08860362015685216\n",
            "Loss S44:  0.09213083282814792\n",
            "Loss S11:  0.07741905852268648\n",
            "Loss S22:  0.08110511040872319\n",
            "Loss S33:  0.08846285684169784\n",
            "Loss S44:  0.09210030806835771\n",
            "Loss S11:  0.07727313277328735\n",
            "Loss S22:  0.08101768166882785\n",
            "Loss S33:  0.08830445187454775\n",
            "Loss S44:  0.09211563987760094\n",
            "Loss S11:  0.07719822465192022\n",
            "Loss S22:  0.08097276214481619\n",
            "Loss S33:  0.08822981010922386\n",
            "Loss S44:  0.09203397877076093\n",
            "Loss S11:  0.07727665681754264\n",
            "Loss S22:  0.08102036156967989\n",
            "Loss S33:  0.08826089130152491\n",
            "Loss S44:  0.09200680967503949\n",
            "Loss S11:  0.07725334826430845\n",
            "Loss S22:  0.08103607936678432\n",
            "Loss S33:  0.08823753746539137\n",
            "Loss S44:  0.09203304805387255\n",
            "Loss S11:  0.07728873523319418\n",
            "Loss S22:  0.08105805594996149\n",
            "Loss S33:  0.08824918199675666\n",
            "Loss S44:  0.09207384490060692\n",
            "Loss S11:  0.07729885786806348\n",
            "Loss S22:  0.08107874026465858\n",
            "Loss S33:  0.08826903490621917\n",
            "Loss S44:  0.09211265848919299\n",
            "Loss S11:  0.07727014025897126\n",
            "Loss S22:  0.08104296750864204\n",
            "Loss S33:  0.08831285345716541\n",
            "Loss S44:  0.09206202665086236\n",
            "Loss S11:  0.07725821571742617\n",
            "Loss S22:  0.08104003753570918\n",
            "Loss S33:  0.08830647198627899\n",
            "Loss S44:  0.09204248434754539\n",
            "Loss S11:  0.07727032616217328\n",
            "Loss S22:  0.08105434744806714\n",
            "Loss S33:  0.08830340251876062\n",
            "Loss S44:  0.09203736191522532\n",
            "Loss S11:  0.07722219237128387\n",
            "Loss S22:  0.08098824323014596\n",
            "Loss S33:  0.08830129750658246\n",
            "Loss S44:  0.09205483256062125\n",
            "Loss S11:  0.07723444629762624\n",
            "Loss S22:  0.08093320143848595\n",
            "Loss S33:  0.08833365088131225\n",
            "Loss S44:  0.09209198735483967\n",
            "Loss S11:  0.07721299179611527\n",
            "Loss S22:  0.08090977785756048\n",
            "Loss S33:  0.08835041503253148\n",
            "Loss S44:  0.09203751412525682\n",
            "Validation: \n",
            " Loss S11:  0.09081800282001495\n",
            " Loss S22:  0.11731931567192078\n",
            " Loss S33:  0.12223067879676819\n",
            " Loss S44:  0.12195989489555359\n",
            " Loss S11:  0.08901576910700117\n",
            " Loss S22:  0.12018291510286785\n",
            " Loss S33:  0.12446886939661843\n",
            " Loss S44:  0.13143942824431828\n",
            " Loss S11:  0.08741820349199016\n",
            " Loss S22:  0.11962943578638681\n",
            " Loss S33:  0.12469091375426548\n",
            " Loss S44:  0.12994243168249364\n",
            " Loss S11:  0.08649993968791649\n",
            " Loss S22:  0.1182143721179884\n",
            " Loss S33:  0.12208233627139545\n",
            " Loss S44:  0.12829393820195903\n",
            " Loss S11:  0.08600364515074978\n",
            " Loss S22:  0.11822894997434852\n",
            " Loss S33:  0.12135223538419347\n",
            " Loss S44:  0.12771238653380193\n",
            "\n",
            "Epoch: 48\n",
            "Loss S11:  0.08767010271549225\n",
            "Loss S22:  0.07476013898849487\n",
            "Loss S33:  0.08329396694898605\n",
            "Loss S44:  0.08608434349298477\n",
            "Loss S11:  0.07890782369808717\n",
            "Loss S22:  0.07988578081130981\n",
            "Loss S33:  0.08916159245100888\n",
            "Loss S44:  0.09409261359409853\n",
            "Loss S11:  0.07706263554947716\n",
            "Loss S22:  0.07979013557944979\n",
            "Loss S33:  0.08895493653558549\n",
            "Loss S44:  0.09265305740492684\n",
            "Loss S11:  0.07655822798129051\n",
            "Loss S22:  0.07857176276945299\n",
            "Loss S33:  0.08772253365285936\n",
            "Loss S44:  0.09193239865764495\n",
            "Loss S11:  0.07663445738030643\n",
            "Loss S22:  0.07930970264644158\n",
            "Loss S33:  0.08867288544410612\n",
            "Loss S44:  0.09115691137749975\n",
            "Loss S11:  0.07690504310177822\n",
            "Loss S22:  0.07987617116932776\n",
            "Loss S33:  0.08803109474041883\n",
            "Loss S44:  0.09024884157321032\n",
            "Loss S11:  0.07689934040679307\n",
            "Loss S22:  0.08012026597241886\n",
            "Loss S33:  0.08786786359841706\n",
            "Loss S44:  0.09071561181154407\n",
            "Loss S11:  0.07653840746678098\n",
            "Loss S22:  0.07981607538293785\n",
            "Loss S33:  0.08779121536604116\n",
            "Loss S44:  0.0903249174146585\n",
            "Loss S11:  0.07645956647616846\n",
            "Loss S22:  0.07975884839708422\n",
            "Loss S33:  0.08777532735724508\n",
            "Loss S44:  0.0904668915418931\n",
            "Loss S11:  0.07661068300296972\n",
            "Loss S22:  0.0795318311551115\n",
            "Loss S33:  0.08802203007124282\n",
            "Loss S44:  0.09054299681396275\n",
            "Loss S11:  0.07643249387493228\n",
            "Loss S22:  0.07941966171902005\n",
            "Loss S33:  0.08770137494153316\n",
            "Loss S44:  0.09042580263449414\n",
            "Loss S11:  0.07622400765214954\n",
            "Loss S22:  0.07929644236962001\n",
            "Loss S33:  0.08738903889247963\n",
            "Loss S44:  0.09055772866751696\n",
            "Loss S11:  0.0761475759473714\n",
            "Loss S22:  0.07931799632458647\n",
            "Loss S33:  0.0874318795755875\n",
            "Loss S44:  0.09078741030505866\n",
            "Loss S11:  0.07647885427675175\n",
            "Loss S22:  0.07966884458793029\n",
            "Loss S33:  0.08794313345019145\n",
            "Loss S44:  0.09086294045657602\n",
            "Loss S11:  0.07661661156948577\n",
            "Loss S22:  0.0796917735579166\n",
            "Loss S33:  0.08797068481749677\n",
            "Loss S44:  0.09099164964459466\n",
            "Loss S11:  0.07676208532409162\n",
            "Loss S22:  0.07981038483365482\n",
            "Loss S33:  0.0879092233761257\n",
            "Loss S44:  0.0908936042067231\n",
            "Loss S11:  0.07660490341530823\n",
            "Loss S22:  0.07957687682431677\n",
            "Loss S33:  0.08773817057194917\n",
            "Loss S44:  0.09084857222826584\n",
            "Loss S11:  0.0764102995700655\n",
            "Loss S22:  0.07941435551957081\n",
            "Loss S33:  0.08770419060312516\n",
            "Loss S44:  0.09060518055805686\n",
            "Loss S11:  0.07643788268799939\n",
            "Loss S22:  0.07940355655402768\n",
            "Loss S33:  0.08793990968505322\n",
            "Loss S44:  0.0906283046165224\n",
            "Loss S11:  0.07662218315673124\n",
            "Loss S22:  0.07944267040303864\n",
            "Loss S33:  0.08800818700908991\n",
            "Loss S44:  0.09092759227877512\n",
            "Loss S11:  0.07657398474379558\n",
            "Loss S22:  0.07953156942307059\n",
            "Loss S33:  0.08800316505615983\n",
            "Loss S44:  0.09101899450098104\n",
            "Loss S11:  0.07655855386531184\n",
            "Loss S22:  0.07969475887115533\n",
            "Loss S33:  0.08798231135047442\n",
            "Loss S44:  0.09108369617383062\n",
            "Loss S11:  0.07665600875451554\n",
            "Loss S22:  0.07984320984571767\n",
            "Loss S33:  0.08794910757385228\n",
            "Loss S44:  0.09125815787061847\n",
            "Loss S11:  0.07689405495534728\n",
            "Loss S22:  0.07992599127225546\n",
            "Loss S33:  0.0879372972108069\n",
            "Loss S44:  0.0913159338297782\n",
            "Loss S11:  0.07706665561468769\n",
            "Loss S22:  0.07990199398573998\n",
            "Loss S33:  0.08800754068550727\n",
            "Loss S44:  0.09144041136958292\n",
            "Loss S11:  0.07707991974940338\n",
            "Loss S22:  0.0799290682986913\n",
            "Loss S33:  0.0880551558448024\n",
            "Loss S44:  0.09134956113845703\n",
            "Loss S11:  0.07705618394003518\n",
            "Loss S22:  0.07990385797517054\n",
            "Loss S33:  0.08799614340523651\n",
            "Loss S44:  0.09134771714836244\n",
            "Loss S11:  0.0771150686726579\n",
            "Loss S22:  0.08010423507857586\n",
            "Loss S33:  0.08817445434547438\n",
            "Loss S44:  0.09144072935699976\n",
            "Loss S11:  0.077110295419273\n",
            "Loss S22:  0.08018207247783281\n",
            "Loss S33:  0.08811926616362406\n",
            "Loss S44:  0.09149912832258435\n",
            "Loss S11:  0.0771214449298136\n",
            "Loss S22:  0.08023099439967539\n",
            "Loss S33:  0.08807408750774115\n",
            "Loss S44:  0.09160832525630996\n",
            "Loss S11:  0.0771649286175685\n",
            "Loss S22:  0.08020930292944972\n",
            "Loss S33:  0.08804742175083224\n",
            "Loss S44:  0.09164030859834728\n",
            "Loss S11:  0.07713643862743086\n",
            "Loss S22:  0.08028150597808828\n",
            "Loss S33:  0.08804405348860565\n",
            "Loss S44:  0.09160948884927958\n",
            "Loss S11:  0.0770732710410687\n",
            "Loss S22:  0.08033244053012115\n",
            "Loss S33:  0.08811089568234678\n",
            "Loss S44:  0.09154163695570093\n",
            "Loss S11:  0.07697125986848713\n",
            "Loss S22:  0.08025319837146296\n",
            "Loss S33:  0.08799512810397364\n",
            "Loss S44:  0.09151335834916625\n",
            "Loss S11:  0.07701224882887606\n",
            "Loss S22:  0.08025001665670152\n",
            "Loss S33:  0.0880278036479027\n",
            "Loss S44:  0.09156062888784493\n",
            "Loss S11:  0.07701594651042566\n",
            "Loss S22:  0.08028608655021062\n",
            "Loss S33:  0.08801138407399511\n",
            "Loss S44:  0.09154504911172763\n",
            "Loss S11:  0.07701488715335933\n",
            "Loss S22:  0.08029996860679497\n",
            "Loss S33:  0.08802875455396658\n",
            "Loss S44:  0.091495461039596\n",
            "Loss S11:  0.07698457952177107\n",
            "Loss S22:  0.08025267803564547\n",
            "Loss S33:  0.08796218210314162\n",
            "Loss S44:  0.09151094199994182\n",
            "Loss S11:  0.07687115034680041\n",
            "Loss S22:  0.08013046909309435\n",
            "Loss S33:  0.08786432656246847\n",
            "Loss S44:  0.09137866755561252\n",
            "Loss S11:  0.07676537429242183\n",
            "Loss S22:  0.08007192337299551\n",
            "Loss S33:  0.08774260442961207\n",
            "Loss S44:  0.09125541464980606\n",
            "Loss S11:  0.07684706789708494\n",
            "Loss S22:  0.0801190411732381\n",
            "Loss S33:  0.08777007523766182\n",
            "Loss S44:  0.09126052957148921\n",
            "Loss S11:  0.07674480239586529\n",
            "Loss S22:  0.08005393140579953\n",
            "Loss S33:  0.0878095028609255\n",
            "Loss S44:  0.09119716942890427\n",
            "Loss S11:  0.07673379036458541\n",
            "Loss S22:  0.0800861379490046\n",
            "Loss S33:  0.08776308960699412\n",
            "Loss S44:  0.09121565442895097\n",
            "Loss S11:  0.07671512442958604\n",
            "Loss S22:  0.08005121801472043\n",
            "Loss S33:  0.08766190728427639\n",
            "Loss S44:  0.09107109171331897\n",
            "Loss S11:  0.07668161458946139\n",
            "Loss S22:  0.08005548777088287\n",
            "Loss S33:  0.08768608966787386\n",
            "Loss S44:  0.09107052332081762\n",
            "Loss S11:  0.0766944631091639\n",
            "Loss S22:  0.08005342465678762\n",
            "Loss S33:  0.08765260450657614\n",
            "Loss S44:  0.09109773944460367\n",
            "Loss S11:  0.07672011360138202\n",
            "Loss S22:  0.08007350352283672\n",
            "Loss S33:  0.08769723026328387\n",
            "Loss S44:  0.09117103964413582\n",
            "Loss S11:  0.07667802417385856\n",
            "Loss S22:  0.07998336081228945\n",
            "Loss S33:  0.08767753065961181\n",
            "Loss S44:  0.09109880792613748\n",
            "Loss S11:  0.07671676471581577\n",
            "Loss S22:  0.07991547470528966\n",
            "Loss S33:  0.08762097389683159\n",
            "Loss S44:  0.09111093119437383\n",
            "Loss S11:  0.0766429387093683\n",
            "Loss S22:  0.07987902718624612\n",
            "Loss S33:  0.08755278816473216\n",
            "Loss S44:  0.09108049756337086\n",
            "Validation: \n",
            " Loss S11:  0.08595001697540283\n",
            " Loss S22:  0.11633237451314926\n",
            " Loss S33:  0.11894526332616806\n",
            " Loss S44:  0.1277245730161667\n",
            " Loss S11:  0.08735378476835433\n",
            " Loss S22:  0.12118308991193771\n",
            " Loss S33:  0.12305108244929995\n",
            " Loss S44:  0.13591932824679784\n",
            " Loss S11:  0.08624332816135592\n",
            " Loss S22:  0.12102967682408124\n",
            " Loss S33:  0.12341901242006116\n",
            " Loss S44:  0.13452804106764676\n",
            " Loss S11:  0.08543993520443557\n",
            " Loss S22:  0.1197174880348268\n",
            " Loss S33:  0.1210593896322563\n",
            " Loss S44:  0.1324699302432967\n",
            " Loss S11:  0.08512167071486697\n",
            " Loss S22:  0.11976967365653426\n",
            " Loss S33:  0.12036910119615955\n",
            " Loss S44:  0.1315612539097115\n",
            "\n",
            "Epoch: 49\n",
            "Loss S11:  0.07246082276105881\n",
            "Loss S22:  0.0732329934835434\n",
            "Loss S33:  0.09181521087884903\n",
            "Loss S44:  0.09870581328868866\n",
            "Loss S11:  0.07859148559245197\n",
            "Loss S22:  0.07972459291869943\n",
            "Loss S33:  0.08926191451874646\n",
            "Loss S44:  0.09312129291621121\n",
            "Loss S11:  0.07868983915873937\n",
            "Loss S22:  0.07980712432236899\n",
            "Loss S33:  0.0897685917360442\n",
            "Loss S44:  0.09123654415210088\n",
            "Loss S11:  0.07812550567811535\n",
            "Loss S22:  0.07899045295292331\n",
            "Loss S33:  0.08807606370218339\n",
            "Loss S44:  0.08944952271638378\n",
            "Loss S11:  0.0770044348588804\n",
            "Loss S22:  0.07899982845637857\n",
            "Loss S33:  0.08790600118113727\n",
            "Loss S44:  0.08959050985371195\n",
            "Loss S11:  0.07710373080244251\n",
            "Loss S22:  0.07985693128669963\n",
            "Loss S33:  0.08781549232263192\n",
            "Loss S44:  0.08971654959753447\n",
            "Loss S11:  0.0767628844888484\n",
            "Loss S22:  0.07996136279868298\n",
            "Loss S33:  0.08741598845016761\n",
            "Loss S44:  0.09025567086016545\n",
            "Loss S11:  0.07636980555007156\n",
            "Loss S22:  0.07984969271740443\n",
            "Loss S33:  0.08695108558930142\n",
            "Loss S44:  0.09006345335026862\n",
            "Loss S11:  0.0763157758815789\n",
            "Loss S22:  0.0797668179428136\n",
            "Loss S33:  0.08683341548398689\n",
            "Loss S44:  0.09020784377683828\n",
            "Loss S11:  0.07641026926237149\n",
            "Loss S22:  0.07969991535275847\n",
            "Loss S33:  0.0868809251012383\n",
            "Loss S44:  0.09048859619504802\n",
            "Loss S11:  0.07625624043221521\n",
            "Loss S22:  0.07965442759565788\n",
            "Loss S33:  0.08660458611084683\n",
            "Loss S44:  0.0900731677613636\n",
            "Loss S11:  0.0761308922088361\n",
            "Loss S22:  0.07935084496532474\n",
            "Loss S33:  0.08654638621452693\n",
            "Loss S44:  0.08994548794654039\n",
            "Loss S11:  0.07592046017612307\n",
            "Loss S22:  0.07922354993248774\n",
            "Loss S33:  0.08628237173576986\n",
            "Loss S44:  0.08969068539536688\n",
            "Loss S11:  0.07605278324193628\n",
            "Loss S22:  0.07944100008893559\n",
            "Loss S33:  0.08642895423046505\n",
            "Loss S44:  0.08987427292434313\n",
            "Loss S11:  0.07591393677161094\n",
            "Loss S22:  0.07941928310385833\n",
            "Loss S33:  0.08645086127815517\n",
            "Loss S44:  0.08985020495052878\n",
            "Loss S11:  0.0761157495049846\n",
            "Loss S22:  0.07942218219997077\n",
            "Loss S33:  0.08649082501597752\n",
            "Loss S44:  0.08995204068572316\n",
            "Loss S11:  0.07606841804263015\n",
            "Loss S22:  0.07921935958058937\n",
            "Loss S33:  0.08619029491399385\n",
            "Loss S44:  0.0897710502425336\n",
            "Loss S11:  0.0758518992721686\n",
            "Loss S22:  0.07911798333999706\n",
            "Loss S33:  0.08604787551520164\n",
            "Loss S44:  0.08959377918675629\n",
            "Loss S11:  0.07589045747373645\n",
            "Loss S22:  0.07911046110711045\n",
            "Loss S33:  0.08611302065421204\n",
            "Loss S44:  0.08973916500642155\n",
            "Loss S11:  0.07592882871315741\n",
            "Loss S22:  0.07915461541236384\n",
            "Loss S33:  0.08613040286521013\n",
            "Loss S44:  0.08963701682877166\n",
            "Loss S11:  0.075848372458522\n",
            "Loss S22:  0.07918738327290288\n",
            "Loss S33:  0.08621253896115431\n",
            "Loss S44:  0.08985722213242184\n",
            "Loss S11:  0.07587994197251108\n",
            "Loss S22:  0.07923245703651442\n",
            "Loss S33:  0.08624701540899503\n",
            "Loss S44:  0.08982379745109387\n",
            "Loss S11:  0.0760240047428403\n",
            "Loss S22:  0.07941044669817476\n",
            "Loss S33:  0.08640651283490712\n",
            "Loss S44:  0.09017095030432913\n",
            "Loss S11:  0.07601968960199522\n",
            "Loss S22:  0.07940792564899374\n",
            "Loss S33:  0.08633107153368202\n",
            "Loss S44:  0.09010930049729038\n",
            "Loss S11:  0.07612960875281655\n",
            "Loss S22:  0.0794937515252606\n",
            "Loss S33:  0.08652147418854148\n",
            "Loss S44:  0.09034514050018738\n",
            "Loss S11:  0.07608777605204943\n",
            "Loss S22:  0.0794919883142192\n",
            "Loss S33:  0.08641526836204338\n",
            "Loss S44:  0.09031564098192876\n",
            "Loss S11:  0.07608287604490002\n",
            "Loss S22:  0.07948498092449031\n",
            "Loss S33:  0.08643620227145966\n",
            "Loss S44:  0.09034464916506946\n",
            "Loss S11:  0.07619111270152334\n",
            "Loss S22:  0.07965618862712075\n",
            "Loss S33:  0.08653548698033794\n",
            "Loss S44:  0.09042982636122686\n",
            "Loss S11:  0.07624042858750794\n",
            "Loss S22:  0.07969274882266954\n",
            "Loss S33:  0.0865483521036406\n",
            "Loss S44:  0.09036133048585301\n",
            "Loss S11:  0.0762592297034575\n",
            "Loss S22:  0.07971934650310945\n",
            "Loss S33:  0.08648759490231059\n",
            "Loss S44:  0.09042521302437864\n",
            "Loss S11:  0.07624403167008562\n",
            "Loss S22:  0.07977916841144578\n",
            "Loss S33:  0.08647292067739258\n",
            "Loss S44:  0.09049980114662766\n",
            "Loss S11:  0.07610344273484405\n",
            "Loss S22:  0.07974936378299231\n",
            "Loss S33:  0.08641784415367715\n",
            "Loss S44:  0.09052101922763506\n",
            "Loss S11:  0.07620266551076438\n",
            "Loss S22:  0.07976578225924219\n",
            "Loss S33:  0.0864555649111204\n",
            "Loss S44:  0.09049994700422911\n",
            "Loss S11:  0.07615858523507132\n",
            "Loss S22:  0.07962861057269609\n",
            "Loss S33:  0.08631299456350754\n",
            "Loss S44:  0.09046039025769133\n",
            "Loss S11:  0.07623370878857252\n",
            "Loss S22:  0.07971480590375987\n",
            "Loss S33:  0.08635544604307745\n",
            "Loss S44:  0.09051763226996419\n",
            "Loss S11:  0.07630857521737064\n",
            "Loss S22:  0.07974198468008273\n",
            "Loss S33:  0.0864488195489954\n",
            "Loss S44:  0.09061422738169672\n",
            "Loss S11:  0.07625986813178023\n",
            "Loss S22:  0.07963945347186271\n",
            "Loss S33:  0.08645408059834113\n",
            "Loss S44:  0.09066599981028618\n",
            "Loss S11:  0.07625420299180434\n",
            "Loss S22:  0.0796324117485725\n",
            "Loss S33:  0.08647879401869529\n",
            "Loss S44:  0.09067425312539638\n",
            "Loss S11:  0.07624088083039432\n",
            "Loss S22:  0.07955125533533222\n",
            "Loss S33:  0.0864595329276533\n",
            "Loss S44:  0.0906638726005404\n",
            "Loss S11:  0.07619427565647208\n",
            "Loss S22:  0.07948008179664612\n",
            "Loss S33:  0.08640404708702545\n",
            "Loss S44:  0.09055923190339447\n",
            "Loss S11:  0.07629707507659075\n",
            "Loss S22:  0.07950640486511507\n",
            "Loss S33:  0.08643268890437343\n",
            "Loss S44:  0.09062984152847989\n",
            "Loss S11:  0.07626259169221794\n",
            "Loss S22:  0.07948784446774318\n",
            "Loss S33:  0.0864419043970514\n",
            "Loss S44:  0.090628167827344\n",
            "Loss S11:  0.07624007278341012\n",
            "Loss S22:  0.07954027346222531\n",
            "Loss S33:  0.08649190478115354\n",
            "Loss S44:  0.09070764871362835\n",
            "Loss S11:  0.07621782717323082\n",
            "Loss S22:  0.07948249590576663\n",
            "Loss S33:  0.08646797207281373\n",
            "Loss S44:  0.09072272868485573\n",
            "Loss S11:  0.07622921568195835\n",
            "Loss S22:  0.07949486721197223\n",
            "Loss S33:  0.08648268513533534\n",
            "Loss S44:  0.0907711340895856\n",
            "Loss S11:  0.07624155354010821\n",
            "Loss S22:  0.07956444883425855\n",
            "Loss S33:  0.0864915189450966\n",
            "Loss S44:  0.09077654123108032\n",
            "Loss S11:  0.07618796710660297\n",
            "Loss S22:  0.07954127115243945\n",
            "Loss S33:  0.08648748545545817\n",
            "Loss S44:  0.09078875979218722\n",
            "Loss S11:  0.07612706543660215\n",
            "Loss S22:  0.07954177075320748\n",
            "Loss S33:  0.08648168751198775\n",
            "Loss S44:  0.09068761522349784\n",
            "Loss S11:  0.0761198874198969\n",
            "Loss S22:  0.07951903024111369\n",
            "Loss S33:  0.08647383648801495\n",
            "Loss S44:  0.09075013091187467\n",
            "Loss S11:  0.07607738019674713\n",
            "Loss S22:  0.07945063344512113\n",
            "Loss S33:  0.08647539176790389\n",
            "Loss S44:  0.09067638464226499\n",
            "Validation: \n",
            " Loss S11:  0.08675861358642578\n",
            " Loss S22:  0.11139651387929916\n",
            " Loss S33:  0.11602137982845306\n",
            " Loss S44:  0.1291133612394333\n",
            " Loss S11:  0.08713043587548393\n",
            " Loss S22:  0.12245829509837287\n",
            " Loss S33:  0.12536479177929105\n",
            " Loss S44:  0.13474808349495843\n",
            " Loss S11:  0.08555099804226945\n",
            " Loss S22:  0.12156111020140531\n",
            " Loss S33:  0.12523988598003621\n",
            " Loss S44:  0.1336073828179662\n",
            " Loss S11:  0.08480117201316552\n",
            " Loss S22:  0.11999845541402941\n",
            " Loss S33:  0.12310046580482702\n",
            " Loss S44:  0.13163246690738398\n",
            " Loss S11:  0.08449256456928489\n",
            " Loss S22:  0.12012189323151554\n",
            " Loss S33:  0.12230677967086251\n",
            " Loss S44:  0.13072784263410686\n",
            "\n",
            "Epoch: 50\n",
            "Loss S11:  0.07166441529989243\n",
            "Loss S22:  0.08043834567070007\n",
            "Loss S33:  0.08614683151245117\n",
            "Loss S44:  0.08671518415212631\n",
            "Loss S11:  0.07471245391802354\n",
            "Loss S22:  0.07944706759669563\n",
            "Loss S33:  0.08988881856203079\n",
            "Loss S44:  0.08917989446358247\n",
            "Loss S11:  0.0766321410025869\n",
            "Loss S22:  0.07973211613439378\n",
            "Loss S33:  0.08843085169792175\n",
            "Loss S44:  0.08978312391610373\n",
            "Loss S11:  0.07597961060462459\n",
            "Loss S22:  0.07907483702705752\n",
            "Loss S33:  0.08718200027942657\n",
            "Loss S44:  0.0892725808005179\n",
            "Loss S11:  0.07542912716545709\n",
            "Loss S22:  0.07908552903227689\n",
            "Loss S33:  0.08730815532730847\n",
            "Loss S44:  0.0894307058031966\n",
            "Loss S11:  0.07515455037355423\n",
            "Loss S22:  0.0794885257879893\n",
            "Loss S33:  0.08746157907972149\n",
            "Loss S44:  0.08955631127544478\n",
            "Loss S11:  0.07544710401628839\n",
            "Loss S22:  0.07904913056580747\n",
            "Loss S33:  0.08720776218859876\n",
            "Loss S44:  0.09043198982711698\n",
            "Loss S11:  0.07522718860229975\n",
            "Loss S22:  0.07849563595274804\n",
            "Loss S33:  0.08720143401706723\n",
            "Loss S44:  0.09041353178695893\n",
            "Loss S11:  0.07571403866196856\n",
            "Loss S22:  0.07859849948206066\n",
            "Loss S33:  0.08703467461429996\n",
            "Loss S44:  0.09022696279449227\n",
            "Loss S11:  0.07590852268449552\n",
            "Loss S22:  0.07875541507542788\n",
            "Loss S33:  0.08721503672691491\n",
            "Loss S44:  0.09050579426380304\n",
            "Loss S11:  0.07582812254676724\n",
            "Loss S22:  0.07853271421229485\n",
            "Loss S33:  0.08677144400259056\n",
            "Loss S44:  0.09013002683030497\n",
            "Loss S11:  0.07564085144717414\n",
            "Loss S22:  0.07821043493511441\n",
            "Loss S33:  0.0865033495802063\n",
            "Loss S44:  0.08995155083972055\n",
            "Loss S11:  0.07551637532050945\n",
            "Loss S22:  0.07813713471751568\n",
            "Loss S33:  0.08627182393034627\n",
            "Loss S44:  0.08954143598059977\n",
            "Loss S11:  0.07551815918156209\n",
            "Loss S22:  0.07832314984034036\n",
            "Loss S33:  0.08652575575668393\n",
            "Loss S44:  0.08963786031453665\n",
            "Loss S11:  0.0754123403565258\n",
            "Loss S22:  0.07845834098386427\n",
            "Loss S33:  0.08642554859108958\n",
            "Loss S44:  0.08953203329591887\n",
            "Loss S11:  0.07565710738005228\n",
            "Loss S22:  0.07867946460941769\n",
            "Loss S33:  0.08653172643374134\n",
            "Loss S44:  0.08968557871335389\n",
            "Loss S11:  0.07559930470696888\n",
            "Loss S22:  0.078496454433995\n",
            "Loss S33:  0.08627639613721681\n",
            "Loss S44:  0.08962158650530051\n",
            "Loss S11:  0.07549991325763931\n",
            "Loss S22:  0.07844699069596174\n",
            "Loss S33:  0.08609525353936424\n",
            "Loss S44:  0.08958777393165387\n",
            "Loss S11:  0.07542910021395315\n",
            "Loss S22:  0.07838593925560378\n",
            "Loss S33:  0.08607592573646683\n",
            "Loss S44:  0.08967670189082952\n",
            "Loss S11:  0.07545849912999812\n",
            "Loss S22:  0.07834241695272985\n",
            "Loss S33:  0.08598998325501436\n",
            "Loss S44:  0.08976694375900698\n",
            "Loss S11:  0.07544713358010226\n",
            "Loss S22:  0.07835826173943666\n",
            "Loss S33:  0.08597310943834817\n",
            "Loss S44:  0.08975613547201773\n",
            "Loss S11:  0.0754823805631902\n",
            "Loss S22:  0.07850884187136781\n",
            "Loss S33:  0.08607463358561575\n",
            "Loss S44:  0.08982851497511163\n",
            "Loss S11:  0.07566033521186712\n",
            "Loss S22:  0.07867926709792193\n",
            "Loss S33:  0.0861558837318852\n",
            "Loss S44:  0.09005802981049767\n",
            "Loss S11:  0.07566812058741396\n",
            "Loss S22:  0.0787569179292365\n",
            "Loss S33:  0.08617472832466101\n",
            "Loss S44:  0.0900633820207604\n",
            "Loss S11:  0.07577427858448127\n",
            "Loss S22:  0.07879466251218963\n",
            "Loss S33:  0.08625352302280205\n",
            "Loss S44:  0.09009435482292255\n",
            "Loss S11:  0.07583495063254558\n",
            "Loss S22:  0.0787985312986184\n",
            "Loss S33:  0.08628159354763677\n",
            "Loss S44:  0.09004324291688037\n",
            "Loss S11:  0.07588169224189159\n",
            "Loss S22:  0.07887066626685789\n",
            "Loss S33:  0.08639677639665275\n",
            "Loss S44:  0.09005621717921619\n",
            "Loss S11:  0.07592361802544541\n",
            "Loss S22:  0.0789371156769485\n",
            "Loss S33:  0.08640815270892808\n",
            "Loss S44:  0.09009783943200904\n",
            "Loss S11:  0.07610947933061267\n",
            "Loss S22:  0.07909542385793665\n",
            "Loss S33:  0.08641364961014099\n",
            "Loss S44:  0.09012590597957054\n",
            "Loss S11:  0.07615916195399162\n",
            "Loss S22:  0.07912669768652965\n",
            "Loss S33:  0.08662902469077881\n",
            "Loss S44:  0.09025544527265214\n",
            "Loss S11:  0.07619319354923461\n",
            "Loss S22:  0.0791869793965967\n",
            "Loss S33:  0.08676543081816644\n",
            "Loss S44:  0.09028174720729308\n",
            "Loss S11:  0.07605643109253749\n",
            "Loss S22:  0.07909912922662171\n",
            "Loss S33:  0.08669548177450799\n",
            "Loss S44:  0.09017982741165008\n",
            "Loss S11:  0.07612254555305216\n",
            "Loss S22:  0.07917880129962696\n",
            "Loss S33:  0.08669697927351681\n",
            "Loss S44:  0.09020211879523744\n",
            "Loss S11:  0.07601121658870101\n",
            "Loss S22:  0.07911760418526716\n",
            "Loss S33:  0.08660795712939202\n",
            "Loss S44:  0.09008326986945288\n",
            "Loss S11:  0.07600252401313125\n",
            "Loss S22:  0.07918467892667065\n",
            "Loss S33:  0.08661642360372628\n",
            "Loss S44:  0.09021029053283228\n",
            "Loss S11:  0.07603303078784902\n",
            "Loss S22:  0.07921687445664338\n",
            "Loss S33:  0.08663803704104192\n",
            "Loss S44:  0.0903238124317593\n",
            "Loss S11:  0.0760045779193042\n",
            "Loss S22:  0.07919018748642005\n",
            "Loss S33:  0.08652890312622129\n",
            "Loss S44:  0.09032225978258904\n",
            "Loss S11:  0.07587547388399708\n",
            "Loss S22:  0.07914966662135728\n",
            "Loss S33:  0.08640279327682408\n",
            "Loss S44:  0.09028457966695899\n",
            "Loss S11:  0.07584439230755245\n",
            "Loss S22:  0.0790645217136761\n",
            "Loss S33:  0.08634430619831786\n",
            "Loss S44:  0.09023229932300061\n",
            "Loss S11:  0.07574391446035841\n",
            "Loss S22:  0.07901901225833331\n",
            "Loss S33:  0.08627161459849618\n",
            "Loss S44:  0.09012930076140577\n",
            "Loss S11:  0.0757903379859621\n",
            "Loss S22:  0.07907244462770714\n",
            "Loss S33:  0.0862715857247462\n",
            "Loss S44:  0.09017658804048624\n",
            "Loss S11:  0.07578208269392776\n",
            "Loss S22:  0.07904805835798709\n",
            "Loss S33:  0.08631842565289959\n",
            "Loss S44:  0.09016279963246226\n",
            "Loss S11:  0.0757879558994906\n",
            "Loss S22:  0.07906845256226079\n",
            "Loss S33:  0.08635433239424313\n",
            "Loss S44:  0.09016386646060649\n",
            "Loss S11:  0.07586096383613943\n",
            "Loss S22:  0.07904090789382254\n",
            "Loss S33:  0.0863162467900119\n",
            "Loss S44:  0.09017312390137715\n",
            "Loss S11:  0.07580751390988325\n",
            "Loss S22:  0.07902976505610408\n",
            "Loss S33:  0.08629378541256565\n",
            "Loss S44:  0.09010842993281055\n",
            "Loss S11:  0.07579631370775188\n",
            "Loss S22:  0.07905968437239759\n",
            "Loss S33:  0.08620960037420171\n",
            "Loss S44:  0.09006370230683201\n",
            "Loss S11:  0.0758306948213541\n",
            "Loss S22:  0.0790505246168362\n",
            "Loss S33:  0.08623966199715588\n",
            "Loss S44:  0.09012371377611367\n",
            "Loss S11:  0.07584747609597356\n",
            "Loss S22:  0.07900692688621533\n",
            "Loss S33:  0.08620863412595858\n",
            "Loss S44:  0.09010538594864483\n",
            "Loss S11:  0.07588277260720606\n",
            "Loss S22:  0.07901438141315247\n",
            "Loss S33:  0.08620247903150233\n",
            "Loss S44:  0.09015534557944276\n",
            "Loss S11:  0.07588759048741364\n",
            "Loss S22:  0.07903131378279685\n",
            "Loss S33:  0.08619419752398237\n",
            "Loss S44:  0.0901374205066813\n",
            "Validation: \n",
            " Loss S11:  0.08249896764755249\n",
            " Loss S22:  0.11261256784200668\n",
            " Loss S33:  0.11889081448316574\n",
            " Loss S44:  0.12712377309799194\n",
            " Loss S11:  0.08352124016909372\n",
            " Loss S22:  0.12379459185259682\n",
            " Loss S33:  0.12497501962241672\n",
            " Loss S44:  0.1314753790696462\n",
            " Loss S11:  0.0820268781446829\n",
            " Loss S22:  0.12326268797240607\n",
            " Loss S33:  0.12514424015109132\n",
            " Loss S44:  0.1305242780505157\n",
            " Loss S11:  0.08133809292902712\n",
            " Loss S22:  0.12198062473144687\n",
            " Loss S33:  0.12321181106762807\n",
            " Loss S44:  0.12900706930238692\n",
            " Loss S11:  0.08100669122772453\n",
            " Loss S22:  0.12191183156805274\n",
            " Loss S33:  0.12257881674133701\n",
            " Loss S44:  0.12796187685963548\n",
            "\n",
            "Epoch: 51\n",
            "Loss S11:  0.07568657398223877\n",
            "Loss S22:  0.08590631186962128\n",
            "Loss S33:  0.07924354821443558\n",
            "Loss S44:  0.0904516950249672\n",
            "Loss S11:  0.07735162837938829\n",
            "Loss S22:  0.08196443996646187\n",
            "Loss S33:  0.08778544041243466\n",
            "Loss S44:  0.09111336144534024\n",
            "Loss S11:  0.07652453616971061\n",
            "Loss S22:  0.08143187456187748\n",
            "Loss S33:  0.08653527746597926\n",
            "Loss S44:  0.09015889111019317\n",
            "Loss S11:  0.07555859571983738\n",
            "Loss S22:  0.08022892354957518\n",
            "Loss S33:  0.08579803065907571\n",
            "Loss S44:  0.08954081107531825\n",
            "Loss S11:  0.07533362989382046\n",
            "Loss S22:  0.08025128757808267\n",
            "Loss S33:  0.08621404519895227\n",
            "Loss S44:  0.08970277465698195\n",
            "Loss S11:  0.07546944558328274\n",
            "Loss S22:  0.08045897998061835\n",
            "Loss S33:  0.0861089124399073\n",
            "Loss S44:  0.08930913461189643\n",
            "Loss S11:  0.07577692613494201\n",
            "Loss S22:  0.08061396232882484\n",
            "Loss S33:  0.0860174943189152\n",
            "Loss S44:  0.08991011183281414\n",
            "Loss S11:  0.07524673845356619\n",
            "Loss S22:  0.08025236637659476\n",
            "Loss S33:  0.08552429793586194\n",
            "Loss S44:  0.08929963000643422\n",
            "Loss S11:  0.07521736925398861\n",
            "Loss S22:  0.07999926621531263\n",
            "Loss S33:  0.085514217891075\n",
            "Loss S44:  0.08929931068861927\n",
            "Loss S11:  0.07507528880467781\n",
            "Loss S22:  0.07948975273213543\n",
            "Loss S33:  0.08564610009665018\n",
            "Loss S44:  0.0890878181044872\n",
            "Loss S11:  0.0748111489090589\n",
            "Loss S22:  0.07908213204971634\n",
            "Loss S33:  0.08519603555450345\n",
            "Loss S44:  0.08844846317378602\n",
            "Loss S11:  0.0746049728613716\n",
            "Loss S22:  0.07869882525892945\n",
            "Loss S33:  0.08517587070798015\n",
            "Loss S44:  0.08842630563555537\n",
            "Loss S11:  0.07462649148973552\n",
            "Loss S22:  0.07860996451013344\n",
            "Loss S33:  0.08513490036745702\n",
            "Loss S44:  0.0882899460097975\n",
            "Loss S11:  0.07482865182611778\n",
            "Loss S22:  0.07872431093954858\n",
            "Loss S33:  0.08542815863176156\n",
            "Loss S44:  0.08831225796282746\n",
            "Loss S11:  0.07497535273432732\n",
            "Loss S22:  0.07886439683378166\n",
            "Loss S33:  0.08537200663952117\n",
            "Loss S44:  0.08848989966279226\n",
            "Loss S11:  0.07530959239167882\n",
            "Loss S22:  0.07903387402461855\n",
            "Loss S33:  0.08564837878903016\n",
            "Loss S44:  0.08873469091409089\n",
            "Loss S11:  0.0750895150190925\n",
            "Loss S22:  0.07881300903347709\n",
            "Loss S33:  0.08544847244246406\n",
            "Loss S44:  0.08869640033992922\n",
            "Loss S11:  0.07495006315453707\n",
            "Loss S22:  0.07877697976447685\n",
            "Loss S33:  0.0852886120739736\n",
            "Loss S44:  0.08861587370871103\n",
            "Loss S11:  0.07497520240548566\n",
            "Loss S22:  0.07888256875767234\n",
            "Loss S33:  0.08545558566217265\n",
            "Loss S44:  0.08892314975762236\n",
            "Loss S11:  0.07507063313850558\n",
            "Loss S22:  0.07895960294061306\n",
            "Loss S33:  0.08545392349908489\n",
            "Loss S44:  0.08901649584788926\n",
            "Loss S11:  0.07513526080185501\n",
            "Loss S22:  0.07892249785919687\n",
            "Loss S33:  0.08547067386445714\n",
            "Loss S44:  0.08897885672785157\n",
            "Loss S11:  0.07517297920810667\n",
            "Loss S22:  0.07891588578645087\n",
            "Loss S33:  0.08551008211916657\n",
            "Loss S44:  0.08893745579708244\n",
            "Loss S11:  0.07530429654194219\n",
            "Loss S22:  0.07895485546052186\n",
            "Loss S33:  0.08552215531535817\n",
            "Loss S44:  0.08909546786169122\n",
            "Loss S11:  0.07532762399032003\n",
            "Loss S22:  0.07904814465930968\n",
            "Loss S33:  0.08559916729663873\n",
            "Loss S44:  0.08910010235779213\n",
            "Loss S11:  0.07536591315850677\n",
            "Loss S22:  0.07922861732820752\n",
            "Loss S33:  0.08584347540412206\n",
            "Loss S44:  0.08923874028738109\n",
            "Loss S11:  0.07538200171285891\n",
            "Loss S22:  0.07915744828097374\n",
            "Loss S33:  0.08586717159505859\n",
            "Loss S44:  0.0891921706942923\n",
            "Loss S11:  0.07545636450136758\n",
            "Loss S22:  0.0791673394369668\n",
            "Loss S33:  0.08587343679648249\n",
            "Loss S44:  0.0891798471182699\n",
            "Loss S11:  0.07545140980849407\n",
            "Loss S22:  0.07918830984357979\n",
            "Loss S33:  0.08596884602220296\n",
            "Loss S44:  0.08934892243894704\n",
            "Loss S11:  0.07544305391636183\n",
            "Loss S22:  0.0792280466981842\n",
            "Loss S33:  0.08594311625295686\n",
            "Loss S44:  0.0894910095531321\n",
            "Loss S11:  0.07544080983476131\n",
            "Loss S22:  0.07928953679515324\n",
            "Loss S33:  0.08596185402771861\n",
            "Loss S44:  0.08949723860242523\n",
            "Loss S11:  0.07542171023563293\n",
            "Loss S22:  0.07928572833983605\n",
            "Loss S33:  0.08594063832811343\n",
            "Loss S44:  0.0895513772617939\n",
            "Loss S11:  0.07531337969460287\n",
            "Loss S22:  0.07921120345927894\n",
            "Loss S33:  0.08581427465585267\n",
            "Loss S44:  0.08946172088097146\n",
            "Loss S11:  0.07531781025764726\n",
            "Loss S22:  0.07923323265349382\n",
            "Loss S33:  0.08591112320389703\n",
            "Loss S44:  0.08948652908159566\n",
            "Loss S11:  0.07524685168869545\n",
            "Loss S22:  0.07911265113607634\n",
            "Loss S33:  0.08573963209491482\n",
            "Loss S44:  0.089385321695819\n",
            "Loss S11:  0.07520411184325246\n",
            "Loss S22:  0.07910261347977297\n",
            "Loss S33:  0.0856602082678999\n",
            "Loss S44:  0.0893502275615144\n",
            "Loss S11:  0.07518990890804858\n",
            "Loss S22:  0.07920753929307658\n",
            "Loss S33:  0.08565374523231446\n",
            "Loss S44:  0.08940778021034691\n",
            "Loss S11:  0.0751883165183325\n",
            "Loss S22:  0.07922615815686718\n",
            "Loss S33:  0.08562981529249049\n",
            "Loss S44:  0.08944049316595136\n",
            "Loss S11:  0.07510947182051576\n",
            "Loss S22:  0.07920360993945373\n",
            "Loss S33:  0.08558183153363251\n",
            "Loss S44:  0.08938961387724889\n",
            "Loss S11:  0.07510934509043618\n",
            "Loss S22:  0.07911951353973917\n",
            "Loss S33:  0.08555187294724106\n",
            "Loss S44:  0.08934347708941758\n",
            "Loss S11:  0.07504755842601857\n",
            "Loss S22:  0.0790600296862595\n",
            "Loss S33:  0.08553895582933255\n",
            "Loss S44:  0.08932305161681627\n",
            "Loss S11:  0.07516665943431439\n",
            "Loss S22:  0.07909009986536164\n",
            "Loss S33:  0.08559955270064441\n",
            "Loss S44:  0.08938535501683442\n",
            "Loss S11:  0.07513657972724189\n",
            "Loss S22:  0.07905059931414551\n",
            "Loss S33:  0.0855656433533288\n",
            "Loss S44:  0.08937188767241155\n",
            "Loss S11:  0.07514411622476974\n",
            "Loss S22:  0.07907606645392126\n",
            "Loss S33:  0.08557535099162059\n",
            "Loss S44:  0.08940863961491052\n",
            "Loss S11:  0.07517581458123543\n",
            "Loss S22:  0.07901103324375684\n",
            "Loss S33:  0.08551576724945807\n",
            "Loss S44:  0.08933308265947687\n",
            "Loss S11:  0.07516475024373354\n",
            "Loss S22:  0.07899132038122401\n",
            "Loss S33:  0.0854674049921317\n",
            "Loss S44:  0.08927864694541274\n",
            "Loss S11:  0.07517813768097407\n",
            "Loss S22:  0.07900293106977269\n",
            "Loss S33:  0.08547371325233823\n",
            "Loss S44:  0.08935203933530794\n",
            "Loss S11:  0.07519135904577187\n",
            "Loss S22:  0.07900997429051781\n",
            "Loss S33:  0.08546101642922052\n",
            "Loss S44:  0.08941119070490115\n",
            "Loss S11:  0.07512269681616193\n",
            "Loss S22:  0.0789780655515928\n",
            "Loss S33:  0.08536884970092976\n",
            "Loss S44:  0.08936984057196133\n",
            "Loss S11:  0.07513960082231093\n",
            "Loss S22:  0.07895347068624536\n",
            "Loss S33:  0.08538071643909645\n",
            "Loss S44:  0.08939535427923757\n",
            "Loss S11:  0.07519969527748112\n",
            "Loss S22:  0.07894393097601694\n",
            "Loss S33:  0.0854750905648754\n",
            "Loss S44:  0.08941737576074853\n",
            "Validation: \n",
            " Loss S11:  0.08993632346391678\n",
            " Loss S22:  0.11082920432090759\n",
            " Loss S33:  0.11636839807033539\n",
            " Loss S44:  0.11734160780906677\n",
            " Loss S11:  0.08785893839030039\n",
            " Loss S22:  0.12230630999519712\n",
            " Loss S33:  0.12249206006526947\n",
            " Loss S44:  0.12857573550371898\n",
            " Loss S11:  0.08678923111136366\n",
            " Loss S22:  0.12161463807995726\n",
            " Loss S33:  0.12234798800654528\n",
            " Loss S44:  0.12739348102633546\n",
            " Loss S11:  0.08605540665935298\n",
            " Loss S22:  0.12021646306651537\n",
            " Loss S33:  0.12004869980890243\n",
            " Loss S44:  0.1259405462712538\n",
            " Loss S11:  0.08587462684990448\n",
            " Loss S22:  0.1201780934208705\n",
            " Loss S33:  0.11939018606035798\n",
            " Loss S44:  0.12507892749559732\n",
            "\n",
            "Epoch: 52\n",
            "Loss S11:  0.07770591229200363\n",
            "Loss S22:  0.08325590938329697\n",
            "Loss S33:  0.09173707664012909\n",
            "Loss S44:  0.09294122457504272\n",
            "Loss S11:  0.07813318276947195\n",
            "Loss S22:  0.07842982086268338\n",
            "Loss S33:  0.08963491293517026\n",
            "Loss S44:  0.09019634263081984\n",
            "Loss S11:  0.07729604591925938\n",
            "Loss S22:  0.07842460771401723\n",
            "Loss S33:  0.08676410395474661\n",
            "Loss S44:  0.08854013504016967\n",
            "Loss S11:  0.07671174455073572\n",
            "Loss S22:  0.0785529363539911\n",
            "Loss S33:  0.08553281378361487\n",
            "Loss S44:  0.08774293166014456\n",
            "Loss S11:  0.07571058120669388\n",
            "Loss S22:  0.07881004454159155\n",
            "Loss S33:  0.08558307497239695\n",
            "Loss S44:  0.08816625359581738\n",
            "Loss S11:  0.07506671168056189\n",
            "Loss S22:  0.07875216343239241\n",
            "Loss S33:  0.08526771500998852\n",
            "Loss S44:  0.08819704371340134\n",
            "Loss S11:  0.07509442260030841\n",
            "Loss S22:  0.07926567851519975\n",
            "Loss S33:  0.08555635125910649\n",
            "Loss S44:  0.08892656374173086\n",
            "Loss S11:  0.07465732506882976\n",
            "Loss S22:  0.07891906031840284\n",
            "Loss S33:  0.08537781994107743\n",
            "Loss S44:  0.08853270176430823\n",
            "Loss S11:  0.07477153488147406\n",
            "Loss S22:  0.07910482870575822\n",
            "Loss S33:  0.08533033544634595\n",
            "Loss S44:  0.08892612334018872\n",
            "Loss S11:  0.07513669354247522\n",
            "Loss S22:  0.07895064190193847\n",
            "Loss S33:  0.08575076847286014\n",
            "Loss S44:  0.0894003424506921\n",
            "Loss S11:  0.07482523223993802\n",
            "Loss S22:  0.07848370768646203\n",
            "Loss S33:  0.08518238432041489\n",
            "Loss S44:  0.08932970958476019\n",
            "Loss S11:  0.07468542245191497\n",
            "Loss S22:  0.07833567456350671\n",
            "Loss S33:  0.08490630816500466\n",
            "Loss S44:  0.08921646065003164\n",
            "Loss S11:  0.07462043546078619\n",
            "Loss S22:  0.07804567702422457\n",
            "Loss S33:  0.08472600376064127\n",
            "Loss S44:  0.08891905474761301\n",
            "Loss S11:  0.07483761277021343\n",
            "Loss S22:  0.07832909304335828\n",
            "Loss S33:  0.08499979779465508\n",
            "Loss S44:  0.08911037325631571\n",
            "Loss S11:  0.07496874243126693\n",
            "Loss S22:  0.07839664174838269\n",
            "Loss S33:  0.0849687248362717\n",
            "Loss S44:  0.08900995578960325\n",
            "Loss S11:  0.07516916108545878\n",
            "Loss S22:  0.07840698802017218\n",
            "Loss S33:  0.08520896742675478\n",
            "Loss S44:  0.08917667119708282\n",
            "Loss S11:  0.0751794944314853\n",
            "Loss S22:  0.07816753848831846\n",
            "Loss S33:  0.08519705866249452\n",
            "Loss S44:  0.0892134171835384\n",
            "Loss S11:  0.07512509559243045\n",
            "Loss S22:  0.07809583165230807\n",
            "Loss S33:  0.08510622373449872\n",
            "Loss S44:  0.08909886556933498\n",
            "Loss S11:  0.07508009966483432\n",
            "Loss S22:  0.07821677293813689\n",
            "Loss S33:  0.08521788233880839\n",
            "Loss S44:  0.08909188918975176\n",
            "Loss S11:  0.0750723776861011\n",
            "Loss S22:  0.07821632592075782\n",
            "Loss S33:  0.0852219429627763\n",
            "Loss S44:  0.08898621146123446\n",
            "Loss S11:  0.07495611661405706\n",
            "Loss S22:  0.07809815079492716\n",
            "Loss S33:  0.08516314577552217\n",
            "Loss S44:  0.08890586231478412\n",
            "Loss S11:  0.07499678277573879\n",
            "Loss S22:  0.07812240686249959\n",
            "Loss S33:  0.08515730630871243\n",
            "Loss S44:  0.08884281110707053\n",
            "Loss S11:  0.0750417061048935\n",
            "Loss S22:  0.07813922806360603\n",
            "Loss S33:  0.08529859444134916\n",
            "Loss S44:  0.08886920519139432\n",
            "Loss S11:  0.07513160761687662\n",
            "Loss S22:  0.078140920584465\n",
            "Loss S33:  0.08518807270697185\n",
            "Loss S44:  0.08889697546953763\n",
            "Loss S11:  0.07523189036801643\n",
            "Loss S22:  0.07813090603814085\n",
            "Loss S33:  0.08535374379504271\n",
            "Loss S44:  0.08901494698158438\n",
            "Loss S11:  0.07527417680656767\n",
            "Loss S22:  0.07806651274937082\n",
            "Loss S33:  0.08523913276979173\n",
            "Loss S44:  0.08903540703523681\n",
            "Loss S11:  0.07526620877085974\n",
            "Loss S22:  0.0781184900908863\n",
            "Loss S33:  0.0853064242621948\n",
            "Loss S44:  0.08906127495327215\n",
            "Loss S11:  0.07528300388162866\n",
            "Loss S22:  0.07824911832864434\n",
            "Loss S33:  0.08546188088800634\n",
            "Loss S44:  0.08923687816106085\n",
            "Loss S11:  0.07531178811584928\n",
            "Loss S22:  0.07836595966457473\n",
            "Loss S33:  0.085555768150876\n",
            "Loss S44:  0.08924790872161499\n",
            "Loss S11:  0.07535181051164967\n",
            "Loss S22:  0.07838835727257007\n",
            "Loss S33:  0.08550219437510696\n",
            "Loss S44:  0.08921843201331667\n",
            "Loss S11:  0.07536532592486306\n",
            "Loss S22:  0.07842593341818284\n",
            "Loss S33:  0.08549180834792382\n",
            "Loss S44:  0.08928202293144905\n",
            "Loss S11:  0.0753223050229994\n",
            "Loss S22:  0.07839752430653266\n",
            "Loss S33:  0.08536042965397574\n",
            "Loss S44:  0.08919658296265402\n",
            "Loss S11:  0.0754557118121523\n",
            "Loss S22:  0.07843051007092927\n",
            "Loss S33:  0.0855144885284507\n",
            "Loss S44:  0.08928063119590468\n",
            "Loss S11:  0.07540782967587972\n",
            "Loss S22:  0.07831936852944582\n",
            "Loss S33:  0.0854120168531049\n",
            "Loss S44:  0.08928781443942349\n",
            "Loss S11:  0.0754901184386347\n",
            "Loss S22:  0.07837084181541222\n",
            "Loss S33:  0.08552817456271292\n",
            "Loss S44:  0.08932491758963929\n",
            "Loss S11:  0.07548947188674215\n",
            "Loss S22:  0.07839728162711503\n",
            "Loss S33:  0.08554468444015226\n",
            "Loss S44:  0.08931295055779297\n",
            "Loss S11:  0.07549073231352334\n",
            "Loss S22:  0.07846328185824807\n",
            "Loss S33:  0.08560678201864301\n",
            "Loss S44:  0.08937167456770868\n",
            "Loss S11:  0.07536288852881229\n",
            "Loss S22:  0.07840881979650242\n",
            "Loss S33:  0.08554813737737522\n",
            "Loss S44:  0.0892513920515374\n",
            "Loss S11:  0.07536891188834283\n",
            "Loss S22:  0.07832507576959652\n",
            "Loss S33:  0.08547331983336001\n",
            "Loss S44:  0.0891983615444714\n",
            "Loss S11:  0.07531602896006821\n",
            "Loss S22:  0.07820657464435034\n",
            "Loss S33:  0.08542527864351297\n",
            "Loss S44:  0.08911942970722228\n",
            "Loss S11:  0.07540308882916658\n",
            "Loss S22:  0.07820016339867192\n",
            "Loss S33:  0.08544238534131253\n",
            "Loss S44:  0.08917558412450804\n",
            "Loss S11:  0.07535551116776873\n",
            "Loss S22:  0.07827077435279704\n",
            "Loss S33:  0.0854478413896259\n",
            "Loss S44:  0.08914722101604271\n",
            "Loss S11:  0.07538432137699988\n",
            "Loss S22:  0.07827896411550583\n",
            "Loss S33:  0.0855272986578828\n",
            "Loss S44:  0.08920626605986982\n",
            "Loss S11:  0.07540236553501364\n",
            "Loss S22:  0.0782358492800227\n",
            "Loss S33:  0.08553556259505433\n",
            "Loss S44:  0.08914474778205779\n",
            "Loss S11:  0.07534588004996717\n",
            "Loss S22:  0.07822113733341635\n",
            "Loss S33:  0.08556959433417742\n",
            "Loss S44:  0.08922267093612493\n",
            "Loss S11:  0.07534203600724891\n",
            "Loss S22:  0.0782519754790546\n",
            "Loss S33:  0.08556010354384615\n",
            "Loss S44:  0.0892421782611479\n",
            "Loss S11:  0.07530841731553223\n",
            "Loss S22:  0.07822854104987194\n",
            "Loss S33:  0.0855998483662233\n",
            "Loss S44:  0.08930205211140328\n",
            "Loss S11:  0.07525597312616188\n",
            "Loss S22:  0.07818781882682677\n",
            "Loss S33:  0.08559844880157215\n",
            "Loss S44:  0.08926715605324748\n",
            "Loss S11:  0.07528151488880358\n",
            "Loss S22:  0.0781572089141347\n",
            "Loss S33:  0.08561551411347677\n",
            "Loss S44:  0.0892431506124207\n",
            "Loss S11:  0.07523696316508557\n",
            "Loss S22:  0.07816160428517947\n",
            "Loss S33:  0.08564164063109886\n",
            "Loss S44:  0.08926722895953418\n",
            "Validation: \n",
            " Loss S11:  0.08561009168624878\n",
            " Loss S22:  0.1098327785730362\n",
            " Loss S33:  0.11803766340017319\n",
            " Loss S44:  0.12460559606552124\n",
            " Loss S11:  0.08368575821320216\n",
            " Loss S22:  0.11894331411236808\n",
            " Loss S33:  0.12306123581670579\n",
            " Loss S44:  0.1303546027768226\n",
            " Loss S11:  0.08264637393195455\n",
            " Loss S22:  0.11858271116890558\n",
            " Loss S33:  0.12270247154846424\n",
            " Loss S44:  0.12806548414433874\n",
            " Loss S11:  0.08193105471427323\n",
            " Loss S22:  0.11695344660614358\n",
            " Loss S33:  0.1204892421355013\n",
            " Loss S44:  0.12705411383363066\n",
            " Loss S11:  0.08185712726395807\n",
            " Loss S22:  0.11719247689585627\n",
            " Loss S33:  0.11992376260919335\n",
            " Loss S44:  0.12626333231175388\n",
            "\n",
            "Epoch: 53\n",
            "Loss S11:  0.07740794122219086\n",
            "Loss S22:  0.06977596879005432\n",
            "Loss S33:  0.08338584005832672\n",
            "Loss S44:  0.09633133560419083\n",
            "Loss S11:  0.07466042109511116\n",
            "Loss S22:  0.07612858577208086\n",
            "Loss S33:  0.08534734086556868\n",
            "Loss S44:  0.09064634212038734\n",
            "Loss S11:  0.07553650580701374\n",
            "Loss S22:  0.07822380356845401\n",
            "Loss S33:  0.08681099932818186\n",
            "Loss S44:  0.09025874308177403\n",
            "Loss S11:  0.07509215356361482\n",
            "Loss S22:  0.07747353673461944\n",
            "Loss S33:  0.08603356658451018\n",
            "Loss S44:  0.08940827197605564\n",
            "Loss S11:  0.07506044109056635\n",
            "Loss S22:  0.07824853235265104\n",
            "Loss S33:  0.08566916588602996\n",
            "Loss S44:  0.08947153516658922\n",
            "Loss S11:  0.07449797329072859\n",
            "Loss S22:  0.07846536012549027\n",
            "Loss S33:  0.08530575767451641\n",
            "Loss S44:  0.08907779671397864\n",
            "Loss S11:  0.07463829030023246\n",
            "Loss S22:  0.07845185226837142\n",
            "Loss S33:  0.08567049537525802\n",
            "Loss S44:  0.0891334852967106\n",
            "Loss S11:  0.07432302302667793\n",
            "Loss S22:  0.0782571035686513\n",
            "Loss S33:  0.08506886923397092\n",
            "Loss S44:  0.08918766698367159\n",
            "Loss S11:  0.07404925540825467\n",
            "Loss S22:  0.07787292768005971\n",
            "Loss S33:  0.08483475621467755\n",
            "Loss S44:  0.08879700431853164\n",
            "Loss S11:  0.0743039326107764\n",
            "Loss S22:  0.07789112590662725\n",
            "Loss S33:  0.0850058573779169\n",
            "Loss S44:  0.08888831300722373\n",
            "Loss S11:  0.07420649973325211\n",
            "Loss S22:  0.0776796134022793\n",
            "Loss S33:  0.08453382721336761\n",
            "Loss S44:  0.08849560174316463\n",
            "Loss S11:  0.07403978963827228\n",
            "Loss S22:  0.0774822313304957\n",
            "Loss S33:  0.08430569410861076\n",
            "Loss S44:  0.08837871609238891\n",
            "Loss S11:  0.07401975893900414\n",
            "Loss S22:  0.07735520961486604\n",
            "Loss S33:  0.08425139358713607\n",
            "Loss S44:  0.0879765373369879\n",
            "Loss S11:  0.07427313261245953\n",
            "Loss S22:  0.07748804841446512\n",
            "Loss S33:  0.0845329659130737\n",
            "Loss S44:  0.08819590207502132\n",
            "Loss S11:  0.07420717742531857\n",
            "Loss S22:  0.07751053394683709\n",
            "Loss S33:  0.08462549650922735\n",
            "Loss S44:  0.08825508668913064\n",
            "Loss S11:  0.07448984122532883\n",
            "Loss S22:  0.07769292452378779\n",
            "Loss S33:  0.08476374096033588\n",
            "Loss S44:  0.08843854291746948\n",
            "Loss S11:  0.0743394940860153\n",
            "Loss S22:  0.07739812017690322\n",
            "Loss S33:  0.08465727404778048\n",
            "Loss S44:  0.08847096835419258\n",
            "Loss S11:  0.074367063523036\n",
            "Loss S22:  0.0772929186578731\n",
            "Loss S33:  0.08452653484037745\n",
            "Loss S44:  0.08844279894354748\n",
            "Loss S11:  0.07440154661954437\n",
            "Loss S22:  0.07737726028699901\n",
            "Loss S33:  0.08464722080125335\n",
            "Loss S44:  0.08861689314822466\n",
            "Loss S11:  0.07451692414689438\n",
            "Loss S22:  0.0771831253943331\n",
            "Loss S33:  0.08459487900684017\n",
            "Loss S44:  0.08857872220078064\n",
            "Loss S11:  0.07447975039926928\n",
            "Loss S22:  0.07717242167882658\n",
            "Loss S33:  0.08458317864445311\n",
            "Loss S44:  0.08863029388052907\n",
            "Loss S11:  0.07450988465010837\n",
            "Loss S22:  0.07723882704337626\n",
            "Loss S33:  0.08456936136099964\n",
            "Loss S44:  0.08849225302725607\n",
            "Loss S11:  0.07453333306636206\n",
            "Loss S22:  0.07751438397560184\n",
            "Loss S33:  0.08463749168145711\n",
            "Loss S44:  0.08875025407626079\n",
            "Loss S11:  0.07457135680627514\n",
            "Loss S22:  0.0776358407587458\n",
            "Loss S33:  0.08461047415480469\n",
            "Loss S44:  0.08876442822156014\n",
            "Loss S11:  0.07453985736392346\n",
            "Loss S22:  0.07766555563481022\n",
            "Loss S33:  0.08462380676595997\n",
            "Loss S44:  0.08888884136043644\n",
            "Loss S11:  0.07449845629799888\n",
            "Loss S22:  0.07761004111503225\n",
            "Loss S33:  0.08459634972046096\n",
            "Loss S44:  0.08883318233774953\n",
            "Loss S11:  0.07455059482048755\n",
            "Loss S22:  0.077630123516038\n",
            "Loss S33:  0.08462774993359358\n",
            "Loss S44:  0.08876351626782582\n",
            "Loss S11:  0.07450021546320282\n",
            "Loss S22:  0.077551367276819\n",
            "Loss S33:  0.08468367497529491\n",
            "Loss S44:  0.08884558067880434\n",
            "Loss S11:  0.07455378978967242\n",
            "Loss S22:  0.07759566683308934\n",
            "Loss S33:  0.08471739379642697\n",
            "Loss S44:  0.08882315817464713\n",
            "Loss S11:  0.07456139082095467\n",
            "Loss S22:  0.07768049056536143\n",
            "Loss S33:  0.08486372578082625\n",
            "Loss S44:  0.0888285170152425\n",
            "Loss S11:  0.07451790525420164\n",
            "Loss S22:  0.0776834441503615\n",
            "Loss S33:  0.0848459101149014\n",
            "Loss S44:  0.08886812723851283\n",
            "Loss S11:  0.07441809496743503\n",
            "Loss S22:  0.07764667187520928\n",
            "Loss S33:  0.08474635260952247\n",
            "Loss S44:  0.08883647968052284\n",
            "Loss S11:  0.07450621959251406\n",
            "Loss S22:  0.07765525051412923\n",
            "Loss S33:  0.08485859964402666\n",
            "Loss S44:  0.08887182807439584\n",
            "Loss S11:  0.07443620477243132\n",
            "Loss S22:  0.07754896568792824\n",
            "Loss S33:  0.08474421660975748\n",
            "Loss S44:  0.08875361062428742\n",
            "Loss S11:  0.0744750221475653\n",
            "Loss S22:  0.07758460369758592\n",
            "Loss S33:  0.0847242416874055\n",
            "Loss S44:  0.08880515477437079\n",
            "Loss S11:  0.0744692252635786\n",
            "Loss S22:  0.07757896926795316\n",
            "Loss S33:  0.0847418578262003\n",
            "Loss S44:  0.08880890126836266\n",
            "Loss S11:  0.07443931555359978\n",
            "Loss S22:  0.07753462733448047\n",
            "Loss S33:  0.08472888340910385\n",
            "Loss S44:  0.08880982527359702\n",
            "Loss S11:  0.07436949935442996\n",
            "Loss S22:  0.07747987897770424\n",
            "Loss S33:  0.08465529337645862\n",
            "Loss S44:  0.08872891520714823\n",
            "Loss S11:  0.07428263557043288\n",
            "Loss S22:  0.07742927206821955\n",
            "Loss S33:  0.08463540090507096\n",
            "Loss S44:  0.088645801006809\n",
            "Loss S11:  0.07420984064907674\n",
            "Loss S22:  0.07739843749214927\n",
            "Loss S33:  0.08455023356258412\n",
            "Loss S44:  0.08856423839431285\n",
            "Loss S11:  0.0743315078485339\n",
            "Loss S22:  0.0774087269240038\n",
            "Loss S33:  0.0846945782960799\n",
            "Loss S44:  0.08861900054085582\n",
            "Loss S11:  0.0743110969735178\n",
            "Loss S22:  0.07741853508672285\n",
            "Loss S33:  0.08473791390512402\n",
            "Loss S44:  0.08856669414580014\n",
            "Loss S11:  0.07431093606912043\n",
            "Loss S22:  0.07743473608498053\n",
            "Loss S33:  0.08474730514432359\n",
            "Loss S44:  0.08862614242311329\n",
            "Loss S11:  0.07426116458224587\n",
            "Loss S22:  0.07736324234552285\n",
            "Loss S33:  0.08469948492373779\n",
            "Loss S44:  0.08857053592628227\n",
            "Loss S11:  0.07426646980298619\n",
            "Loss S22:  0.07740930760967758\n",
            "Loss S33:  0.08469974937217306\n",
            "Loss S44:  0.08856204234891468\n",
            "Loss S11:  0.07430535262479486\n",
            "Loss S22:  0.07746544008673956\n",
            "Loss S33:  0.08472487419645432\n",
            "Loss S44:  0.08856071224498115\n",
            "Loss S11:  0.07436785074415538\n",
            "Loss S22:  0.07746257670950993\n",
            "Loss S33:  0.08470179623805002\n",
            "Loss S44:  0.08856151128539294\n",
            "Loss S11:  0.07434260405426066\n",
            "Loss S22:  0.07743816768192942\n",
            "Loss S33:  0.08467437945417032\n",
            "Loss S44:  0.08855079932260919\n",
            "Loss S11:  0.07434984676059715\n",
            "Loss S22:  0.07741899939404952\n",
            "Loss S33:  0.0846993471660386\n",
            "Loss S44:  0.08860587791816608\n",
            "Loss S11:  0.07432823332288357\n",
            "Loss S22:  0.07736379765505218\n",
            "Loss S33:  0.08464944861874804\n",
            "Loss S44:  0.08855912231986246\n",
            "Validation: \n",
            " Loss S11:  0.0859384685754776\n",
            " Loss S22:  0.11219392716884613\n",
            " Loss S33:  0.11927661299705505\n",
            " Loss S44:  0.12935256958007812\n",
            " Loss S11:  0.0854409769887016\n",
            " Loss S22:  0.12384836695023946\n",
            " Loss S33:  0.1253113356374559\n",
            " Loss S44:  0.1326579146441959\n",
            " Loss S11:  0.08386463599234092\n",
            " Loss S22:  0.12286034462655462\n",
            " Loss S33:  0.1254454520417423\n",
            " Loss S44:  0.1305742249256227\n",
            " Loss S11:  0.08286062518104179\n",
            " Loss S22:  0.12123207125018855\n",
            " Loss S33:  0.12301349664320711\n",
            " Loss S44:  0.12875513097301858\n",
            " Loss S11:  0.08255576995419867\n",
            " Loss S22:  0.12130078728552218\n",
            " Loss S33:  0.12204988797505696\n",
            " Loss S44:  0.12781308618960557\n",
            "\n",
            "Epoch: 54\n",
            "Loss S11:  0.07417045533657074\n",
            "Loss S22:  0.07282872498035431\n",
            "Loss S33:  0.08446639776229858\n",
            "Loss S44:  0.08820060640573502\n",
            "Loss S11:  0.0750688761472702\n",
            "Loss S22:  0.07835436883297833\n",
            "Loss S33:  0.08413914252411235\n",
            "Loss S44:  0.08786570551720532\n",
            "Loss S11:  0.07475541461081732\n",
            "Loss S22:  0.07812969918761935\n",
            "Loss S33:  0.08291229428279967\n",
            "Loss S44:  0.08762167713471822\n",
            "Loss S11:  0.0732844321718139\n",
            "Loss S22:  0.0776746837842849\n",
            "Loss S33:  0.08237484457992739\n",
            "Loss S44:  0.08674690776294278\n",
            "Loss S11:  0.07276755362385656\n",
            "Loss S22:  0.07766848239229947\n",
            "Loss S33:  0.08236561043233406\n",
            "Loss S44:  0.08760738808934282\n",
            "Loss S11:  0.07295294230182965\n",
            "Loss S22:  0.07776622225840886\n",
            "Loss S33:  0.08253128724355324\n",
            "Loss S44:  0.08778208172788807\n",
            "Loss S11:  0.07357117784072141\n",
            "Loss S22:  0.07801332520168336\n",
            "Loss S33:  0.08276335601923895\n",
            "Loss S44:  0.08850202049876822\n",
            "Loss S11:  0.07310685058924514\n",
            "Loss S22:  0.07784258112521239\n",
            "Loss S33:  0.08272626786164834\n",
            "Loss S44:  0.08806213148882691\n",
            "Loss S11:  0.07309030474703988\n",
            "Loss S22:  0.0778923765928657\n",
            "Loss S33:  0.08318506871108655\n",
            "Loss S44:  0.08794797129101223\n",
            "Loss S11:  0.07334913002265679\n",
            "Loss S22:  0.07804371711317\n",
            "Loss S33:  0.08325148873276762\n",
            "Loss S44:  0.08787373232317496\n",
            "Loss S11:  0.07308483765561982\n",
            "Loss S22:  0.0776477093891342\n",
            "Loss S33:  0.08286798428190817\n",
            "Loss S44:  0.08746123306526996\n",
            "Loss S11:  0.07302620919706586\n",
            "Loss S22:  0.07742077112197876\n",
            "Loss S33:  0.08278780470828752\n",
            "Loss S44:  0.08734374983353657\n",
            "Loss S11:  0.07305156556535358\n",
            "Loss S22:  0.076995998036024\n",
            "Loss S33:  0.08262241650218806\n",
            "Loss S44:  0.08701089006071248\n",
            "Loss S11:  0.07330420351665438\n",
            "Loss S22:  0.07705862032434413\n",
            "Loss S33:  0.0829235027537091\n",
            "Loss S44:  0.08689689573668341\n",
            "Loss S11:  0.07344772846352124\n",
            "Loss S22:  0.07706852117223097\n",
            "Loss S33:  0.08307147115891707\n",
            "Loss S44:  0.08694388558889957\n",
            "Loss S11:  0.07371848806837536\n",
            "Loss S22:  0.0770626396976954\n",
            "Loss S33:  0.08318712961989523\n",
            "Loss S44:  0.0870496052878582\n",
            "Loss S11:  0.07368962792968899\n",
            "Loss S22:  0.07692070923144033\n",
            "Loss S33:  0.0831091566393094\n",
            "Loss S44:  0.08701137370019225\n",
            "Loss S11:  0.07349696818586679\n",
            "Loss S22:  0.07680312790281592\n",
            "Loss S33:  0.08314824601014455\n",
            "Loss S44:  0.08708279043958898\n",
            "Loss S11:  0.07351247396176033\n",
            "Loss S22:  0.07695185654482789\n",
            "Loss S33:  0.08310977525967919\n",
            "Loss S44:  0.08713982888183541\n",
            "Loss S11:  0.07351035041568792\n",
            "Loss S22:  0.07686249133570032\n",
            "Loss S33:  0.08327317284663935\n",
            "Loss S44:  0.08732073964716876\n",
            "Loss S11:  0.073503528642862\n",
            "Loss S22:  0.07689222910288554\n",
            "Loss S33:  0.08343190818431959\n",
            "Loss S44:  0.08725680318193056\n",
            "Loss S11:  0.07364573963516131\n",
            "Loss S22:  0.0769503867492009\n",
            "Loss S33:  0.08351238142540104\n",
            "Loss S44:  0.08734335824494113\n",
            "Loss S11:  0.07381479066219265\n",
            "Loss S22:  0.07710951214394958\n",
            "Loss S33:  0.08362106287776075\n",
            "Loss S44:  0.08746439198293297\n",
            "Loss S11:  0.07395304008931308\n",
            "Loss S22:  0.07722891381744182\n",
            "Loss S33:  0.08354299199400526\n",
            "Loss S44:  0.0875046660244723\n",
            "Loss S11:  0.07399722313918018\n",
            "Loss S22:  0.07732334437768489\n",
            "Loss S33:  0.08370103711780176\n",
            "Loss S44:  0.0875929833014971\n",
            "Loss S11:  0.07404982430704561\n",
            "Loss S22:  0.07727259449452993\n",
            "Loss S33:  0.08374296749136838\n",
            "Loss S44:  0.08764512361995727\n",
            "Loss S11:  0.07405392370290226\n",
            "Loss S22:  0.07719640612202586\n",
            "Loss S33:  0.08376616145344987\n",
            "Loss S44:  0.0876038827544428\n",
            "Loss S11:  0.07400239933955713\n",
            "Loss S22:  0.07729226440136283\n",
            "Loss S33:  0.08392259217386316\n",
            "Loss S44:  0.08769916297654824\n",
            "Loss S11:  0.07402767984733463\n",
            "Loss S22:  0.07733204914422646\n",
            "Loss S33:  0.0839146901173948\n",
            "Loss S44:  0.08765598529704526\n",
            "Loss S11:  0.07396928919959314\n",
            "Loss S22:  0.0773790049465866\n",
            "Loss S33:  0.08392008068840119\n",
            "Loss S44:  0.0876623420524843\n",
            "Loss S11:  0.07401924949151734\n",
            "Loss S22:  0.07735745736283321\n",
            "Loss S33:  0.08395244398980442\n",
            "Loss S44:  0.08778781166603399\n",
            "Loss S11:  0.07403064299961762\n",
            "Loss S22:  0.07732691190585829\n",
            "Loss S33:  0.0839781579336936\n",
            "Loss S44:  0.08784059351280188\n",
            "Loss S11:  0.07402286598476294\n",
            "Loss S22:  0.07731825216583374\n",
            "Loss S33:  0.08404523707531694\n",
            "Loss S44:  0.08787004807470744\n",
            "Loss S11:  0.07407460617199765\n",
            "Loss S22:  0.07726724948902865\n",
            "Loss S33:  0.08395187703054117\n",
            "Loss S44:  0.08783099192596995\n",
            "Loss S11:  0.0740318766240675\n",
            "Loss S22:  0.07732414315479243\n",
            "Loss S33:  0.0840078416851259\n",
            "Loss S44:  0.08781871949035751\n",
            "Loss S11:  0.07404571878816667\n",
            "Loss S22:  0.07734285321543019\n",
            "Loss S33:  0.08397814527451143\n",
            "Loss S44:  0.08787241603574182\n",
            "Loss S11:  0.0740962365430643\n",
            "Loss S22:  0.07732011081645694\n",
            "Loss S33:  0.08399197562414523\n",
            "Loss S44:  0.08797257661902012\n",
            "Loss S11:  0.07410005718027807\n",
            "Loss S22:  0.07728911479407886\n",
            "Loss S33:  0.08393992003645859\n",
            "Loss S44:  0.08792106281072302\n",
            "Loss S11:  0.07401309882014442\n",
            "Loss S22:  0.07720585399138646\n",
            "Loss S33:  0.08384148665065841\n",
            "Loss S44:  0.08787094400780095\n",
            "Loss S11:  0.07398423528694131\n",
            "Loss S22:  0.07713901887045187\n",
            "Loss S33:  0.08377728771294474\n",
            "Loss S44:  0.08778142504146337\n",
            "Loss S11:  0.07409350623253574\n",
            "Loss S22:  0.07720156860284674\n",
            "Loss S33:  0.0838360482654964\n",
            "Loss S44:  0.08786540411579936\n",
            "Loss S11:  0.07409896085224592\n",
            "Loss S22:  0.07721360663174133\n",
            "Loss S33:  0.08390074121763526\n",
            "Loss S44:  0.08788036308511911\n",
            "Loss S11:  0.07409305199803762\n",
            "Loss S22:  0.07726371427223688\n",
            "Loss S33:  0.0839053238089464\n",
            "Loss S44:  0.08794013017925684\n",
            "Loss S11:  0.07408450908630464\n",
            "Loss S22:  0.0772268625620208\n",
            "Loss S33:  0.0839303089543451\n",
            "Loss S44:  0.08792844938498916\n",
            "Loss S11:  0.07403293782486126\n",
            "Loss S22:  0.07729030087117165\n",
            "Loss S33:  0.08390225664355587\n",
            "Loss S44:  0.08792951150613577\n",
            "Loss S11:  0.07405352846052324\n",
            "Loss S22:  0.0773472468845728\n",
            "Loss S33:  0.08395290193034381\n",
            "Loss S44:  0.08798558152633866\n",
            "Loss S11:  0.07401576491957632\n",
            "Loss S22:  0.07738999447860066\n",
            "Loss S33:  0.08397108775416062\n",
            "Loss S44:  0.08797850870323284\n",
            "Loss S11:  0.07402981617616493\n",
            "Loss S22:  0.07740923263243303\n",
            "Loss S33:  0.08402169772647242\n",
            "Loss S44:  0.08798720635426273\n",
            "Loss S11:  0.07402581046964671\n",
            "Loss S22:  0.0774033177806657\n",
            "Loss S33:  0.0840984728504863\n",
            "Loss S44:  0.08800931927010323\n",
            "Loss S11:  0.07404796216947969\n",
            "Loss S22:  0.07736077535207063\n",
            "Loss S33:  0.08413730018493357\n",
            "Loss S44:  0.08798385355356025\n",
            "Validation: \n",
            " Loss S11:  0.08272159099578857\n",
            " Loss S22:  0.11232022941112518\n",
            " Loss S33:  0.11577340960502625\n",
            " Loss S44:  0.1233636736869812\n",
            " Loss S11:  0.08374168865737461\n",
            " Loss S22:  0.11855780404238474\n",
            " Loss S33:  0.1252901894705636\n",
            " Loss S44:  0.12952203622886113\n",
            " Loss S11:  0.08284344342423648\n",
            " Loss S22:  0.11808163327414815\n",
            " Loss S33:  0.1254612807093597\n",
            " Loss S44:  0.12864433701445416\n",
            " Loss S11:  0.08199048823997622\n",
            " Loss S22:  0.1164750437267491\n",
            " Loss S33:  0.12328425342919397\n",
            " Loss S44:  0.127252780145309\n",
            " Loss S11:  0.08192274526313499\n",
            " Loss S22:  0.11650951979336915\n",
            " Loss S33:  0.12254187253154354\n",
            " Loss S44:  0.1264125844578684\n",
            "\n",
            "Epoch: 55\n",
            "Loss S11:  0.07797732949256897\n",
            "Loss S22:  0.07531044632196426\n",
            "Loss S33:  0.08489207923412323\n",
            "Loss S44:  0.08922583609819412\n",
            "Loss S11:  0.07770193537527864\n",
            "Loss S22:  0.07875902747566049\n",
            "Loss S33:  0.08906713128089905\n",
            "Loss S44:  0.09257760508493944\n",
            "Loss S11:  0.07575000237141337\n",
            "Loss S22:  0.07795443314881552\n",
            "Loss S33:  0.08695686537595022\n",
            "Loss S44:  0.09096497900429226\n",
            "Loss S11:  0.07418670132756233\n",
            "Loss S22:  0.07612216064045506\n",
            "Loss S33:  0.08477001276708418\n",
            "Loss S44:  0.08913101159757184\n",
            "Loss S11:  0.07447676805824768\n",
            "Loss S22:  0.07683220360337234\n",
            "Loss S33:  0.08508885415588938\n",
            "Loss S44:  0.08836219223534189\n",
            "Loss S11:  0.07433904334902763\n",
            "Loss S22:  0.07757413124336916\n",
            "Loss S33:  0.08501689296727087\n",
            "Loss S44:  0.08824131331023048\n",
            "Loss S11:  0.0743907914176339\n",
            "Loss S22:  0.07746291380436694\n",
            "Loss S33:  0.08492054094056614\n",
            "Loss S44:  0.08833318313614266\n",
            "Loss S11:  0.07421637929871049\n",
            "Loss S22:  0.07708941662395505\n",
            "Loss S33:  0.08493096098093919\n",
            "Loss S44:  0.08804717439580971\n",
            "Loss S11:  0.07424726884490178\n",
            "Loss S22:  0.0771039659962242\n",
            "Loss S33:  0.0848574608012482\n",
            "Loss S44:  0.0880428738432166\n",
            "Loss S11:  0.07433394887126409\n",
            "Loss S22:  0.07696779874654916\n",
            "Loss S33:  0.08531364258174058\n",
            "Loss S44:  0.08843372115394571\n",
            "Loss S11:  0.07396509424589648\n",
            "Loss S22:  0.07672171287312365\n",
            "Loss S33:  0.0850216345769344\n",
            "Loss S44:  0.08801303508848247\n",
            "Loss S11:  0.07382749665427853\n",
            "Loss S22:  0.07619164427658459\n",
            "Loss S33:  0.08452694473771362\n",
            "Loss S44:  0.08771854954528378\n",
            "Loss S11:  0.07375109269599284\n",
            "Loss S22:  0.07611399778157227\n",
            "Loss S33:  0.08434310937223356\n",
            "Loss S44:  0.08754640134159199\n",
            "Loss S11:  0.07381582965377632\n",
            "Loss S22:  0.07646947587264404\n",
            "Loss S33:  0.08428944425500987\n",
            "Loss S44:  0.0874859967645798\n",
            "Loss S11:  0.07386130094528198\n",
            "Loss S22:  0.07657869878812884\n",
            "Loss S33:  0.08406299500600665\n",
            "Loss S44:  0.0874742495147049\n",
            "Loss S11:  0.07392947809980405\n",
            "Loss S22:  0.07660766148211941\n",
            "Loss S33:  0.0840640114928713\n",
            "Loss S44:  0.08745698532126597\n",
            "Loss S11:  0.07380924216523674\n",
            "Loss S22:  0.07655020238634963\n",
            "Loss S33:  0.08407164198456343\n",
            "Loss S44:  0.08751733999074617\n",
            "Loss S11:  0.07359466812851136\n",
            "Loss S22:  0.07657879503846865\n",
            "Loss S33:  0.08398342193567265\n",
            "Loss S44:  0.08747970979464681\n",
            "Loss S11:  0.07362892600136567\n",
            "Loss S22:  0.07647367945527503\n",
            "Loss S33:  0.08415538014957259\n",
            "Loss S44:  0.08761666319976195\n",
            "Loss S11:  0.07362500944842842\n",
            "Loss S22:  0.07646583808654266\n",
            "Loss S33:  0.0841101851138769\n",
            "Loss S44:  0.08762756957433611\n",
            "Loss S11:  0.07346109278611283\n",
            "Loss S22:  0.0765014659261229\n",
            "Loss S33:  0.08411591214623618\n",
            "Loss S44:  0.0875960968397743\n",
            "Loss S11:  0.0735035862826623\n",
            "Loss S22:  0.07652724777917726\n",
            "Loss S33:  0.08405366780068638\n",
            "Loss S44:  0.0874602559477232\n",
            "Loss S11:  0.07363328691656233\n",
            "Loss S22:  0.07666597734479343\n",
            "Loss S33:  0.08420430326084206\n",
            "Loss S44:  0.08767543254394876\n",
            "Loss S11:  0.07371509078390155\n",
            "Loss S22:  0.07670309275259703\n",
            "Loss S33:  0.08424782649779217\n",
            "Loss S44:  0.08777711992010925\n",
            "Loss S11:  0.07373719322359908\n",
            "Loss S22:  0.07695147664096841\n",
            "Loss S33:  0.08443985913428033\n",
            "Loss S44:  0.08791365369350583\n",
            "Loss S11:  0.07376987035887175\n",
            "Loss S22:  0.07707830874687172\n",
            "Loss S33:  0.08449367974027694\n",
            "Loss S44:  0.08795203197287374\n",
            "Loss S11:  0.07378433604806776\n",
            "Loss S22:  0.07716292074356956\n",
            "Loss S33:  0.08454690839367351\n",
            "Loss S44:  0.08789372566901861\n",
            "Loss S11:  0.07381932756337732\n",
            "Loss S22:  0.07726394662777876\n",
            "Loss S33:  0.08470870276768709\n",
            "Loss S44:  0.08786128892999734\n",
            "Loss S11:  0.07379010515917238\n",
            "Loss S22:  0.07731525774952355\n",
            "Loss S33:  0.08469700383758205\n",
            "Loss S44:  0.08786461947757579\n",
            "Loss S11:  0.07377250594986264\n",
            "Loss S22:  0.07738064762038463\n",
            "Loss S33:  0.08462586750586827\n",
            "Loss S44:  0.08789136688324184\n",
            "Loss S11:  0.07370913637634924\n",
            "Loss S22:  0.07738481158780497\n",
            "Loss S33:  0.08464113657557687\n",
            "Loss S44:  0.08791391197530138\n",
            "Loss S11:  0.07366824229143057\n",
            "Loss S22:  0.07730907416180782\n",
            "Loss S33:  0.08462927967214125\n",
            "Loss S44:  0.08782574026530962\n",
            "Loss S11:  0.07370628437427716\n",
            "Loss S22:  0.07732440168008997\n",
            "Loss S33:  0.08462348603570945\n",
            "Loss S44:  0.08774901148964682\n",
            "Loss S11:  0.0736579667225345\n",
            "Loss S22:  0.07728662640081431\n",
            "Loss S33:  0.08451934446921161\n",
            "Loss S44:  0.0877006535398636\n",
            "Loss S11:  0.07369354397845058\n",
            "Loss S22:  0.07733420401680505\n",
            "Loss S33:  0.08456651661053431\n",
            "Loss S44:  0.08775899825819776\n",
            "Loss S11:  0.07373210150971372\n",
            "Loss S22:  0.07736360115076063\n",
            "Loss S33:  0.08456194122717252\n",
            "Loss S44:  0.08779831880178207\n",
            "Loss S11:  0.07370010439378732\n",
            "Loss S22:  0.07732076659138183\n",
            "Loss S33:  0.08446934486937985\n",
            "Loss S44:  0.087744236570316\n",
            "Loss S11:  0.07369187331665558\n",
            "Loss S22:  0.07731594184458737\n",
            "Loss S33:  0.08443183542261869\n",
            "Loss S44:  0.08770082135888123\n",
            "Loss S11:  0.07364305853843689\n",
            "Loss S22:  0.0771626119362557\n",
            "Loss S33:  0.08430832301772485\n",
            "Loss S44:  0.0877389727732328\n",
            "Loss S11:  0.0736374090356595\n",
            "Loss S22:  0.07708891928005401\n",
            "Loss S33:  0.08426124131892954\n",
            "Loss S44:  0.08769574368853703\n",
            "Loss S11:  0.07357517630790832\n",
            "Loss S22:  0.07706416518127829\n",
            "Loss S33:  0.0842511584596741\n",
            "Loss S44:  0.087648044649205\n",
            "Loss S11:  0.07352934961735187\n",
            "Loss S22:  0.0770307237377567\n",
            "Loss S33:  0.08421909535380756\n",
            "Loss S44:  0.08770281810612574\n",
            "Loss S11:  0.07354718412221753\n",
            "Loss S22:  0.0770454483102308\n",
            "Loss S33:  0.08424920342738725\n",
            "Loss S44:  0.0877637595398409\n",
            "Loss S11:  0.07350689164965722\n",
            "Loss S22:  0.07697789741813998\n",
            "Loss S33:  0.08419543406180605\n",
            "Loss S44:  0.0877243092849747\n",
            "Loss S11:  0.07346423832771459\n",
            "Loss S22:  0.07693348381490935\n",
            "Loss S33:  0.08415482394278996\n",
            "Loss S44:  0.08776752985253626\n",
            "Loss S11:  0.07350053521752886\n",
            "Loss S22:  0.07694547217826621\n",
            "Loss S33:  0.08414202684045102\n",
            "Loss S44:  0.08776392642317749\n",
            "Loss S11:  0.07360438082100512\n",
            "Loss S22:  0.07695735523117597\n",
            "Loss S33:  0.08417835558508069\n",
            "Loss S44:  0.0878366540285098\n",
            "Loss S11:  0.07355258970535232\n",
            "Loss S22:  0.07693717675382418\n",
            "Loss S33:  0.08410458572034876\n",
            "Loss S44:  0.08781568571577153\n",
            "Loss S11:  0.07360899743474943\n",
            "Loss S22:  0.07691905208381199\n",
            "Loss S33:  0.0841411450827444\n",
            "Loss S44:  0.08788553106437849\n",
            "Loss S11:  0.07356071078394921\n",
            "Loss S22:  0.07690648099525155\n",
            "Loss S33:  0.08411721492189982\n",
            "Loss S44:  0.08785250103158038\n",
            "Validation: \n",
            " Loss S11:  0.09131576120853424\n",
            " Loss S22:  0.11137218028306961\n",
            " Loss S33:  0.11044151335954666\n",
            " Loss S44:  0.12079203128814697\n",
            " Loss S11:  0.0891780707807768\n",
            " Loss S22:  0.12498765474274046\n",
            " Loss S33:  0.12330876574629829\n",
            " Loss S44:  0.12867842756566547\n",
            " Loss S11:  0.0882030527402715\n",
            " Loss S22:  0.12398028537267591\n",
            " Loss S33:  0.12395372096358276\n",
            " Loss S44:  0.1274334779236375\n",
            " Loss S11:  0.08736568823701045\n",
            " Loss S22:  0.12229737285219255\n",
            " Loss S33:  0.12190961410276226\n",
            " Loss S44:  0.12599077258930833\n",
            " Loss S11:  0.08710659709241655\n",
            " Loss S22:  0.12239851931362976\n",
            " Loss S33:  0.12109458464899181\n",
            " Loss S44:  0.12506978094209859\n",
            "\n",
            "Epoch: 56\n",
            "Loss S11:  0.07096344232559204\n",
            "Loss S22:  0.0639454796910286\n",
            "Loss S33:  0.0809713751077652\n",
            "Loss S44:  0.08753971755504608\n",
            "Loss S11:  0.07603362541307103\n",
            "Loss S22:  0.0769504921680147\n",
            "Loss S33:  0.08526175333694978\n",
            "Loss S44:  0.08841378038579767\n",
            "Loss S11:  0.07500325923874265\n",
            "Loss S22:  0.07684518201720147\n",
            "Loss S33:  0.08412983516852061\n",
            "Loss S44:  0.08623937056178138\n",
            "Loss S11:  0.07394240676395354\n",
            "Loss S22:  0.07615390792489052\n",
            "Loss S33:  0.08278559989506198\n",
            "Loss S44:  0.08610980861609982\n",
            "Loss S11:  0.07371861414938438\n",
            "Loss S22:  0.07617912995742589\n",
            "Loss S33:  0.08325038813963169\n",
            "Loss S44:  0.08600214014693004\n",
            "Loss S11:  0.07409487518609739\n",
            "Loss S22:  0.07681555220601606\n",
            "Loss S33:  0.08377735492061167\n",
            "Loss S44:  0.08609287163206175\n",
            "Loss S11:  0.07422118133208791\n",
            "Loss S22:  0.07704471741787723\n",
            "Loss S33:  0.08313382294822912\n",
            "Loss S44:  0.08656480395403064\n",
            "Loss S11:  0.07343860395567518\n",
            "Loss S22:  0.07673321260322988\n",
            "Loss S33:  0.08284241349344522\n",
            "Loss S44:  0.08626417270008947\n",
            "Loss S11:  0.07314783104775864\n",
            "Loss S22:  0.07643540088593224\n",
            "Loss S33:  0.0825860341206009\n",
            "Loss S44:  0.08613808701435725\n",
            "Loss S11:  0.07348878689847149\n",
            "Loss S22:  0.07662689002169358\n",
            "Loss S33:  0.08302932969488941\n",
            "Loss S44:  0.08643570886208461\n",
            "Loss S11:  0.0732597434653504\n",
            "Loss S22:  0.07645531157308286\n",
            "Loss S33:  0.08270554893677777\n",
            "Loss S44:  0.08618816630084916\n",
            "Loss S11:  0.07310025236225343\n",
            "Loss S22:  0.07631212523257411\n",
            "Loss S33:  0.08255118957242451\n",
            "Loss S44:  0.08619705225164825\n",
            "Loss S11:  0.07279195377895654\n",
            "Loss S22:  0.07599345123595443\n",
            "Loss S33:  0.08245713232962554\n",
            "Loss S44:  0.0859822930136988\n",
            "Loss S11:  0.07291647006992165\n",
            "Loss S22:  0.07606010113394897\n",
            "Loss S33:  0.08269741494237011\n",
            "Loss S44:  0.0861481059029812\n",
            "Loss S11:  0.07284711470417943\n",
            "Loss S22:  0.07598867608194655\n",
            "Loss S33:  0.08266704085659474\n",
            "Loss S44:  0.08636042945985253\n",
            "Loss S11:  0.07313738546229356\n",
            "Loss S22:  0.07612824383259609\n",
            "Loss S33:  0.08284363372633789\n",
            "Loss S44:  0.08676714553738273\n",
            "Loss S11:  0.07301705585133215\n",
            "Loss S22:  0.07588066324961852\n",
            "Loss S33:  0.08273258833041103\n",
            "Loss S44:  0.08678121198408352\n",
            "Loss S11:  0.07286180673461211\n",
            "Loss S22:  0.07571843987581325\n",
            "Loss S33:  0.08256069888845521\n",
            "Loss S44:  0.08661969890545683\n",
            "Loss S11:  0.07286880996839776\n",
            "Loss S22:  0.0755953444425243\n",
            "Loss S33:  0.08285262159879694\n",
            "Loss S44:  0.08671038696614418\n",
            "Loss S11:  0.07290571221974508\n",
            "Loss S22:  0.07550749350436695\n",
            "Loss S33:  0.08273276671065086\n",
            "Loss S44:  0.08653723651357971\n",
            "Loss S11:  0.07284307748598245\n",
            "Loss S22:  0.0755982467561812\n",
            "Loss S33:  0.08282824587169571\n",
            "Loss S44:  0.08647387875105018\n",
            "Loss S11:  0.07285676121499866\n",
            "Loss S22:  0.07577083651770912\n",
            "Loss S33:  0.08294902095721231\n",
            "Loss S44:  0.0864757321315919\n",
            "Loss S11:  0.07289416643019715\n",
            "Loss S22:  0.07576961488340775\n",
            "Loss S33:  0.08307767838104818\n",
            "Loss S44:  0.08655371796761163\n",
            "Loss S11:  0.07290727861257859\n",
            "Loss S22:  0.07579553170831173\n",
            "Loss S33:  0.0831238440685458\n",
            "Loss S44:  0.08664666280730979\n",
            "Loss S11:  0.07292008090810657\n",
            "Loss S22:  0.07588551949354128\n",
            "Loss S33:  0.08324097931632363\n",
            "Loss S44:  0.08681903325052182\n",
            "Loss S11:  0.07296875371519788\n",
            "Loss S22:  0.07593176761768254\n",
            "Loss S33:  0.08333829766606905\n",
            "Loss S44:  0.08677439572682893\n",
            "Loss S11:  0.07302595545106007\n",
            "Loss S22:  0.07600624178504121\n",
            "Loss S33:  0.08341107905710338\n",
            "Loss S44:  0.0867581304690847\n",
            "Loss S11:  0.07307404531269056\n",
            "Loss S22:  0.07615240653667503\n",
            "Loss S33:  0.0834114969436533\n",
            "Loss S44:  0.0868828893257683\n",
            "Loss S11:  0.07309249352358839\n",
            "Loss S22:  0.0761685162066777\n",
            "Loss S33:  0.08345117022240289\n",
            "Loss S44:  0.08688433309254698\n",
            "Loss S11:  0.0730732042355226\n",
            "Loss S22:  0.07625331111329119\n",
            "Loss S33:  0.08351018010955497\n",
            "Loss S44:  0.08692231364676223\n",
            "Loss S11:  0.07306867436911181\n",
            "Loss S22:  0.07628405177068473\n",
            "Loss S33:  0.08342325830974452\n",
            "Loss S44:  0.08704360000219852\n",
            "Loss S11:  0.07300309475116025\n",
            "Loss S22:  0.0762631373893218\n",
            "Loss S33:  0.08344977636525101\n",
            "Loss S44:  0.08703648064201668\n",
            "Loss S11:  0.07311154769681324\n",
            "Loss S22:  0.07636217887852793\n",
            "Loss S33:  0.08348603938877397\n",
            "Loss S44:  0.08701178436153031\n",
            "Loss S11:  0.07309480616963522\n",
            "Loss S22:  0.07631354802639823\n",
            "Loss S33:  0.08341599548421237\n",
            "Loss S44:  0.08695664963603379\n",
            "Loss S11:  0.07314500660578177\n",
            "Loss S22:  0.07639159632524437\n",
            "Loss S33:  0.08345042300189345\n",
            "Loss S44:  0.08703454926915882\n",
            "Loss S11:  0.07316210394741124\n",
            "Loss S22:  0.07642756384449807\n",
            "Loss S33:  0.08347707171725412\n",
            "Loss S44:  0.08708870120113052\n",
            "Loss S11:  0.07306797146136741\n",
            "Loss S22:  0.0764495975072199\n",
            "Loss S33:  0.08352264988950746\n",
            "Loss S44:  0.08702278432340833\n",
            "Loss S11:  0.07303933662706308\n",
            "Loss S22:  0.07646479060988863\n",
            "Loss S33:  0.0834250311766352\n",
            "Loss S44:  0.08705398080885571\n",
            "Loss S11:  0.07302640308076003\n",
            "Loss S22:  0.07644556972181077\n",
            "Loss S33:  0.0833906891151989\n",
            "Loss S44:  0.08703513009341683\n",
            "Loss S11:  0.07299316810715534\n",
            "Loss S22:  0.07635930896072132\n",
            "Loss S33:  0.08337913259215977\n",
            "Loss S44:  0.08702246478908812\n",
            "Loss S11:  0.07300555485851153\n",
            "Loss S22:  0.07641992300860305\n",
            "Loss S33:  0.08345127748580942\n",
            "Loss S44:  0.0870714582632902\n",
            "Loss S11:  0.0729675256531604\n",
            "Loss S22:  0.0764348853624215\n",
            "Loss S33:  0.08342234937161425\n",
            "Loss S44:  0.08705486714332353\n",
            "Loss S11:  0.07302756333223714\n",
            "Loss S22:  0.07647421909164363\n",
            "Loss S33:  0.08343877995523874\n",
            "Loss S44:  0.08711703394201192\n",
            "Loss S11:  0.07301457782909102\n",
            "Loss S22:  0.07644577970049773\n",
            "Loss S33:  0.08345367080097685\n",
            "Loss S44:  0.08711536932226677\n",
            "Loss S11:  0.07302479998392312\n",
            "Loss S22:  0.07647931110325043\n",
            "Loss S33:  0.08336566143633287\n",
            "Loss S44:  0.08711511592746052\n",
            "Loss S11:  0.07305101330026555\n",
            "Loss S22:  0.07648615244189545\n",
            "Loss S33:  0.08337336169047789\n",
            "Loss S44:  0.0871306047372173\n",
            "Loss S11:  0.07301417092536122\n",
            "Loss S22:  0.07652382370803723\n",
            "Loss S33:  0.08345573479531902\n",
            "Loss S44:  0.08716577301185716\n",
            "Loss S11:  0.07296321763322865\n",
            "Loss S22:  0.07653571922339958\n",
            "Loss S33:  0.08343302306665737\n",
            "Loss S44:  0.08711085751684355\n",
            "Loss S11:  0.07300123628887219\n",
            "Loss S22:  0.07656501688202552\n",
            "Loss S33:  0.08348933542282815\n",
            "Loss S44:  0.08714468970801875\n",
            "Loss S11:  0.07298088633662086\n",
            "Loss S22:  0.07649986243326892\n",
            "Loss S33:  0.08342721070449367\n",
            "Loss S44:  0.0870884200311484\n",
            "Validation: \n",
            " Loss S11:  0.08833646774291992\n",
            " Loss S22:  0.11364715546369553\n",
            " Loss S33:  0.10886401683092117\n",
            " Loss S44:  0.11804511398077011\n",
            " Loss S11:  0.08681587058873404\n",
            " Loss S22:  0.12436079907984961\n",
            " Loss S33:  0.11983614627804075\n",
            " Loss S44:  0.12877175495738075\n",
            " Loss S11:  0.08581876663900004\n",
            " Loss S22:  0.12341331236246156\n",
            " Loss S33:  0.1212167921589642\n",
            " Loss S44:  0.12811219528680895\n",
            " Loss S11:  0.0851035367269985\n",
            " Loss S22:  0.12175161135001261\n",
            " Loss S33:  0.11890501260268883\n",
            " Loss S44:  0.12650686013894002\n",
            " Loss S11:  0.08488424554651167\n",
            " Loss S22:  0.12149218028342282\n",
            " Loss S33:  0.11821442115821956\n",
            " Loss S44:  0.12538178458257956\n",
            "\n",
            "Epoch: 57\n",
            "Loss S11:  0.07044633477926254\n",
            "Loss S22:  0.06716584414243698\n",
            "Loss S33:  0.0718328207731247\n",
            "Loss S44:  0.0827670618891716\n",
            "Loss S11:  0.07406477223743092\n",
            "Loss S22:  0.07757204906506972\n",
            "Loss S33:  0.08463216979395259\n",
            "Loss S44:  0.08761811459606345\n",
            "Loss S11:  0.07292531661334492\n",
            "Loss S22:  0.07716819573016394\n",
            "Loss S33:  0.08387521867241178\n",
            "Loss S44:  0.08643740663925807\n",
            "Loss S11:  0.07190368048125698\n",
            "Loss S22:  0.07644900078735044\n",
            "Loss S33:  0.0828535609668301\n",
            "Loss S44:  0.08549815728779762\n",
            "Loss S11:  0.07224084518668128\n",
            "Loss S22:  0.07672799651215716\n",
            "Loss S33:  0.08288019768348555\n",
            "Loss S44:  0.08585293536506049\n",
            "Loss S11:  0.07220248972960547\n",
            "Loss S22:  0.07697226092511532\n",
            "Loss S33:  0.08289384958790798\n",
            "Loss S44:  0.08550839140719059\n",
            "Loss S11:  0.07230128610476119\n",
            "Loss S22:  0.07674397308318341\n",
            "Loss S33:  0.0830807029956677\n",
            "Loss S44:  0.0861041633809199\n",
            "Loss S11:  0.07207105943644551\n",
            "Loss S22:  0.07680427178110875\n",
            "Loss S33:  0.08270660787820816\n",
            "Loss S44:  0.08598582960770164\n",
            "Loss S11:  0.07224836554608227\n",
            "Loss S22:  0.07652336487799515\n",
            "Loss S33:  0.08287645296918021\n",
            "Loss S44:  0.0860763277720522\n",
            "Loss S11:  0.07233386277988717\n",
            "Loss S22:  0.07651016705638759\n",
            "Loss S33:  0.08262146026878567\n",
            "Loss S44:  0.08625844906974625\n",
            "Loss S11:  0.07211202320338476\n",
            "Loss S22:  0.07645477088961271\n",
            "Loss S33:  0.08244782678856707\n",
            "Loss S44:  0.08585484296378523\n",
            "Loss S11:  0.07212619473402565\n",
            "Loss S22:  0.07604864978038513\n",
            "Loss S33:  0.08234775422124176\n",
            "Loss S44:  0.0858060810882766\n",
            "Loss S11:  0.0719315004496535\n",
            "Loss S22:  0.07575076763048645\n",
            "Loss S33:  0.0822060510885617\n",
            "Loss S44:  0.08554070787735221\n",
            "Loss S11:  0.07219183405164543\n",
            "Loss S22:  0.07599130415051948\n",
            "Loss S33:  0.08243237276568667\n",
            "Loss S44:  0.08556338388273735\n",
            "Loss S11:  0.07227810218613198\n",
            "Loss S22:  0.07615353732455707\n",
            "Loss S33:  0.08240334140070786\n",
            "Loss S44:  0.08557977553800489\n",
            "Loss S11:  0.07259388998249508\n",
            "Loss S22:  0.07646774702908977\n",
            "Loss S33:  0.08258726403413229\n",
            "Loss S44:  0.08587150177024058\n",
            "Loss S11:  0.07254335659075968\n",
            "Loss S22:  0.07639585676030343\n",
            "Loss S33:  0.08257405876372911\n",
            "Loss S44:  0.08593033568829483\n",
            "Loss S11:  0.07251114511524724\n",
            "Loss S22:  0.07623232110289105\n",
            "Loss S33:  0.08243964442558456\n",
            "Loss S44:  0.08590621597062774\n",
            "Loss S11:  0.0725990685310153\n",
            "Loss S22:  0.07624022574138246\n",
            "Loss S33:  0.0824411021576402\n",
            "Loss S44:  0.08609227671478335\n",
            "Loss S11:  0.07275006900595121\n",
            "Loss S22:  0.07620447554201355\n",
            "Loss S33:  0.0825039245851377\n",
            "Loss S44:  0.08629334312302904\n",
            "Loss S11:  0.07268192454134646\n",
            "Loss S22:  0.07619955572322827\n",
            "Loss S33:  0.08243775371443574\n",
            "Loss S44:  0.08621117761775629\n",
            "Loss S11:  0.07267756479399465\n",
            "Loss S22:  0.07613408918629326\n",
            "Loss S33:  0.08254718759331094\n",
            "Loss S44:  0.08613649109528528\n",
            "Loss S11:  0.07288796452982393\n",
            "Loss S22:  0.07627420356640449\n",
            "Loss S33:  0.08256795938333235\n",
            "Loss S44:  0.08638013612765533\n",
            "Loss S11:  0.07289968487439734\n",
            "Loss S22:  0.07619359202328183\n",
            "Loss S33:  0.0824846951734452\n",
            "Loss S44:  0.08641381906869608\n",
            "Loss S11:  0.07292725697654412\n",
            "Loss S22:  0.07639310586254626\n",
            "Loss S33:  0.08268484303194458\n",
            "Loss S44:  0.08648426169181761\n",
            "Loss S11:  0.07295487599959412\n",
            "Loss S22:  0.07639806037880036\n",
            "Loss S33:  0.08267518806742483\n",
            "Loss S44:  0.08652377921152875\n",
            "Loss S11:  0.07303484365115677\n",
            "Loss S22:  0.07640243955384726\n",
            "Loss S33:  0.08281412198283207\n",
            "Loss S44:  0.08657176410101383\n",
            "Loss S11:  0.07302108391899464\n",
            "Loss S22:  0.07645933877168107\n",
            "Loss S33:  0.0827668010913578\n",
            "Loss S44:  0.08650579550939293\n",
            "Loss S11:  0.07297240487992551\n",
            "Loss S22:  0.07641492532135329\n",
            "Loss S33:  0.08280025616129098\n",
            "Loss S44:  0.0865685690022024\n",
            "Loss S11:  0.07299111656767805\n",
            "Loss S22:  0.07641450638316341\n",
            "Loss S33:  0.08281709980923695\n",
            "Loss S44:  0.08650549140173136\n",
            "Loss S11:  0.07293839997726421\n",
            "Loss S22:  0.0763428398838075\n",
            "Loss S33:  0.08280912113447125\n",
            "Loss S44:  0.08648801810321617\n",
            "Loss S11:  0.07290245527910649\n",
            "Loss S22:  0.07629413979421475\n",
            "Loss S33:  0.08279468111957385\n",
            "Loss S44:  0.08637926794612523\n",
            "Loss S11:  0.07292927973366972\n",
            "Loss S22:  0.07629510059349262\n",
            "Loss S33:  0.08285906903933142\n",
            "Loss S44:  0.08639772152789285\n",
            "Loss S11:  0.07288033036892147\n",
            "Loss S22:  0.07626874767401427\n",
            "Loss S33:  0.08283291516257196\n",
            "Loss S44:  0.08639639944648454\n",
            "Loss S11:  0.07290092026514391\n",
            "Loss S22:  0.07634262198751623\n",
            "Loss S33:  0.08293054307485956\n",
            "Loss S44:  0.0864190069461498\n",
            "Loss S11:  0.07293702697015217\n",
            "Loss S22:  0.07636377455247434\n",
            "Loss S33:  0.08293359021940122\n",
            "Loss S44:  0.0864877773613332\n",
            "Loss S11:  0.07290820911567958\n",
            "Loss S22:  0.07633502250975849\n",
            "Loss S33:  0.08297177009965574\n",
            "Loss S44:  0.08650914057470094\n",
            "Loss S11:  0.07286018068899684\n",
            "Loss S22:  0.07631541525498234\n",
            "Loss S33:  0.08293354941024934\n",
            "Loss S44:  0.08644824726080316\n",
            "Loss S11:  0.07277119984933397\n",
            "Loss S22:  0.07620711237426818\n",
            "Loss S33:  0.0828467263911027\n",
            "Loss S44:  0.08640902407291368\n",
            "Loss S11:  0.0727469587836729\n",
            "Loss S22:  0.07618133500790047\n",
            "Loss S33:  0.08280125993978033\n",
            "Loss S44:  0.08635590823791216\n",
            "Loss S11:  0.07279740413302495\n",
            "Loss S22:  0.07622742866563084\n",
            "Loss S33:  0.0828701078222874\n",
            "Loss S44:  0.08632318383812013\n",
            "Loss S11:  0.07274643368928392\n",
            "Loss S22:  0.07622294131566719\n",
            "Loss S33:  0.08282778847174052\n",
            "Loss S44:  0.08627378472881596\n",
            "Loss S11:  0.07281020374840343\n",
            "Loss S22:  0.07620176237580895\n",
            "Loss S33:  0.08285179155526988\n",
            "Loss S44:  0.08631960148624457\n",
            "Loss S11:  0.0727965624748001\n",
            "Loss S22:  0.07614307681189336\n",
            "Loss S33:  0.0827876713533136\n",
            "Loss S44:  0.08631625386028445\n",
            "Loss S11:  0.07279446318137403\n",
            "Loss S22:  0.07612170667605064\n",
            "Loss S33:  0.08280363547153213\n",
            "Loss S44:  0.08629866439401428\n",
            "Loss S11:  0.07281728836391824\n",
            "Loss S22:  0.07614741908869564\n",
            "Loss S33:  0.08285498967654426\n",
            "Loss S44:  0.08625906317633165\n",
            "Loss S11:  0.07284867751397953\n",
            "Loss S22:  0.07610415182442055\n",
            "Loss S33:  0.08281316695464148\n",
            "Loss S44:  0.08623247480508821\n",
            "Loss S11:  0.0728512125014626\n",
            "Loss S22:  0.07608319724061686\n",
            "Loss S33:  0.08283988903662201\n",
            "Loss S44:  0.08620350474131841\n",
            "Loss S11:  0.07289343608423231\n",
            "Loss S22:  0.07604338454853224\n",
            "Loss S33:  0.0828680093100066\n",
            "Loss S44:  0.08624081722540072\n",
            "Loss S11:  0.0728624359972001\n",
            "Loss S22:  0.07602644248916758\n",
            "Loss S33:  0.08285495224589\n",
            "Loss S44:  0.08621518653789023\n",
            "Validation: \n",
            " Loss S11:  0.09081098437309265\n",
            " Loss S22:  0.10945359617471695\n",
            " Loss S33:  0.10881166160106659\n",
            " Loss S44:  0.12137120962142944\n",
            " Loss S11:  0.08625575509809312\n",
            " Loss S22:  0.12226000286283947\n",
            " Loss S33:  0.11908867139191855\n",
            " Loss S44:  0.12912754395178386\n",
            " Loss S11:  0.08548324736880093\n",
            " Loss S22:  0.12115215928089328\n",
            " Loss S33:  0.11932393781295637\n",
            " Loss S44:  0.1278910251652322\n",
            " Loss S11:  0.08444523408276136\n",
            " Loss S22:  0.11980058631447495\n",
            " Loss S33:  0.11655663540128802\n",
            " Loss S44:  0.12603438022683877\n",
            " Loss S11:  0.0840718353420128\n",
            " Loss S22:  0.11971343170713496\n",
            " Loss S33:  0.11610657234250764\n",
            " Loss S44:  0.1249551464010168\n",
            "\n",
            "Epoch: 58\n",
            "Loss S11:  0.0744103193283081\n",
            "Loss S22:  0.0710122361779213\n",
            "Loss S33:  0.07422754913568497\n",
            "Loss S44:  0.08009803295135498\n",
            "Loss S11:  0.07446956634521484\n",
            "Loss S22:  0.07601808146996931\n",
            "Loss S33:  0.0811057605526664\n",
            "Loss S44:  0.08734453266317194\n",
            "Loss S11:  0.0735937122787748\n",
            "Loss S22:  0.07658562330263001\n",
            "Loss S33:  0.08186892987716765\n",
            "Loss S44:  0.08650804222339675\n",
            "Loss S11:  0.07263727402014117\n",
            "Loss S22:  0.07481541905191637\n",
            "Loss S33:  0.08093861851000017\n",
            "Loss S44:  0.08490992517721269\n",
            "Loss S11:  0.07255585119128227\n",
            "Loss S22:  0.07512683703041659\n",
            "Loss S33:  0.08082200877550172\n",
            "Loss S44:  0.08529912816678606\n",
            "Loss S11:  0.07274316273191396\n",
            "Loss S22:  0.07543530906824504\n",
            "Loss S33:  0.08105096396277933\n",
            "Loss S44:  0.0851006815392597\n",
            "Loss S11:  0.0728777144409594\n",
            "Loss S22:  0.07572681511767575\n",
            "Loss S33:  0.08113626305197107\n",
            "Loss S44:  0.08534969139050265\n",
            "Loss S11:  0.07271804875681098\n",
            "Loss S22:  0.0756347467466979\n",
            "Loss S33:  0.08105150452801879\n",
            "Loss S44:  0.08534365929138492\n",
            "Loss S11:  0.07266408485578901\n",
            "Loss S22:  0.07581544285755099\n",
            "Loss S33:  0.08098767633423393\n",
            "Loss S44:  0.08566863324355196\n",
            "Loss S11:  0.07261974663361088\n",
            "Loss S22:  0.07577540827812729\n",
            "Loss S33:  0.08116275841718192\n",
            "Loss S44:  0.08582732241068568\n",
            "Loss S11:  0.07226772296546709\n",
            "Loss S22:  0.07543983793642262\n",
            "Loss S33:  0.08100599633290036\n",
            "Loss S44:  0.08560673416693612\n",
            "Loss S11:  0.07209270330028492\n",
            "Loss S22:  0.07504405299419756\n",
            "Loss S33:  0.08085270548189008\n",
            "Loss S44:  0.08545270787031801\n",
            "Loss S11:  0.0719701707794154\n",
            "Loss S22:  0.07504173876209692\n",
            "Loss S33:  0.08077623036282122\n",
            "Loss S44:  0.0850495324462406\n",
            "Loss S11:  0.07217541391044173\n",
            "Loss S22:  0.07520110259643038\n",
            "Loss S33:  0.08101090458968213\n",
            "Loss S44:  0.08517710219016512\n",
            "Loss S11:  0.07219374541800919\n",
            "Loss S22:  0.07530610968774938\n",
            "Loss S33:  0.08114164486421761\n",
            "Loss S44:  0.08527065800331163\n",
            "Loss S11:  0.07236132640021527\n",
            "Loss S22:  0.07532104353932355\n",
            "Loss S33:  0.08142006145605188\n",
            "Loss S44:  0.08526647152588857\n",
            "Loss S11:  0.07220924391139368\n",
            "Loss S22:  0.07522500868632186\n",
            "Loss S33:  0.08131926821440644\n",
            "Loss S44:  0.08502027602947276\n",
            "Loss S11:  0.07214128474394481\n",
            "Loss S22:  0.0752060158254459\n",
            "Loss S33:  0.08148775515500566\n",
            "Loss S44:  0.08508314922103408\n",
            "Loss S11:  0.07223321748537254\n",
            "Loss S22:  0.07520735364674863\n",
            "Loss S33:  0.08153505705667465\n",
            "Loss S44:  0.08528276506132183\n",
            "Loss S11:  0.07215288839259072\n",
            "Loss S22:  0.07521959006084197\n",
            "Loss S33:  0.08165310689916161\n",
            "Loss S44:  0.08526525340233174\n",
            "Loss S11:  0.07217426212569375\n",
            "Loss S22:  0.0752375682083825\n",
            "Loss S33:  0.08164262808674011\n",
            "Loss S44:  0.08538623652722112\n",
            "Loss S11:  0.07224634339176648\n",
            "Loss S22:  0.0753498445860865\n",
            "Loss S33:  0.0818159464764369\n",
            "Loss S44:  0.08541959720129651\n",
            "Loss S11:  0.07233789060854803\n",
            "Loss S22:  0.07545445443076246\n",
            "Loss S33:  0.08193330692624615\n",
            "Loss S44:  0.0855192951183783\n",
            "Loss S11:  0.07238674450875361\n",
            "Loss S22:  0.07556089889500048\n",
            "Loss S33:  0.08203151002848819\n",
            "Loss S44:  0.08566346043696652\n",
            "Loss S11:  0.07248107728807264\n",
            "Loss S22:  0.07562637002945441\n",
            "Loss S33:  0.08214081527039223\n",
            "Loss S44:  0.08585605608044324\n",
            "Loss S11:  0.07255250792876183\n",
            "Loss S22:  0.07558946191255315\n",
            "Loss S33:  0.08222478951709679\n",
            "Loss S44:  0.08591486880206967\n",
            "Loss S11:  0.0725850647465251\n",
            "Loss S22:  0.07563255674480479\n",
            "Loss S33:  0.08232713322301477\n",
            "Loss S44:  0.08583949424925892\n",
            "Loss S11:  0.07267461058673383\n",
            "Loss S22:  0.07568553916257686\n",
            "Loss S33:  0.0824381797900939\n",
            "Loss S44:  0.0858517584848008\n",
            "Loss S11:  0.07270474712036258\n",
            "Loss S22:  0.0757573491287189\n",
            "Loss S33:  0.08251191055223187\n",
            "Loss S44:  0.08603706861783177\n",
            "Loss S11:  0.07268732280512036\n",
            "Loss S22:  0.07581920172903955\n",
            "Loss S33:  0.08244602548604979\n",
            "Loss S44:  0.08609867670742917\n",
            "Loss S11:  0.07269280315980167\n",
            "Loss S22:  0.07577858195698935\n",
            "Loss S33:  0.08248225448535526\n",
            "Loss S44:  0.08614021865186897\n",
            "Loss S11:  0.07261512762720179\n",
            "Loss S22:  0.07571622340002628\n",
            "Loss S33:  0.08249439155365493\n",
            "Loss S44:  0.08607349823860877\n",
            "Loss S11:  0.07268146918709405\n",
            "Loss S22:  0.07569327561701199\n",
            "Loss S33:  0.08264839876663647\n",
            "Loss S44:  0.08607483252511589\n",
            "Loss S11:  0.07264243576146685\n",
            "Loss S22:  0.07558276309572678\n",
            "Loss S33:  0.08255538846340785\n",
            "Loss S44:  0.08603254337640327\n",
            "Loss S11:  0.07266038023700112\n",
            "Loss S22:  0.07554688749294128\n",
            "Loss S33:  0.08254146134573694\n",
            "Loss S44:  0.08609762168393806\n",
            "Loss S11:  0.07260734454179421\n",
            "Loss S22:  0.07552811217910883\n",
            "Loss S33:  0.0825234880389651\n",
            "Loss S44:  0.08617359668313608\n",
            "Loss S11:  0.07257029915698017\n",
            "Loss S22:  0.07554265837136068\n",
            "Loss S33:  0.0825060241110107\n",
            "Loss S44:  0.08615000053744898\n",
            "Loss S11:  0.07254116004207385\n",
            "Loss S22:  0.07549374949217164\n",
            "Loss S33:  0.08241926404745431\n",
            "Loss S44:  0.08612500941456168\n",
            "Loss S11:  0.0724833225464727\n",
            "Loss S22:  0.07543014980402832\n",
            "Loss S33:  0.0823323623988572\n",
            "Loss S44:  0.08608038302051427\n",
            "Loss S11:  0.07244557015540655\n",
            "Loss S22:  0.07542138930667391\n",
            "Loss S33:  0.08229240900872613\n",
            "Loss S44:  0.08608832859131686\n",
            "Loss S11:  0.07249534220209443\n",
            "Loss S22:  0.07545133209102171\n",
            "Loss S33:  0.08234333018412317\n",
            "Loss S44:  0.08611725128611127\n",
            "Loss S11:  0.07246559779465633\n",
            "Loss S22:  0.07544256374239922\n",
            "Loss S33:  0.08230303759049905\n",
            "Loss S44:  0.08618450356044619\n",
            "Loss S11:  0.07245264635056328\n",
            "Loss S22:  0.07547894634324129\n",
            "Loss S33:  0.08231085073424065\n",
            "Loss S44:  0.08625209712890151\n",
            "Loss S11:  0.0724210297373359\n",
            "Loss S22:  0.0754446290744568\n",
            "Loss S33:  0.08224393600239278\n",
            "Loss S44:  0.08619575071950246\n",
            "Loss S11:  0.07247276931088797\n",
            "Loss S22:  0.07543392530948667\n",
            "Loss S33:  0.08221540355087678\n",
            "Loss S44:  0.08614385926297732\n",
            "Loss S11:  0.07244128714875213\n",
            "Loss S22:  0.07545312709627289\n",
            "Loss S33:  0.08216564021922004\n",
            "Loss S44:  0.08616923744854271\n",
            "Loss S11:  0.07247562569903448\n",
            "Loss S22:  0.07550731937273723\n",
            "Loss S33:  0.08220403568734316\n",
            "Loss S44:  0.08623564787160548\n",
            "Loss S11:  0.07246186880833784\n",
            "Loss S22:  0.07550100041770884\n",
            "Loss S33:  0.08217343787668617\n",
            "Loss S44:  0.08621929365263623\n",
            "Loss S11:  0.07247481681793742\n",
            "Loss S22:  0.07555331189859187\n",
            "Loss S33:  0.08218781506011491\n",
            "Loss S44:  0.08625514968734026\n",
            "Loss S11:  0.07247851609182213\n",
            "Loss S22:  0.07550495158114646\n",
            "Loss S33:  0.08221205710879169\n",
            "Loss S44:  0.0861941754529651\n",
            "Validation: \n",
            " Loss S11:  0.08690822124481201\n",
            " Loss S22:  0.10853908956050873\n",
            " Loss S33:  0.10479797422885895\n",
            " Loss S44:  0.11776373535394669\n",
            " Loss S11:  0.08419472582283474\n",
            " Loss S22:  0.12094260149058841\n",
            " Loss S33:  0.11791364813134783\n",
            " Loss S44:  0.12953507119701022\n",
            " Loss S11:  0.08259214787948423\n",
            " Loss S22:  0.12050606982737053\n",
            " Loss S33:  0.11795591217715566\n",
            " Loss S44:  0.12887371785757018\n",
            " Loss S11:  0.08208388016849268\n",
            " Loss S22:  0.1189667925727172\n",
            " Loss S33:  0.11565086648601\n",
            " Loss S44:  0.12760046551950643\n",
            " Loss S11:  0.08183540449834165\n",
            " Loss S22:  0.11901373269013417\n",
            " Loss S33:  0.11497576571541068\n",
            " Loss S44:  0.12666342904170355\n",
            "\n",
            "Epoch: 59\n",
            "Loss S11:  0.07759861648082733\n",
            "Loss S22:  0.0667889192700386\n",
            "Loss S33:  0.0773136243224144\n",
            "Loss S44:  0.09270386397838593\n",
            "Loss S11:  0.07391620156439868\n",
            "Loss S22:  0.07525752146135677\n",
            "Loss S33:  0.08259575881741264\n",
            "Loss S44:  0.08570458130402998\n",
            "Loss S11:  0.07257010894162315\n",
            "Loss S22:  0.0768479225890977\n",
            "Loss S33:  0.08170989155769348\n",
            "Loss S44:  0.0851792207076436\n",
            "Loss S11:  0.07228848001649303\n",
            "Loss S22:  0.07583307426783346\n",
            "Loss S33:  0.0806977885865396\n",
            "Loss S44:  0.08426451683044434\n",
            "Loss S11:  0.07213008276573042\n",
            "Loss S22:  0.075682467622001\n",
            "Loss S33:  0.08148641975187673\n",
            "Loss S44:  0.08493996447906262\n",
            "Loss S11:  0.07196251660877583\n",
            "Loss S22:  0.07602972025964774\n",
            "Loss S33:  0.08157827150003583\n",
            "Loss S44:  0.08500369068454294\n",
            "Loss S11:  0.0721892207005962\n",
            "Loss S22:  0.07590293737708545\n",
            "Loss S33:  0.08158888533467153\n",
            "Loss S44:  0.0851183211217161\n",
            "Loss S11:  0.07188453137035102\n",
            "Loss S22:  0.07592984551275281\n",
            "Loss S33:  0.08138657843982669\n",
            "Loss S44:  0.08459780627573041\n",
            "Loss S11:  0.07232708980639775\n",
            "Loss S22:  0.0759218767469312\n",
            "Loss S33:  0.08158090462287267\n",
            "Loss S44:  0.08481368220146791\n",
            "Loss S11:  0.07245146253934273\n",
            "Loss S22:  0.07591427453271635\n",
            "Loss S33:  0.08190346378218996\n",
            "Loss S44:  0.08495674503373575\n",
            "Loss S11:  0.0721999482426903\n",
            "Loss S22:  0.07557855871054206\n",
            "Loss S33:  0.08154324574930834\n",
            "Loss S44:  0.08480413619539526\n",
            "Loss S11:  0.07205641977824606\n",
            "Loss S22:  0.07514378082779077\n",
            "Loss S33:  0.08116197404829231\n",
            "Loss S44:  0.08470066032699637\n",
            "Loss S11:  0.07206200041677341\n",
            "Loss S22:  0.07507810972569402\n",
            "Loss S33:  0.08117298529414106\n",
            "Loss S44:  0.08439543424559033\n",
            "Loss S11:  0.07214291288538743\n",
            "Loss S22:  0.07513263068012609\n",
            "Loss S33:  0.08117258156301412\n",
            "Loss S44:  0.08419638656022894\n",
            "Loss S11:  0.07215744264899417\n",
            "Loss S22:  0.07514481249113455\n",
            "Loss S33:  0.08120082498442197\n",
            "Loss S44:  0.08432915954725116\n",
            "Loss S11:  0.07231796175142788\n",
            "Loss S22:  0.07523893246686222\n",
            "Loss S33:  0.08136255215138\n",
            "Loss S44:  0.08463405723998088\n",
            "Loss S11:  0.07216842359174853\n",
            "Loss S22:  0.07498305311762028\n",
            "Loss S33:  0.08124413452348354\n",
            "Loss S44:  0.08453179794068662\n",
            "Loss S11:  0.07215579886708343\n",
            "Loss S22:  0.07487026381998034\n",
            "Loss S33:  0.08109285869793585\n",
            "Loss S44:  0.08453108235235103\n",
            "Loss S11:  0.07216455186434213\n",
            "Loss S22:  0.07478878764404777\n",
            "Loss S33:  0.08124610822833045\n",
            "Loss S44:  0.08478446699967042\n",
            "Loss S11:  0.07219922456753816\n",
            "Loss S22:  0.07481110413895227\n",
            "Loss S33:  0.08112179113464206\n",
            "Loss S44:  0.08479035384367897\n",
            "Loss S11:  0.07223065260482665\n",
            "Loss S22:  0.07480252917800377\n",
            "Loss S33:  0.08114195099814021\n",
            "Loss S44:  0.08470189196998207\n",
            "Loss S11:  0.07217662417775647\n",
            "Loss S22:  0.07484935670737972\n",
            "Loss S33:  0.08118622686484414\n",
            "Loss S44:  0.08479612016988591\n",
            "Loss S11:  0.07228676101605816\n",
            "Loss S22:  0.07495280222048586\n",
            "Loss S33:  0.08150291284419832\n",
            "Loss S44:  0.08503410263973124\n",
            "Loss S11:  0.07243434875171421\n",
            "Loss S22:  0.07492669944316793\n",
            "Loss S33:  0.08162307119988776\n",
            "Loss S44:  0.08511589461074763\n",
            "Loss S11:  0.0725378031799902\n",
            "Loss S22:  0.07498282248239299\n",
            "Loss S33:  0.08176179811420282\n",
            "Loss S44:  0.08527065424008982\n",
            "Loss S11:  0.07254483249498553\n",
            "Loss S22:  0.07500484590214562\n",
            "Loss S33:  0.08183459216261289\n",
            "Loss S44:  0.08524091547703838\n",
            "Loss S11:  0.07256295547688602\n",
            "Loss S22:  0.07509644527944569\n",
            "Loss S33:  0.08188382410569209\n",
            "Loss S44:  0.0852470129899595\n",
            "Loss S11:  0.07258933531017321\n",
            "Loss S22:  0.07518533687365012\n",
            "Loss S33:  0.08193450091179887\n",
            "Loss S44:  0.08539046666059107\n",
            "Loss S11:  0.07261701987466354\n",
            "Loss S22:  0.07520513293528897\n",
            "Loss S33:  0.0819388908529621\n",
            "Loss S44:  0.08540561052828073\n",
            "Loss S11:  0.0725686335840176\n",
            "Loss S22:  0.07528056648537465\n",
            "Loss S33:  0.0819350911169937\n",
            "Loss S44:  0.08538868936271601\n",
            "Loss S11:  0.07260125334625228\n",
            "Loss S22:  0.07531421200876616\n",
            "Loss S33:  0.08192765461497528\n",
            "Loss S44:  0.08547749909946689\n",
            "Loss S11:  0.07254715517786155\n",
            "Loss S22:  0.07533896671326597\n",
            "Loss S33:  0.08194144730159707\n",
            "Loss S44:  0.08549624195148707\n",
            "Loss S11:  0.07261530121910238\n",
            "Loss S22:  0.07533470761738834\n",
            "Loss S33:  0.08200288200721934\n",
            "Loss S44:  0.08551061900790978\n",
            "Loss S11:  0.0725617232279475\n",
            "Loss S22:  0.0752871046208543\n",
            "Loss S33:  0.08191639305376575\n",
            "Loss S44:  0.08547269109512744\n",
            "Loss S11:  0.07257440056968645\n",
            "Loss S22:  0.07536112418797009\n",
            "Loss S33:  0.08196304392211598\n",
            "Loss S44:  0.0854991273490215\n",
            "Loss S11:  0.07259259592646207\n",
            "Loss S22:  0.07538930734253338\n",
            "Loss S33:  0.0820052274675281\n",
            "Loss S44:  0.085520708344431\n",
            "Loss S11:  0.07250602267827023\n",
            "Loss S22:  0.07535871694127609\n",
            "Loss S33:  0.08193561739167018\n",
            "Loss S44:  0.08547468136717408\n",
            "Loss S11:  0.0724532087396258\n",
            "Loss S22:  0.07524366641663155\n",
            "Loss S33:  0.08185634341321865\n",
            "Loss S44:  0.08535779888096524\n",
            "Loss S11:  0.07237757225166469\n",
            "Loss S22:  0.07517266102389401\n",
            "Loss S33:  0.08176264021144765\n",
            "Loss S44:  0.08527590586679189\n",
            "Loss S11:  0.072268491225017\n",
            "Loss S22:  0.07511431801006617\n",
            "Loss S33:  0.08175346296271095\n",
            "Loss S44:  0.08523904743706784\n",
            "Loss S11:  0.07235813653677181\n",
            "Loss S22:  0.07516621308693862\n",
            "Loss S33:  0.08177504540790347\n",
            "Loss S44:  0.08534862448524061\n",
            "Loss S11:  0.0723312708148121\n",
            "Loss S22:  0.07521493804534567\n",
            "Loss S33:  0.08178417019346625\n",
            "Loss S44:  0.08536510856554747\n",
            "Loss S11:  0.07237833766642772\n",
            "Loss S22:  0.07528057702398923\n",
            "Loss S33:  0.08180912393819974\n",
            "Loss S44:  0.0854228912120477\n",
            "Loss S11:  0.07234243435780853\n",
            "Loss S22:  0.07527457093798368\n",
            "Loss S33:  0.08180584330673561\n",
            "Loss S44:  0.08532989704484453\n",
            "Loss S11:  0.07231626197560574\n",
            "Loss S22:  0.0753115086041205\n",
            "Loss S33:  0.08180085879638622\n",
            "Loss S44:  0.08536684041728779\n",
            "Loss S11:  0.07233911243054661\n",
            "Loss S22:  0.07531715766784886\n",
            "Loss S33:  0.0818047279909154\n",
            "Loss S44:  0.08546518838392393\n",
            "Loss S11:  0.07231313033017316\n",
            "Loss S22:  0.07525019880198346\n",
            "Loss S33:  0.08183880379007333\n",
            "Loss S44:  0.08553103921599606\n",
            "Loss S11:  0.07228886297332507\n",
            "Loss S22:  0.07524130916551167\n",
            "Loss S33:  0.08180981019468318\n",
            "Loss S44:  0.08554913130472167\n",
            "Loss S11:  0.07228258918978568\n",
            "Loss S22:  0.0752646505228571\n",
            "Loss S33:  0.08179891108692064\n",
            "Loss S44:  0.08557537112872962\n",
            "Loss S11:  0.07221881184127568\n",
            "Loss S22:  0.07520258239263672\n",
            "Loss S33:  0.08175860616224846\n",
            "Loss S44:  0.08552450419625536\n",
            "Validation: \n",
            " Loss S11:  0.08333948254585266\n",
            " Loss S22:  0.11089292168617249\n",
            " Loss S33:  0.10915091633796692\n",
            " Loss S44:  0.12540841102600098\n",
            " Loss S11:  0.08178366330407914\n",
            " Loss S22:  0.12095996240774791\n",
            " Loss S33:  0.1202745387951533\n",
            " Loss S44:  0.13139734381721133\n",
            " Loss S11:  0.08107172388856004\n",
            " Loss S22:  0.12024795136800627\n",
            " Loss S33:  0.12036505287013403\n",
            " Loss S44:  0.12964914793648372\n",
            " Loss S11:  0.08063229796339254\n",
            " Loss S22:  0.11867116025236786\n",
            " Loss S33:  0.11757837272569781\n",
            " Loss S44:  0.12818699049167945\n",
            " Loss S11:  0.08036249885220587\n",
            " Loss S22:  0.11839693665136526\n",
            " Loss S33:  0.11694429989582227\n",
            " Loss S44:  0.12719821782759677\n",
            "\n",
            "Epoch: 60\n",
            "Loss S11:  0.07922925055027008\n",
            "Loss S22:  0.07829036563634872\n",
            "Loss S33:  0.08731076866388321\n",
            "Loss S44:  0.09518645703792572\n",
            "Loss S11:  0.07513078844005411\n",
            "Loss S22:  0.07496015930717642\n",
            "Loss S33:  0.08264652300964702\n",
            "Loss S44:  0.08706603673371402\n",
            "Loss S11:  0.07349805445188567\n",
            "Loss S22:  0.07487304526425544\n",
            "Loss S33:  0.08233449324255898\n",
            "Loss S44:  0.08489400467702321\n",
            "Loss S11:  0.07306850208870826\n",
            "Loss S22:  0.07336067035794258\n",
            "Loss S33:  0.08167558764257739\n",
            "Loss S44:  0.08431711168058457\n",
            "Loss S11:  0.07285664148810433\n",
            "Loss S22:  0.07359908966393006\n",
            "Loss S33:  0.08147322041232412\n",
            "Loss S44:  0.08468373247036119\n",
            "Loss S11:  0.07270133926295767\n",
            "Loss S22:  0.0740090377044444\n",
            "Loss S33:  0.08126779368110731\n",
            "Loss S44:  0.08430876831213634\n",
            "Loss S11:  0.0726124567941564\n",
            "Loss S22:  0.07398521100155643\n",
            "Loss S33:  0.08093648733662777\n",
            "Loss S44:  0.08455187900633108\n",
            "Loss S11:  0.07226045193596625\n",
            "Loss S22:  0.07401975260024339\n",
            "Loss S33:  0.08105463712987765\n",
            "Loss S44:  0.08412439701422839\n",
            "Loss S11:  0.0721988768581255\n",
            "Loss S22:  0.07404069091986727\n",
            "Loss S33:  0.08102944566879744\n",
            "Loss S44:  0.0842740183443199\n",
            "Loss S11:  0.07211447830547343\n",
            "Loss S22:  0.07394163527495258\n",
            "Loss S33:  0.08151458113730609\n",
            "Loss S44:  0.08449890128858797\n",
            "Loss S11:  0.07190958288784075\n",
            "Loss S22:  0.07392752911932397\n",
            "Loss S33:  0.08139024719153301\n",
            "Loss S44:  0.08454465917726554\n",
            "Loss S11:  0.07170750378622665\n",
            "Loss S22:  0.07383126741996757\n",
            "Loss S33:  0.08120966548318262\n",
            "Loss S44:  0.0846004218265817\n",
            "Loss S11:  0.07158039673423965\n",
            "Loss S22:  0.07364491127000368\n",
            "Loss S33:  0.0811318800099625\n",
            "Loss S44:  0.0843867686661807\n",
            "Loss S11:  0.071796657958331\n",
            "Loss S22:  0.07389716235280946\n",
            "Loss S33:  0.08120205367111978\n",
            "Loss S44:  0.08441322131921317\n",
            "Loss S11:  0.07175194923865034\n",
            "Loss S22:  0.07387851083532293\n",
            "Loss S33:  0.08108168227452758\n",
            "Loss S44:  0.08434072078753871\n",
            "Loss S11:  0.07197548387362468\n",
            "Loss S22:  0.07405605941813513\n",
            "Loss S33:  0.08127679266282264\n",
            "Loss S44:  0.0845526322130336\n",
            "Loss S11:  0.07188144423391508\n",
            "Loss S22:  0.07393256024174068\n",
            "Loss S33:  0.0810838407888916\n",
            "Loss S44:  0.08456718648627679\n",
            "Loss S11:  0.07184289173598875\n",
            "Loss S22:  0.07397597714474327\n",
            "Loss S33:  0.08100538352253842\n",
            "Loss S44:  0.08461941281954448\n",
            "Loss S11:  0.07181459224849775\n",
            "Loss S22:  0.07406796185680516\n",
            "Loss S33:  0.08107309562066642\n",
            "Loss S44:  0.08466494359199514\n",
            "Loss S11:  0.07179285558574487\n",
            "Loss S22:  0.07402389551176451\n",
            "Loss S33:  0.08116171384201\n",
            "Loss S44:  0.08481652184306639\n",
            "Loss S11:  0.07173790386067101\n",
            "Loss S22:  0.07403758194167816\n",
            "Loss S33:  0.08113943718707384\n",
            "Loss S44:  0.08482227555999708\n",
            "Loss S11:  0.07178566071659467\n",
            "Loss S22:  0.07405503475553052\n",
            "Loss S33:  0.08116563275385807\n",
            "Loss S44:  0.08488073930921147\n",
            "Loss S11:  0.07190297213614796\n",
            "Loss S22:  0.07416501472708327\n",
            "Loss S33:  0.08130688378713789\n",
            "Loss S44:  0.08490646782234244\n",
            "Loss S11:  0.07194825832719927\n",
            "Loss S22:  0.0742544737277609\n",
            "Loss S33:  0.08133261192799647\n",
            "Loss S44:  0.08495973208636949\n",
            "Loss S11:  0.07202190939816201\n",
            "Loss S22:  0.07418999561259361\n",
            "Loss S33:  0.08146827590663404\n",
            "Loss S44:  0.08507283863561282\n",
            "Loss S11:  0.072009469482647\n",
            "Loss S22:  0.074164169851646\n",
            "Loss S33:  0.08143689251872173\n",
            "Loss S44:  0.08511845426136753\n",
            "Loss S11:  0.07199144703104121\n",
            "Loss S22:  0.0742470057845344\n",
            "Loss S33:  0.08140363498551635\n",
            "Loss S44:  0.08515132241436348\n",
            "Loss S11:  0.07201064971870162\n",
            "Loss S22:  0.0742915420212209\n",
            "Loss S33:  0.08146567439241163\n",
            "Loss S44:  0.08515535767887791\n",
            "Loss S11:  0.07202130167668824\n",
            "Loss S22:  0.07435717811522959\n",
            "Loss S33:  0.08153068918562445\n",
            "Loss S44:  0.08515123654408811\n",
            "Loss S11:  0.07199599652765543\n",
            "Loss S22:  0.0743283937041907\n",
            "Loss S33:  0.08146729199951867\n",
            "Loss S44:  0.08514760401343152\n",
            "Loss S11:  0.07194382885464798\n",
            "Loss S22:  0.07437549998049324\n",
            "Loss S33:  0.08147764051227871\n",
            "Loss S44:  0.0851804079655001\n",
            "Loss S11:  0.07186578845431567\n",
            "Loss S22:  0.0743828967956293\n",
            "Loss S33:  0.08142532028521372\n",
            "Loss S44:  0.0852120272887098\n",
            "Loss S11:  0.07187179400588493\n",
            "Loss S22:  0.07444280377847383\n",
            "Loss S33:  0.08145246762677888\n",
            "Loss S44:  0.08520211868939741\n",
            "Loss S11:  0.07179355939844584\n",
            "Loss S22:  0.07442983799245423\n",
            "Loss S33:  0.08134933683762377\n",
            "Loss S44:  0.08522041261286174\n",
            "Loss S11:  0.07184746469264505\n",
            "Loss S22:  0.0745088955458483\n",
            "Loss S33:  0.08145660337858186\n",
            "Loss S44:  0.08527212371487072\n",
            "Loss S11:  0.07188291020924889\n",
            "Loss S22:  0.07452455257460942\n",
            "Loss S33:  0.08146215645208997\n",
            "Loss S44:  0.08532503896798843\n",
            "Loss S11:  0.07180719260239865\n",
            "Loss S22:  0.07447611265028943\n",
            "Loss S33:  0.08146269969291305\n",
            "Loss S44:  0.08536700594788443\n",
            "Loss S11:  0.07182363929814406\n",
            "Loss S22:  0.07445583525653476\n",
            "Loss S33:  0.08145931103038338\n",
            "Loss S44:  0.08534220893267351\n",
            "Loss S11:  0.07176840120530505\n",
            "Loss S22:  0.0743805354954846\n",
            "Loss S33:  0.08139618428477778\n",
            "Loss S44:  0.08526879089476243\n",
            "Loss S11:  0.07173670636837745\n",
            "Loss S22:  0.07439427785670666\n",
            "Loss S33:  0.08134337368866672\n",
            "Loss S44:  0.08525254849887565\n",
            "Loss S11:  0.07177314659269374\n",
            "Loss S22:  0.07441782103177913\n",
            "Loss S33:  0.08143980963680512\n",
            "Loss S44:  0.0852583322739066\n",
            "Loss S11:  0.07174006582140342\n",
            "Loss S22:  0.07443587908888386\n",
            "Loss S33:  0.08147443237277133\n",
            "Loss S44:  0.08523572143847055\n",
            "Loss S11:  0.07176208358939357\n",
            "Loss S22:  0.07444366441946698\n",
            "Loss S33:  0.08149012062265018\n",
            "Loss S44:  0.08523941094900819\n",
            "Loss S11:  0.07175603402635891\n",
            "Loss S22:  0.07446146606880386\n",
            "Loss S33:  0.08145555448469873\n",
            "Loss S44:  0.08516648142077807\n",
            "Loss S11:  0.0717213410385747\n",
            "Loss S22:  0.0745202475343567\n",
            "Loss S33:  0.08148438908379364\n",
            "Loss S44:  0.08523089570455811\n",
            "Loss S11:  0.07171323751729237\n",
            "Loss S22:  0.0745495276199344\n",
            "Loss S33:  0.0814606186638518\n",
            "Loss S44:  0.08523306650359457\n",
            "Loss S11:  0.07175502334590073\n",
            "Loss S22:  0.07457368391002336\n",
            "Loss S33:  0.08150779479334773\n",
            "Loss S44:  0.08526577845765299\n",
            "Loss S11:  0.07172358331849873\n",
            "Loss S22:  0.07460296025891212\n",
            "Loss S33:  0.0814869686122786\n",
            "Loss S44:  0.08525967289497897\n",
            "Loss S11:  0.07170880523949799\n",
            "Loss S22:  0.07458626288386244\n",
            "Loss S33:  0.081448999479444\n",
            "Loss S44:  0.08527209701136591\n",
            "Loss S11:  0.0716989797192897\n",
            "Loss S22:  0.0746062579744945\n",
            "Loss S33:  0.08142637219393811\n",
            "Loss S44:  0.08524632434191869\n",
            "Validation: \n",
            " Loss S11:  0.08539916574954987\n",
            " Loss S22:  0.11329961568117142\n",
            " Loss S33:  0.1004212275147438\n",
            " Loss S44:  0.12064185738563538\n",
            " Loss S11:  0.08480273470992133\n",
            " Loss S22:  0.12159929069734755\n",
            " Loss S33:  0.12061372328372229\n",
            " Loss S44:  0.12948292564778102\n",
            " Loss S11:  0.0837036701964169\n",
            " Loss S22:  0.12080917725475823\n",
            " Loss S33:  0.12078031116142506\n",
            " Loss S44:  0.12762445829263547\n",
            " Loss S11:  0.08313863199265277\n",
            " Loss S22:  0.11899260321601493\n",
            " Loss S33:  0.11814352857773422\n",
            " Loss S44:  0.12607381272999968\n",
            " Loss S11:  0.08287959610238488\n",
            " Loss S22:  0.1188049418506799\n",
            " Loss S33:  0.11743843684225906\n",
            " Loss S44:  0.1251421820970229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net2(nn.Module):\n",
        "    def __init__(self, s11,s22,s33,s44):\n",
        "        super(Net2, self).__init__()\n",
        "        self.s11 = s11\n",
        "        self.s22 = s22\n",
        "        self.s33 = s33\n",
        "        self.s44 = s44\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.s11(x)\n",
        "        out2 = self.s22(x)\n",
        "        out3 = self.s33(x)\n",
        "        out4 = self.s44(x)\n",
        "\n",
        "        out = torch.cat((out1,out2,out3,out4),1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "net4students = Net2(s11,s22,s33,s44)\n",
        "net4students.to(device)\n",
        "summary(net4students, (3, 32, 32))"
      ],
      "metadata": {
        "id": "HuuyhtB_NQoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffe3916a-c5d3-4da6-cae7-a877754d8f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "              VGG-32                  [-1, 128]               0\n",
            "           Conv2d-33           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-34           [-1, 32, 32, 32]              64\n",
            "             ReLU-35           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-36           [-1, 32, 16, 16]               0\n",
            "           Conv2d-37           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-38           [-1, 32, 16, 16]              64\n",
            "             ReLU-39           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-40             [-1, 32, 8, 8]               0\n",
            "           Conv2d-41             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-42             [-1, 64, 8, 8]             128\n",
            "             ReLU-43             [-1, 64, 8, 8]               0\n",
            "           Conv2d-44             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-45             [-1, 64, 8, 8]             128\n",
            "             ReLU-46             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-47             [-1, 64, 4, 4]               0\n",
            "           Conv2d-48            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-49            [-1, 128, 4, 4]             256\n",
            "             ReLU-50            [-1, 128, 4, 4]               0\n",
            "           Conv2d-51            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-52            [-1, 128, 4, 4]             256\n",
            "             ReLU-53            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-54            [-1, 128, 2, 2]               0\n",
            "           Conv2d-55            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-56            [-1, 128, 2, 2]             256\n",
            "             ReLU-57            [-1, 128, 2, 2]               0\n",
            "           Conv2d-58            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-59            [-1, 128, 2, 2]             256\n",
            "             ReLU-60            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-61            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-62            [-1, 128, 1, 1]               0\n",
            "           Linear-63                  [-1, 128]          16,512\n",
            "              VGG-64                  [-1, 128]               0\n",
            "           Conv2d-65           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-66           [-1, 32, 32, 32]              64\n",
            "             ReLU-67           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-68           [-1, 32, 16, 16]               0\n",
            "           Conv2d-69           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-70           [-1, 32, 16, 16]              64\n",
            "             ReLU-71           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-72             [-1, 32, 8, 8]               0\n",
            "           Conv2d-73             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-74             [-1, 64, 8, 8]             128\n",
            "             ReLU-75             [-1, 64, 8, 8]               0\n",
            "           Conv2d-76             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-77             [-1, 64, 8, 8]             128\n",
            "             ReLU-78             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-79             [-1, 64, 4, 4]               0\n",
            "           Conv2d-80            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-81            [-1, 128, 4, 4]             256\n",
            "             ReLU-82            [-1, 128, 4, 4]               0\n",
            "           Conv2d-83            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-84            [-1, 128, 4, 4]             256\n",
            "             ReLU-85            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-86            [-1, 128, 2, 2]               0\n",
            "           Conv2d-87            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-88            [-1, 128, 2, 2]             256\n",
            "             ReLU-89            [-1, 128, 2, 2]               0\n",
            "           Conv2d-90            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-91            [-1, 128, 2, 2]             256\n",
            "             ReLU-92            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-93            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-94            [-1, 128, 1, 1]               0\n",
            "           Linear-95                  [-1, 128]          16,512\n",
            "              VGG-96                  [-1, 128]               0\n",
            "           Conv2d-97           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-98           [-1, 32, 32, 32]              64\n",
            "             ReLU-99           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-100           [-1, 32, 16, 16]               0\n",
            "          Conv2d-101           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-102           [-1, 32, 16, 16]              64\n",
            "            ReLU-103           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-104             [-1, 32, 8, 8]               0\n",
            "          Conv2d-105             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-106             [-1, 64, 8, 8]             128\n",
            "            ReLU-107             [-1, 64, 8, 8]               0\n",
            "          Conv2d-108             [-1, 64, 8, 8]          36,928\n",
            "     BatchNorm2d-109             [-1, 64, 8, 8]             128\n",
            "            ReLU-110             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-111             [-1, 64, 4, 4]               0\n",
            "          Conv2d-112            [-1, 128, 4, 4]          73,856\n",
            "     BatchNorm2d-113            [-1, 128, 4, 4]             256\n",
            "            ReLU-114            [-1, 128, 4, 4]               0\n",
            "          Conv2d-115            [-1, 128, 4, 4]         147,584\n",
            "     BatchNorm2d-116            [-1, 128, 4, 4]             256\n",
            "            ReLU-117            [-1, 128, 4, 4]               0\n",
            "       MaxPool2d-118            [-1, 128, 2, 2]               0\n",
            "          Conv2d-119            [-1, 128, 2, 2]         147,584\n",
            "     BatchNorm2d-120            [-1, 128, 2, 2]             256\n",
            "            ReLU-121            [-1, 128, 2, 2]               0\n",
            "          Conv2d-122            [-1, 128, 2, 2]         147,584\n",
            "     BatchNorm2d-123            [-1, 128, 2, 2]             256\n",
            "            ReLU-124            [-1, 128, 2, 2]               0\n",
            "       MaxPool2d-125            [-1, 128, 1, 1]               0\n",
            "       AvgPool2d-126            [-1, 128, 1, 1]               0\n",
            "          Linear-127                  [-1, 128]          16,512\n",
            "             VGG-128                  [-1, 128]               0\n",
            "          Linear-129                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 2,405,514\n",
            "Trainable params: 2,405,514\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 5.34\n",
            "Params size (MB): 9.18\n",
            "Estimated Total Size (MB): 14.53\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for m in net4students.modules():\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        m.weight.requires_grad = False\n",
        "        if m.bias is not None:\n",
        "            m.bias.requires_grad = False\n",
        "net4students = net4students.to(device)\n",
        "summary(net4students, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMUeIq5X02oI",
        "outputId": "e3f08660-84fe-4b9d-9947-f9869e896b8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "              ReLU-7           [-1, 32, 16, 16]               0\n",
            "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 8, 8]             128\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 64, 4, 4]               0\n",
            "           Conv2d-16            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-17            [-1, 128, 4, 4]             256\n",
            "             ReLU-18            [-1, 128, 4, 4]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-22            [-1, 128, 2, 2]               0\n",
            "           Conv2d-23            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-24            [-1, 128, 2, 2]             256\n",
            "             ReLU-25            [-1, 128, 2, 2]               0\n",
            "           Conv2d-26            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 2, 2]             256\n",
            "             ReLU-28            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-29            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-30            [-1, 128, 1, 1]               0\n",
            "           Linear-31                  [-1, 128]          16,512\n",
            "              VGG-32                  [-1, 128]               0\n",
            "           Conv2d-33           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-34           [-1, 32, 32, 32]              64\n",
            "             ReLU-35           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-36           [-1, 32, 16, 16]               0\n",
            "           Conv2d-37           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-38           [-1, 32, 16, 16]              64\n",
            "             ReLU-39           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-40             [-1, 32, 8, 8]               0\n",
            "           Conv2d-41             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-42             [-1, 64, 8, 8]             128\n",
            "             ReLU-43             [-1, 64, 8, 8]               0\n",
            "           Conv2d-44             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-45             [-1, 64, 8, 8]             128\n",
            "             ReLU-46             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-47             [-1, 64, 4, 4]               0\n",
            "           Conv2d-48            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-49            [-1, 128, 4, 4]             256\n",
            "             ReLU-50            [-1, 128, 4, 4]               0\n",
            "           Conv2d-51            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-52            [-1, 128, 4, 4]             256\n",
            "             ReLU-53            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-54            [-1, 128, 2, 2]               0\n",
            "           Conv2d-55            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-56            [-1, 128, 2, 2]             256\n",
            "             ReLU-57            [-1, 128, 2, 2]               0\n",
            "           Conv2d-58            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-59            [-1, 128, 2, 2]             256\n",
            "             ReLU-60            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-61            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-62            [-1, 128, 1, 1]               0\n",
            "           Linear-63                  [-1, 128]          16,512\n",
            "              VGG-64                  [-1, 128]               0\n",
            "           Conv2d-65           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-66           [-1, 32, 32, 32]              64\n",
            "             ReLU-67           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-68           [-1, 32, 16, 16]               0\n",
            "           Conv2d-69           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-70           [-1, 32, 16, 16]              64\n",
            "             ReLU-71           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-72             [-1, 32, 8, 8]               0\n",
            "           Conv2d-73             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-74             [-1, 64, 8, 8]             128\n",
            "             ReLU-75             [-1, 64, 8, 8]               0\n",
            "           Conv2d-76             [-1, 64, 8, 8]          36,928\n",
            "      BatchNorm2d-77             [-1, 64, 8, 8]             128\n",
            "             ReLU-78             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-79             [-1, 64, 4, 4]               0\n",
            "           Conv2d-80            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-81            [-1, 128, 4, 4]             256\n",
            "             ReLU-82            [-1, 128, 4, 4]               0\n",
            "           Conv2d-83            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-84            [-1, 128, 4, 4]             256\n",
            "             ReLU-85            [-1, 128, 4, 4]               0\n",
            "        MaxPool2d-86            [-1, 128, 2, 2]               0\n",
            "           Conv2d-87            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-88            [-1, 128, 2, 2]             256\n",
            "             ReLU-89            [-1, 128, 2, 2]               0\n",
            "           Conv2d-90            [-1, 128, 2, 2]         147,584\n",
            "      BatchNorm2d-91            [-1, 128, 2, 2]             256\n",
            "             ReLU-92            [-1, 128, 2, 2]               0\n",
            "        MaxPool2d-93            [-1, 128, 1, 1]               0\n",
            "        AvgPool2d-94            [-1, 128, 1, 1]               0\n",
            "           Linear-95                  [-1, 128]          16,512\n",
            "              VGG-96                  [-1, 128]               0\n",
            "           Conv2d-97           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-98           [-1, 32, 32, 32]              64\n",
            "             ReLU-99           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-100           [-1, 32, 16, 16]               0\n",
            "          Conv2d-101           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-102           [-1, 32, 16, 16]              64\n",
            "            ReLU-103           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-104             [-1, 32, 8, 8]               0\n",
            "          Conv2d-105             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-106             [-1, 64, 8, 8]             128\n",
            "            ReLU-107             [-1, 64, 8, 8]               0\n",
            "          Conv2d-108             [-1, 64, 8, 8]          36,928\n",
            "     BatchNorm2d-109             [-1, 64, 8, 8]             128\n",
            "            ReLU-110             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-111             [-1, 64, 4, 4]               0\n",
            "          Conv2d-112            [-1, 128, 4, 4]          73,856\n",
            "     BatchNorm2d-113            [-1, 128, 4, 4]             256\n",
            "            ReLU-114            [-1, 128, 4, 4]               0\n",
            "          Conv2d-115            [-1, 128, 4, 4]         147,584\n",
            "     BatchNorm2d-116            [-1, 128, 4, 4]             256\n",
            "            ReLU-117            [-1, 128, 4, 4]               0\n",
            "       MaxPool2d-118            [-1, 128, 2, 2]               0\n",
            "          Conv2d-119            [-1, 128, 2, 2]         147,584\n",
            "     BatchNorm2d-120            [-1, 128, 2, 2]             256\n",
            "            ReLU-121            [-1, 128, 2, 2]               0\n",
            "          Conv2d-122            [-1, 128, 2, 2]         147,584\n",
            "     BatchNorm2d-123            [-1, 128, 2, 2]             256\n",
            "            ReLU-124            [-1, 128, 2, 2]               0\n",
            "       MaxPool2d-125            [-1, 128, 1, 1]               0\n",
            "       AvgPool2d-126            [-1, 128, 1, 1]               0\n",
            "          Linear-127                  [-1, 128]          16,512\n",
            "             VGG-128                  [-1, 128]               0\n",
            "          Linear-129                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 2,405,514\n",
            "Trainable params: 76,810\n",
            "Non-trainable params: 2,328,704\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 5.34\n",
            "Params size (MB): 9.18\n",
            "Estimated Total Size (MB): 14.53\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net4students.parameters(), lr=0.0001)\n",
        "\n",
        "def train41(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    net4students.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        net4students.zero_grad()\n",
        "        outputs = net4students(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if(batch_idx % 200 == 0):\n",
        "          print(\"Accuracy : \",100.*correct/total,\" Loss : \", train_loss/(batch_idx+1))\n",
        "def test42(epoch):\n",
        "    net4students.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net4students(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\"Accuracy : \",100.*correct/total,\" Loss : \", test_loss/(batch_idx+1))"
      ],
      "metadata": {
        "id": "Nl6WfyLk04H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "    train41(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test42(epoch)"
      ],
      "metadata": {
        "id": "wi8hV35xQ6Fg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34968889-4b3d-4970-8c69-f792831c08e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Accuracy :  5.0  Loss :  3.1348931789398193\n",
            "Accuracy :  74.7910447761194  Loss :  0.9859186075813141\n",
            "Accuracy :  79.98004987531172  Loss :  0.7337499610949633\n",
            "Validation: \n",
            "Accuracy :  84.0  Loss :  0.48206543922424316\n",
            "Accuracy :  84.33333333333333  Loss :  0.47359646218163626\n",
            "Accuracy :  84.09756097560975  Loss :  0.48078606622975045\n",
            "Accuracy :  84.24590163934427  Loss :  0.47811746059871113\n",
            "Accuracy :  84.18518518518519  Loss :  0.4750321595757096\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy :  85.0  Loss :  0.4314382076263428\n",
            "Accuracy :  85.3681592039801  Loss :  0.43474845091501874\n",
            "Accuracy :  85.40399002493766  Loss :  0.4293050227171168\n",
            "Validation: \n",
            "Accuracy :  85.0  Loss :  0.459880530834198\n",
            "Accuracy :  84.0952380952381  Loss :  0.4645976878347851\n",
            "Accuracy :  84.21951219512195  Loss :  0.4723944685808042\n",
            "Accuracy :  84.59016393442623  Loss :  0.4690202230312785\n",
            "Accuracy :  84.5925925925926  Loss :  0.4652766394026486\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy :  87.0  Loss :  0.40599119663238525\n",
            "Accuracy :  85.45771144278606  Loss :  0.4199742463542454\n",
            "Accuracy :  85.64588528678304  Loss :  0.41800879766964855\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.44932422041893005\n",
            "Accuracy :  84.28571428571429  Loss :  0.46260308225949603\n",
            "Accuracy :  84.29268292682927  Loss :  0.4711952812787963\n",
            "Accuracy :  84.59016393442623  Loss :  0.46756840875891387\n",
            "Accuracy :  84.55555555555556  Loss :  0.46366815324182864\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy :  87.0  Loss :  0.385283887386322\n",
            "Accuracy :  85.6268656716418  Loss :  0.417750111118478\n",
            "Accuracy :  85.6783042394015  Loss :  0.41355142796277405\n",
            "Validation: \n",
            "Accuracy :  85.0  Loss :  0.44593119621276855\n",
            "Accuracy :  84.42857142857143  Loss :  0.4614195752711523\n",
            "Accuracy :  84.26829268292683  Loss :  0.4694606436461937\n",
            "Accuracy :  84.60655737704919  Loss :  0.4652544093913719\n",
            "Accuracy :  84.66666666666667  Loss :  0.46158800338521416\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy :  88.0  Loss :  0.35701924562454224\n",
            "Accuracy :  85.70646766169155  Loss :  0.4104348841442991\n",
            "Accuracy :  85.81546134663341  Loss :  0.409137911369973\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.43898722529411316\n",
            "Accuracy :  84.57142857142857  Loss :  0.45783395568529767\n",
            "Accuracy :  84.39024390243902  Loss :  0.4663506651796946\n",
            "Accuracy :  84.80327868852459  Loss :  0.46277669951563977\n",
            "Accuracy :  84.8395061728395  Loss :  0.45882677884749423\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy :  86.0  Loss :  0.3574029803276062\n",
            "Accuracy :  85.66666666666667  Loss :  0.4115464338170948\n",
            "Accuracy :  85.89775561097257  Loss :  0.4075902670324592\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.4337872266769409\n",
            "Accuracy :  84.52380952380952  Loss :  0.45870826357886907\n",
            "Accuracy :  84.41463414634147  Loss :  0.46699990077716547\n",
            "Accuracy :  84.72131147540983  Loss :  0.46298682640810485\n",
            "Accuracy :  84.79012345679013  Loss :  0.4590163635618893\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy :  90.0  Loss :  0.34302130341529846\n",
            "Accuracy :  86.17910447761194  Loss :  0.4035620232719687\n",
            "Accuracy :  86.16209476309227  Loss :  0.4025969258196039\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.43309420347213745\n",
            "Accuracy :  84.66666666666667  Loss :  0.45561722204798744\n",
            "Accuracy :  84.4390243902439  Loss :  0.4636267619888957\n",
            "Accuracy :  84.85245901639344  Loss :  0.45978526942065506\n",
            "Accuracy :  84.91358024691358  Loss :  0.4555993609958225\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy :  87.0  Loss :  0.37572306394577026\n",
            "Accuracy :  86.11940298507463  Loss :  0.4040832221508026\n",
            "Accuracy :  86.00997506234414  Loss :  0.40377512362383844\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.43257075548171997\n",
            "Accuracy :  84.9047619047619  Loss :  0.45447832062130883\n",
            "Accuracy :  84.5609756097561  Loss :  0.4624679386615753\n",
            "Accuracy :  84.95081967213115  Loss :  0.4583203479891918\n",
            "Accuracy :  84.98765432098766  Loss :  0.4542261435661787\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy :  89.0  Loss :  0.34988537430763245\n",
            "Accuracy :  86.1094527363184  Loss :  0.40243693963805244\n",
            "Accuracy :  86.16957605985037  Loss :  0.4006188688358464\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4316571056842804\n",
            "Accuracy :  85.04761904761905  Loss :  0.45289809363228933\n",
            "Accuracy :  84.65853658536585  Loss :  0.4603659830442289\n",
            "Accuracy :  85.14754098360656  Loss :  0.4567553317937695\n",
            "Accuracy :  85.09876543209876  Loss :  0.45257968372768825\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy :  87.0  Loss :  0.3378726840019226\n",
            "Accuracy :  86.40796019900498  Loss :  0.39760752726550125\n",
            "Accuracy :  86.3291770573566  Loss :  0.3962104617434547\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4305574297904968\n",
            "Accuracy :  85.0952380952381  Loss :  0.45300325183641343\n",
            "Accuracy :  84.70731707317073  Loss :  0.46126663394090606\n",
            "Accuracy :  85.1639344262295  Loss :  0.45737563098063233\n",
            "Accuracy :  85.12345679012346  Loss :  0.4531210142153281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiple Students PART 2-- 8 students now**\n"
      ],
      "metadata": {
        "id": "J3x1rV4fS6Mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "    'VGGS1': [32, 32, 'M', 64, 64, 'M', 128, 128, 'M', 256, 256, 'M','M'],\n",
        "    'VGGS33': [32,32,'M', 32,32,'M',32,64,'M', 64,64,'M',64,64,64,'M'],\n",
        "    'VGGS2': [32,'M', 32, 'M', 64, 64, 'M', 128,128,'M', 256,'M'],\n",
        "\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(64, 64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        #out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "ss11 = VGG('VGGS33')\n",
        "ss11 = ss11.to(device)\n",
        "summary(ss11, (3, 32, 32))\n",
        "ss22 = VGG('VGGS33')\n",
        "ss22 = ss22.to(device)\n",
        "summary(ss22, (3,32,32))\n",
        "ss33 = VGG('VGGS33')\n",
        "ss33 = ss33.to(device)\n",
        "summary(ss33, (3, 32, 32))\n",
        "ss44 = VGG('VGGS33')\n",
        "ss44 = ss44.to(device)\n",
        "summary(ss44, (3, 32, 32))\n",
        "ss55 = VGG('VGGS33')\n",
        "ss55 = ss55.to(device)\n",
        "summary(ss55, (3, 32, 32))\n",
        "ss66 = VGG('VGGS33')\n",
        "ss66 = ss66.to(device)\n",
        "summary(ss66, (3,32,32))\n",
        "ss77 = VGG('VGGS33')\n",
        "ss77 = ss77.to(device)\n",
        "summary(ss77, (3, 32, 32))\n",
        "ss88 = VGG('VGGS33')\n",
        "ss88 = ss88.to(device)\n",
        "summary(ss88, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf6ZMLwaDqm0",
        "outputId": "4eb7ea93-3998-49e6-99f4-c5d280afb099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,112\n",
            "Trainable params: 242,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.17\n",
            "Params size (MB): 0.92\n",
            "Estimated Total Size (MB): 3.10\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TAA1_WOH = nn.Sequential(*list(s11.children())[:-1],nn.Flatten())\n",
        "# summary(s11, (3, 32, 32))\n",
        "# summary(TAA1_WOH, (3, 32, 32))\n",
        "\n",
        "# TAA2_WOH = nn.Sequential(*list(s22.children())[:-1],nn.Flatten())\n",
        "# summary(s22, (3, 32, 32))\n",
        "# summary(TAA2_WOH, (3, 32, 32))\n",
        "\n",
        "# TAA3_WOH = nn.Sequential(*list(s33.children())[:-1],nn.Flatten())\n",
        "# summary(s33, (3, 32, 32))\n",
        "# summary(TAA3_WOH, (3, 32, 32))\n",
        "\n",
        "# TAA4_WOH = nn.Sequential(*list(s44.children())[:-1],nn.Flatten())\n",
        "# summary(s44, (3, 32, 32))\n",
        "# summary(TAA4_WOH, (3, 32, 32))"
      ],
      "metadata": {
        "id": "5oDdCLJkFvsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TAA1_WOH.eval()\n",
        "# TAA2_WOH.eval()\n",
        "# TAA3_WOH.eval()\n",
        "# TAA4_WOH.eval()\n",
        "# TA2DenseTrain = None\n",
        "# TA2DenseTest = None\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "#         inputs, targets = inputs.to(device), targets.to(device)\n",
        "#         outputs1 = TAA1_WOH(inputs)\n",
        "#         outputs2 = TAA2_WOH(inputs)\n",
        "#         outputs3 = TAA3_WOH(inputs)\n",
        "#         outputs4 = TAA4_WOH(inputs)\n",
        "#         if(TA2DenseTrain == None):\n",
        "#             TA2DenseTrain = torch.cat((outputs1,outputs2,outputs3,outputs4),1) \n",
        "#         else:\n",
        "#             totalOUTPUT = torch.cat((outputs1,outputs2,outputs3,outputs4),1)         \n",
        "#             TA2DenseTrain = torch.cat((TA2DenseTrain,totalOUTPUT))\n",
        "           \n",
        "# with torch.no_grad():\n",
        "#     for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "#         inputs, targets = inputs.to(device), targets.to(device)\n",
        "#         outputs1 = TAA1_WOH(inputs)\n",
        "#         outputs2 = TAA2_WOH(inputs)\n",
        "#         outputs3 = TAA3_WOH(inputs)\n",
        "#         outputs4 = TAA4_WOH(inputs)\n",
        "#         if(TA2DenseTest == None):\n",
        "#             TA2DenseTest = torch.cat((outputs1,outputs2,outputs3,outputs4),1) \n",
        "#         else:\n",
        "#             totalOUTPUT = torch.cat((outputs1,outputs2,outputs3,outputs4),1)      \n",
        "#             TA2DenseTest = torch.cat((TA2DenseTest,totalOUTPUT))\n",
        "           "
      ],
      "metadata": {
        "id": "-X97wNRHGSkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(TA2DenseTrain.shape)"
      ],
      "metadata": {
        "id": "XgpdmBzrHEZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer1 = optim.Adam(ss11.parameters(), lr=0.0001)\n",
        "optimizer2 = optim.Adam(ss22.parameters(), lr=0.0001)\n",
        "optimizer3 = optim.Adam(ss33.parameters(), lr=0.0001)\n",
        "optimizer4 = optim.Adam(ss44.parameters(), lr=0.0001)\n",
        "optimizer5 = optim.Adam(ss55.parameters(), lr=0.0001)\n",
        "optimizer6 = optim.Adam(ss66.parameters(), lr=0.0001)\n",
        "optimizer7 = optim.Adam(ss77.parameters(), lr=0.0001)\n",
        "optimizer8 = optim.Adam(ss88.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def train8(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    s11.train()\n",
        "    train_loss1 = 0\n",
        "    train_loss2 = 0\n",
        "    train_loss3 = 0\n",
        "    train_loss4= 0\n",
        "    train_loss5 = 0\n",
        "    train_loss6 = 0\n",
        "    train_loss7 = 0\n",
        "    train_loss8= 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        #code here\n",
        "        targets = DenseTrain[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "        targets.to(device)\n",
        "        ss11.zero_grad()\n",
        "        ss22.zero_grad()\n",
        "        ss33.zero_grad()\n",
        "        ss44.zero_grad()\n",
        "        ss55.zero_grad()\n",
        "        ss66.zero_grad()\n",
        "        ss77.zero_grad()\n",
        "        ss88.zero_grad()\n",
        "        output1 = ss11(inputs)\n",
        "        output2 = ss22(inputs)\n",
        "        output3 = ss33(inputs)\n",
        "        output4 = ss44(inputs)\n",
        "        output5 = ss55(inputs)\n",
        "        output6 = ss66(inputs)\n",
        "        output7 = ss77(inputs)\n",
        "        output8 = ss88(inputs)\n",
        "        \n",
        "        loss1 = criterion(output1, targets[:,:64])\n",
        "        loss2 = criterion(output2, targets[:,64:128])\n",
        "        loss3 = criterion(output3, targets[:,128:192])\n",
        "        loss4 = criterion(output4, targets[:,192:256])\n",
        "        loss5 = criterion(output5, targets[:,256:320])\n",
        "        loss6 = criterion(output6, targets[:,320:384])\n",
        "        loss7 = criterion(output7, targets[:,384:448])\n",
        "        loss8 = criterion(output8, targets[:,448:512])\n",
        "\n",
        "        loss1.backward()\n",
        "        loss2.backward()\n",
        "        loss3.backward()\n",
        "        loss4.backward()\n",
        "        loss5.backward()\n",
        "        loss6.backward()\n",
        "        loss7.backward()\n",
        "        loss8.backward()\n",
        "        optimizer1.step()\n",
        "        optimizer2.step()\n",
        "        optimizer3.step()\n",
        "        optimizer4.step()\n",
        "        optimizer5.step()\n",
        "        optimizer6.step()\n",
        "        optimizer7.step()\n",
        "        optimizer8.step()\n",
        "      \n",
        "\n",
        "        train_loss1 += loss1.item()\n",
        "        train_loss2 += loss2.item()\n",
        "        train_loss3 += loss3.item()\n",
        "        train_loss4 += loss4.item()\n",
        "        train_loss5 += loss5.item()\n",
        "        train_loss6 += loss6.item()\n",
        "        train_loss7 += loss7.item()\n",
        "        train_loss8 += loss8.item()\n",
        "        if(batch_idx % 10 == 0):\n",
        "          print(\"Loss SS11: \", train_loss1/(batch_idx+1))\n",
        "          print(\"Loss SS22: \", train_loss2/(batch_idx+1))\n",
        "          print(\"Loss SS33: \", train_loss3/(batch_idx+1))\n",
        "          print(\"Loss SS44: \", train_loss4/(batch_idx+1))\n",
        "          print(\"Loss SS55: \", train_loss5/(batch_idx+1))\n",
        "          print(\"Loss SS66: \", train_loss6/(batch_idx+1))\n",
        "          print(\"Loss SS77: \", train_loss7/(batch_idx+1))\n",
        "          print(\"Loss SS88: \", train_loss8/(batch_idx+1))\n",
        "def test8(epoch):\n",
        "    ss11.eval()\n",
        "    \n",
        "    test_loss1 = 0\n",
        "    test_loss2 = 0\n",
        "    test_loss3 = 0\n",
        "    test_loss4= 0\n",
        "    test_loss5 = 0\n",
        "    test_loss6 = 0\n",
        "    test_loss7 = 0\n",
        "    test_loss8= 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            #code here\n",
        "            targets = DenseTest[batch_idx*batch_size:(batch_idx*batch_size)+batch_size]\n",
        "            targets.to(device)\n",
        "            output1 = ss11(inputs)\n",
        "            output2 = ss22(inputs)\n",
        "            output3 = ss33(inputs)\n",
        "            output4 = ss44(inputs)\n",
        "            output5 = ss55(inputs)\n",
        "            output6 = ss66(inputs)\n",
        "            output7 = ss77(inputs)\n",
        "            output8 = ss88(inputs)\n",
        "            loss1 = criterion(output1, targets[:,:64])\n",
        "            loss2 = criterion(output2, targets[:,64:128])\n",
        "            loss3 = criterion(output3, targets[:,128:192])\n",
        "            loss4 = criterion(output4, targets[:,192:256])\n",
        "            loss5 = criterion(output5, targets[:,256:320])\n",
        "            loss6 = criterion(output6, targets[:,320:384])\n",
        "            loss7 = criterion(output7, targets[:,384:448])\n",
        "            loss8 = criterion(output8, targets[:,448:512])\n",
        "            test_loss1 += loss1.item()\n",
        "            test_loss2 += loss2.item()\n",
        "            test_loss3 += loss3.item()\n",
        "            test_loss4 += loss4.item()\n",
        "\n",
        "            test_loss5 += loss5.item()\n",
        "            test_loss6 += loss6.item()\n",
        "            test_loss7 += loss7.item()\n",
        "            test_loss8 += loss8.item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\" Loss SS11: \", test_loss1/(batch_idx+1))\n",
        "              print(\" Loss SS22: \", test_loss2/(batch_idx+1))\n",
        "              print(\" Loss SS33: \", test_loss3/(batch_idx+1))\n",
        "              print(\" Loss SS55: \", test_loss4/(batch_idx+1))\n",
        "              print(\" Loss SS66: \", test_loss5/(batch_idx+1))\n",
        "              print(\" Loss SS77: \", test_loss6/(batch_idx+1))\n",
        "              print(\" Loss SS88: \", test_loss7/(batch_idx+1))\n",
        "              print(\" Loss SS99: \", test_loss8/(batch_idx+1))"
      ],
      "metadata": {
        "id": "EsiP7r2XHExk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+60):\n",
        "    train8(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test8(epoch)"
      ],
      "metadata": {
        "id": "lpbuGedJI_2y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56d7309f-53c7-4380-c942-8cd61e7099d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Loss SS33:  0.08963167063339288\n",
            "Loss SS44:  0.07983872005739785\n",
            "Loss SS55:  0.09231702977662806\n",
            "Loss SS66:  0.0926281665940114\n",
            "Loss SS77:  0.0918359872706406\n",
            "Loss SS88:  0.09868154406090222\n",
            "Loss SS11:  0.09086160623744836\n",
            "Loss SS22:  0.0773140923526519\n",
            "Loss SS33:  0.08967367652571409\n",
            "Loss SS44:  0.07987845788461312\n",
            "Loss SS55:  0.09236862021788694\n",
            "Loss SS66:  0.09265280840105547\n",
            "Loss SS77:  0.09188429448298385\n",
            "Loss SS88:  0.09872677639832818\n",
            "Loss SS11:  0.0908877167787285\n",
            "Loss SS22:  0.07728102435233239\n",
            "Loss SS33:  0.08963967685716866\n",
            "Loss SS44:  0.0799155311518487\n",
            "Loss SS55:  0.0923093012794671\n",
            "Loss SS66:  0.09268131418892357\n",
            "Loss SS77:  0.09188184071413792\n",
            "Loss SS88:  0.09868300736965634\n",
            "Loss SS11:  0.09094025281361899\n",
            "Loss SS22:  0.07727609332341197\n",
            "Loss SS33:  0.08973356867275442\n",
            "Loss SS44:  0.07998500736939086\n",
            "Loss SS55:  0.09228611999410348\n",
            "Loss SS66:  0.09276709795847642\n",
            "Loss SS77:  0.09194647830484032\n",
            "Loss SS88:  0.09875799198756592\n",
            "Loss SS11:  0.09087116516728966\n",
            "Loss SS22:  0.07724227372496421\n",
            "Loss SS33:  0.08970935573572349\n",
            "Loss SS44:  0.07991841640280184\n",
            "Loss SS55:  0.09225748911534551\n",
            "Loss SS66:  0.09278959829334317\n",
            "Loss SS77:  0.09189383991563957\n",
            "Loss SS88:  0.09875761236986816\n",
            "Loss SS11:  0.09080921806175422\n",
            "Loss SS22:  0.07720256922872158\n",
            "Loss SS33:  0.08974037174126999\n",
            "Loss SS44:  0.07982164688087375\n",
            "Loss SS55:  0.09225415032939846\n",
            "Loss SS66:  0.09279857763425023\n",
            "Loss SS77:  0.0918629921617962\n",
            "Loss SS88:  0.09874014184723635\n",
            "Loss SS11:  0.09081473502179736\n",
            "Loss SS22:  0.07718295869964718\n",
            "Loss SS33:  0.08975901289485774\n",
            "Loss SS44:  0.07977199973228766\n",
            "Loss SS55:  0.09227611889397755\n",
            "Loss SS66:  0.09272375494704278\n",
            "Loss SS77:  0.09189217757359312\n",
            "Loss SS88:  0.09887309185747561\n",
            "Loss SS11:  0.09067213170052608\n",
            "Loss SS22:  0.07710581105566076\n",
            "Loss SS33:  0.08979938177638354\n",
            "Loss SS44:  0.07970767751853275\n",
            "Loss SS55:  0.09226208744235256\n",
            "Loss SS66:  0.09265744427538228\n",
            "Loss SS77:  0.09185599527622769\n",
            "Loss SS88:  0.09884367799164158\n",
            "Loss SS11:  0.09057844589976495\n",
            "Loss SS22:  0.07708691610240885\n",
            "Loss SS33:  0.08973292193341913\n",
            "Loss SS44:  0.07964638626151531\n",
            "Loss SS55:  0.09223635037550248\n",
            "Loss SS66:  0.09264266623812877\n",
            "Loss SS77:  0.09181069736141562\n",
            "Loss SS88:  0.09875666650680473\n",
            "Loss SS11:  0.09053681422172118\n",
            "Loss SS22:  0.07706585380088019\n",
            "Loss SS33:  0.08975059298988192\n",
            "Loss SS44:  0.07966839871417708\n",
            "Loss SS55:  0.09220655104474565\n",
            "Loss SS66:  0.09266971545816707\n",
            "Loss SS77:  0.09180876225418955\n",
            "Loss SS88:  0.0987561783460966\n",
            "Loss SS11:  0.09044491303913947\n",
            "Loss SS22:  0.07703666292631699\n",
            "Loss SS33:  0.08975602127990995\n",
            "Loss SS44:  0.0796266110223204\n",
            "Loss SS55:  0.09213621416669757\n",
            "Loss SS66:  0.09267531781478229\n",
            "Loss SS77:  0.09178584700264417\n",
            "Loss SS88:  0.0987052310356783\n",
            "Validation: \n",
            " Loss SS11:  0.07970812916755676\n",
            " Loss SS22:  0.10548847168684006\n",
            " Loss SS33:  0.11463329941034317\n",
            " Loss SS55:  0.10043271631002426\n",
            " Loss SS66:  0.13015957176685333\n",
            " Loss SS77:  0.12151698768138885\n",
            " Loss SS88:  0.12048734724521637\n",
            " Loss SS99:  0.16201816499233246\n",
            " Loss SS11:  0.09708293066138313\n",
            " Loss SS22:  0.11549103721266701\n",
            " Loss SS33:  0.12422753480218705\n",
            " Loss SS55:  0.11742688431626275\n",
            " Loss SS66:  0.14683056658222562\n",
            " Loss SS77:  0.13603486794800984\n",
            " Loss SS88:  0.14537487711225236\n",
            " Loss SS99:  0.1571636703752336\n",
            " Loss SS11:  0.09425772490297876\n",
            " Loss SS22:  0.11335386844669901\n",
            " Loss SS33:  0.12309496958808201\n",
            " Loss SS55:  0.11669535971269375\n",
            " Loss SS66:  0.14524984014470402\n",
            " Loss SS77:  0.13749017003105907\n",
            " Loss SS88:  0.1462952040317582\n",
            " Loss SS99:  0.15656633656926272\n",
            " Loss SS11:  0.09339501879742888\n",
            " Loss SS22:  0.11215930456509356\n",
            " Loss SS33:  0.12180490547516307\n",
            " Loss SS55:  0.11575541633074402\n",
            " Loss SS66:  0.14375392323146102\n",
            " Loss SS77:  0.13570375716099975\n",
            " Loss SS88:  0.1452948264166957\n",
            " Loss SS99:  0.15368356567914368\n",
            " Loss SS11:  0.09294035331702527\n",
            " Loss SS22:  0.111535652368157\n",
            " Loss SS33:  0.1218303625046471\n",
            " Loss SS55:  0.11577801516762486\n",
            " Loss SS66:  0.14194929112254837\n",
            " Loss SS77:  0.13532723854353398\n",
            " Loss SS88:  0.14389496499005658\n",
            " Loss SS99:  0.15344910893911196\n",
            "\n",
            "Epoch: 50\n",
            "Loss SS11:  0.09485349804162979\n",
            "Loss SS22:  0.07959970831871033\n",
            "Loss SS33:  0.08470440655946732\n",
            "Loss SS44:  0.07644728571176529\n",
            "Loss SS55:  0.08964141458272934\n",
            "Loss SS66:  0.09120982885360718\n",
            "Loss SS77:  0.09088388085365295\n",
            "Loss SS88:  0.09799321740865707\n",
            "Loss SS11:  0.09184404462575912\n",
            "Loss SS22:  0.07716220346364108\n",
            "Loss SS33:  0.08886727758429268\n",
            "Loss SS44:  0.07930505885319276\n",
            "Loss SS55:  0.09422792358831926\n",
            "Loss SS66:  0.09432405233383179\n",
            "Loss SS77:  0.09478252042423595\n",
            "Loss SS88:  0.09982578524134376\n",
            "Loss SS11:  0.09025469386861437\n",
            "Loss SS22:  0.07707589864730835\n",
            "Loss SS33:  0.09055444882029579\n",
            "Loss SS44:  0.08004033033336912\n",
            "Loss SS55:  0.09324153548195249\n",
            "Loss SS66:  0.09541681799150649\n",
            "Loss SS77:  0.09487400345859073\n",
            "Loss SS88:  0.1003344857266971\n",
            "Loss SS11:  0.08960220121568249\n",
            "Loss SS22:  0.07584407733332726\n",
            "Loss SS33:  0.08993227275148515\n",
            "Loss SS44:  0.07896362557526558\n",
            "Loss SS55:  0.09223538084376243\n",
            "Loss SS66:  0.09271430055941304\n",
            "Loss SS77:  0.0928597683387418\n",
            "Loss SS88:  0.09900515887045092\n",
            "Loss SS11:  0.0907134646322669\n",
            "Loss SS22:  0.07601573827063166\n",
            "Loss SS33:  0.08945816737122653\n",
            "Loss SS44:  0.07903456996853758\n",
            "Loss SS55:  0.0915633961558342\n",
            "Loss SS66:  0.09238364801901143\n",
            "Loss SS77:  0.09187504649162292\n",
            "Loss SS88:  0.09771338668538303\n",
            "Loss SS11:  0.09045388082078859\n",
            "Loss SS22:  0.07623803016601824\n",
            "Loss SS33:  0.0906417609429827\n",
            "Loss SS44:  0.07953804380753461\n",
            "Loss SS55:  0.09214383159198013\n",
            "Loss SS66:  0.09258960318915985\n",
            "Loss SS77:  0.09225681380313985\n",
            "Loss SS88:  0.09828325667802025\n",
            "Loss SS11:  0.09001793993301078\n",
            "Loss SS22:  0.076513337307289\n",
            "Loss SS33:  0.09062493629142886\n",
            "Loss SS44:  0.07919503687346569\n",
            "Loss SS55:  0.09183668162001939\n",
            "Loss SS66:  0.09262150074126291\n",
            "Loss SS77:  0.09211068881339714\n",
            "Loss SS88:  0.09858069153594189\n",
            "Loss SS11:  0.08980351140801336\n",
            "Loss SS22:  0.07645616827296539\n",
            "Loss SS33:  0.09040399798205201\n",
            "Loss SS44:  0.07911171764135361\n",
            "Loss SS55:  0.09161832687300696\n",
            "Loss SS66:  0.09209106222424708\n",
            "Loss SS77:  0.09145516346038228\n",
            "Loss SS88:  0.09837248260286492\n",
            "Loss SS11:  0.08990410899306521\n",
            "Loss SS22:  0.07637879344416254\n",
            "Loss SS33:  0.09037606565304744\n",
            "Loss SS44:  0.07888436271452609\n",
            "Loss SS55:  0.09204599675205019\n",
            "Loss SS66:  0.0923278037412667\n",
            "Loss SS77:  0.09120425977456717\n",
            "Loss SS88:  0.098158153578823\n",
            "Loss SS11:  0.0901481555058406\n",
            "Loss SS22:  0.0763269915849298\n",
            "Loss SS33:  0.09035576912727984\n",
            "Loss SS44:  0.07904175849078776\n",
            "Loss SS55:  0.09245782754905932\n",
            "Loss SS66:  0.09274203453090164\n",
            "Loss SS77:  0.09123570791312627\n",
            "Loss SS88:  0.09797150396056227\n",
            "Loss SS11:  0.08968872761372293\n",
            "Loss SS22:  0.07584558718717925\n",
            "Loss SS33:  0.08980631592250106\n",
            "Loss SS44:  0.07875677485867302\n",
            "Loss SS55:  0.09194190851827659\n",
            "Loss SS66:  0.09219905645540445\n",
            "Loss SS77:  0.09067080885466963\n",
            "Loss SS88:  0.09768441710436698\n",
            "Loss SS11:  0.08925692790800387\n",
            "Loss SS22:  0.07555398475882169\n",
            "Loss SS33:  0.0893354188348796\n",
            "Loss SS44:  0.07849163058641795\n",
            "Loss SS55:  0.09154131572257292\n",
            "Loss SS66:  0.09181861485446896\n",
            "Loss SS77:  0.09033514901592925\n",
            "Loss SS88:  0.0972940059261279\n",
            "Loss SS11:  0.08910255866848733\n",
            "Loss SS22:  0.07575566666431663\n",
            "Loss SS33:  0.0891651394199734\n",
            "Loss SS44:  0.07844581855230096\n",
            "Loss SS55:  0.09134010170116898\n",
            "Loss SS66:  0.09186685744149625\n",
            "Loss SS77:  0.09031554988839409\n",
            "Loss SS88:  0.09725535094491707\n",
            "Loss SS11:  0.0892991101696291\n",
            "Loss SS22:  0.07584056320763727\n",
            "Loss SS33:  0.08934729699870102\n",
            "Loss SS44:  0.07857610962090601\n",
            "Loss SS55:  0.09136609812729231\n",
            "Loss SS66:  0.09190423894702023\n",
            "Loss SS77:  0.09060422712154971\n",
            "Loss SS88:  0.09743729882112896\n",
            "Loss SS11:  0.08912313697820014\n",
            "Loss SS22:  0.07573177591494634\n",
            "Loss SS33:  0.0891884519910136\n",
            "Loss SS44:  0.07856226923829275\n",
            "Loss SS55:  0.09119663007081823\n",
            "Loss SS66:  0.09199218901125251\n",
            "Loss SS77:  0.0903070554242912\n",
            "Loss SS88:  0.09723185092633498\n",
            "Loss SS11:  0.08933193799083596\n",
            "Loss SS22:  0.07592119676190497\n",
            "Loss SS33:  0.08939487669641609\n",
            "Loss SS44:  0.07881672172159548\n",
            "Loss SS55:  0.0913300509780448\n",
            "Loss SS66:  0.0922644803954276\n",
            "Loss SS77:  0.09047368292184856\n",
            "Loss SS88:  0.097557358690445\n",
            "Loss SS11:  0.08906047948584053\n",
            "Loss SS22:  0.0758870982605478\n",
            "Loss SS33:  0.08916799131757724\n",
            "Loss SS44:  0.0787682240331395\n",
            "Loss SS55:  0.09099661179395936\n",
            "Loss SS66:  0.09212092717426905\n",
            "Loss SS77:  0.09032971513752612\n",
            "Loss SS88:  0.09747380050628082\n",
            "Loss SS11:  0.08882673716510249\n",
            "Loss SS22:  0.07570722485669175\n",
            "Loss SS33:  0.08905710406296434\n",
            "Loss SS44:  0.07874053285310142\n",
            "Loss SS55:  0.09093788396893886\n",
            "Loss SS66:  0.0919310598694093\n",
            "Loss SS77:  0.0903371853089472\n",
            "Loss SS88:  0.09734196063370733\n",
            "Loss SS11:  0.08870744083632422\n",
            "Loss SS22:  0.07584184519329124\n",
            "Loss SS33:  0.08913705135741945\n",
            "Loss SS44:  0.07876495346015329\n",
            "Loss SS55:  0.09110799501778671\n",
            "Loss SS66:  0.091975845520009\n",
            "Loss SS77:  0.09041667589660507\n",
            "Loss SS88:  0.09725427364117532\n",
            "Loss SS11:  0.08862270586465666\n",
            "Loss SS22:  0.07592546764312615\n",
            "Loss SS33:  0.08896357651936446\n",
            "Loss SS44:  0.07879477618408452\n",
            "Loss SS55:  0.09093550370313734\n",
            "Loss SS66:  0.09217094208713601\n",
            "Loss SS77:  0.09026799815175421\n",
            "Loss SS88:  0.0973390394246391\n",
            "Loss SS11:  0.08857676197788608\n",
            "Loss SS22:  0.07583961669188827\n",
            "Loss SS33:  0.08898740232139085\n",
            "Loss SS44:  0.07881411042676043\n",
            "Loss SS55:  0.09093170842869365\n",
            "Loss SS66:  0.09207321677486695\n",
            "Loss SS77:  0.09047094265471643\n",
            "Loss SS88:  0.09725731340658605\n",
            "Loss SS11:  0.08867511831188654\n",
            "Loss SS22:  0.0758705258157581\n",
            "Loss SS33:  0.08899448748448449\n",
            "Loss SS44:  0.07886425486955598\n",
            "Loss SS55:  0.09094384824636423\n",
            "Loss SS66:  0.09190081130554326\n",
            "Loss SS77:  0.09038208569819328\n",
            "Loss SS88:  0.09715139950621185\n",
            "Loss SS11:  0.08880003717015772\n",
            "Loss SS22:  0.07598078813892684\n",
            "Loss SS33:  0.08905610317423333\n",
            "Loss SS44:  0.0789765012345163\n",
            "Loss SS55:  0.09114211790971627\n",
            "Loss SS66:  0.09198216884923736\n",
            "Loss SS77:  0.09055393489237824\n",
            "Loss SS88:  0.09736331124111539\n",
            "Loss SS11:  0.08902302642405291\n",
            "Loss SS22:  0.07608494723772073\n",
            "Loss SS33:  0.08901256561537325\n",
            "Loss SS44:  0.078989172039868\n",
            "Loss SS55:  0.09115802161234282\n",
            "Loss SS66:  0.09201431029286736\n",
            "Loss SS77:  0.09070948353319457\n",
            "Loss SS88:  0.09743571494306837\n",
            "Loss SS11:  0.08912151588196576\n",
            "Loss SS22:  0.07619983588511518\n",
            "Loss SS33:  0.08905769504328487\n",
            "Loss SS44:  0.07905529909361447\n",
            "Loss SS55:  0.09120388981341326\n",
            "Loss SS66:  0.09207609818313131\n",
            "Loss SS77:  0.09084294495246223\n",
            "Loss SS88:  0.09754626947567176\n",
            "Loss SS11:  0.08913072537022283\n",
            "Loss SS22:  0.07623789655378616\n",
            "Loss SS33:  0.08909638089487752\n",
            "Loss SS44:  0.07909100366184911\n",
            "Loss SS55:  0.09132825013294638\n",
            "Loss SS66:  0.09218987493044827\n",
            "Loss SS77:  0.09099810929649854\n",
            "Loss SS88:  0.09780254223907137\n",
            "Loss SS11:  0.08908400304929506\n",
            "Loss SS22:  0.07627099917994605\n",
            "Loss SS33:  0.08915574020809597\n",
            "Loss SS44:  0.07920630316167955\n",
            "Loss SS55:  0.09144021416532583\n",
            "Loss SS66:  0.09225320108092151\n",
            "Loss SS77:  0.09101385055145542\n",
            "Loss SS88:  0.09791998057073104\n",
            "Loss SS11:  0.08904866002692507\n",
            "Loss SS22:  0.07642894298717984\n",
            "Loss SS33:  0.0893958881222454\n",
            "Loss SS44:  0.07926268586923275\n",
            "Loss SS55:  0.09152773861185651\n",
            "Loss SS66:  0.09234970604581587\n",
            "Loss SS77:  0.09104995231566834\n",
            "Loss SS88:  0.09800976960201545\n",
            "Loss SS11:  0.08903158445481303\n",
            "Loss SS22:  0.07632668104248115\n",
            "Loss SS33:  0.08938698178932761\n",
            "Loss SS44:  0.07928338453332724\n",
            "Loss SS55:  0.09152634534653395\n",
            "Loss SS66:  0.09237656644230635\n",
            "Loss SS77:  0.09094655009987515\n",
            "Loss SS88:  0.09800564786717561\n",
            "Loss SS11:  0.08889836001232318\n",
            "Loss SS22:  0.07639897176899861\n",
            "Loss SS33:  0.08946836225150787\n",
            "Loss SS44:  0.07934479551114577\n",
            "Loss SS55:  0.09167239410463478\n",
            "Loss SS66:  0.09244363153308528\n",
            "Loss SS77:  0.09109575682898977\n",
            "Loss SS88:  0.09805116162369751\n",
            "Loss SS11:  0.08892708638082707\n",
            "Loss SS22:  0.07645413876074889\n",
            "Loss SS33:  0.08950946274587879\n",
            "Loss SS44:  0.07940255286190596\n",
            "Loss SS55:  0.09185848647474847\n",
            "Loss SS66:  0.0924093746564713\n",
            "Loss SS77:  0.09115299235942752\n",
            "Loss SS88:  0.09810982946541619\n",
            "Loss SS11:  0.08898113778166448\n",
            "Loss SS22:  0.07642084381683846\n",
            "Loss SS33:  0.08941539031995455\n",
            "Loss SS44:  0.07935136772237024\n",
            "Loss SS55:  0.091785445378141\n",
            "Loss SS66:  0.09234587421754549\n",
            "Loss SS77:  0.09107237888973242\n",
            "Loss SS88:  0.09810584189424178\n",
            "Loss SS11:  0.08906864551276061\n",
            "Loss SS22:  0.0764460924583432\n",
            "Loss SS33:  0.08947395753823337\n",
            "Loss SS44:  0.07940565047327232\n",
            "Loss SS55:  0.09187081595447576\n",
            "Loss SS66:  0.09234346950722631\n",
            "Loss SS77:  0.09113446929465945\n",
            "Loss SS88:  0.09810925646455859\n",
            "Loss SS11:  0.08901840079226883\n",
            "Loss SS22:  0.0764217456391931\n",
            "Loss SS33:  0.08939050923571486\n",
            "Loss SS44:  0.07928373594838686\n",
            "Loss SS55:  0.09182237176466565\n",
            "Loss SS66:  0.0922102992988425\n",
            "Loss SS77:  0.09107980829891481\n",
            "Loss SS88:  0.09809682770348027\n",
            "Loss SS11:  0.08906068832031792\n",
            "Loss SS22:  0.07644009745278317\n",
            "Loss SS33:  0.08940119541023483\n",
            "Loss SS44:  0.07930365377419855\n",
            "Loss SS55:  0.09176843126148772\n",
            "Loss SS66:  0.09219658252430801\n",
            "Loss SS77:  0.09115003514761799\n",
            "Loss SS88:  0.09809303170075515\n",
            "Loss SS11:  0.0889985997155521\n",
            "Loss SS22:  0.07646248812264526\n",
            "Loss SS33:  0.08933648541357443\n",
            "Loss SS44:  0.07930737593744555\n",
            "Loss SS55:  0.09177738714676636\n",
            "Loss SS66:  0.09219100258374147\n",
            "Loss SS77:  0.091169236477284\n",
            "Loss SS88:  0.09811162649311571\n",
            "Loss SS11:  0.0890658933081125\n",
            "Loss SS22:  0.07638039439916611\n",
            "Loss SS33:  0.08925447559340179\n",
            "Loss SS44:  0.07921510257879452\n",
            "Loss SS55:  0.0916767728584625\n",
            "Loss SS66:  0.09213207437143431\n",
            "Loss SS77:  0.09117109365658087\n",
            "Loss SS88:  0.09802153110091376\n",
            "Loss SS11:  0.0890167486675666\n",
            "Loss SS22:  0.07629969105566287\n",
            "Loss SS33:  0.0892175341431664\n",
            "Loss SS44:  0.07914194514889601\n",
            "Loss SS55:  0.09161777079426696\n",
            "Loss SS66:  0.09206631106909395\n",
            "Loss SS77:  0.09107634674185369\n",
            "Loss SS88:  0.09796478761977584\n",
            "Loss SS11:  0.08899658321943182\n",
            "Loss SS22:  0.07626459368179477\n",
            "Loss SS33:  0.08912278854549713\n",
            "Loss SS44:  0.07901977622524647\n",
            "Loss SS55:  0.09157490622731332\n",
            "Loss SS66:  0.09197073932394893\n",
            "Loss SS77:  0.09094309816601395\n",
            "Loss SS88:  0.09782463894897872\n",
            "Loss SS11:  0.08894079372934673\n",
            "Loss SS22:  0.07621372119545022\n",
            "Loss SS33:  0.0889945466004674\n",
            "Loss SS44:  0.07897898636739273\n",
            "Loss SS55:  0.0914430113704613\n",
            "Loss SS66:  0.09190084592765554\n",
            "Loss SS77:  0.09085951023318274\n",
            "Loss SS88:  0.09762253153049733\n",
            "Loss SS11:  0.0890117335014807\n",
            "Loss SS22:  0.07626256692290603\n",
            "Loss SS33:  0.0890551333006778\n",
            "Loss SS44:  0.07900228395946603\n",
            "Loss SS55:  0.09143932299349374\n",
            "Loss SS66:  0.09189792733611608\n",
            "Loss SS77:  0.0908919109816563\n",
            "Loss SS88:  0.09758982831551845\n",
            "Loss SS11:  0.08904382004572527\n",
            "Loss SS22:  0.07625110519918502\n",
            "Loss SS33:  0.08906819169248688\n",
            "Loss SS44:  0.0790155809508623\n",
            "Loss SS55:  0.09137085691058143\n",
            "Loss SS66:  0.0918077761783217\n",
            "Loss SS77:  0.09075229169025908\n",
            "Loss SS88:  0.0974923514935512\n",
            "Loss SS11:  0.08914959251951435\n",
            "Loss SS22:  0.0762587785189905\n",
            "Loss SS33:  0.08916323614304536\n",
            "Loss SS44:  0.07903755220834546\n",
            "Loss SS55:  0.09144827327082672\n",
            "Loss SS66:  0.0918768417452973\n",
            "Loss SS77:  0.09079255390195552\n",
            "Loss SS88:  0.09759064073588106\n",
            "Loss SS11:  0.08918471968021857\n",
            "Loss SS22:  0.07625118425232628\n",
            "Loss SS33:  0.08917511482733861\n",
            "Loss SS44:  0.07898099210519526\n",
            "Loss SS55:  0.09144472626411722\n",
            "Loss SS66:  0.09184362675363669\n",
            "Loss SS77:  0.09070288172108391\n",
            "Loss SS88:  0.09758394638512914\n",
            "Loss SS11:  0.08924061073467575\n",
            "Loss SS22:  0.07623248984889379\n",
            "Loss SS33:  0.08927107509436791\n",
            "Loss SS44:  0.07895484902893875\n",
            "Loss SS55:  0.09146846302039499\n",
            "Loss SS66:  0.09186069584555095\n",
            "Loss SS77:  0.09072855905610688\n",
            "Loss SS88:  0.09763159771827884\n",
            "Loss SS11:  0.08932894063564203\n",
            "Loss SS22:  0.07624338752282432\n",
            "Loss SS33:  0.08928879039845287\n",
            "Loss SS44:  0.07898577540584255\n",
            "Loss SS55:  0.09148885899464994\n",
            "Loss SS66:  0.09188846292226119\n",
            "Loss SS77:  0.09076065933559\n",
            "Loss SS88:  0.09769873935605364\n",
            "Loss SS11:  0.08927444242538961\n",
            "Loss SS22:  0.07624079518165609\n",
            "Loss SS33:  0.08936381790940519\n",
            "Loss SS44:  0.07901192973660802\n",
            "Loss SS55:  0.09158367230486197\n",
            "Loss SS66:  0.09191722566678054\n",
            "Loss SS77:  0.09084220321085344\n",
            "Loss SS88:  0.09774395527718124\n",
            "Loss SS11:  0.0892737237954089\n",
            "Loss SS22:  0.07628978425023915\n",
            "Loss SS33:  0.08936745981881573\n",
            "Loss SS44:  0.07904004180633845\n",
            "Loss SS55:  0.0915532752934788\n",
            "Loss SS66:  0.09191690374268595\n",
            "Loss SS77:  0.09084291319558575\n",
            "Loss SS88:  0.09770490630923309\n",
            "Loss SS11:  0.08929841993318526\n",
            "Loss SS22:  0.07627239381882851\n",
            "Loss SS33:  0.08938762420118475\n",
            "Loss SS44:  0.07905765223218102\n",
            "Loss SS55:  0.09150244168580941\n",
            "Loss SS66:  0.09196606956871532\n",
            "Loss SS77:  0.09082946906218657\n",
            "Loss SS88:  0.09771646587120024\n",
            "Loss SS11:  0.08927543635827218\n",
            "Loss SS22:  0.07625065150608114\n",
            "Loss SS33:  0.08930792519917556\n",
            "Loss SS44:  0.0790376036205991\n",
            "Loss SS55:  0.09145859630433216\n",
            "Loss SS66:  0.09195024159010462\n",
            "Loss SS77:  0.09082842173620057\n",
            "Loss SS88:  0.09771488028789004\n",
            "Validation: \n",
            " Loss SS11:  0.07981235533952713\n",
            " Loss SS22:  0.11070980131626129\n",
            " Loss SS33:  0.11271093040704727\n",
            " Loss SS55:  0.10308006405830383\n",
            " Loss SS66:  0.12459588050842285\n",
            " Loss SS77:  0.11456446349620819\n",
            " Loss SS88:  0.11935174465179443\n",
            " Loss SS99:  0.15785858035087585\n",
            " Loss SS11:  0.0964758289711816\n",
            " Loss SS22:  0.11861307138488406\n",
            " Loss SS33:  0.12378734350204468\n",
            " Loss SS55:  0.11827992931717918\n",
            " Loss SS66:  0.14037043814148223\n",
            " Loss SS77:  0.13307729257004602\n",
            " Loss SS88:  0.13917829734938486\n",
            " Loss SS99:  0.1555831219468798\n",
            " Loss SS11:  0.09372065961360931\n",
            " Loss SS22:  0.11602625995874405\n",
            " Loss SS33:  0.12263172265233063\n",
            " Loss SS55:  0.11748063891399198\n",
            " Loss SS66:  0.1394040682693807\n",
            " Loss SS77:  0.13406518301585826\n",
            " Loss SS88:  0.13906033518837718\n",
            " Loss SS99:  0.1547176950588459\n",
            " Loss SS11:  0.09296010312486867\n",
            " Loss SS22:  0.1147231366790709\n",
            " Loss SS33:  0.12127407689075001\n",
            " Loss SS55:  0.11645608883900721\n",
            " Loss SS66:  0.13787594755164911\n",
            " Loss SS77:  0.13244640851607087\n",
            " Loss SS88:  0.13837927494381294\n",
            " Loss SS99:  0.1517577028421105\n",
            " Loss SS11:  0.09254960669779483\n",
            " Loss SS22:  0.11423845230429261\n",
            " Loss SS33:  0.12136535456886997\n",
            " Loss SS55:  0.11645137454256599\n",
            " Loss SS66:  0.13664409269889197\n",
            " Loss SS77:  0.1323697570665383\n",
            " Loss SS88:  0.13734713435908893\n",
            " Loss SS99:  0.1513893743172104\n",
            "\n",
            "Epoch: 51\n",
            "Loss SS11:  0.08765102177858353\n",
            "Loss SS22:  0.07125680893659592\n",
            "Loss SS33:  0.07666753232479095\n",
            "Loss SS44:  0.0729869082570076\n",
            "Loss SS55:  0.08298486471176147\n",
            "Loss SS66:  0.08336111158132553\n",
            "Loss SS77:  0.0830434188246727\n",
            "Loss SS88:  0.09171754866838455\n",
            "Loss SS11:  0.09208137135614049\n",
            "Loss SS22:  0.07724686373363841\n",
            "Loss SS33:  0.09061164679852399\n",
            "Loss SS44:  0.08253720538182692\n",
            "Loss SS55:  0.0944796312939037\n",
            "Loss SS66:  0.09673330865123055\n",
            "Loss SS77:  0.09472806548530405\n",
            "Loss SS88:  0.1006856777451255\n",
            "Loss SS11:  0.09058681698072524\n",
            "Loss SS22:  0.07736614381983167\n",
            "Loss SS33:  0.09145915934017726\n",
            "Loss SS44:  0.08147356907526652\n",
            "Loss SS55:  0.09398502537182399\n",
            "Loss SS66:  0.09758589842489787\n",
            "Loss SS77:  0.093733939741339\n",
            "Loss SS88:  0.09926539411147435\n",
            "Loss SS11:  0.09048126085150626\n",
            "Loss SS22:  0.07651878196385599\n",
            "Loss SS33:  0.08991417601223915\n",
            "Loss SS44:  0.07894793140792078\n",
            "Loss SS55:  0.09194944462468548\n",
            "Loss SS66:  0.09444012444826864\n",
            "Loss SS77:  0.09244289441454795\n",
            "Loss SS88:  0.0978375262310428\n",
            "Loss SS11:  0.09089635839549506\n",
            "Loss SS22:  0.07661784876410555\n",
            "Loss SS33:  0.08923240551134436\n",
            "Loss SS44:  0.07928113521235745\n",
            "Loss SS55:  0.09177429374398255\n",
            "Loss SS66:  0.09363661760964044\n",
            "Loss SS77:  0.09073082830120878\n",
            "Loss SS88:  0.09730089619392301\n",
            "Loss SS11:  0.09135334208315495\n",
            "Loss SS22:  0.07649282219947554\n",
            "Loss SS33:  0.09031664024965436\n",
            "Loss SS44:  0.07967960286666365\n",
            "Loss SS55:  0.0923301070636394\n",
            "Loss SS66:  0.09363777365754633\n",
            "Loss SS77:  0.09099442482578989\n",
            "Loss SS88:  0.09768559360036663\n",
            "Loss SS11:  0.09121348171449098\n",
            "Loss SS22:  0.07650043815374374\n",
            "Loss SS33:  0.08996148383031126\n",
            "Loss SS44:  0.07956170565525039\n",
            "Loss SS55:  0.09224705178229535\n",
            "Loss SS66:  0.09373208733855701\n",
            "Loss SS77:  0.0913149791418529\n",
            "Loss SS88:  0.09793250262737274\n",
            "Loss SS11:  0.09105446514948992\n",
            "Loss SS22:  0.07638820092862761\n",
            "Loss SS33:  0.08996125142759001\n",
            "Loss SS44:  0.07954717946934028\n",
            "Loss SS55:  0.09221861868257254\n",
            "Loss SS66:  0.09383571693595026\n",
            "Loss SS77:  0.09148693084716797\n",
            "Loss SS88:  0.09795473327099437\n",
            "Loss SS11:  0.09078016260891784\n",
            "Loss SS22:  0.07608853685267178\n",
            "Loss SS33:  0.08965250417406176\n",
            "Loss SS44:  0.07950552520744594\n",
            "Loss SS55:  0.0919305286657663\n",
            "Loss SS66:  0.09349152730938828\n",
            "Loss SS77:  0.09154543253006758\n",
            "Loss SS88:  0.09751940251868448\n",
            "Loss SS11:  0.0911779346374365\n",
            "Loss SS22:  0.07607914875824373\n",
            "Loss SS33:  0.089140959247783\n",
            "Loss SS44:  0.07922335497134335\n",
            "Loss SS55:  0.09195975746427264\n",
            "Loss SS66:  0.09320612751193098\n",
            "Loss SS77:  0.09144747732104835\n",
            "Loss SS88:  0.0976519967009733\n",
            "Loss SS11:  0.09049189577598384\n",
            "Loss SS22:  0.07583888339819295\n",
            "Loss SS33:  0.08871400754640599\n",
            "Loss SS44:  0.07895872575959356\n",
            "Loss SS55:  0.0916541981520039\n",
            "Loss SS66:  0.09263175829212264\n",
            "Loss SS77:  0.09120449320514604\n",
            "Loss SS88:  0.09726955203136595\n",
            "Loss SS11:  0.0901635323424597\n",
            "Loss SS22:  0.07573171909730714\n",
            "Loss SS33:  0.08830565878668346\n",
            "Loss SS44:  0.07858780929231429\n",
            "Loss SS55:  0.09110656957905572\n",
            "Loss SS66:  0.09209715742785651\n",
            "Loss SS77:  0.09090587268541525\n",
            "Loss SS88:  0.09707972910758611\n",
            "Loss SS11:  0.08980350091683963\n",
            "Loss SS22:  0.07560502445279074\n",
            "Loss SS33:  0.08824989224268386\n",
            "Loss SS44:  0.07846572234734031\n",
            "Loss SS55:  0.09114384158583712\n",
            "Loss SS66:  0.09185005157939659\n",
            "Loss SS77:  0.09073635627908155\n",
            "Loss SS88:  0.09683817323328049\n",
            "Loss SS11:  0.08971641565324696\n",
            "Loss SS22:  0.07555112171605344\n",
            "Loss SS33:  0.08822758114747419\n",
            "Loss SS44:  0.07853474738147423\n",
            "Loss SS55:  0.0912444275867848\n",
            "Loss SS66:  0.09197177868762998\n",
            "Loss SS77:  0.09094215173075218\n",
            "Loss SS88:  0.09699529904218121\n",
            "Loss SS11:  0.08944986956127991\n",
            "Loss SS22:  0.07542630916474559\n",
            "Loss SS33:  0.08808995852022307\n",
            "Loss SS44:  0.07851200151845072\n",
            "Loss SS55:  0.0910437396126436\n",
            "Loss SS66:  0.09174813219207398\n",
            "Loss SS77:  0.09090364069169295\n",
            "Loss SS88:  0.09677254276495453\n",
            "Loss SS11:  0.08965490575855141\n",
            "Loss SS22:  0.07568971499405949\n",
            "Loss SS33:  0.08823844385857614\n",
            "Loss SS44:  0.07857357180177771\n",
            "Loss SS55:  0.09099693798663601\n",
            "Loss SS66:  0.09190492480006439\n",
            "Loss SS77:  0.09079689631201573\n",
            "Loss SS88:  0.09705988988773712\n",
            "Loss SS11:  0.08944285628588303\n",
            "Loss SS22:  0.07556780101442188\n",
            "Loss SS33:  0.08798749980904301\n",
            "Loss SS44:  0.07850816538152487\n",
            "Loss SS55:  0.09090745518481509\n",
            "Loss SS66:  0.09177093258740739\n",
            "Loss SS77:  0.09075866542432619\n",
            "Loss SS88:  0.09719437990129365\n",
            "Loss SS11:  0.08918164056121257\n",
            "Loss SS22:  0.07538791072734616\n",
            "Loss SS33:  0.08784540973560155\n",
            "Loss SS44:  0.07831096115429499\n",
            "Loss SS55:  0.09069383771795976\n",
            "Loss SS66:  0.09160393920906804\n",
            "Loss SS77:  0.09059529446544703\n",
            "Loss SS88:  0.09707926100457621\n",
            "Loss SS11:  0.08934042664522625\n",
            "Loss SS22:  0.07550016146420774\n",
            "Loss SS33:  0.08810108439039789\n",
            "Loss SS44:  0.07835101618210255\n",
            "Loss SS55:  0.09088813669938409\n",
            "Loss SS66:  0.09160147306833478\n",
            "Loss SS77:  0.09070486740018781\n",
            "Loss SS88:  0.09710148421440336\n",
            "Loss SS11:  0.08928186937932568\n",
            "Loss SS22:  0.07558472674944638\n",
            "Loss SS33:  0.08813836511361037\n",
            "Loss SS44:  0.07843016823314872\n",
            "Loss SS55:  0.09079350195630058\n",
            "Loss SS66:  0.0917322716007682\n",
            "Loss SS77:  0.0907346836793485\n",
            "Loss SS88:  0.09721021865206864\n",
            "Loss SS11:  0.08916540755264794\n",
            "Loss SS22:  0.07564036924373452\n",
            "Loss SS33:  0.08810145767470498\n",
            "Loss SS44:  0.07850334854490722\n",
            "Loss SS55:  0.09077224572796133\n",
            "Loss SS66:  0.091593957140078\n",
            "Loss SS77:  0.09072178702301054\n",
            "Loss SS88:  0.09709198202066753\n",
            "Loss SS11:  0.08918591260345061\n",
            "Loss SS22:  0.07573526428562205\n",
            "Loss SS33:  0.08818913915004775\n",
            "Loss SS44:  0.07863544271949909\n",
            "Loss SS55:  0.09064147218849987\n",
            "Loss SS66:  0.09162745790741456\n",
            "Loss SS77:  0.09075873645278515\n",
            "Loss SS88:  0.09704719045998361\n",
            "Loss SS11:  0.08939724698730184\n",
            "Loss SS22:  0.07566688841891504\n",
            "Loss SS33:  0.0883572578632454\n",
            "Loss SS44:  0.07860373137560905\n",
            "Loss SS55:  0.09072838710174301\n",
            "Loss SS66:  0.09174124984180226\n",
            "Loss SS77:  0.09091883998920475\n",
            "Loss SS88:  0.09728004766535435\n",
            "Loss SS11:  0.08950480557494349\n",
            "Loss SS22:  0.07570766710809299\n",
            "Loss SS33:  0.08837326216104227\n",
            "Loss SS44:  0.07871222229940551\n",
            "Loss SS55:  0.09073990048010112\n",
            "Loss SS66:  0.09176785666576195\n",
            "Loss SS77:  0.09099091241112003\n",
            "Loss SS88:  0.09728406156812396\n",
            "Loss SS11:  0.08960256604980137\n",
            "Loss SS22:  0.0758772848088959\n",
            "Loss SS33:  0.08841331766848742\n",
            "Loss SS44:  0.07878078370358935\n",
            "Loss SS55:  0.09090237718276463\n",
            "Loss SS66:  0.0919215589205259\n",
            "Loss SS77:  0.0911073489926168\n",
            "Loss SS88:  0.0975071501558747\n",
            "Loss SS11:  0.08941412070001739\n",
            "Loss SS22:  0.07586230689370299\n",
            "Loss SS33:  0.08842777125388977\n",
            "Loss SS44:  0.07884813247033325\n",
            "Loss SS55:  0.09097175101241267\n",
            "Loss SS66:  0.09201035437830891\n",
            "Loss SS77:  0.0912675975090954\n",
            "Loss SS88:  0.09754908072995949\n",
            "Loss SS11:  0.08935527700459821\n",
            "Loss SS22:  0.07591921038269084\n",
            "Loss SS33:  0.08855196362596819\n",
            "Loss SS44:  0.07886015168494648\n",
            "Loss SS55:  0.09106820901691685\n",
            "Loss SS66:  0.09193618806842643\n",
            "Loss SS77:  0.09123997261811947\n",
            "Loss SS88:  0.09749082413098821\n",
            "Loss SS11:  0.08936396428139887\n",
            "Loss SS22:  0.07607819985364636\n",
            "Loss SS33:  0.08863419327348801\n",
            "Loss SS44:  0.0789120982356397\n",
            "Loss SS55:  0.09104145859880201\n",
            "Loss SS66:  0.09188583204557095\n",
            "Loss SS77:  0.09138804173777464\n",
            "Loss SS88:  0.09744724546081465\n",
            "Loss SS11:  0.08933291483498129\n",
            "Loss SS22:  0.07605638579064417\n",
            "Loss SS33:  0.08874221501401311\n",
            "Loss SS44:  0.0789748844806622\n",
            "Loss SS55:  0.09098824837453849\n",
            "Loss SS66:  0.09193422860098055\n",
            "Loss SS77:  0.09128244884073522\n",
            "Loss SS88:  0.09743315567431501\n",
            "Loss SS11:  0.08931119463492915\n",
            "Loss SS22:  0.07602568898874869\n",
            "Loss SS33:  0.0888017898623886\n",
            "Loss SS44:  0.07904959417546738\n",
            "Loss SS55:  0.09096736284260898\n",
            "Loss SS66:  0.09201094200930644\n",
            "Loss SS77:  0.0913898358537569\n",
            "Loss SS88:  0.09751878637833283\n",
            "Loss SS11:  0.08933799664344502\n",
            "Loss SS22:  0.07606805643171963\n",
            "Loss SS33:  0.08881294004544864\n",
            "Loss SS44:  0.07898995865668569\n",
            "Loss SS55:  0.09104860997279221\n",
            "Loss SS66:  0.09201449098876148\n",
            "Loss SS77:  0.0914538076698186\n",
            "Loss SS88:  0.09746798702153653\n",
            "Loss SS11:  0.08928666520157044\n",
            "Loss SS22:  0.07597205196162896\n",
            "Loss SS33:  0.08869443989547503\n",
            "Loss SS44:  0.07884739929888027\n",
            "Loss SS55:  0.09095170902285929\n",
            "Loss SS66:  0.09177400626936909\n",
            "Loss SS77:  0.09131217903646242\n",
            "Loss SS88:  0.09727741178975612\n",
            "Loss SS11:  0.08934769907101664\n",
            "Loss SS22:  0.07604741755397149\n",
            "Loss SS33:  0.08868790998172908\n",
            "Loss SS44:  0.07889697557994138\n",
            "Loss SS55:  0.09100321373928373\n",
            "Loss SS66:  0.0918411745971237\n",
            "Loss SS77:  0.09136900321995357\n",
            "Loss SS88:  0.09737553855990323\n",
            "Loss SS11:  0.08933024209701763\n",
            "Loss SS22:  0.07600750542299264\n",
            "Loss SS33:  0.0885930583106067\n",
            "Loss SS44:  0.0787552020363397\n",
            "Loss SS55:  0.09084718775713192\n",
            "Loss SS66:  0.09170525649343013\n",
            "Loss SS77:  0.09113846532437736\n",
            "Loss SS88:  0.09731028940292283\n",
            "Loss SS11:  0.08942212424407607\n",
            "Loss SS22:  0.07602972578792629\n",
            "Loss SS33:  0.0886700431709765\n",
            "Loss SS44:  0.07875599706671105\n",
            "Loss SS55:  0.09082909336974544\n",
            "Loss SS66:  0.09179600345686384\n",
            "Loss SS77:  0.09118033263865105\n",
            "Loss SS88:  0.09738228789085517\n",
            "Loss SS11:  0.08928561121480078\n",
            "Loss SS22:  0.07603292054685093\n",
            "Loss SS33:  0.08855714194710099\n",
            "Loss SS44:  0.07869125716918894\n",
            "Loss SS55:  0.09071982989453861\n",
            "Loss SS66:  0.09179116719723428\n",
            "Loss SS77:  0.0911646248013885\n",
            "Loss SS88:  0.09727428545724293\n",
            "Loss SS11:  0.08935429841032319\n",
            "Loss SS22:  0.0759305039890255\n",
            "Loss SS33:  0.08847977064157787\n",
            "Loss SS44:  0.07860574630860477\n",
            "Loss SS55:  0.09062896576036707\n",
            "Loss SS66:  0.09169503542855176\n",
            "Loss SS77:  0.09111363816376869\n",
            "Loss SS88:  0.09720934370217892\n",
            "Loss SS11:  0.08934621400428268\n",
            "Loss SS22:  0.07593512796166772\n",
            "Loss SS33:  0.08845037561301594\n",
            "Loss SS44:  0.07855282263011945\n",
            "Loss SS55:  0.09058959316211249\n",
            "Loss SS66:  0.0915732689583398\n",
            "Loss SS77:  0.0911478882610316\n",
            "Loss SS88:  0.09716457967970249\n",
            "Loss SS11:  0.0892845264335317\n",
            "Loss SS22:  0.07590983987949651\n",
            "Loss SS33:  0.0884279335342993\n",
            "Loss SS44:  0.07846183946791284\n",
            "Loss SS55:  0.09048659794443235\n",
            "Loss SS66:  0.09148116818484985\n",
            "Loss SS77:  0.0910621120782662\n",
            "Loss SS88:  0.09711237556821718\n",
            "Loss SS11:  0.0893064583735088\n",
            "Loss SS22:  0.07584363647053004\n",
            "Loss SS33:  0.08839231384608447\n",
            "Loss SS44:  0.07841111268000224\n",
            "Loss SS55:  0.09043279955225528\n",
            "Loss SS66:  0.09142793958906627\n",
            "Loss SS77:  0.09093342389901886\n",
            "Loss SS88:  0.09698734240001425\n",
            "Loss SS11:  0.08932425904823955\n",
            "Loss SS22:  0.07591086211718823\n",
            "Loss SS33:  0.08841185791980001\n",
            "Loss SS44:  0.07843586326834567\n",
            "Loss SS55:  0.09054266884365582\n",
            "Loss SS66:  0.09144104049167133\n",
            "Loss SS77:  0.09098490801386702\n",
            "Loss SS88:  0.0970288672277755\n",
            "Loss SS11:  0.08942817394460785\n",
            "Loss SS22:  0.07592018952914978\n",
            "Loss SS33:  0.08848261532267225\n",
            "Loss SS44:  0.07844336269255682\n",
            "Loss SS55:  0.09053625887907915\n",
            "Loss SS66:  0.09147873807272482\n",
            "Loss SS77:  0.09094957278592743\n",
            "Loss SS88:  0.09702327912741333\n",
            "Loss SS11:  0.08951414017241245\n",
            "Loss SS22:  0.07593818122162671\n",
            "Loss SS33:  0.08855919380652366\n",
            "Loss SS44:  0.07850315943891249\n",
            "Loss SS55:  0.09056556709730144\n",
            "Loss SS66:  0.09152757659589876\n",
            "Loss SS77:  0.09100707604454703\n",
            "Loss SS88:  0.0971656648216508\n",
            "Loss SS11:  0.08954095489533484\n",
            "Loss SS22:  0.07593029203312579\n",
            "Loss SS33:  0.08857222165903747\n",
            "Loss SS44:  0.07847134777606225\n",
            "Loss SS55:  0.09052572314672847\n",
            "Loss SS66:  0.09142534915366471\n",
            "Loss SS77:  0.09094546320405195\n",
            "Loss SS88:  0.0971490984000352\n",
            "Loss SS11:  0.08949412723111998\n",
            "Loss SS22:  0.0759412120867208\n",
            "Loss SS33:  0.08860774268976955\n",
            "Loss SS44:  0.0784588207801183\n",
            "Loss SS55:  0.09047431206486933\n",
            "Loss SS66:  0.0913353311130249\n",
            "Loss SS77:  0.09095200401247223\n",
            "Loss SS88:  0.09712495752810892\n",
            "Loss SS11:  0.08950432725333321\n",
            "Loss SS22:  0.07591322652516502\n",
            "Loss SS33:  0.08858309370649363\n",
            "Loss SS44:  0.07851315628398549\n",
            "Loss SS55:  0.09050085144468527\n",
            "Loss SS66:  0.09131113274597011\n",
            "Loss SS77:  0.09091271071833147\n",
            "Loss SS88:  0.09710187616937704\n",
            "Loss SS11:  0.08946949542538463\n",
            "Loss SS22:  0.07593085054397065\n",
            "Loss SS33:  0.08867242377970586\n",
            "Loss SS44:  0.07847123659355262\n",
            "Loss SS55:  0.09058158403492801\n",
            "Loss SS66:  0.09128786553982282\n",
            "Loss SS77:  0.09096743677158935\n",
            "Loss SS88:  0.09712732214795793\n",
            "Loss SS11:  0.08946369034722605\n",
            "Loss SS22:  0.07594192970836745\n",
            "Loss SS33:  0.08863256724586913\n",
            "Loss SS44:  0.07845180067223854\n",
            "Loss SS55:  0.09055095238733697\n",
            "Loss SS66:  0.09123890603360603\n",
            "Loss SS77:  0.09096099373008541\n",
            "Loss SS88:  0.09706052772368595\n",
            "Loss SS11:  0.08938688348026137\n",
            "Loss SS22:  0.07591594778884225\n",
            "Loss SS33:  0.08858198079274747\n",
            "Loss SS44:  0.07841853913609055\n",
            "Loss SS55:  0.090528868216239\n",
            "Loss SS66:  0.0912623216946011\n",
            "Loss SS77:  0.09099601299004346\n",
            "Loss SS88:  0.09706587785866554\n",
            "Loss SS11:  0.08934150841049167\n",
            "Loss SS22:  0.07593148018095013\n",
            "Loss SS33:  0.08859323990927695\n",
            "Loss SS44:  0.07835620348844412\n",
            "Loss SS55:  0.09053011331138203\n",
            "Loss SS66:  0.09125497406102004\n",
            "Loss SS77:  0.09098511315169014\n",
            "Loss SS88:  0.09702444181240746\n",
            "Validation: \n",
            " Loss SS11:  0.0871233344078064\n",
            " Loss SS22:  0.11082644015550613\n",
            " Loss SS33:  0.11439952254295349\n",
            " Loss SS55:  0.1036057248711586\n",
            " Loss SS66:  0.1300574243068695\n",
            " Loss SS77:  0.11844965070486069\n",
            " Loss SS88:  0.11962388455867767\n",
            " Loss SS99:  0.1570875644683838\n",
            " Loss SS11:  0.09951026631253106\n",
            " Loss SS22:  0.11469784343526476\n",
            " Loss SS33:  0.12332079524085635\n",
            " Loss SS55:  0.1163580488590967\n",
            " Loss SS66:  0.14138820483571007\n",
            " Loss SS77:  0.13551176481303714\n",
            " Loss SS88:  0.1419404578350839\n",
            " Loss SS99:  0.15838547973405748\n",
            " Loss SS11:  0.0963693281135908\n",
            " Loss SS22:  0.11221668832912678\n",
            " Loss SS33:  0.12190797089076624\n",
            " Loss SS55:  0.11508181691169739\n",
            " Loss SS66:  0.1393555726219968\n",
            " Loss SS77:  0.13661554819200097\n",
            " Loss SS88:  0.14187034928217168\n",
            " Loss SS99:  0.1568689575282539\n",
            " Loss SS11:  0.0954179938455097\n",
            " Loss SS22:  0.11077563342500905\n",
            " Loss SS33:  0.12047969781961597\n",
            " Loss SS55:  0.11385238329406644\n",
            " Loss SS66:  0.13766846072966935\n",
            " Loss SS77:  0.13483470543974735\n",
            " Loss SS88:  0.14072897761571604\n",
            " Loss SS99:  0.15370721394409897\n",
            " Loss SS11:  0.09497753567533729\n",
            " Loss SS22:  0.11032418961878176\n",
            " Loss SS33:  0.12064883370458344\n",
            " Loss SS55:  0.11377656312636387\n",
            " Loss SS66:  0.13640320383840138\n",
            " Loss SS77:  0.13430319708070637\n",
            " Loss SS88:  0.13952056530081194\n",
            " Loss SS99:  0.15339897784553927\n",
            "\n",
            "Epoch: 52\n",
            "Loss SS11:  0.08352696895599365\n",
            "Loss SS22:  0.07496749609708786\n",
            "Loss SS33:  0.0835757702589035\n",
            "Loss SS44:  0.07830628007650375\n",
            "Loss SS55:  0.08813521265983582\n",
            "Loss SS66:  0.08099305629730225\n",
            "Loss SS77:  0.08395368605852127\n",
            "Loss SS88:  0.10605960339307785\n",
            "Loss SS11:  0.08793240853331306\n",
            "Loss SS22:  0.07736937227574261\n",
            "Loss SS33:  0.08963921666145325\n",
            "Loss SS44:  0.08064546368338844\n",
            "Loss SS55:  0.09442789243026213\n",
            "Loss SS66:  0.0950507799332792\n",
            "Loss SS77:  0.0928806804797866\n",
            "Loss SS88:  0.10162279958074744\n",
            "Loss SS11:  0.08642723092011043\n",
            "Loss SS22:  0.07665764504954928\n",
            "Loss SS33:  0.08993865833396003\n",
            "Loss SS44:  0.07847174150603158\n",
            "Loss SS55:  0.09195702097245625\n",
            "Loss SS66:  0.0936064390199525\n",
            "Loss SS77:  0.09146474408251899\n",
            "Loss SS88:  0.09883290954998561\n",
            "Loss SS11:  0.08730482886875829\n",
            "Loss SS22:  0.0753929556137131\n",
            "Loss SS33:  0.08865003239723944\n",
            "Loss SS44:  0.07675367954277224\n",
            "Loss SS55:  0.09022018289373766\n",
            "Loss SS66:  0.09124458292799612\n",
            "Loss SS77:  0.08999051538205916\n",
            "Loss SS88:  0.0966754368235988\n",
            "Loss SS11:  0.08856054886085231\n",
            "Loss SS22:  0.07554733126265246\n",
            "Loss SS33:  0.08845292268002905\n",
            "Loss SS44:  0.07726632276686227\n",
            "Loss SS55:  0.09008405993624431\n",
            "Loss SS66:  0.09102413203658127\n",
            "Loss SS77:  0.0894707515835762\n",
            "Loss SS88:  0.09692483122755842\n",
            "Loss SS11:  0.08889541117584004\n",
            "Loss SS22:  0.07575782132791538\n",
            "Loss SS33:  0.08939304334275863\n",
            "Loss SS44:  0.07801672670186735\n",
            "Loss SS55:  0.09094903311308693\n",
            "Loss SS66:  0.0919156693944744\n",
            "Loss SS77:  0.09025992613797094\n",
            "Loss SS88:  0.09746464151962131\n",
            "Loss SS11:  0.08902302188951461\n",
            "Loss SS22:  0.07554674643229266\n",
            "Loss SS33:  0.08918014168739319\n",
            "Loss SS44:  0.07831262601692168\n",
            "Loss SS55:  0.09070997440912684\n",
            "Loss SS66:  0.09160360107656385\n",
            "Loss SS77:  0.09039359564175371\n",
            "Loss SS88:  0.09785091950268042\n",
            "Loss SS11:  0.08851559743495055\n",
            "Loss SS22:  0.07557301858151463\n",
            "Loss SS33:  0.0888789146089218\n",
            "Loss SS44:  0.07851303379300614\n",
            "Loss SS55:  0.09085933391896772\n",
            "Loss SS66:  0.09176894655110131\n",
            "Loss SS77:  0.08999249307622373\n",
            "Loss SS88:  0.09758909896645747\n",
            "Loss SS11:  0.08837664550469246\n",
            "Loss SS22:  0.07567555967856336\n",
            "Loss SS33:  0.08860460861965462\n",
            "Loss SS44:  0.07844244238035178\n",
            "Loss SS55:  0.09067841748028625\n",
            "Loss SS66:  0.0915473343597518\n",
            "Loss SS77:  0.09008928038823752\n",
            "Loss SS88:  0.09713546168289067\n",
            "Loss SS11:  0.0886134677208387\n",
            "Loss SS22:  0.075735899159214\n",
            "Loss SS33:  0.08855505967205697\n",
            "Loss SS44:  0.07857529819011688\n",
            "Loss SS55:  0.09065452643803187\n",
            "Loss SS66:  0.09159501388177767\n",
            "Loss SS77:  0.09020854049659037\n",
            "Loss SS88:  0.09712706891062496\n",
            "Loss SS11:  0.08851339431977508\n",
            "Loss SS22:  0.07539051512975505\n",
            "Loss SS33:  0.08806190470067582\n",
            "Loss SS44:  0.07822920109080796\n",
            "Loss SS55:  0.09048469932657657\n",
            "Loss SS66:  0.09101286869828064\n",
            "Loss SS77:  0.08978674405872232\n",
            "Loss SS88:  0.09663124699698816\n",
            "Loss SS11:  0.08844840378911646\n",
            "Loss SS22:  0.07533325147521389\n",
            "Loss SS33:  0.08776897903498229\n",
            "Loss SS44:  0.07826711680438067\n",
            "Loss SS55:  0.09024354761785215\n",
            "Loss SS66:  0.09086420494425404\n",
            "Loss SS77:  0.0894362802306811\n",
            "Loss SS88:  0.09633123169879655\n",
            "Loss SS11:  0.08835737290214901\n",
            "Loss SS22:  0.07536604144602768\n",
            "Loss SS33:  0.08797049466982361\n",
            "Loss SS44:  0.07819538618907455\n",
            "Loss SS55:  0.09009424966475195\n",
            "Loss SS66:  0.09082771696088728\n",
            "Loss SS77:  0.08927562962139933\n",
            "Loss SS88:  0.09606731276620518\n",
            "Loss SS11:  0.08814303548508928\n",
            "Loss SS22:  0.07551619393452433\n",
            "Loss SS33:  0.0880788511788572\n",
            "Loss SS44:  0.07844824855791703\n",
            "Loss SS55:  0.09027815012986423\n",
            "Loss SS66:  0.09089108812444993\n",
            "Loss SS77:  0.08954075297326532\n",
            "Loss SS88:  0.09640191262232438\n",
            "Loss SS11:  0.08811945700687719\n",
            "Loss SS22:  0.07528961492134324\n",
            "Loss SS33:  0.08794463832750388\n",
            "Loss SS44:  0.07836079840541732\n",
            "Loss SS55:  0.09004097648546205\n",
            "Loss SS66:  0.09087390331089074\n",
            "Loss SS77:  0.08929787428243786\n",
            "Loss SS88:  0.09624184926985004\n",
            "Loss SS11:  0.08817439935854729\n",
            "Loss SS22:  0.07546947871809763\n",
            "Loss SS33:  0.08809594563301036\n",
            "Loss SS44:  0.07857179330871594\n",
            "Loss SS55:  0.09009715924594576\n",
            "Loss SS66:  0.09092366157580685\n",
            "Loss SS77:  0.08933520331880115\n",
            "Loss SS88:  0.09627245085326251\n",
            "Loss SS11:  0.0880030342233107\n",
            "Loss SS22:  0.07539309556624904\n",
            "Loss SS33:  0.08798429767907777\n",
            "Loss SS44:  0.07855359724034434\n",
            "Loss SS55:  0.09011220709877725\n",
            "Loss SS66:  0.09090677940327188\n",
            "Loss SS77:  0.08935557005012998\n",
            "Loss SS88:  0.096229571502031\n",
            "Loss SS11:  0.08772845945337363\n",
            "Loss SS22:  0.0752404014748788\n",
            "Loss SS33:  0.08778397599507494\n",
            "Loss SS44:  0.07851330942490645\n",
            "Loss SS55:  0.09012738061927215\n",
            "Loss SS66:  0.09089895385747765\n",
            "Loss SS77:  0.08941380522753063\n",
            "Loss SS88:  0.09629263258294056\n",
            "Loss SS11:  0.0877758154223637\n",
            "Loss SS22:  0.0753625586258443\n",
            "Loss SS33:  0.08795965627576764\n",
            "Loss SS44:  0.07859908906712058\n",
            "Loss SS55:  0.09015596139332208\n",
            "Loss SS66:  0.09080429334008233\n",
            "Loss SS77:  0.08946534995380687\n",
            "Loss SS88:  0.0964130818514534\n",
            "Loss SS11:  0.08785509981254008\n",
            "Loss SS22:  0.07536712700393812\n",
            "Loss SS33:  0.08808134804377381\n",
            "Loss SS44:  0.07875097330406074\n",
            "Loss SS55:  0.09023027660334922\n",
            "Loss SS66:  0.09109550843220107\n",
            "Loss SS77:  0.08957446073986473\n",
            "Loss SS88:  0.09653099700418442\n",
            "Loss SS11:  0.08782310843171172\n",
            "Loss SS22:  0.07539991902845417\n",
            "Loss SS33:  0.08801728060737771\n",
            "Loss SS44:  0.07875399782660589\n",
            "Loss SS55:  0.09027184212385718\n",
            "Loss SS66:  0.09092595557964857\n",
            "Loss SS77:  0.0897944879843228\n",
            "Loss SS88:  0.09655921976661208\n",
            "Loss SS11:  0.08804139713822948\n",
            "Loss SS22:  0.0753901043103487\n",
            "Loss SS33:  0.0880030742471252\n",
            "Loss SS44:  0.07873136121161742\n",
            "Loss SS55:  0.09028179191441332\n",
            "Loss SS66:  0.09093293526443826\n",
            "Loss SS77:  0.08990654914300024\n",
            "Loss SS88:  0.09642367612270382\n",
            "Loss SS11:  0.08816814304611802\n",
            "Loss SS22:  0.07538869692122235\n",
            "Loss SS33:  0.08805351508823456\n",
            "Loss SS44:  0.07873651403849481\n",
            "Loss SS55:  0.0905385214953401\n",
            "Loss SS66:  0.09115732734289644\n",
            "Loss SS77:  0.0901382186900976\n",
            "Loss SS88:  0.09654677241231521\n",
            "Loss SS11:  0.0883076542190143\n",
            "Loss SS22:  0.07533208088892879\n",
            "Loss SS33:  0.08812207577032444\n",
            "Loss SS44:  0.07882820000444656\n",
            "Loss SS55:  0.0904885573046548\n",
            "Loss SS66:  0.09120130303489181\n",
            "Loss SS77:  0.09020759620062717\n",
            "Loss SS88:  0.09667039082034842\n",
            "Loss SS11:  0.08853166306167223\n",
            "Loss SS22:  0.07544697383689188\n",
            "Loss SS33:  0.08817861705276481\n",
            "Loss SS44:  0.07892271006070233\n",
            "Loss SS55:  0.09070422695138147\n",
            "Loss SS66:  0.09141970466034047\n",
            "Loss SS77:  0.09046171466468281\n",
            "Loss SS88:  0.09681976441278498\n",
            "Loss SS11:  0.08840534171141476\n",
            "Loss SS22:  0.07555396327993784\n",
            "Loss SS33:  0.08835685941802553\n",
            "Loss SS44:  0.07891212402290082\n",
            "Loss SS55:  0.09081156465755516\n",
            "Loss SS66:  0.09149250934323466\n",
            "Loss SS77:  0.0905681789158825\n",
            "Loss SS88:  0.0969260267586822\n",
            "Loss SS11:  0.08848337183966948\n",
            "Loss SS22:  0.07567154143676447\n",
            "Loss SS33:  0.08840713904050118\n",
            "Loss SS44:  0.07889749881922056\n",
            "Loss SS55:  0.09084226739132541\n",
            "Loss SS66:  0.09152818910806115\n",
            "Loss SS77:  0.09053435536294148\n",
            "Loss SS88:  0.096866342328289\n",
            "Loss SS11:  0.08857617018627505\n",
            "Loss SS22:  0.07583290048465957\n",
            "Loss SS33:  0.08854475383718956\n",
            "Loss SS44:  0.07907067097695991\n",
            "Loss SS55:  0.09096852439795913\n",
            "Loss SS66:  0.09149852202607257\n",
            "Loss SS77:  0.09061548358509901\n",
            "Loss SS88:  0.09702681675828251\n",
            "Loss SS11:  0.08867723953469368\n",
            "Loss SS22:  0.07582019732144804\n",
            "Loss SS33:  0.08864136705725219\n",
            "Loss SS44:  0.07914003765614856\n",
            "Loss SS55:  0.09094280431385142\n",
            "Loss SS66:  0.0916248921344713\n",
            "Loss SS77:  0.09059207966001008\n",
            "Loss SS88:  0.09706568052442048\n",
            "Loss SS11:  0.0886037598309648\n",
            "Loss SS22:  0.07588271293597124\n",
            "Loss SS33:  0.08880060995669709\n",
            "Loss SS44:  0.0792295680960634\n",
            "Loss SS55:  0.09097634573367863\n",
            "Loss SS66:  0.09171743429813188\n",
            "Loss SS77:  0.09066842744747798\n",
            "Loss SS88:  0.09712345196619067\n",
            "Loss SS11:  0.08857609084080224\n",
            "Loss SS22:  0.0758697028175937\n",
            "Loss SS33:  0.0887863112000928\n",
            "Loss SS44:  0.07925294256735085\n",
            "Loss SS55:  0.09103441480980363\n",
            "Loss SS66:  0.09168120792933873\n",
            "Loss SS77:  0.09067421007988065\n",
            "Loss SS88:  0.09709337645788921\n",
            "Loss SS11:  0.08851232226351066\n",
            "Loss SS22:  0.07578188475592727\n",
            "Loss SS33:  0.08871874282693556\n",
            "Loss SS44:  0.07921844820261385\n",
            "Loss SS55:  0.09093529675934475\n",
            "Loss SS66:  0.09155391108759729\n",
            "Loss SS77:  0.09050238458289978\n",
            "Loss SS88:  0.09692337905383187\n",
            "Loss SS11:  0.08861850647725791\n",
            "Loss SS22:  0.07582202697654379\n",
            "Loss SS33:  0.08874369990602832\n",
            "Loss SS44:  0.07923159754118445\n",
            "Loss SS55:  0.09104232831257526\n",
            "Loss SS66:  0.09155949430302296\n",
            "Loss SS77:  0.0905920356001438\n",
            "Loss SS88:  0.09697847293451939\n",
            "Loss SS11:  0.08861110201865165\n",
            "Loss SS22:  0.075798123714787\n",
            "Loss SS33:  0.0886570290631398\n",
            "Loss SS44:  0.07907466600533698\n",
            "Loss SS55:  0.0910147418158292\n",
            "Loss SS66:  0.09133673785676409\n",
            "Loss SS77:  0.09053678551739436\n",
            "Loss SS88:  0.09695379548562617\n",
            "Loss SS11:  0.08866624459842783\n",
            "Loss SS22:  0.07580600285634967\n",
            "Loss SS33:  0.0886546773426344\n",
            "Loss SS44:  0.0790451087844162\n",
            "Loss SS55:  0.09107876413903278\n",
            "Loss SS66:  0.09136206750646016\n",
            "Loss SS77:  0.09056959749142096\n",
            "Loss SS88:  0.09690339102947816\n",
            "Loss SS11:  0.08868384087442333\n",
            "Loss SS22:  0.07578981640162291\n",
            "Loss SS33:  0.08864542941676926\n",
            "Loss SS44:  0.07902764975812361\n",
            "Loss SS55:  0.0910661612395887\n",
            "Loss SS66:  0.09148584643745014\n",
            "Loss SS77:  0.09061867726749165\n",
            "Loss SS88:  0.09689068257214337\n",
            "Loss SS11:  0.08878929248476954\n",
            "Loss SS22:  0.07580233338466971\n",
            "Loss SS33:  0.08859086080023457\n",
            "Loss SS44:  0.0790223943332721\n",
            "Loss SS55:  0.09110162648632916\n",
            "Loss SS66:  0.09153142412531079\n",
            "Loss SS77:  0.09059720779472441\n",
            "Loss SS88:  0.0968754933811621\n",
            "Loss SS11:  0.08886456023650671\n",
            "Loss SS22:  0.07574073795600722\n",
            "Loss SS33:  0.0885718629728109\n",
            "Loss SS44:  0.07901353747496065\n",
            "Loss SS55:  0.09100316444818543\n",
            "Loss SS66:  0.09149605264721533\n",
            "Loss SS77:  0.09052587033442731\n",
            "Loss SS88:  0.09686478526605108\n",
            "Loss SS11:  0.0888477159453815\n",
            "Loss SS22:  0.07570640991131465\n",
            "Loss SS33:  0.08846501878944282\n",
            "Loss SS44:  0.0789367941127518\n",
            "Loss SS55:  0.09084321943715488\n",
            "Loss SS66:  0.0913579372048691\n",
            "Loss SS77:  0.09042201220519899\n",
            "Loss SS88:  0.09676408531080706\n",
            "Loss SS11:  0.0888176524578153\n",
            "Loss SS22:  0.07567627775623366\n",
            "Loss SS33:  0.08835969396564357\n",
            "Loss SS44:  0.07886275259392037\n",
            "Loss SS55:  0.0907220407139005\n",
            "Loss SS66:  0.09123402921592488\n",
            "Loss SS77:  0.09030241427747794\n",
            "Loss SS88:  0.09672517321832344\n",
            "Loss SS11:  0.08897236312862644\n",
            "Loss SS22:  0.07567683265467831\n",
            "Loss SS33:  0.08830910358122757\n",
            "Loss SS44:  0.07884560812478351\n",
            "Loss SS55:  0.09076664362688017\n",
            "Loss SS66:  0.09124692343639912\n",
            "Loss SS77:  0.09027669275936641\n",
            "Loss SS88:  0.09677039491862728\n",
            "Loss SS11:  0.08897738769614204\n",
            "Loss SS22:  0.07561871476489553\n",
            "Loss SS33:  0.08827486278946962\n",
            "Loss SS44:  0.07877475707599137\n",
            "Loss SS55:  0.09074245291759787\n",
            "Loss SS66:  0.09120174209131812\n",
            "Loss SS77:  0.09021835752429753\n",
            "Loss SS88:  0.09678002941782457\n",
            "Loss SS11:  0.0889778178441836\n",
            "Loss SS22:  0.07565372083291871\n",
            "Loss SS33:  0.08830506051945856\n",
            "Loss SS44:  0.07879250191346766\n",
            "Loss SS55:  0.09081101969698546\n",
            "Loss SS66:  0.09127049937384145\n",
            "Loss SS77:  0.09026393080196585\n",
            "Loss SS88:  0.09694637349339959\n",
            "Loss SS11:  0.08895231588049997\n",
            "Loss SS22:  0.07558546377791053\n",
            "Loss SS33:  0.08823512095240595\n",
            "Loss SS44:  0.0787891743060746\n",
            "Loss SS55:  0.09073715809118997\n",
            "Loss SS66:  0.09118175053679473\n",
            "Loss SS77:  0.09019034715442813\n",
            "Loss SS88:  0.09692682920503505\n",
            "Loss SS11:  0.08888563817861128\n",
            "Loss SS22:  0.07556391324923963\n",
            "Loss SS33:  0.08828321993013089\n",
            "Loss SS44:  0.07874917922342986\n",
            "Loss SS55:  0.09073340387639005\n",
            "Loss SS66:  0.0911437020449141\n",
            "Loss SS77:  0.09023089029418638\n",
            "Loss SS88:  0.09697645634007292\n",
            "Loss SS11:  0.0889591473070058\n",
            "Loss SS22:  0.07555581878730304\n",
            "Loss SS33:  0.08833044058665997\n",
            "Loss SS44:  0.07877604208242338\n",
            "Loss SS55:  0.09074626669717205\n",
            "Loss SS66:  0.09119597419336471\n",
            "Loss SS77:  0.09017515806956196\n",
            "Loss SS88:  0.0970529323637882\n",
            "Loss SS11:  0.08891879477883627\n",
            "Loss SS22:  0.07553556561470032\n",
            "Loss SS33:  0.08837794590086843\n",
            "Loss SS44:  0.07871889613100091\n",
            "Loss SS55:  0.0907700845159574\n",
            "Loss SS66:  0.09121120822920975\n",
            "Loss SS77:  0.09015562252794067\n",
            "Loss SS88:  0.09701287694711747\n",
            "Loss SS11:  0.08889811739104063\n",
            "Loss SS22:  0.07552581414928355\n",
            "Loss SS33:  0.0883426492412885\n",
            "Loss SS44:  0.07869432068522822\n",
            "Loss SS55:  0.09079198069443369\n",
            "Loss SS66:  0.09114971136844335\n",
            "Loss SS77:  0.09011287802154092\n",
            "Loss SS88:  0.09698099616100328\n",
            "Loss SS11:  0.08888208353407913\n",
            "Loss SS22:  0.0755397452200277\n",
            "Loss SS33:  0.08841155074428372\n",
            "Loss SS44:  0.07868428197602224\n",
            "Loss SS55:  0.0908260456830449\n",
            "Loss SS66:  0.09122266561536432\n",
            "Loss SS77:  0.09016376081660483\n",
            "Loss SS88:  0.09694117945841593\n",
            "Loss SS11:  0.08883139176973258\n",
            "Loss SS22:  0.0754705525464293\n",
            "Loss SS33:  0.08838407312901103\n",
            "Loss SS44:  0.078598029296959\n",
            "Loss SS55:  0.09074461349341145\n",
            "Loss SS66:  0.09119300896305658\n",
            "Loss SS77:  0.09014559213959758\n",
            "Loss SS88:  0.09694762751004847\n",
            "Validation: \n",
            " Loss SS11:  0.0825003832578659\n",
            " Loss SS22:  0.10586384683847427\n",
            " Loss SS33:  0.11292264610528946\n",
            " Loss SS55:  0.10091809928417206\n",
            " Loss SS66:  0.12334218621253967\n",
            " Loss SS77:  0.11854076385498047\n",
            " Loss SS88:  0.12220313400030136\n",
            " Loss SS99:  0.15342512726783752\n",
            " Loss SS11:  0.09894647768565587\n",
            " Loss SS22:  0.11470620937290646\n",
            " Loss SS33:  0.12388201448179427\n",
            " Loss SS55:  0.11720355635597593\n",
            " Loss SS66:  0.1417412090869177\n",
            " Loss SS77:  0.13201739568085896\n",
            " Loss SS88:  0.14247928524301165\n",
            " Loss SS99:  0.156329611937205\n",
            " Loss SS11:  0.09564363520319868\n",
            " Loss SS22:  0.11242892520456779\n",
            " Loss SS33:  0.1222228854894638\n",
            " Loss SS55:  0.11654490614082755\n",
            " Loss SS66:  0.1399134784573462\n",
            " Loss SS77:  0.1334116715120106\n",
            " Loss SS88:  0.14356415191801583\n",
            " Loss SS99:  0.15592505146817462\n",
            " Loss SS11:  0.09490280671686423\n",
            " Loss SS22:  0.11084261219032475\n",
            " Loss SS33:  0.12044775253925168\n",
            " Loss SS55:  0.11499375764463769\n",
            " Loss SS66:  0.13795895410365747\n",
            " Loss SS77:  0.13174466092566975\n",
            " Loss SS88:  0.1426738245321102\n",
            " Loss SS99:  0.15302709526703007\n",
            " Loss SS11:  0.0946569730654175\n",
            " Loss SS22:  0.11039548220089924\n",
            " Loss SS33:  0.12050266490306383\n",
            " Loss SS55:  0.1149139028088546\n",
            " Loss SS66:  0.13650187758015997\n",
            " Loss SS77:  0.131049247527564\n",
            " Loss SS88:  0.14114232922409786\n",
            " Loss SS99:  0.15255027733467244\n",
            "\n",
            "Epoch: 53\n",
            "Loss SS11:  0.08349054306745529\n",
            "Loss SS22:  0.07389430701732635\n",
            "Loss SS33:  0.0795726478099823\n",
            "Loss SS44:  0.07902738451957703\n",
            "Loss SS55:  0.08962999284267426\n",
            "Loss SS66:  0.08237762004137039\n",
            "Loss SS77:  0.08453897386789322\n",
            "Loss SS88:  0.09530714154243469\n",
            "Loss SS11:  0.09119057181206616\n",
            "Loss SS22:  0.07697183706543663\n",
            "Loss SS33:  0.08633770048618317\n",
            "Loss SS44:  0.07905027541247281\n",
            "Loss SS55:  0.09206243265758861\n",
            "Loss SS66:  0.09426597234877673\n",
            "Loss SS77:  0.09451486712152307\n",
            "Loss SS88:  0.09680744734677402\n",
            "Loss SS11:  0.09001583038341432\n",
            "Loss SS22:  0.07700416924698013\n",
            "Loss SS33:  0.08910250486362548\n",
            "Loss SS44:  0.07881676583063035\n",
            "Loss SS55:  0.09177649198543458\n",
            "Loss SS66:  0.09564089597690673\n",
            "Loss SS77:  0.09219132079964593\n",
            "Loss SS88:  0.09645566947403408\n",
            "Loss SS11:  0.0896157213757115\n",
            "Loss SS22:  0.07530416728508088\n",
            "Loss SS33:  0.08800319341882583\n",
            "Loss SS44:  0.07802395522594452\n",
            "Loss SS55:  0.08927642169498629\n",
            "Loss SS66:  0.0921945869922638\n",
            "Loss SS77:  0.08961740592795034\n",
            "Loss SS88:  0.09485161208337353\n",
            "Loss SS11:  0.08973725303644087\n",
            "Loss SS22:  0.07539562944595407\n",
            "Loss SS33:  0.0878571958803549\n",
            "Loss SS44:  0.07812076189169069\n",
            "Loss SS55:  0.08983247072958364\n",
            "Loss SS66:  0.0914381742477417\n",
            "Loss SS77:  0.08986120289418756\n",
            "Loss SS88:  0.09520264805817022\n",
            "Loss SS11:  0.08932860429380454\n",
            "Loss SS22:  0.07615856848218862\n",
            "Loss SS33:  0.08878178631558138\n",
            "Loss SS44:  0.07874471606577144\n",
            "Loss SS55:  0.0902933396545111\n",
            "Loss SS66:  0.09143310872947469\n",
            "Loss SS77:  0.09018727976317499\n",
            "Loss SS88:  0.09546353142051135\n",
            "Loss SS11:  0.08938965042595004\n",
            "Loss SS22:  0.07618387367148868\n",
            "Loss SS33:  0.08863804440517895\n",
            "Loss SS44:  0.07881427556276321\n",
            "Loss SS55:  0.08986761848457524\n",
            "Loss SS66:  0.09110927325291712\n",
            "Loss SS77:  0.09018624256380268\n",
            "Loss SS88:  0.09618834967984528\n",
            "Loss SS11:  0.0890636790386388\n",
            "Loss SS22:  0.07601686855646926\n",
            "Loss SS33:  0.08793149059507209\n",
            "Loss SS44:  0.07850059331722663\n",
            "Loss SS55:  0.0898464098572731\n",
            "Loss SS66:  0.09080554387519058\n",
            "Loss SS77:  0.09008062379041189\n",
            "Loss SS88:  0.0961103233653055\n",
            "Loss SS11:  0.08906414920901075\n",
            "Loss SS22:  0.07590240814987524\n",
            "Loss SS33:  0.08793901412943263\n",
            "Loss SS44:  0.07848216547274295\n",
            "Loss SS55:  0.08962061053808824\n",
            "Loss SS66:  0.09093769207412813\n",
            "Loss SS77:  0.09037371973196666\n",
            "Loss SS88:  0.09612218253774407\n",
            "Loss SS11:  0.08914654832947386\n",
            "Loss SS22:  0.07600868526068362\n",
            "Loss SS33:  0.08813639263530354\n",
            "Loss SS44:  0.07849153000247347\n",
            "Loss SS55:  0.08984481322241354\n",
            "Loss SS66:  0.09089204980121864\n",
            "Loss SS77:  0.09051399399618526\n",
            "Loss SS88:  0.09618743713740464\n",
            "Loss SS11:  0.08865928273684907\n",
            "Loss SS22:  0.07560334701349239\n",
            "Loss SS33:  0.08767315996165323\n",
            "Loss SS44:  0.07809936432260098\n",
            "Loss SS55:  0.08931742475764586\n",
            "Loss SS66:  0.09060154906889\n",
            "Loss SS77:  0.08991538172606195\n",
            "Loss SS88:  0.09580285707027605\n",
            "Loss SS11:  0.08848216633001964\n",
            "Loss SS22:  0.07512251660227776\n",
            "Loss SS33:  0.08730579899238036\n",
            "Loss SS44:  0.07790129799563605\n",
            "Loss SS55:  0.08916847152752919\n",
            "Loss SS66:  0.0903621835483087\n",
            "Loss SS77:  0.08959657416955845\n",
            "Loss SS88:  0.09578878452648988\n",
            "Loss SS11:  0.08835413984277031\n",
            "Loss SS22:  0.07516480030969155\n",
            "Loss SS33:  0.08731501198504582\n",
            "Loss SS44:  0.0778576631006623\n",
            "Loss SS55:  0.08918839317461676\n",
            "Loss SS66:  0.0904368357348048\n",
            "Loss SS77:  0.08959080063360782\n",
            "Loss SS88:  0.09549103104624866\n",
            "Loss SS11:  0.08824083794846789\n",
            "Loss SS22:  0.07503664769402897\n",
            "Loss SS33:  0.08742050985798581\n",
            "Loss SS44:  0.07794926500388684\n",
            "Loss SS55:  0.08923655327494817\n",
            "Loss SS66:  0.09030367217901099\n",
            "Loss SS77:  0.08962390321584149\n",
            "Loss SS88:  0.09555831989486709\n",
            "Loss SS11:  0.08816381528022442\n",
            "Loss SS22:  0.07518909573026583\n",
            "Loss SS33:  0.08740583680411602\n",
            "Loss SS44:  0.07802703789045624\n",
            "Loss SS55:  0.08925521701362961\n",
            "Loss SS66:  0.09038432803771175\n",
            "Loss SS77:  0.08971744157532428\n",
            "Loss SS88:  0.09532336811435983\n",
            "Loss SS11:  0.08846077288420784\n",
            "Loss SS22:  0.07540660344113577\n",
            "Loss SS33:  0.08760845128274122\n",
            "Loss SS44:  0.07829365442624155\n",
            "Loss SS55:  0.08959961342101065\n",
            "Loss SS66:  0.09060366752723985\n",
            "Loss SS77:  0.08988477630133661\n",
            "Loss SS88:  0.09567303506546462\n",
            "Loss SS11:  0.08834112107013323\n",
            "Loss SS22:  0.07537224192334258\n",
            "Loss SS33:  0.08734802633338834\n",
            "Loss SS44:  0.07798945843618109\n",
            "Loss SS55:  0.08948000886055253\n",
            "Loss SS66:  0.09049598178508118\n",
            "Loss SS77:  0.08970911444529243\n",
            "Loss SS88:  0.09560762521643076\n",
            "Loss SS11:  0.08823645650817637\n",
            "Loss SS22:  0.07532557969292004\n",
            "Loss SS33:  0.08735775873500701\n",
            "Loss SS44:  0.0780295096991355\n",
            "Loss SS55:  0.08952285074873974\n",
            "Loss SS66:  0.09037708752510841\n",
            "Loss SS77:  0.08977251961740138\n",
            "Loss SS88:  0.09562416126330693\n",
            "Loss SS11:  0.08822553135578146\n",
            "Loss SS22:  0.07540962875959623\n",
            "Loss SS33:  0.08756732121521597\n",
            "Loss SS44:  0.07824845537954932\n",
            "Loss SS55:  0.08966296318486251\n",
            "Loss SS66:  0.09045438563132155\n",
            "Loss SS77:  0.08974636798899477\n",
            "Loss SS88:  0.09559420183383299\n",
            "Loss SS11:  0.08806744409013169\n",
            "Loss SS22:  0.07533797093164858\n",
            "Loss SS33:  0.0875924719565826\n",
            "Loss SS44:  0.07825946885877878\n",
            "Loss SS55:  0.08964560383745514\n",
            "Loss SS66:  0.09052356106292515\n",
            "Loss SS77:  0.08964785488800228\n",
            "Loss SS88:  0.09566357174000815\n",
            "Loss SS11:  0.0879429437183029\n",
            "Loss SS22:  0.07527754706010889\n",
            "Loss SS33:  0.08765841416310315\n",
            "Loss SS44:  0.07832719055722602\n",
            "Loss SS55:  0.08973840646334548\n",
            "Loss SS66:  0.09052382991533374\n",
            "Loss SS77:  0.08983209206541973\n",
            "Loss SS88:  0.09566366498298313\n",
            "Loss SS11:  0.08815044012821116\n",
            "Loss SS22:  0.07524505492423383\n",
            "Loss SS33:  0.08767737419966838\n",
            "Loss SS44:  0.0782357637563023\n",
            "Loss SS55:  0.08977252742831741\n",
            "Loss SS66:  0.09057693744863944\n",
            "Loss SS77:  0.08988190866053387\n",
            "Loss SS88:  0.09556933040443755\n",
            "Loss SS11:  0.0882768494443656\n",
            "Loss SS22:  0.07532972085597288\n",
            "Loss SS33:  0.08775288476798329\n",
            "Loss SS44:  0.07830513983425512\n",
            "Loss SS55:  0.08979796072072034\n",
            "Loss SS66:  0.09067457988521092\n",
            "Loss SS77:  0.09005840271037088\n",
            "Loss SS88:  0.09583841045113171\n",
            "Loss SS11:  0.0883249623370377\n",
            "Loss SS22:  0.07530573692608189\n",
            "Loss SS33:  0.08787113931271937\n",
            "Loss SS44:  0.07830108750563164\n",
            "Loss SS55:  0.08985915483334364\n",
            "Loss SS66:  0.09074496739096456\n",
            "Loss SS77:  0.09022159461593215\n",
            "Loss SS88:  0.09601411013763188\n",
            "Loss SS11:  0.08829851989553182\n",
            "Loss SS22:  0.0754230440868629\n",
            "Loss SS33:  0.0878007776756999\n",
            "Loss SS44:  0.07833423301639399\n",
            "Loss SS55:  0.08994846927056174\n",
            "Loss SS66:  0.0908623379481284\n",
            "Loss SS77:  0.09028983548963714\n",
            "Loss SS88:  0.09615824739468048\n",
            "Loss SS11:  0.0881485656141285\n",
            "Loss SS22:  0.07546596568953469\n",
            "Loss SS33:  0.08779852433627346\n",
            "Loss SS44:  0.07840479245461315\n",
            "Loss SS55:  0.09008642443385258\n",
            "Loss SS66:  0.09107618615684282\n",
            "Loss SS77:  0.09046703048673758\n",
            "Loss SS88:  0.0962821164454122\n",
            "Loss SS11:  0.08827276325202993\n",
            "Loss SS22:  0.07547624653269505\n",
            "Loss SS33:  0.08781452599727331\n",
            "Loss SS44:  0.07848785272389079\n",
            "Loss SS55:  0.09014755236234702\n",
            "Loss SS66:  0.09102315281542782\n",
            "Loss SS77:  0.09051208337947327\n",
            "Loss SS88:  0.09622184932231903\n",
            "Loss SS11:  0.08823849293798539\n",
            "Loss SS22:  0.07560029368833862\n",
            "Loss SS33:  0.08790084950598404\n",
            "Loss SS44:  0.07849638738645398\n",
            "Loss SS55:  0.09029803526797418\n",
            "Loss SS66:  0.09106431478063999\n",
            "Loss SS77:  0.09059763246136837\n",
            "Loss SS88:  0.0962576202470878\n",
            "Loss SS11:  0.0882493956445375\n",
            "Loss SS22:  0.07558967800167955\n",
            "Loss SS33:  0.08797395703949538\n",
            "Loss SS44:  0.07857190268743929\n",
            "Loss SS55:  0.09030447701436345\n",
            "Loss SS66:  0.09116253681030137\n",
            "Loss SS77:  0.0905471020162742\n",
            "Loss SS88:  0.09619359422833046\n",
            "Loss SS11:  0.08816304636472688\n",
            "Loss SS22:  0.07553934058833778\n",
            "Loss SS33:  0.0880454610713159\n",
            "Loss SS44:  0.0785625108431295\n",
            "Loss SS55:  0.09034422797845401\n",
            "Loss SS66:  0.09120302322189423\n",
            "Loss SS77:  0.09053327662940697\n",
            "Loss SS88:  0.09620859320323492\n",
            "Loss SS11:  0.08815359938085278\n",
            "Loss SS22:  0.07552716976334882\n",
            "Loss SS33:  0.0880307774013063\n",
            "Loss SS44:  0.07851315795929725\n",
            "Loss SS55:  0.09036775752654504\n",
            "Loss SS66:  0.0911093823537478\n",
            "Loss SS77:  0.0904647439866763\n",
            "Loss SS88:  0.0961156254046383\n",
            "Loss SS11:  0.08813511707200115\n",
            "Loss SS22:  0.07551618132416842\n",
            "Loss SS33:  0.08798726710380082\n",
            "Loss SS44:  0.0784937945760523\n",
            "Loss SS55:  0.0904003746352395\n",
            "Loss SS66:  0.0910947657283096\n",
            "Loss SS77:  0.09030013830907092\n",
            "Loss SS88:  0.09612588665876358\n",
            "Loss SS11:  0.08827385424732048\n",
            "Loss SS22:  0.07557465777822372\n",
            "Loss SS33:  0.08803408049812941\n",
            "Loss SS44:  0.07850562749761288\n",
            "Loss SS55:  0.09046188844048716\n",
            "Loss SS66:  0.09114908057952596\n",
            "Loss SS77:  0.09030733547663763\n",
            "Loss SS88:  0.0962229785843059\n",
            "Loss SS11:  0.08825820673178329\n",
            "Loss SS22:  0.07552074590343363\n",
            "Loss SS33:  0.08796637993744853\n",
            "Loss SS44:  0.07839835926980411\n",
            "Loss SS55:  0.09037978328156687\n",
            "Loss SS66:  0.0910354095238213\n",
            "Loss SS77:  0.09021306479085248\n",
            "Loss SS88:  0.09617790154369936\n",
            "Loss SS11:  0.08833115161164415\n",
            "Loss SS22:  0.07552221966934694\n",
            "Loss SS33:  0.08800110721256027\n",
            "Loss SS44:  0.07834490228040128\n",
            "Loss SS55:  0.09038692756482233\n",
            "Loss SS66:  0.0910188710628367\n",
            "Loss SS77:  0.09019439975537163\n",
            "Loss SS88:  0.09614179075813013\n",
            "Loss SS11:  0.08822585805783584\n",
            "Loss SS22:  0.07545638427223235\n",
            "Loss SS33:  0.08789595570998994\n",
            "Loss SS44:  0.07823276200313174\n",
            "Loss SS55:  0.09033387455760244\n",
            "Loss SS66:  0.0909967608006931\n",
            "Loss SS77:  0.09020756441898156\n",
            "Loss SS88:  0.09604938612704264\n",
            "Loss SS11:  0.08823593293118015\n",
            "Loss SS22:  0.07538617244428875\n",
            "Loss SS33:  0.08778137255490982\n",
            "Loss SS44:  0.07820948224690152\n",
            "Loss SS55:  0.09028655523821258\n",
            "Loss SS66:  0.0909145935915844\n",
            "Loss SS77:  0.09011261251824715\n",
            "Loss SS88:  0.09606738204853686\n",
            "Loss SS11:  0.08813793903010875\n",
            "Loss SS22:  0.07532262031962929\n",
            "Loss SS33:  0.08767092828400373\n",
            "Loss SS44:  0.07816085435269335\n",
            "Loss SS55:  0.09027698115920121\n",
            "Loss SS66:  0.09087632696904904\n",
            "Loss SS77:  0.09002233467333401\n",
            "Loss SS88:  0.09601117971772453\n",
            "Loss SS11:  0.08814724088888469\n",
            "Loss SS22:  0.07524389078415285\n",
            "Loss SS33:  0.08758746333948271\n",
            "Loss SS44:  0.0781104332838315\n",
            "Loss SS55:  0.09022562798317962\n",
            "Loss SS66:  0.09073904096063354\n",
            "Loss SS77:  0.08996919560150837\n",
            "Loss SS88:  0.09593524259766882\n",
            "Loss SS11:  0.08817844298642005\n",
            "Loss SS22:  0.0751608814139043\n",
            "Loss SS33:  0.08750069364333701\n",
            "Loss SS44:  0.07807782021782282\n",
            "Loss SS55:  0.0900916524631593\n",
            "Loss SS66:  0.09070835203465903\n",
            "Loss SS77:  0.08982096871604091\n",
            "Loss SS88:  0.09583310660956156\n",
            "Loss SS11:  0.08828747660813487\n",
            "Loss SS22:  0.07522934302687645\n",
            "Loss SS33:  0.08749493228229799\n",
            "Loss SS44:  0.0781010839149839\n",
            "Loss SS55:  0.09018054916674359\n",
            "Loss SS66:  0.09082324295641478\n",
            "Loss SS77:  0.08980605940792032\n",
            "Loss SS88:  0.09589965602480563\n",
            "Loss SS11:  0.08830152292466222\n",
            "Loss SS22:  0.07521999164183064\n",
            "Loss SS33:  0.08744211235693192\n",
            "Loss SS44:  0.07807347602652807\n",
            "Loss SS55:  0.09008083407316185\n",
            "Loss SS66:  0.09074224446449257\n",
            "Loss SS77:  0.08977354064112453\n",
            "Loss SS88:  0.09589128754579818\n",
            "Loss SS11:  0.08839675858439855\n",
            "Loss SS22:  0.07522136656978917\n",
            "Loss SS33:  0.08751578400721176\n",
            "Loss SS44:  0.0780650604861366\n",
            "Loss SS55:  0.09007286768840214\n",
            "Loss SS66:  0.09086318239746638\n",
            "Loss SS77:  0.0898202335961924\n",
            "Loss SS88:  0.09590680175184355\n",
            "Loss SS11:  0.08843882300668415\n",
            "Loss SS22:  0.07519217631552723\n",
            "Loss SS33:  0.08744381157262143\n",
            "Loss SS44:  0.07798901978197341\n",
            "Loss SS55:  0.08997782630945303\n",
            "Loss SS66:  0.09085054294554097\n",
            "Loss SS77:  0.08976682527919932\n",
            "Loss SS88:  0.0958889256255256\n",
            "Loss SS11:  0.08840516833018283\n",
            "Loss SS22:  0.07517856568131857\n",
            "Loss SS33:  0.08746892828789968\n",
            "Loss SS44:  0.07796737277149614\n",
            "Loss SS55:  0.0900111890771762\n",
            "Loss SS66:  0.09086159554198216\n",
            "Loss SS77:  0.08977559915070091\n",
            "Loss SS88:  0.09590213703595592\n",
            "Loss SS11:  0.08840392044272496\n",
            "Loss SS22:  0.07515807007904063\n",
            "Loss SS33:  0.08745604370227146\n",
            "Loss SS44:  0.07799052386782122\n",
            "Loss SS55:  0.09006750694333582\n",
            "Loss SS66:  0.09089222755241817\n",
            "Loss SS77:  0.08979180954115883\n",
            "Loss SS88:  0.09594472958613394\n",
            "Loss SS11:  0.08823984083992281\n",
            "Loss SS22:  0.07507121557301141\n",
            "Loss SS33:  0.08747060280172013\n",
            "Loss SS44:  0.07790767666510026\n",
            "Loss SS55:  0.09011607308449818\n",
            "Loss SS66:  0.09088727085166277\n",
            "Loss SS77:  0.08977789606357603\n",
            "Loss SS88:  0.09590937517211649\n",
            "Loss SS11:  0.08823693051712782\n",
            "Loss SS22:  0.07504873078181992\n",
            "Loss SS33:  0.08752398120116782\n",
            "Loss SS44:  0.07795442390846852\n",
            "Loss SS55:  0.09018856287002563\n",
            "Loss SS66:  0.09087064363964044\n",
            "Loss SS77:  0.08983640681216672\n",
            "Loss SS88:  0.09591105319322295\n",
            "Loss SS11:  0.0882605872196368\n",
            "Loss SS22:  0.07504549198115938\n",
            "Loss SS33:  0.08755108745083244\n",
            "Loss SS44:  0.0779710197337204\n",
            "Loss SS55:  0.09024343692277424\n",
            "Loss SS66:  0.09081849271505142\n",
            "Loss SS77:  0.08985291065148653\n",
            "Loss SS88:  0.09597833013150399\n",
            "Loss SS11:  0.08826338715259509\n",
            "Loss SS22:  0.07504129767903485\n",
            "Loss SS33:  0.08755680709288222\n",
            "Loss SS44:  0.07795787941049898\n",
            "Loss SS55:  0.0901806737522001\n",
            "Loss SS66:  0.09072526219602516\n",
            "Loss SS77:  0.08985277844968492\n",
            "Loss SS88:  0.09601975107933498\n",
            "Validation: \n",
            " Loss SS11:  0.07761403173208237\n",
            " Loss SS22:  0.10354255139827728\n",
            " Loss SS33:  0.11082012206315994\n",
            " Loss SS55:  0.10653474926948547\n",
            " Loss SS66:  0.12991617619991302\n",
            " Loss SS77:  0.1200924664735794\n",
            " Loss SS88:  0.12119413167238235\n",
            " Loss SS99:  0.15139514207839966\n",
            " Loss SS11:  0.09677024930715561\n",
            " Loss SS22:  0.11418561850275312\n",
            " Loss SS33:  0.12003762345938455\n",
            " Loss SS55:  0.11976570955344609\n",
            " Loss SS66:  0.1423745368208204\n",
            " Loss SS77:  0.13656404543490636\n",
            " Loss SS88:  0.1407148614525795\n",
            " Loss SS99:  0.15394646709873563\n",
            " Loss SS11:  0.09351776140492137\n",
            " Loss SS22:  0.11186411249928357\n",
            " Loss SS33:  0.1191909200534588\n",
            " Loss SS55:  0.11859546947043116\n",
            " Loss SS66:  0.14060332698792946\n",
            " Loss SS77:  0.13771343612816275\n",
            " Loss SS88:  0.1406008530317283\n",
            " Loss SS99:  0.15309689866333473\n",
            " Loss SS11:  0.0922776035598067\n",
            " Loss SS22:  0.11046573495278593\n",
            " Loss SS33:  0.11755363936307\n",
            " Loss SS55:  0.1171568474564396\n",
            " Loss SS66:  0.1386052462898317\n",
            " Loss SS77:  0.13577448442334034\n",
            " Loss SS88:  0.13973435930541303\n",
            " Loss SS99:  0.15023750017900936\n",
            " Loss SS11:  0.09161833590931362\n",
            " Loss SS22:  0.10976582186089621\n",
            " Loss SS33:  0.11757853737583866\n",
            " Loss SS55:  0.1171552468965083\n",
            " Loss SS66:  0.13714653998613358\n",
            " Loss SS77:  0.135470215001224\n",
            " Loss SS88:  0.13834248326810789\n",
            " Loss SS99:  0.15009353116706567\n",
            "\n",
            "Epoch: 54\n",
            "Loss SS11:  0.08309003710746765\n",
            "Loss SS22:  0.07626856863498688\n",
            "Loss SS33:  0.07706929743289948\n",
            "Loss SS44:  0.07584081590175629\n",
            "Loss SS55:  0.0810919925570488\n",
            "Loss SS66:  0.087793730199337\n",
            "Loss SS77:  0.08640587329864502\n",
            "Loss SS88:  0.0958804115653038\n",
            "Loss SS11:  0.08699999004602432\n",
            "Loss SS22:  0.07790002023631876\n",
            "Loss SS33:  0.08566924997351387\n",
            "Loss SS44:  0.07978616180745038\n",
            "Loss SS55:  0.09023807942867279\n",
            "Loss SS66:  0.09497896920550954\n",
            "Loss SS77:  0.09315838800235228\n",
            "Loss SS88:  0.09915961460633711\n",
            "Loss SS11:  0.08740393959340595\n",
            "Loss SS22:  0.0761122519061679\n",
            "Loss SS33:  0.08657315870126088\n",
            "Loss SS44:  0.07880318697009768\n",
            "Loss SS55:  0.09115146632705416\n",
            "Loss SS66:  0.09424042098578952\n",
            "Loss SS77:  0.08999645035891306\n",
            "Loss SS88:  0.0976898397008578\n",
            "Loss SS11:  0.0870987510969562\n",
            "Loss SS22:  0.07537890682297368\n",
            "Loss SS33:  0.08568509932487242\n",
            "Loss SS44:  0.07763827688271\n",
            "Loss SS55:  0.08960305899381638\n",
            "Loss SS66:  0.09257549672357497\n",
            "Loss SS77:  0.0886203569750632\n",
            "Loss SS88:  0.09568447787915507\n",
            "Loss SS11:  0.08783402602847029\n",
            "Loss SS22:  0.07589897022741597\n",
            "Loss SS33:  0.08646914544628888\n",
            "Loss SS44:  0.07792429916742372\n",
            "Loss SS55:  0.08988292188179202\n",
            "Loss SS66:  0.09230340126811004\n",
            "Loss SS77:  0.08885208699034482\n",
            "Loss SS88:  0.09549971360985826\n",
            "Loss SS11:  0.08796756831454296\n",
            "Loss SS22:  0.07594283363398384\n",
            "Loss SS33:  0.08781532402716431\n",
            "Loss SS44:  0.0784715852316688\n",
            "Loss SS55:  0.09044801502251158\n",
            "Loss SS66:  0.09202474706313189\n",
            "Loss SS77:  0.08925972426054525\n",
            "Loss SS88:  0.09555861821361616\n",
            "Loss SS11:  0.08819722005578338\n",
            "Loss SS22:  0.07606326935232663\n",
            "Loss SS33:  0.08749581971129433\n",
            "Loss SS44:  0.07862487679622213\n",
            "Loss SS55:  0.09032565917147965\n",
            "Loss SS66:  0.09201085274336768\n",
            "Loss SS77:  0.08959814576340504\n",
            "Loss SS88:  0.09649253282390657\n",
            "Loss SS11:  0.08766905291819237\n",
            "Loss SS22:  0.07559479552675301\n",
            "Loss SS33:  0.0869964332857602\n",
            "Loss SS44:  0.07849214939584195\n",
            "Loss SS55:  0.09002717028201467\n",
            "Loss SS66:  0.09158820270652503\n",
            "Loss SS77:  0.0890472853477572\n",
            "Loss SS88:  0.09606119486647592\n",
            "Loss SS11:  0.08776755741348972\n",
            "Loss SS22:  0.0753360421937189\n",
            "Loss SS33:  0.08690962866868501\n",
            "Loss SS44:  0.07825418451318035\n",
            "Loss SS55:  0.08967237349277661\n",
            "Loss SS66:  0.09139114176785504\n",
            "Loss SS77:  0.08907731870810191\n",
            "Loss SS88:  0.09575564451055762\n",
            "Loss SS11:  0.08803157598435224\n",
            "Loss SS22:  0.07516316987655974\n",
            "Loss SS33:  0.08696159163674155\n",
            "Loss SS44:  0.0781099952675484\n",
            "Loss SS55:  0.08977579964058739\n",
            "Loss SS66:  0.09125272430233904\n",
            "Loss SS77:  0.08921108059175722\n",
            "Loss SS88:  0.09557186460101998\n",
            "Loss SS11:  0.08781267717333123\n",
            "Loss SS22:  0.0748123955903667\n",
            "Loss SS33:  0.08667922440436807\n",
            "Loss SS44:  0.07793329899558926\n",
            "Loss SS55:  0.08956092750967139\n",
            "Loss SS66:  0.09083218606981901\n",
            "Loss SS77:  0.0890093012759001\n",
            "Loss SS88:  0.09539967251591164\n",
            "Loss SS11:  0.08809721107418472\n",
            "Loss SS22:  0.07473609140058896\n",
            "Loss SS33:  0.08634735408935461\n",
            "Loss SS44:  0.0776312620387421\n",
            "Loss SS55:  0.08924587299157907\n",
            "Loss SS66:  0.09042379669509493\n",
            "Loss SS77:  0.08850484958908579\n",
            "Loss SS88:  0.09509900541187406\n",
            "Loss SS11:  0.08829179085975836\n",
            "Loss SS22:  0.07477421473619367\n",
            "Loss SS33:  0.08633560619570992\n",
            "Loss SS44:  0.07761072626044928\n",
            "Loss SS55:  0.0891866887773364\n",
            "Loss SS66:  0.090335437766284\n",
            "Loss SS77:  0.08838187404407942\n",
            "Loss SS88:  0.09505326891979896\n",
            "Loss SS11:  0.08825530595224322\n",
            "Loss SS22:  0.07494080174742764\n",
            "Loss SS33:  0.08655104029724617\n",
            "Loss SS44:  0.07781343014185665\n",
            "Loss SS55:  0.08938053307187466\n",
            "Loss SS66:  0.09046308645082794\n",
            "Loss SS77:  0.0885775962289963\n",
            "Loss SS88:  0.0953639728649882\n",
            "Loss SS11:  0.08829814195632935\n",
            "Loss SS22:  0.0748638502249481\n",
            "Loss SS33:  0.08641830773641032\n",
            "Loss SS44:  0.07770293264101583\n",
            "Loss SS55:  0.08918621496739962\n",
            "Loss SS66:  0.09028394382896153\n",
            "Loss SS77:  0.08853828146102581\n",
            "Loss SS88:  0.09512463153888148\n",
            "Loss SS11:  0.08851068392889389\n",
            "Loss SS22:  0.07503669689230572\n",
            "Loss SS33:  0.08675447121163868\n",
            "Loss SS44:  0.07774323920737829\n",
            "Loss SS55:  0.0893347825830346\n",
            "Loss SS66:  0.09025366234266205\n",
            "Loss SS77:  0.08862269378655793\n",
            "Loss SS88:  0.09520921920309004\n",
            "Loss SS11:  0.08804979781556574\n",
            "Loss SS22:  0.0747795799365325\n",
            "Loss SS33:  0.08656184629809043\n",
            "Loss SS44:  0.07738939094247285\n",
            "Loss SS55:  0.08908301610383929\n",
            "Loss SS66:  0.08995555406031401\n",
            "Loss SS77:  0.08854884649655834\n",
            "Loss SS88:  0.09500271484533453\n",
            "Loss SS11:  0.08785029082444676\n",
            "Loss SS22:  0.0746279171074343\n",
            "Loss SS33:  0.08646431077293486\n",
            "Loss SS44:  0.07731163009391194\n",
            "Loss SS55:  0.08900721819951521\n",
            "Loss SS66:  0.08980115093508659\n",
            "Loss SS77:  0.0885987945815973\n",
            "Loss SS88:  0.09506334177060434\n",
            "Loss SS11:  0.08797255961111237\n",
            "Loss SS22:  0.07471984097970784\n",
            "Loss SS33:  0.08642097443177556\n",
            "Loss SS44:  0.07732544007880912\n",
            "Loss SS55:  0.08903986606644003\n",
            "Loss SS66:  0.08980127300511408\n",
            "Loss SS77:  0.08851626910557404\n",
            "Loss SS88:  0.09502332711088064\n",
            "Loss SS11:  0.08788927987764018\n",
            "Loss SS22:  0.07467493301754846\n",
            "Loss SS33:  0.08653952286661608\n",
            "Loss SS44:  0.07740113572144383\n",
            "Loss SS55:  0.08885881148707804\n",
            "Loss SS66:  0.08984599508228103\n",
            "Loss SS77:  0.08846895833602127\n",
            "Loss SS88:  0.09514513518173658\n",
            "Loss SS11:  0.08770069357619356\n",
            "Loss SS22:  0.07468659986755741\n",
            "Loss SS33:  0.08649122158982861\n",
            "Loss SS44:  0.07734052251227459\n",
            "Loss SS55:  0.08885399300364119\n",
            "Loss SS66:  0.089796013833575\n",
            "Loss SS77:  0.08869286381931447\n",
            "Loss SS88:  0.09499001480750184\n",
            "Loss SS11:  0.08789408980246403\n",
            "Loss SS22:  0.07457763219684786\n",
            "Loss SS33:  0.08662326284353201\n",
            "Loss SS44:  0.07740326824275803\n",
            "Loss SS55:  0.0888136074088196\n",
            "Loss SS66:  0.08979435088510197\n",
            "Loss SS77:  0.08876149790688148\n",
            "Loss SS88:  0.0950449724191738\n",
            "Loss SS11:  0.08809402794050415\n",
            "Loss SS22:  0.07463536966103235\n",
            "Loss SS33:  0.08675075807857298\n",
            "Loss SS44:  0.07750354978630986\n",
            "Loss SS55:  0.08896968557554133\n",
            "Loss SS66:  0.09001237103437407\n",
            "Loss SS77:  0.08905767248227046\n",
            "Loss SS88:  0.09518094411397951\n",
            "Loss SS11:  0.0882110745037273\n",
            "Loss SS22:  0.07465505579010749\n",
            "Loss SS33:  0.08670119654177587\n",
            "Loss SS44:  0.07750320994170197\n",
            "Loss SS55:  0.0890904532108472\n",
            "Loss SS66:  0.09002557471072004\n",
            "Loss SS77:  0.08911459754297743\n",
            "Loss SS88:  0.09535294587348962\n",
            "Loss SS11:  0.0883442860606795\n",
            "Loss SS22:  0.07477690797314604\n",
            "Loss SS33:  0.08675912919133531\n",
            "Loss SS44:  0.0775711856844267\n",
            "Loss SS55:  0.08918940040951448\n",
            "Loss SS66:  0.090162576543345\n",
            "Loss SS77:  0.08917279835187548\n",
            "Loss SS88:  0.09555962391166767\n",
            "Loss SS11:  0.0882701173840291\n",
            "Loss SS22:  0.07487857990352756\n",
            "Loss SS33:  0.08681332204208905\n",
            "Loss SS44:  0.07759181732141164\n",
            "Loss SS55:  0.0894199973974095\n",
            "Loss SS66:  0.09027067616403815\n",
            "Loss SS77:  0.08933664086567929\n",
            "Loss SS88:  0.09574820778046945\n",
            "Loss SS11:  0.0882371881855402\n",
            "Loss SS22:  0.07491727759954574\n",
            "Loss SS33:  0.086793745043634\n",
            "Loss SS44:  0.07762552739274456\n",
            "Loss SS55:  0.08951203305259975\n",
            "Loss SS66:  0.09021119599255566\n",
            "Loss SS77:  0.08926490485896553\n",
            "Loss SS88:  0.09573008830862484\n",
            "Loss SS11:  0.0881244935837619\n",
            "Loss SS22:  0.0749427331577148\n",
            "Loss SS33:  0.0868459576500298\n",
            "Loss SS44:  0.07765352834579689\n",
            "Loss SS55:  0.08957147980418152\n",
            "Loss SS66:  0.09022886770677742\n",
            "Loss SS77:  0.08927889786853122\n",
            "Loss SS88:  0.0957902547563134\n",
            "Loss SS11:  0.08818492235025902\n",
            "Loss SS22:  0.07487775052622545\n",
            "Loss SS33:  0.08694166069671352\n",
            "Loss SS44:  0.07775602862238884\n",
            "Loss SS55:  0.08955845292458754\n",
            "Loss SS66:  0.09032837293538334\n",
            "Loss SS77:  0.08924657411316536\n",
            "Loss SS88:  0.09577122526873048\n",
            "Loss SS11:  0.0880153479752262\n",
            "Loss SS22:  0.0748944397874919\n",
            "Loss SS33:  0.08706088551028897\n",
            "Loss SS44:  0.07772934834930495\n",
            "Loss SS55:  0.08978554801023293\n",
            "Loss SS66:  0.09042974325901863\n",
            "Loss SS77:  0.08937528557896204\n",
            "Loss SS88:  0.0958292581832286\n",
            "Loss SS11:  0.08809084687815157\n",
            "Loss SS22:  0.07488761822448617\n",
            "Loss SS33:  0.08703228618417468\n",
            "Loss SS44:  0.07771159397852777\n",
            "Loss SS55:  0.0898218939817229\n",
            "Loss SS66:  0.09040487763097516\n",
            "Loss SS77:  0.0893604038809225\n",
            "Loss SS88:  0.0958209149910762\n",
            "Loss SS11:  0.08811014660685008\n",
            "Loss SS22:  0.07484025999280801\n",
            "Loss SS33:  0.08692537732062999\n",
            "Loss SS44:  0.07771209432141574\n",
            "Loss SS55:  0.08971465084330445\n",
            "Loss SS66:  0.09033519372200276\n",
            "Loss SS77:  0.08924449985050312\n",
            "Loss SS88:  0.0958076084206342\n",
            "Loss SS11:  0.08813197838833027\n",
            "Loss SS22:  0.07484439637430731\n",
            "Loss SS33:  0.08701978238572212\n",
            "Loss SS44:  0.0776981467328896\n",
            "Loss SS55:  0.08977250084048863\n",
            "Loss SS66:  0.0904275677935728\n",
            "Loss SS77:  0.08923948189458372\n",
            "Loss SS88:  0.09583188433234936\n",
            "Loss SS11:  0.08800738464669519\n",
            "Loss SS22:  0.07484448476050196\n",
            "Loss SS33:  0.08692561772024524\n",
            "Loss SS44:  0.0776094230302691\n",
            "Loss SS55:  0.08962579150304334\n",
            "Loss SS66:  0.09027221744874453\n",
            "Loss SS77:  0.08914781247020848\n",
            "Loss SS88:  0.0957420240311104\n",
            "Loss SS11:  0.08800892704480555\n",
            "Loss SS22:  0.07484756506258441\n",
            "Loss SS33:  0.08701608262278816\n",
            "Loss SS44:  0.07752863267474859\n",
            "Loss SS55:  0.08969489093924547\n",
            "Loss SS66:  0.09030731889771576\n",
            "Loss SS77:  0.08917885933541132\n",
            "Loss SS88:  0.09571007438156962\n",
            "Loss SS11:  0.08810706665882698\n",
            "Loss SS22:  0.07485522397732802\n",
            "Loss SS33:  0.08708038584374295\n",
            "Loss SS44:  0.07757550919497455\n",
            "Loss SS55:  0.08975526064294695\n",
            "Loss SS66:  0.09034299595743163\n",
            "Loss SS77:  0.08931993954202049\n",
            "Loss SS88:  0.09576234028295234\n",
            "Loss SS11:  0.0880904793120157\n",
            "Loss SS22:  0.07486106954783284\n",
            "Loss SS33:  0.08708043481091714\n",
            "Loss SS44:  0.07752904714887492\n",
            "Loss SS55:  0.08968376374475844\n",
            "Loss SS66:  0.09023418318779515\n",
            "Loss SS77:  0.08930591314288057\n",
            "Loss SS88:  0.09578074847149387\n",
            "Loss SS11:  0.08802521528018453\n",
            "Loss SS22:  0.07482498866126865\n",
            "Loss SS33:  0.08699336854836369\n",
            "Loss SS44:  0.07742343228460964\n",
            "Loss SS55:  0.08955866743049853\n",
            "Loss SS66:  0.09008174855513071\n",
            "Loss SS77:  0.08922027645888675\n",
            "Loss SS88:  0.09576454979550807\n",
            "Loss SS11:  0.08806631913569969\n",
            "Loss SS22:  0.07474191547457001\n",
            "Loss SS33:  0.08692747591126936\n",
            "Loss SS44:  0.07735735983673356\n",
            "Loss SS55:  0.08939692962748486\n",
            "Loss SS66:  0.09000213289745837\n",
            "Loss SS77:  0.08910623838113049\n",
            "Loss SS88:  0.09565130632928037\n",
            "Loss SS11:  0.08808158341880955\n",
            "Loss SS22:  0.07471848368797156\n",
            "Loss SS33:  0.08690317522000779\n",
            "Loss SS44:  0.0773111015482022\n",
            "Loss SS55:  0.08926648938137552\n",
            "Loss SS66:  0.08989772170096103\n",
            "Loss SS77:  0.08897424780804178\n",
            "Loss SS88:  0.09556828736496703\n",
            "Loss SS11:  0.08813924453873884\n",
            "Loss SS22:  0.07474945526467891\n",
            "Loss SS33:  0.08690845007611034\n",
            "Loss SS44:  0.07737406722253695\n",
            "Loss SS55:  0.08923191356094104\n",
            "Loss SS66:  0.08997586330496463\n",
            "Loss SS77:  0.08902505554834804\n",
            "Loss SS88:  0.09560500980909924\n",
            "Loss SS11:  0.08816385278229005\n",
            "Loss SS22:  0.074766122946774\n",
            "Loss SS33:  0.08685259153010491\n",
            "Loss SS44:  0.07736963365417328\n",
            "Loss SS55:  0.08931843298340075\n",
            "Loss SS66:  0.08996083830322373\n",
            "Loss SS77:  0.08899895022022754\n",
            "Loss SS88:  0.09561678749076351\n",
            "Loss SS11:  0.08820759305053538\n",
            "Loss SS22:  0.07479572154563849\n",
            "Loss SS33:  0.0868841776903055\n",
            "Loss SS44:  0.07740082312154091\n",
            "Loss SS55:  0.0894041822033087\n",
            "Loss SS66:  0.09001721197522451\n",
            "Loss SS77:  0.08906589414827047\n",
            "Loss SS88:  0.0956764097486992\n",
            "Loss SS11:  0.08822451466197481\n",
            "Loss SS22:  0.07472757500728155\n",
            "Loss SS33:  0.08684585870971813\n",
            "Loss SS44:  0.0773069358100194\n",
            "Loss SS55:  0.0893422402355068\n",
            "Loss SS66:  0.08997626887880733\n",
            "Loss SS77:  0.08896043511181033\n",
            "Loss SS88:  0.09561300111757354\n",
            "Loss SS11:  0.08827010254943722\n",
            "Loss SS22:  0.07469804446443139\n",
            "Loss SS33:  0.08686026643598431\n",
            "Loss SS44:  0.07723140952234366\n",
            "Loss SS55:  0.08933171643421493\n",
            "Loss SS66:  0.08995717221579584\n",
            "Loss SS77:  0.08899183331433337\n",
            "Loss SS88:  0.09562272597062074\n",
            "Loss SS11:  0.08826196955273262\n",
            "Loss SS22:  0.07468214735157475\n",
            "Loss SS33:  0.08691661388607089\n",
            "Loss SS44:  0.07731511477719653\n",
            "Loss SS55:  0.08939435788969242\n",
            "Loss SS66:  0.09000325743150288\n",
            "Loss SS77:  0.08901473876510121\n",
            "Loss SS88:  0.09563679958460336\n",
            "Loss SS11:  0.0881627727938319\n",
            "Loss SS22:  0.07464791641221129\n",
            "Loss SS33:  0.08700065508711105\n",
            "Loss SS44:  0.07725482260295212\n",
            "Loss SS55:  0.08940242925848722\n",
            "Loss SS66:  0.08999878160025189\n",
            "Loss SS77:  0.0889864090725036\n",
            "Loss SS88:  0.09567967818447928\n",
            "Loss SS11:  0.08814771217145738\n",
            "Loss SS22:  0.07465654928751306\n",
            "Loss SS33:  0.08705301345533627\n",
            "Loss SS44:  0.07726989926141538\n",
            "Loss SS55:  0.0894306436687265\n",
            "Loss SS66:  0.08995882833016652\n",
            "Loss SS77:  0.08899017875804516\n",
            "Loss SS88:  0.09564824873727851\n",
            "Loss SS11:  0.08811923835170987\n",
            "Loss SS22:  0.07466608337794668\n",
            "Loss SS33:  0.08701468446522394\n",
            "Loss SS44:  0.07723637963809739\n",
            "Loss SS55:  0.08936566941579513\n",
            "Loss SS66:  0.08995588143935075\n",
            "Loss SS77:  0.08902052061907219\n",
            "Loss SS88:  0.09565058150744983\n",
            "Loss SS11:  0.0880426491654332\n",
            "Loss SS22:  0.07464809803758765\n",
            "Loss SS33:  0.08701795580855698\n",
            "Loss SS44:  0.07722309545107384\n",
            "Loss SS55:  0.08935135334365965\n",
            "Loss SS66:  0.08992697774999739\n",
            "Loss SS77:  0.08900585159082762\n",
            "Loss SS88:  0.09566120402754688\n",
            "Validation: \n",
            " Loss SS11:  0.08055076748132706\n",
            " Loss SS22:  0.11114653199911118\n",
            " Loss SS33:  0.11262165755033493\n",
            " Loss SS55:  0.10560611635446548\n",
            " Loss SS66:  0.12490200996398926\n",
            " Loss SS77:  0.11557475477457047\n",
            " Loss SS88:  0.12264514714479446\n",
            " Loss SS99:  0.1529131978750229\n",
            " Loss SS11:  0.09685791851509185\n",
            " Loss SS22:  0.11934175484237217\n",
            " Loss SS33:  0.12177936910163789\n",
            " Loss SS55:  0.11866593893085207\n",
            " Loss SS66:  0.1396258356315749\n",
            " Loss SS77:  0.13409966407787233\n",
            " Loss SS88:  0.14179186345565886\n",
            " Loss SS99:  0.15578497449556986\n",
            " Loss SS11:  0.09375160187482834\n",
            " Loss SS22:  0.11627985682429337\n",
            " Loss SS33:  0.12092011639984643\n",
            " Loss SS55:  0.11781669607976587\n",
            " Loss SS66:  0.1378806224319993\n",
            " Loss SS77:  0.1354445557041866\n",
            " Loss SS88:  0.14209733012972808\n",
            " Loss SS99:  0.15535341203212738\n",
            " Loss SS11:  0.09241017980165168\n",
            " Loss SS22:  0.11485729415397175\n",
            " Loss SS33:  0.11915436228279208\n",
            " Loss SS55:  0.11652847318375698\n",
            " Loss SS66:  0.13590592133705734\n",
            " Loss SS77:  0.13369240098800816\n",
            " Loss SS88:  0.14079493406366128\n",
            " Loss SS99:  0.15234075486660004\n",
            " Loss SS11:  0.09171017231764617\n",
            " Loss SS22:  0.11431734604232105\n",
            " Loss SS33:  0.11945769062012802\n",
            " Loss SS55:  0.11641333353372267\n",
            " Loss SS66:  0.13470617323010056\n",
            " Loss SS77:  0.13319589869107729\n",
            " Loss SS88:  0.13963790394273806\n",
            " Loss SS99:  0.1520378780953678\n",
            "\n",
            "Epoch: 55\n",
            "Loss SS11:  0.08295329660177231\n",
            "Loss SS22:  0.07780623435974121\n",
            "Loss SS33:  0.08356660604476929\n",
            "Loss SS44:  0.07231973856687546\n",
            "Loss SS55:  0.07993465662002563\n",
            "Loss SS66:  0.08382204174995422\n",
            "Loss SS77:  0.09011609852313995\n",
            "Loss SS88:  0.08977457880973816\n",
            "Loss SS11:  0.08845849199728532\n",
            "Loss SS22:  0.07813876325433905\n",
            "Loss SS33:  0.08757843280380423\n",
            "Loss SS44:  0.07927746393463829\n",
            "Loss SS55:  0.08832651241259142\n",
            "Loss SS66:  0.09195109389045021\n",
            "Loss SS77:  0.09297787939960306\n",
            "Loss SS88:  0.09866878932172601\n",
            "Loss SS11:  0.08768286520526522\n",
            "Loss SS22:  0.07784201914355868\n",
            "Loss SS33:  0.08828914236454737\n",
            "Loss SS44:  0.0784375366000902\n",
            "Loss SS55:  0.08957268475067048\n",
            "Loss SS66:  0.09206311865931466\n",
            "Loss SS77:  0.09152635044994809\n",
            "Loss SS88:  0.09662478133326485\n",
            "Loss SS11:  0.08820475349503179\n",
            "Loss SS22:  0.07604367297983938\n",
            "Loss SS33:  0.08728219136115044\n",
            "Loss SS44:  0.07681408248120739\n",
            "Loss SS55:  0.08762409538030624\n",
            "Loss SS66:  0.09026253055180272\n",
            "Loss SS77:  0.08970684918665117\n",
            "Loss SS88:  0.09438074332091116\n",
            "Loss SS11:  0.08885057870207763\n",
            "Loss SS22:  0.07574892325735674\n",
            "Loss SS33:  0.08718021995410687\n",
            "Loss SS44:  0.07709836859892054\n",
            "Loss SS55:  0.08836003247557617\n",
            "Loss SS66:  0.09016902108744877\n",
            "Loss SS77:  0.0887795470473243\n",
            "Loss SS88:  0.0946674314213962\n",
            "Loss SS11:  0.08870254530041825\n",
            "Loss SS22:  0.07568377117608108\n",
            "Loss SS33:  0.08766858265096066\n",
            "Loss SS44:  0.07704173737004691\n",
            "Loss SS55:  0.08868105694943783\n",
            "Loss SS66:  0.09022931608499266\n",
            "Loss SS77:  0.08946353723021115\n",
            "Loss SS88:  0.09501777545494192\n",
            "Loss SS11:  0.08850314761282968\n",
            "Loss SS22:  0.075166718087724\n",
            "Loss SS33:  0.08727253094071248\n",
            "Loss SS44:  0.0766437728507597\n",
            "Loss SS55:  0.08862812023182384\n",
            "Loss SS66:  0.09014926081309553\n",
            "Loss SS77:  0.0897222199156636\n",
            "Loss SS88:  0.09549126419864717\n",
            "Loss SS11:  0.0880293394898025\n",
            "Loss SS22:  0.07480958056911616\n",
            "Loss SS33:  0.08690045438182185\n",
            "Loss SS44:  0.0768113431691284\n",
            "Loss SS55:  0.08829972431273528\n",
            "Loss SS66:  0.09003020235350434\n",
            "Loss SS77:  0.08916195114733468\n",
            "Loss SS88:  0.09515199915204249\n",
            "Loss SS11:  0.08770346816307233\n",
            "Loss SS22:  0.0749896559174414\n",
            "Loss SS33:  0.08655463517815978\n",
            "Loss SS44:  0.07708047831683983\n",
            "Loss SS55:  0.08835861399585818\n",
            "Loss SS66:  0.09001901046729383\n",
            "Loss SS77:  0.08903961380322774\n",
            "Loss SS88:  0.09481463719297338\n",
            "Loss SS11:  0.08775777214176052\n",
            "Loss SS22:  0.0749189474589222\n",
            "Loss SS33:  0.08668890660935706\n",
            "Loss SS44:  0.07716681468454036\n",
            "Loss SS55:  0.08847631555009675\n",
            "Loss SS66:  0.0902551402757456\n",
            "Loss SS77:  0.08908122401316088\n",
            "Loss SS88:  0.09462547670681398\n",
            "Loss SS11:  0.08765807489652445\n",
            "Loss SS22:  0.07462167171853604\n",
            "Loss SS33:  0.08635125368243397\n",
            "Loss SS44:  0.07682637689579831\n",
            "Loss SS55:  0.08780319597756508\n",
            "Loss SS66:  0.08990374893540203\n",
            "Loss SS77:  0.08864666669085475\n",
            "Loss SS88:  0.09430360683415195\n",
            "Loss SS11:  0.08731328977926357\n",
            "Loss SS22:  0.07424174309582324\n",
            "Loss SS33:  0.08586388141722293\n",
            "Loss SS44:  0.07669367698264552\n",
            "Loss SS55:  0.08768779737455351\n",
            "Loss SS66:  0.089552001775922\n",
            "Loss SS77:  0.08868429581592749\n",
            "Loss SS88:  0.09399741129563735\n",
            "Loss SS11:  0.087344377001455\n",
            "Loss SS22:  0.07428905885081646\n",
            "Loss SS33:  0.08592189534390268\n",
            "Loss SS44:  0.07669677299039424\n",
            "Loss SS55:  0.08774671992979759\n",
            "Loss SS66:  0.08969578017626913\n",
            "Loss SS77:  0.08862957850960661\n",
            "Loss SS88:  0.0940289856358008\n",
            "Loss SS11:  0.08747728254276378\n",
            "Loss SS22:  0.07438576716275616\n",
            "Loss SS33:  0.08616755811767723\n",
            "Loss SS44:  0.07692393225454192\n",
            "Loss SS55:  0.08767623761455522\n",
            "Loss SS66:  0.0900875831605824\n",
            "Loss SS77:  0.08877926843084452\n",
            "Loss SS88:  0.09438944454411514\n",
            "Loss SS11:  0.0872608630898151\n",
            "Loss SS22:  0.07437573621670406\n",
            "Loss SS33:  0.08623247049379011\n",
            "Loss SS44:  0.07710681097410249\n",
            "Loss SS55:  0.08765232251256916\n",
            "Loss SS66:  0.0899315854230671\n",
            "Loss SS77:  0.08870819882086828\n",
            "Loss SS88:  0.0942475085562848\n",
            "Loss SS11:  0.08762128049174682\n",
            "Loss SS22:  0.07455434211042543\n",
            "Loss SS33:  0.0862573998554653\n",
            "Loss SS44:  0.07732703021129235\n",
            "Loss SS55:  0.0878146734360038\n",
            "Loss SS66:  0.0900515660840944\n",
            "Loss SS77:  0.08874260777274505\n",
            "Loss SS88:  0.09439699228433583\n",
            "Loss SS11:  0.08733763309739391\n",
            "Loss SS22:  0.0743505908234149\n",
            "Loss SS33:  0.0859825855456524\n",
            "Loss SS44:  0.0770370499549075\n",
            "Loss SS55:  0.08756136935875282\n",
            "Loss SS66:  0.08970305749348231\n",
            "Loss SS77:  0.08851894676130011\n",
            "Loss SS88:  0.09438132874158599\n",
            "Loss SS11:  0.0871723962631839\n",
            "Loss SS22:  0.07425759438621371\n",
            "Loss SS33:  0.0858781204411858\n",
            "Loss SS44:  0.0769477328666818\n",
            "Loss SS55:  0.08750192821025848\n",
            "Loss SS66:  0.08945936317506589\n",
            "Loss SS77:  0.08843048339524465\n",
            "Loss SS88:  0.09447648807575829\n",
            "Loss SS11:  0.08709191876715718\n",
            "Loss SS22:  0.07426960986539803\n",
            "Loss SS33:  0.08604145741594431\n",
            "Loss SS44:  0.0770142227205453\n",
            "Loss SS55:  0.08766266114968621\n",
            "Loss SS66:  0.0895553224241536\n",
            "Loss SS77:  0.08846903374181926\n",
            "Loss SS88:  0.09451727145119926\n",
            "Loss SS11:  0.08702831473494076\n",
            "Loss SS22:  0.0743696731539609\n",
            "Loss SS33:  0.08614525113118257\n",
            "Loss SS44:  0.07713459909976465\n",
            "Loss SS55:  0.08771432039001226\n",
            "Loss SS66:  0.08983163387363494\n",
            "Loss SS77:  0.08862117369761642\n",
            "Loss SS88:  0.09475637110272003\n",
            "Loss SS11:  0.08667882539294845\n",
            "Loss SS22:  0.07427072478689957\n",
            "Loss SS33:  0.08618241971108451\n",
            "Loss SS44:  0.07691399009889038\n",
            "Loss SS55:  0.08771494957641583\n",
            "Loss SS66:  0.08974950842151594\n",
            "Loss SS77:  0.08868791263050108\n",
            "Loss SS88:  0.09474675913355243\n",
            "Loss SS11:  0.0866495494452698\n",
            "Loss SS22:  0.07428875224779567\n",
            "Loss SS33:  0.08616635521158787\n",
            "Loss SS44:  0.07691003422813393\n",
            "Loss SS55:  0.08773529402452622\n",
            "Loss SS66:  0.08980545528692092\n",
            "Loss SS77:  0.08873062771502264\n",
            "Loss SS88:  0.09454178799526386\n",
            "Loss SS11:  0.08674559539934089\n",
            "Loss SS22:  0.07435457325831257\n",
            "Loss SS33:  0.08623803477751184\n",
            "Loss SS44:  0.07702098849200016\n",
            "Loss SS55:  0.08804145897001163\n",
            "Loss SS66:  0.08989990792527996\n",
            "Loss SS77:  0.08908099222641724\n",
            "Loss SS88:  0.09470652721451418\n",
            "Loss SS11:  0.08685795601570245\n",
            "Loss SS22:  0.07433812831451883\n",
            "Loss SS33:  0.08630848692093061\n",
            "Loss SS44:  0.07715283777742159\n",
            "Loss SS55:  0.08817335321402653\n",
            "Loss SS66:  0.09000221723730946\n",
            "Loss SS77:  0.08929104686944515\n",
            "Loss SS88:  0.09475163009130594\n",
            "Loss SS11:  0.08698223565252984\n",
            "Loss SS22:  0.07443240858398038\n",
            "Loss SS33:  0.08640416424056802\n",
            "Loss SS44:  0.07716491705587296\n",
            "Loss SS55:  0.08838325822378096\n",
            "Loss SS66:  0.0901586815591175\n",
            "Loss SS77:  0.08934755215259013\n",
            "Loss SS88:  0.09497314682268998\n",
            "Loss SS11:  0.08696792449846685\n",
            "Loss SS22:  0.07438370619577715\n",
            "Loss SS33:  0.08645654508317134\n",
            "Loss SS44:  0.07717878524347606\n",
            "Loss SS55:  0.08846859319276544\n",
            "Loss SS66:  0.09018310622508782\n",
            "Loss SS77:  0.08938636384399763\n",
            "Loss SS88:  0.09508927000233852\n",
            "Loss SS11:  0.08690144090245967\n",
            "Loss SS22:  0.07438787790836046\n",
            "Loss SS33:  0.08647042661334363\n",
            "Loss SS44:  0.07724442479996389\n",
            "Loss SS55:  0.0885831051117159\n",
            "Loss SS66:  0.09017284731184386\n",
            "Loss SS77:  0.08930794674888881\n",
            "Loss SS88:  0.09504753060039432\n",
            "Loss SS11:  0.0867905735804586\n",
            "Loss SS22:  0.07457030565921231\n",
            "Loss SS33:  0.08650025618142307\n",
            "Loss SS44:  0.07729135523807959\n",
            "Loss SS55:  0.08866845244521145\n",
            "Loss SS66:  0.09016601059489585\n",
            "Loss SS77:  0.08929042727085057\n",
            "Loss SS88:  0.09498447108312727\n",
            "Loss SS11:  0.08673353861765505\n",
            "Loss SS22:  0.07462543642870896\n",
            "Loss SS33:  0.0864860835713848\n",
            "Loss SS44:  0.07731288283055787\n",
            "Loss SS55:  0.0887202222523316\n",
            "Loss SS66:  0.09021618479723607\n",
            "Loss SS77:  0.08923831070339128\n",
            "Loss SS88:  0.09496531659385912\n",
            "Loss SS11:  0.0867625860516558\n",
            "Loss SS22:  0.0746642244256444\n",
            "Loss SS33:  0.08659370225626987\n",
            "Loss SS44:  0.0773013370220399\n",
            "Loss SS55:  0.0889225336643019\n",
            "Loss SS66:  0.09029398026437693\n",
            "Loss SS77:  0.08938776724731799\n",
            "Loss SS88:  0.09504789882099506\n",
            "Loss SS11:  0.08679210486206106\n",
            "Loss SS22:  0.07465601000823452\n",
            "Loss SS33:  0.08651223860408777\n",
            "Loss SS44:  0.07725128079718134\n",
            "Loss SS55:  0.08898197302588592\n",
            "Loss SS66:  0.09027975938743928\n",
            "Loss SS77:  0.08937504768767626\n",
            "Loss SS88:  0.09496902268491314\n",
            "Loss SS11:  0.0867947908028529\n",
            "Loss SS22:  0.0745639535224131\n",
            "Loss SS33:  0.08636764829852574\n",
            "Loss SS44:  0.07723213860411736\n",
            "Loss SS55:  0.08888416655864746\n",
            "Loss SS66:  0.09012684487165745\n",
            "Loss SS77:  0.08925611377241527\n",
            "Loss SS88:  0.09481820828278348\n",
            "Loss SS11:  0.08689962137153959\n",
            "Loss SS22:  0.0745928920573349\n",
            "Loss SS33:  0.08643796218332843\n",
            "Loss SS44:  0.07721249669836688\n",
            "Loss SS55:  0.08890659236740843\n",
            "Loss SS66:  0.09016209831490324\n",
            "Loss SS77:  0.08926183651448039\n",
            "Loss SS88:  0.09492439282274692\n",
            "Loss SS11:  0.08689165241408564\n",
            "Loss SS22:  0.07461571768826589\n",
            "Loss SS33:  0.08642494698755691\n",
            "Loss SS44:  0.07712972224929182\n",
            "Loss SS55:  0.088929425418557\n",
            "Loss SS66:  0.09008209105704847\n",
            "Loss SS77:  0.0892883016758815\n",
            "Loss SS88:  0.09489911691119664\n",
            "Loss SS11:  0.08703791747781887\n",
            "Loss SS22:  0.07466072880845964\n",
            "Loss SS33:  0.08651620516679154\n",
            "Loss SS44:  0.07713840482346823\n",
            "Loss SS55:  0.08903734193694207\n",
            "Loss SS66:  0.09006758444994425\n",
            "Loss SS77:  0.089317002953672\n",
            "Loss SS88:  0.09487591432598678\n",
            "Loss SS11:  0.08695829763711348\n",
            "Loss SS22:  0.07464114246502561\n",
            "Loss SS33:  0.08643565348155818\n",
            "Loss SS44:  0.07711827287986407\n",
            "Loss SS55:  0.0889976377628128\n",
            "Loss SS66:  0.09001252592204304\n",
            "Loss SS77:  0.0892777358449762\n",
            "Loss SS88:  0.09479157692431724\n",
            "Loss SS11:  0.08689656915592024\n",
            "Loss SS22:  0.07451863480613172\n",
            "Loss SS33:  0.08639058148761866\n",
            "Loss SS44:  0.07705215783660761\n",
            "Loss SS55:  0.08887474756904586\n",
            "Loss SS66:  0.08994284261874545\n",
            "Loss SS77:  0.08924616697835129\n",
            "Loss SS88:  0.0947557564752584\n",
            "Loss SS11:  0.08692127594729318\n",
            "Loss SS22:  0.07448711934075201\n",
            "Loss SS33:  0.08637697864173237\n",
            "Loss SS44:  0.07700825366128808\n",
            "Loss SS55:  0.08873929005025853\n",
            "Loss SS66:  0.08989257666701576\n",
            "Loss SS77:  0.08917557761434595\n",
            "Loss SS88:  0.09476018086517596\n",
            "Loss SS11:  0.08702950844458082\n",
            "Loss SS22:  0.07443343549771259\n",
            "Loss SS33:  0.08634497753278476\n",
            "Loss SS44:  0.07694719651630857\n",
            "Loss SS55:  0.08868077139216145\n",
            "Loss SS66:  0.08977405157849544\n",
            "Loss SS77:  0.08908500745305865\n",
            "Loss SS88:  0.09470822405940278\n",
            "Loss SS11:  0.08696407119712561\n",
            "Loss SS22:  0.07438142506210395\n",
            "Loss SS33:  0.08630899395174382\n",
            "Loss SS44:  0.07696303433698157\n",
            "Loss SS55:  0.08849945778736983\n",
            "Loss SS66:  0.08967311240141959\n",
            "Loss SS77:  0.08897527892266394\n",
            "Loss SS88:  0.0945562085189173\n",
            "Loss SS11:  0.08707219572212928\n",
            "Loss SS22:  0.07442128483642664\n",
            "Loss SS33:  0.0862840923696682\n",
            "Loss SS44:  0.07696578244764608\n",
            "Loss SS55:  0.08852723837270404\n",
            "Loss SS66:  0.08970623472682258\n",
            "Loss SS77:  0.08896413835652749\n",
            "Loss SS88:  0.09454014378964455\n",
            "Loss SS11:  0.08708717556185386\n",
            "Loss SS22:  0.07440709491280743\n",
            "Loss SS33:  0.08631166393376904\n",
            "Loss SS44:  0.07699387932055768\n",
            "Loss SS55:  0.08846070635565297\n",
            "Loss SS66:  0.08971822878160036\n",
            "Loss SS77:  0.08886369292390897\n",
            "Loss SS88:  0.09451281901113598\n",
            "Loss SS11:  0.08708677991269037\n",
            "Loss SS22:  0.07438350585676429\n",
            "Loss SS33:  0.08630385961286245\n",
            "Loss SS44:  0.0769754303110467\n",
            "Loss SS55:  0.08846476160857003\n",
            "Loss SS66:  0.08977123141076389\n",
            "Loss SS77:  0.0888329893704548\n",
            "Loss SS88:  0.09457890616813917\n",
            "Loss SS11:  0.0870500977762614\n",
            "Loss SS22:  0.07435124268076812\n",
            "Loss SS33:  0.08633012179363078\n",
            "Loss SS44:  0.0769717668263099\n",
            "Loss SS55:  0.08846964388048566\n",
            "Loss SS66:  0.08973637128166699\n",
            "Loss SS77:  0.08876906772708118\n",
            "Loss SS88:  0.09458808595716538\n",
            "Loss SS11:  0.08700198974130915\n",
            "Loss SS22:  0.07430888650740355\n",
            "Loss SS33:  0.08631331061357274\n",
            "Loss SS44:  0.07690896980819248\n",
            "Loss SS55:  0.08848190354922461\n",
            "Loss SS66:  0.08976138788623875\n",
            "Loss SS77:  0.08879171664955395\n",
            "Loss SS88:  0.09465424405609399\n",
            "Loss SS11:  0.08697086452050114\n",
            "Loss SS22:  0.07435275197128234\n",
            "Loss SS33:  0.08641064713839153\n",
            "Loss SS44:  0.07692985173868234\n",
            "Loss SS55:  0.08857708193047877\n",
            "Loss SS66:  0.08982270779273992\n",
            "Loss SS77:  0.08880574606408566\n",
            "Loss SS88:  0.09474776857047282\n",
            "Loss SS11:  0.08690713499541397\n",
            "Loss SS22:  0.07435766365904353\n",
            "Loss SS33:  0.08644346036259368\n",
            "Loss SS44:  0.07690675420900746\n",
            "Loss SS55:  0.08862916449662661\n",
            "Loss SS66:  0.08985039506131812\n",
            "Loss SS77:  0.0887742886024804\n",
            "Loss SS88:  0.09472872834014272\n",
            "Loss SS11:  0.08690024363450437\n",
            "Loss SS22:  0.07430004133129069\n",
            "Loss SS33:  0.08645648254583849\n",
            "Loss SS44:  0.07690533465703835\n",
            "Loss SS55:  0.08863299819314556\n",
            "Loss SS66:  0.08979915617124797\n",
            "Loss SS77:  0.08871941009018325\n",
            "Loss SS88:  0.0946470016411915\n",
            "Loss SS11:  0.08695183285432645\n",
            "Loss SS22:  0.07430830958014714\n",
            "Loss SS33:  0.0864863987090434\n",
            "Loss SS44:  0.07686884823621187\n",
            "Loss SS55:  0.08865529710810298\n",
            "Loss SS66:  0.08986139283544556\n",
            "Loss SS77:  0.08868665362779911\n",
            "Loss SS88:  0.09466587550736763\n",
            "Loss SS11:  0.08689034469198063\n",
            "Loss SS22:  0.0742833506232488\n",
            "Loss SS33:  0.08645211764120037\n",
            "Loss SS44:  0.0768395844095836\n",
            "Loss SS55:  0.0885707437536867\n",
            "Loss SS66:  0.08989058594293362\n",
            "Loss SS77:  0.08870231272186135\n",
            "Loss SS88:  0.094647345817866\n",
            "Validation: \n",
            " Loss SS11:  0.08126775920391083\n",
            " Loss SS22:  0.10591308027505875\n",
            " Loss SS33:  0.10921867936849594\n",
            " Loss SS55:  0.10401585698127747\n",
            " Loss SS66:  0.12706582248210907\n",
            " Loss SS77:  0.1174110472202301\n",
            " Loss SS88:  0.12000732123851776\n",
            " Loss SS99:  0.16271331906318665\n",
            " Loss SS11:  0.09656481977019991\n",
            " Loss SS22:  0.1137857756444386\n",
            " Loss SS33:  0.12244591187863123\n",
            " Loss SS55:  0.11687314758698146\n",
            " Loss SS66:  0.13811388931104115\n",
            " Loss SS77:  0.13251796471221106\n",
            " Loss SS88:  0.13965880906298048\n",
            " Loss SS99:  0.15855368404161363\n",
            " Loss SS11:  0.09361721375366537\n",
            " Loss SS22:  0.11124409880579972\n",
            " Loss SS33:  0.12125520717080046\n",
            " Loss SS55:  0.11654667847040223\n",
            " Loss SS66:  0.13713025901375747\n",
            " Loss SS77:  0.1349049639411089\n",
            " Loss SS88:  0.14071423124249388\n",
            " Loss SS99:  0.15839401796096708\n",
            " Loss SS11:  0.09269753525980183\n",
            " Loss SS22:  0.10969997991303929\n",
            " Loss SS33:  0.11907971222869686\n",
            " Loss SS55:  0.11527878107106099\n",
            " Loss SS66:  0.13528914578625414\n",
            " Loss SS77:  0.13296397921980405\n",
            " Loss SS88:  0.13912972246037156\n",
            " Loss SS99:  0.15524785648115347\n",
            " Loss SS11:  0.0922030434012413\n",
            " Loss SS22:  0.10921079205510056\n",
            " Loss SS33:  0.1189233613786874\n",
            " Loss SS55:  0.11533324797580272\n",
            " Loss SS66:  0.13395277944243986\n",
            " Loss SS77:  0.1324615032400614\n",
            " Loss SS88:  0.1379477713762978\n",
            " Loss SS99:  0.15500408512206726\n",
            "\n",
            "Epoch: 56\n",
            "Loss SS11:  0.08918933570384979\n",
            "Loss SS22:  0.0720413476228714\n",
            "Loss SS33:  0.07760535180568695\n",
            "Loss SS44:  0.07359039783477783\n",
            "Loss SS55:  0.08491397649049759\n",
            "Loss SS66:  0.08077022433280945\n",
            "Loss SS77:  0.08619454503059387\n",
            "Loss SS88:  0.09164940565824509\n",
            "Loss SS11:  0.08869702301242134\n",
            "Loss SS22:  0.07655755565925078\n",
            "Loss SS33:  0.08697631887414238\n",
            "Loss SS44:  0.07737783613530072\n",
            "Loss SS55:  0.08899433098056099\n",
            "Loss SS66:  0.09420923753218217\n",
            "Loss SS77:  0.09188222614201633\n",
            "Loss SS88:  0.09546178917993199\n",
            "Loss SS11:  0.08736558577844075\n",
            "Loss SS22:  0.07545433441797893\n",
            "Loss SS33:  0.08884286738577343\n",
            "Loss SS44:  0.07797124378737949\n",
            "Loss SS55:  0.08897727479537328\n",
            "Loss SS66:  0.09451457751648766\n",
            "Loss SS77:  0.0905478788273675\n",
            "Loss SS88:  0.09511319122144155\n",
            "Loss SS11:  0.08747901358912068\n",
            "Loss SS22:  0.07415332585092514\n",
            "Loss SS33:  0.08786349383092695\n",
            "Loss SS44:  0.07655926938018491\n",
            "Loss SS55:  0.08817713996095042\n",
            "Loss SS66:  0.09170985582374758\n",
            "Loss SS77:  0.08766882097528826\n",
            "Loss SS88:  0.0933007225394249\n",
            "Loss SS11:  0.0880804848743648\n",
            "Loss SS22:  0.07450292804619162\n",
            "Loss SS33:  0.08750119241999417\n",
            "Loss SS44:  0.07675879394135825\n",
            "Loss SS55:  0.08801272202555727\n",
            "Loss SS66:  0.09101269594052942\n",
            "Loss SS77:  0.08706696331501007\n",
            "Loss SS88:  0.09288771123420901\n",
            "Loss SS11:  0.08747161574223462\n",
            "Loss SS22:  0.07447513718815411\n",
            "Loss SS33:  0.08794542854907465\n",
            "Loss SS44:  0.0772747439788837\n",
            "Loss SS55:  0.08905541750730253\n",
            "Loss SS66:  0.09077459193912207\n",
            "Loss SS77:  0.0877230943125837\n",
            "Loss SS88:  0.0937225751432718\n",
            "Loss SS11:  0.08729716402585389\n",
            "Loss SS22:  0.0746727896029832\n",
            "Loss SS33:  0.08760861722660847\n",
            "Loss SS44:  0.07749203751321698\n",
            "Loss SS55:  0.08880818647439362\n",
            "Loss SS66:  0.09068764074415457\n",
            "Loss SS77:  0.08813986695203625\n",
            "Loss SS88:  0.09455741697647532\n",
            "Loss SS11:  0.08689266695103175\n",
            "Loss SS22:  0.07449198944467894\n",
            "Loss SS33:  0.0871405121935925\n",
            "Loss SS44:  0.07734644171637549\n",
            "Loss SS55:  0.08867558459161033\n",
            "Loss SS66:  0.09034168909133321\n",
            "Loss SS77:  0.08780119064408289\n",
            "Loss SS88:  0.09458570738493556\n",
            "Loss SS11:  0.08660666543392488\n",
            "Loss SS22:  0.07452496923046348\n",
            "Loss SS33:  0.08697469504895033\n",
            "Loss SS44:  0.07722698722356632\n",
            "Loss SS55:  0.08861517013958943\n",
            "Loss SS66:  0.09003264778926048\n",
            "Loss SS77:  0.08789083766348568\n",
            "Loss SS88:  0.09477722506832194\n",
            "Loss SS11:  0.08644165958349521\n",
            "Loss SS22:  0.07421103922220376\n",
            "Loss SS33:  0.08691353793000127\n",
            "Loss SS44:  0.07728700781916524\n",
            "Loss SS55:  0.0887152000770464\n",
            "Loss SS66:  0.08976225645005048\n",
            "Loss SS77:  0.08777955886754361\n",
            "Loss SS88:  0.09473390803559796\n",
            "Loss SS11:  0.0865070884770686\n",
            "Loss SS22:  0.07380967266341247\n",
            "Loss SS33:  0.08653284963404778\n",
            "Loss SS44:  0.07694704793762452\n",
            "Loss SS55:  0.08858912836502095\n",
            "Loss SS66:  0.08944000296368457\n",
            "Loss SS77:  0.08749495918797974\n",
            "Loss SS88:  0.09434140414589702\n",
            "Loss SS11:  0.08639651684610693\n",
            "Loss SS22:  0.07370951533451811\n",
            "Loss SS33:  0.0865404368923591\n",
            "Loss SS44:  0.07676126270949303\n",
            "Loss SS55:  0.08849312747652466\n",
            "Loss SS66:  0.08911783013257894\n",
            "Loss SS77:  0.08715889159892057\n",
            "Loss SS88:  0.09406310017849948\n",
            "Loss SS11:  0.08639125199603641\n",
            "Loss SS22:  0.07360741702362525\n",
            "Loss SS33:  0.0864406561186491\n",
            "Loss SS44:  0.07661364834047546\n",
            "Loss SS55:  0.08835132253810393\n",
            "Loss SS66:  0.08894186663972445\n",
            "Loss SS77:  0.08717511224845224\n",
            "Loss SS88:  0.09414675480816975\n",
            "Loss SS11:  0.08635239968545565\n",
            "Loss SS22:  0.07366765978682133\n",
            "Loss SS33:  0.08667112181659874\n",
            "Loss SS44:  0.07653617446545426\n",
            "Loss SS55:  0.08840937770277489\n",
            "Loss SS66:  0.08915601136120221\n",
            "Loss SS77:  0.08733699010073684\n",
            "Loss SS88:  0.09448318871605488\n",
            "Loss SS11:  0.08639545876083644\n",
            "Loss SS22:  0.0735375740109606\n",
            "Loss SS33:  0.08648303494597158\n",
            "Loss SS44:  0.07647675384126656\n",
            "Loss SS55:  0.08809200499920135\n",
            "Loss SS66:  0.0890639305960202\n",
            "Loss SS77:  0.08721617096704794\n",
            "Loss SS88:  0.09440160785795104\n",
            "Loss SS11:  0.08674341948419217\n",
            "Loss SS22:  0.07374074413681662\n",
            "Loss SS33:  0.08652092832208469\n",
            "Loss SS44:  0.07669692484057503\n",
            "Loss SS55:  0.08835608709529535\n",
            "Loss SS66:  0.0892600573549997\n",
            "Loss SS77:  0.08718766800023073\n",
            "Loss SS88:  0.09459561657234533\n",
            "Loss SS11:  0.08657290874430852\n",
            "Loss SS22:  0.07363153399425264\n",
            "Loss SS33:  0.08634187063630323\n",
            "Loss SS44:  0.07650827586373186\n",
            "Loss SS55:  0.08815568130208838\n",
            "Loss SS66:  0.08899196214187219\n",
            "Loss SS77:  0.08707957168728668\n",
            "Loss SS88:  0.0944941086891275\n",
            "Loss SS11:  0.08623533848433466\n",
            "Loss SS22:  0.07349692332988594\n",
            "Loss SS33:  0.08613128162789763\n",
            "Loss SS44:  0.07647598876852041\n",
            "Loss SS55:  0.08802767167663017\n",
            "Loss SS66:  0.08882113415421101\n",
            "Loss SS77:  0.0872090156925352\n",
            "Loss SS88:  0.0944810457832632\n",
            "Loss SS11:  0.08630506706665893\n",
            "Loss SS22:  0.07356481142959542\n",
            "Loss SS33:  0.08618304351581395\n",
            "Loss SS44:  0.07646578926984118\n",
            "Loss SS55:  0.08799887617319328\n",
            "Loss SS66:  0.08886706759093216\n",
            "Loss SS77:  0.08718625678377256\n",
            "Loss SS88:  0.09458027833568457\n",
            "Loss SS11:  0.08613642738127583\n",
            "Loss SS22:  0.07361055153826769\n",
            "Loss SS33:  0.08618481188076328\n",
            "Loss SS44:  0.07643659886811416\n",
            "Loss SS55:  0.08780546736030678\n",
            "Loss SS66:  0.0888078357068656\n",
            "Loss SS77:  0.08708746980934243\n",
            "Loss SS88:  0.094657390295523\n",
            "Loss SS11:  0.08602895087270594\n",
            "Loss SS22:  0.07367742736244676\n",
            "Loss SS33:  0.08602559981654533\n",
            "Loss SS44:  0.07637648304822434\n",
            "Loss SS55:  0.08775103748289508\n",
            "Loss SS66:  0.08866768589808573\n",
            "Loss SS77:  0.0872064279531365\n",
            "Loss SS88:  0.09455484478034783\n",
            "Loss SS11:  0.08617840061114297\n",
            "Loss SS22:  0.07368012053353526\n",
            "Loss SS33:  0.08603190655391928\n",
            "Loss SS44:  0.0763285988279711\n",
            "Loss SS55:  0.08779074008007186\n",
            "Loss SS66:  0.08867356721400084\n",
            "Loss SS77:  0.08723064776845453\n",
            "Loss SS88:  0.09453513339091252\n",
            "Loss SS11:  0.08643160952566975\n",
            "Loss SS22:  0.07372887690479939\n",
            "Loss SS33:  0.0861370723306863\n",
            "Loss SS44:  0.07634241290693909\n",
            "Loss SS55:  0.08802826888016446\n",
            "Loss SS66:  0.08890370212124483\n",
            "Loss SS77:  0.08752687309122732\n",
            "Loss SS88:  0.09468932753234967\n",
            "Loss SS11:  0.08650887286637253\n",
            "Loss SS22:  0.07369347918149713\n",
            "Loss SS33:  0.086122080619201\n",
            "Loss SS44:  0.0763440127464342\n",
            "Loss SS55:  0.08793509270979728\n",
            "Loss SS66:  0.08881898137269081\n",
            "Loss SS77:  0.08758128940801084\n",
            "Loss SS88:  0.09473439664035649\n",
            "Loss SS11:  0.08648859632213086\n",
            "Loss SS22:  0.07386195583895035\n",
            "Loss SS33:  0.08617634128361817\n",
            "Loss SS44:  0.07641165154788998\n",
            "Loss SS55:  0.0879814025648402\n",
            "Loss SS66:  0.08896153689172753\n",
            "Loss SS77:  0.08774601726858448\n",
            "Loss SS88:  0.09493033343701937\n",
            "Loss SS11:  0.08635687130499646\n",
            "Loss SS22:  0.07394058353813046\n",
            "Loss SS33:  0.08622447721867922\n",
            "Loss SS44:  0.07642029773488462\n",
            "Loss SS55:  0.08789862760628363\n",
            "Loss SS66:  0.08896529353947279\n",
            "Loss SS77:  0.08788113011069507\n",
            "Loss SS88:  0.09491247680795145\n",
            "Loss SS11:  0.08632619873088895\n",
            "Loss SS22:  0.07392281632142506\n",
            "Loss SS33:  0.08618288056147053\n",
            "Loss SS44:  0.07639229398977254\n",
            "Loss SS55:  0.08790035046265957\n",
            "Loss SS66:  0.08903397311424387\n",
            "Loss SS77:  0.08798425033745638\n",
            "Loss SS88:  0.09483272548037014\n",
            "Loss SS11:  0.08631185684146916\n",
            "Loss SS22:  0.07400452685147194\n",
            "Loss SS33:  0.08626783207553779\n",
            "Loss SS44:  0.07644871939867184\n",
            "Loss SS55:  0.08806401546150996\n",
            "Loss SS66:  0.08907126492349864\n",
            "Loss SS77:  0.08809188901828224\n",
            "Loss SS88:  0.09485227070281427\n",
            "Loss SS11:  0.08633607702853417\n",
            "Loss SS22:  0.07397443704386623\n",
            "Loss SS33:  0.08631869519520485\n",
            "Loss SS44:  0.07647186187609659\n",
            "Loss SS55:  0.08815952520353514\n",
            "Loss SS66:  0.08902175965788525\n",
            "Loss SS77:  0.08808052868605508\n",
            "Loss SS88:  0.09491285029467314\n",
            "Loss SS11:  0.08627999568825326\n",
            "Loss SS22:  0.07400585530498593\n",
            "Loss SS33:  0.08642003618667216\n",
            "Loss SS44:  0.07648345506734044\n",
            "Loss SS55:  0.08826829119236608\n",
            "Loss SS66:  0.0890623782457355\n",
            "Loss SS77:  0.08813160880632007\n",
            "Loss SS88:  0.09503304812404298\n",
            "Loss SS11:  0.08630552499298241\n",
            "Loss SS22:  0.07400354494238613\n",
            "Loss SS33:  0.08629675568437259\n",
            "Loss SS44:  0.07647463371141805\n",
            "Loss SS55:  0.08828557166348264\n",
            "Loss SS66:  0.08901946058503021\n",
            "Loss SS77:  0.08807989545290257\n",
            "Loss SS88:  0.09505031703417484\n",
            "Loss SS11:  0.08620688364724255\n",
            "Loss SS22:  0.07389587353184292\n",
            "Loss SS33:  0.0861985903125484\n",
            "Loss SS44:  0.07637838047848254\n",
            "Loss SS55:  0.08825646544480246\n",
            "Loss SS66:  0.08888413103063773\n",
            "Loss SS77:  0.08788419619106787\n",
            "Loss SS88:  0.09492086259785956\n",
            "Loss SS11:  0.08634561070493449\n",
            "Loss SS22:  0.07396417533590044\n",
            "Loss SS33:  0.08625922245307133\n",
            "Loss SS44:  0.07641582284993101\n",
            "Loss SS55:  0.08834247934019826\n",
            "Loss SS66:  0.08893333728254027\n",
            "Loss SS77:  0.08799017850901479\n",
            "Loss SS88:  0.09496025750384524\n",
            "Loss SS11:  0.08618584700671568\n",
            "Loss SS22:  0.07391148902408307\n",
            "Loss SS33:  0.08610442373057507\n",
            "Loss SS44:  0.07642711895965378\n",
            "Loss SS55:  0.08825304194879675\n",
            "Loss SS66:  0.08874933052459152\n",
            "Loss SS77:  0.0879237490545767\n",
            "Loss SS88:  0.09482713093210203\n",
            "Loss SS11:  0.08627071297151252\n",
            "Loss SS22:  0.07393446477277538\n",
            "Loss SS33:  0.08617116255232316\n",
            "Loss SS44:  0.07644779952871135\n",
            "Loss SS55:  0.08823900455690899\n",
            "Loss SS66:  0.08882281992617241\n",
            "Loss SS77:  0.08797247923058499\n",
            "Loss SS88:  0.09484356235374104\n",
            "Loss SS11:  0.08618568113216987\n",
            "Loss SS22:  0.07388108034758827\n",
            "Loss SS33:  0.0861065993112037\n",
            "Loss SS44:  0.07640988736325859\n",
            "Loss SS55:  0.08819643611836638\n",
            "Loss SS66:  0.08875836160278049\n",
            "Loss SS77:  0.0878936340262917\n",
            "Loss SS88:  0.09479433777834954\n",
            "Loss SS11:  0.08626030550604051\n",
            "Loss SS22:  0.07384122745069441\n",
            "Loss SS33:  0.08610887534482987\n",
            "Loss SS44:  0.07640874276448485\n",
            "Loss SS55:  0.08819016640866563\n",
            "Loss SS66:  0.08874318443259374\n",
            "Loss SS77:  0.08794766161951992\n",
            "Loss SS88:  0.09479606461161721\n",
            "Loss SS11:  0.08624281878821612\n",
            "Loss SS22:  0.07380559002212438\n",
            "Loss SS33:  0.08611231325369961\n",
            "Loss SS44:  0.07632804830681603\n",
            "Loss SS55:  0.08812766482004258\n",
            "Loss SS66:  0.08876257301421178\n",
            "Loss SS77:  0.08795110856225548\n",
            "Loss SS88:  0.09476866954661443\n",
            "Loss SS11:  0.08618920918290071\n",
            "Loss SS22:  0.0737836676554417\n",
            "Loss SS33:  0.08601917543514508\n",
            "Loss SS44:  0.07625278807061864\n",
            "Loss SS55:  0.08802758343028896\n",
            "Loss SS66:  0.08869061793085783\n",
            "Loss SS77:  0.0878760225366889\n",
            "Loss SS88:  0.0947392759869105\n",
            "Loss SS11:  0.0861860390018929\n",
            "Loss SS22:  0.07374745542588441\n",
            "Loss SS33:  0.08594438824278619\n",
            "Loss SS44:  0.0762285956507906\n",
            "Loss SS55:  0.08796288452261244\n",
            "Loss SS66:  0.08868266748802742\n",
            "Loss SS77:  0.08788122049035013\n",
            "Loss SS88:  0.09463064894651818\n",
            "Loss SS11:  0.08628953001148386\n",
            "Loss SS22:  0.07380523601375019\n",
            "Loss SS33:  0.08590696018161322\n",
            "Loss SS44:  0.07622383370475282\n",
            "Loss SS55:  0.08799844034517793\n",
            "Loss SS66:  0.08872729621623222\n",
            "Loss SS77:  0.08786796855547481\n",
            "Loss SS88:  0.09459353687980228\n",
            "Loss SS11:  0.08633070999253405\n",
            "Loss SS22:  0.07379600974910161\n",
            "Loss SS33:  0.08593487014445655\n",
            "Loss SS44:  0.07624358657539036\n",
            "Loss SS55:  0.08797104174493293\n",
            "Loss SS66:  0.08867082422159595\n",
            "Loss SS77:  0.08787814932909326\n",
            "Loss SS88:  0.09451452321814795\n",
            "Loss SS11:  0.08644906709284794\n",
            "Loss SS22:  0.07380360702933156\n",
            "Loss SS33:  0.0860217885067797\n",
            "Loss SS44:  0.07628769485054276\n",
            "Loss SS55:  0.08805252965583937\n",
            "Loss SS66:  0.08873526831633122\n",
            "Loss SS77:  0.08795370250904928\n",
            "Loss SS88:  0.09459432729491145\n",
            "Loss SS11:  0.0865289984819784\n",
            "Loss SS22:  0.07379050490612774\n",
            "Loss SS33:  0.08601488947453466\n",
            "Loss SS44:  0.07624484165604042\n",
            "Loss SS55:  0.08797275797051232\n",
            "Loss SS66:  0.08861452717930424\n",
            "Loss SS77:  0.0878850491047597\n",
            "Loss SS88:  0.09457618746638574\n",
            "Loss SS11:  0.08651833197359596\n",
            "Loss SS22:  0.07378561899505234\n",
            "Loss SS33:  0.08600624189490363\n",
            "Loss SS44:  0.07625546293700633\n",
            "Loss SS55:  0.08800232342851946\n",
            "Loss SS66:  0.08862942551809644\n",
            "Loss SS77:  0.08785997610750382\n",
            "Loss SS88:  0.09458198553783283\n",
            "Loss SS11:  0.08651583701239986\n",
            "Loss SS22:  0.07377234319361245\n",
            "Loss SS33:  0.08604510886772244\n",
            "Loss SS44:  0.07628077758422448\n",
            "Loss SS55:  0.08801667570671327\n",
            "Loss SS66:  0.08859196607593423\n",
            "Loss SS77:  0.08781873638805687\n",
            "Loss SS88:  0.09462591496909536\n",
            "Loss SS11:  0.08645004665967958\n",
            "Loss SS22:  0.07386209203209122\n",
            "Loss SS33:  0.08610557205571011\n",
            "Loss SS44:  0.07626998009980371\n",
            "Loss SS55:  0.08811876998806206\n",
            "Loss SS66:  0.0886390017357669\n",
            "Loss SS77:  0.08787725598363452\n",
            "Loss SS88:  0.09465473864445717\n",
            "Loss SS11:  0.08642820922179334\n",
            "Loss SS22:  0.07386790942044774\n",
            "Loss SS33:  0.08607366064171883\n",
            "Loss SS44:  0.07629876515003527\n",
            "Loss SS55:  0.08813901894158366\n",
            "Loss SS66:  0.0885952287956661\n",
            "Loss SS77:  0.08781250957755526\n",
            "Loss SS88:  0.09462458917163233\n",
            "Loss SS11:  0.08639130846688256\n",
            "Loss SS22:  0.07388437630728724\n",
            "Loss SS33:  0.08604752707630087\n",
            "Loss SS44:  0.07630178235517718\n",
            "Loss SS55:  0.0881495911961038\n",
            "Loss SS66:  0.08869256063707158\n",
            "Loss SS77:  0.08777023341483485\n",
            "Loss SS88:  0.0946559358287502\n",
            "Loss SS11:  0.08635351641479189\n",
            "Loss SS22:  0.07389026139758512\n",
            "Loss SS33:  0.08605639214671071\n",
            "Loss SS44:  0.07630382406159233\n",
            "Loss SS55:  0.08812617924815526\n",
            "Loss SS66:  0.08871159012108859\n",
            "Loss SS77:  0.08776183968861331\n",
            "Loss SS88:  0.09463883384060714\n",
            "Validation: \n",
            " Loss SS11:  0.08175089955329895\n",
            " Loss SS22:  0.10748367011547089\n",
            " Loss SS33:  0.11332403123378754\n",
            " Loss SS55:  0.1022171601653099\n",
            " Loss SS66:  0.12477927654981613\n",
            " Loss SS77:  0.12031058967113495\n",
            " Loss SS88:  0.1250830590724945\n",
            " Loss SS99:  0.1489209085702896\n",
            " Loss SS11:  0.09821970689864386\n",
            " Loss SS22:  0.11264181279000782\n",
            " Loss SS33:  0.12181672808669862\n",
            " Loss SS55:  0.11706593561740149\n",
            " Loss SS66:  0.139852755126499\n",
            " Loss SS77:  0.13881713435763404\n",
            " Loss SS88:  0.1397891342639923\n",
            " Loss SS99:  0.15116581178846814\n",
            " Loss SS11:  0.09499236432517447\n",
            " Loss SS22:  0.1097780440638705\n",
            " Loss SS33:  0.1211868033903401\n",
            " Loss SS55:  0.11656033211364979\n",
            " Loss SS66:  0.13860910095092727\n",
            " Loss SS77:  0.14000720130961117\n",
            " Loss SS88:  0.1402171461320505\n",
            " Loss SS99:  0.1511769507352899\n",
            " Loss SS11:  0.09399690869890276\n",
            " Loss SS22:  0.10843563043191785\n",
            " Loss SS33:  0.11943667736209806\n",
            " Loss SS55:  0.1155070988614051\n",
            " Loss SS66:  0.13708780229580206\n",
            " Loss SS77:  0.13806575683296704\n",
            " Loss SS88:  0.13909257325481195\n",
            " Loss SS99:  0.14841881272245627\n",
            " Loss SS11:  0.09344912375564929\n",
            " Loss SS22:  0.10794101857844694\n",
            " Loss SS33:  0.11937024591513622\n",
            " Loss SS55:  0.11550495083685275\n",
            " Loss SS66:  0.13573006172606975\n",
            " Loss SS77:  0.13774381633158084\n",
            " Loss SS88:  0.13773346379583265\n",
            " Loss SS99:  0.14831136406203846\n",
            "\n",
            "Epoch: 57\n",
            "Loss SS11:  0.08283837884664536\n",
            "Loss SS22:  0.07289556413888931\n",
            "Loss SS33:  0.08156074583530426\n",
            "Loss SS44:  0.07699048519134521\n",
            "Loss SS55:  0.08975240588188171\n",
            "Loss SS66:  0.07793314009904861\n",
            "Loss SS77:  0.0876343697309494\n",
            "Loss SS88:  0.09386951476335526\n",
            "Loss SS11:  0.08179325678131798\n",
            "Loss SS22:  0.07564616203308105\n",
            "Loss SS33:  0.08296652409163388\n",
            "Loss SS44:  0.0768249210986224\n",
            "Loss SS55:  0.08821676942435178\n",
            "Loss SS66:  0.09112589467655528\n",
            "Loss SS77:  0.08540645309469917\n",
            "Loss SS88:  0.09549133276397531\n",
            "Loss SS11:  0.08360624419791358\n",
            "Loss SS22:  0.07599828498704093\n",
            "Loss SS33:  0.08506914831343151\n",
            "Loss SS44:  0.07578046265102568\n",
            "Loss SS55:  0.08966048026368731\n",
            "Loss SS66:  0.09071021739925657\n",
            "Loss SS77:  0.08729672822214309\n",
            "Loss SS88:  0.09598926916008904\n",
            "Loss SS11:  0.08530276821505639\n",
            "Loss SS22:  0.07421413893180509\n",
            "Loss SS33:  0.08427672352521651\n",
            "Loss SS44:  0.07587013994493792\n",
            "Loss SS55:  0.08769781310712138\n",
            "Loss SS66:  0.08877973114290545\n",
            "Loss SS77:  0.08641900987394395\n",
            "Loss SS88:  0.09553268744099525\n",
            "Loss SS11:  0.085527843454989\n",
            "Loss SS22:  0.07423097049681152\n",
            "Loss SS33:  0.08500829429888143\n",
            "Loss SS44:  0.07612017760189568\n",
            "Loss SS55:  0.08794844968289864\n",
            "Loss SS66:  0.08872082720442516\n",
            "Loss SS77:  0.08682376855030292\n",
            "Loss SS88:  0.09571277004916494\n",
            "Loss SS11:  0.08598239939002429\n",
            "Loss SS22:  0.07414121933135331\n",
            "Loss SS33:  0.08555266027357064\n",
            "Loss SS44:  0.07606694029242385\n",
            "Loss SS55:  0.08810538433346093\n",
            "Loss SS66:  0.08915521818048813\n",
            "Loss SS77:  0.08735986784392712\n",
            "Loss SS88:  0.09537448836307899\n",
            "Loss SS11:  0.08604244765688161\n",
            "Loss SS22:  0.0741478829476677\n",
            "Loss SS33:  0.08547884299129736\n",
            "Loss SS44:  0.07588896204213627\n",
            "Loss SS55:  0.08780076831090645\n",
            "Loss SS66:  0.08941401822156593\n",
            "Loss SS77:  0.08699074363122221\n",
            "Loss SS88:  0.09509811843516397\n",
            "Loss SS11:  0.08557051546137098\n",
            "Loss SS22:  0.07410937636880807\n",
            "Loss SS33:  0.08536014517008418\n",
            "Loss SS44:  0.07563028274707391\n",
            "Loss SS55:  0.08730650074045423\n",
            "Loss SS66:  0.08876625196614736\n",
            "Loss SS77:  0.08660359670158843\n",
            "Loss SS88:  0.09521170841975951\n",
            "Loss SS11:  0.08546810118872443\n",
            "Loss SS22:  0.07409393920758624\n",
            "Loss SS33:  0.08554631306065454\n",
            "Loss SS44:  0.07571700150951927\n",
            "Loss SS55:  0.08709986912615506\n",
            "Loss SS66:  0.08882837080293232\n",
            "Loss SS77:  0.08692106080275995\n",
            "Loss SS88:  0.09494663213874087\n",
            "Loss SS11:  0.08557914549505317\n",
            "Loss SS22:  0.0739091449713969\n",
            "Loss SS33:  0.08554271927901677\n",
            "Loss SS44:  0.07574751097094881\n",
            "Loss SS55:  0.0870880058833531\n",
            "Loss SS66:  0.08889254518262632\n",
            "Loss SS77:  0.08692507052814567\n",
            "Loss SS88:  0.09525149864154858\n",
            "Loss SS11:  0.08514099870577897\n",
            "Loss SS22:  0.07354322876227964\n",
            "Loss SS33:  0.08507679941335528\n",
            "Loss SS44:  0.07545603774856813\n",
            "Loss SS55:  0.0866611133707632\n",
            "Loss SS66:  0.08835575736985349\n",
            "Loss SS77:  0.08662780898042244\n",
            "Loss SS88:  0.09463920798337105\n",
            "Loss SS11:  0.08495325400485648\n",
            "Loss SS22:  0.07321366389189754\n",
            "Loss SS33:  0.08496154938732181\n",
            "Loss SS44:  0.07521568628044815\n",
            "Loss SS55:  0.0866381174272245\n",
            "Loss SS66:  0.08793542198501192\n",
            "Loss SS77:  0.08647248504666595\n",
            "Loss SS88:  0.09454740087191264\n",
            "Loss SS11:  0.08511891105204575\n",
            "Loss SS22:  0.07317218218337405\n",
            "Loss SS33:  0.08498304866808505\n",
            "Loss SS44:  0.07507243740164544\n",
            "Loss SS55:  0.08674887352245898\n",
            "Loss SS66:  0.08770143881070712\n",
            "Loss SS77:  0.08652781652024955\n",
            "Loss SS88:  0.09461176487778829\n",
            "Loss SS11:  0.08489202315570744\n",
            "Loss SS22:  0.07334146446740354\n",
            "Loss SS33:  0.08509297253748843\n",
            "Loss SS44:  0.07535580289727858\n",
            "Loss SS55:  0.08681128742813154\n",
            "Loss SS66:  0.08793160273828579\n",
            "Loss SS77:  0.08683630448716287\n",
            "Loss SS88:  0.09455885527698138\n",
            "Loss SS11:  0.08478946265176679\n",
            "Loss SS22:  0.07323603107468456\n",
            "Loss SS33:  0.08485781232304607\n",
            "Loss SS44:  0.07527083153208942\n",
            "Loss SS55:  0.0867457371865604\n",
            "Loss SS66:  0.08807301457892074\n",
            "Loss SS77:  0.08677795006239668\n",
            "Loss SS88:  0.09431819226724882\n",
            "Loss SS11:  0.0848633637590124\n",
            "Loss SS22:  0.07340922638379185\n",
            "Loss SS33:  0.08499317552080217\n",
            "Loss SS44:  0.07528187105099098\n",
            "Loss SS55:  0.08684998864172311\n",
            "Loss SS66:  0.08815375764835749\n",
            "Loss SS77:  0.08685324967696967\n",
            "Loss SS88:  0.09437575620531247\n",
            "Loss SS11:  0.08438570103289918\n",
            "Loss SS22:  0.07316017581254058\n",
            "Loss SS33:  0.08464246562549047\n",
            "Loss SS44:  0.07511585136378034\n",
            "Loss SS55:  0.08673077951306882\n",
            "Loss SS66:  0.08786951625569267\n",
            "Loss SS77:  0.08661386042092897\n",
            "Loss SS88:  0.09410368220776505\n",
            "Loss SS11:  0.08422281026665927\n",
            "Loss SS22:  0.07296506243578174\n",
            "Loss SS33:  0.08455589221932037\n",
            "Loss SS44:  0.07505205487124404\n",
            "Loss SS55:  0.08665577161033251\n",
            "Loss SS66:  0.08777328537047258\n",
            "Loss SS77:  0.08666711479251148\n",
            "Loss SS88:  0.09380316873740034\n",
            "Loss SS11:  0.08420316483928354\n",
            "Loss SS22:  0.07308269318872394\n",
            "Loss SS33:  0.08463180225692402\n",
            "Loss SS44:  0.07524886562679355\n",
            "Loss SS55:  0.08693836547063859\n",
            "Loss SS66:  0.08788152425987286\n",
            "Loss SS77:  0.0867555007256197\n",
            "Loss SS88:  0.09388469781170893\n",
            "Loss SS11:  0.0841968179685283\n",
            "Loss SS22:  0.07302401394276095\n",
            "Loss SS33:  0.08463427510249052\n",
            "Loss SS44:  0.0754282093016889\n",
            "Loss SS55:  0.08676937398018013\n",
            "Loss SS66:  0.08791265734203199\n",
            "Loss SS77:  0.08657546251700186\n",
            "Loss SS88:  0.09391469758062462\n",
            "Loss SS11:  0.08427536928683371\n",
            "Loss SS22:  0.07303293253207088\n",
            "Loss SS33:  0.08468299707518288\n",
            "Loss SS44:  0.07538074214215303\n",
            "Loss SS55:  0.08681660349392772\n",
            "Loss SS66:  0.08788833044358153\n",
            "Loss SS77:  0.0865650248542354\n",
            "Loss SS88:  0.09397358035863336\n",
            "Loss SS11:  0.08450080489660326\n",
            "Loss SS22:  0.07302322577667462\n",
            "Loss SS33:  0.08489142573699002\n",
            "Loss SS44:  0.07546486610202428\n",
            "Loss SS55:  0.0870037523067393\n",
            "Loss SS66:  0.08811484916374017\n",
            "Loss SS77:  0.08677224055709432\n",
            "Loss SS88:  0.09408367238056038\n",
            "Loss SS11:  0.0847557157412913\n",
            "Loss SS22:  0.07313708620508332\n",
            "Loss SS33:  0.08503758209458304\n",
            "Loss SS44:  0.07556408994338092\n",
            "Loss SS55:  0.08726195283065555\n",
            "Loss SS66:  0.08819660267948565\n",
            "Loss SS77:  0.08709981656586963\n",
            "Loss SS88:  0.09432697724298114\n",
            "Loss SS11:  0.08492839891157108\n",
            "Loss SS22:  0.07316979307890971\n",
            "Loss SS33:  0.08507110581640558\n",
            "Loss SS44:  0.07557673503955205\n",
            "Loss SS55:  0.08733173295274957\n",
            "Loss SS66:  0.08836691175059323\n",
            "Loss SS77:  0.0871936251848807\n",
            "Loss SS88:  0.09443263957897823\n",
            "Loss SS11:  0.0851777537235086\n",
            "Loss SS22:  0.07330120626945219\n",
            "Loss SS33:  0.08525410906655165\n",
            "Loss SS44:  0.07569964078575743\n",
            "Loss SS55:  0.08746720399104708\n",
            "Loss SS66:  0.08855793770418127\n",
            "Loss SS77:  0.08723689068526153\n",
            "Loss SS88:  0.09472313594397667\n",
            "Loss SS11:  0.08511442949334938\n",
            "Loss SS22:  0.07339424859598813\n",
            "Loss SS33:  0.08536274048436686\n",
            "Loss SS44:  0.07569951534389975\n",
            "Loss SS55:  0.08756895847173327\n",
            "Loss SS66:  0.08860151122409508\n",
            "Loss SS77:  0.08738600774352769\n",
            "Loss SS88:  0.0949367079661187\n",
            "Loss SS11:  0.08508053296370524\n",
            "Loss SS22:  0.07340914159442273\n",
            "Loss SS33:  0.08545615598273917\n",
            "Loss SS44:  0.07572260213537692\n",
            "Loss SS55:  0.08776378537389054\n",
            "Loss SS66:  0.08865440194405814\n",
            "Loss SS77:  0.0874388693507147\n",
            "Loss SS88:  0.09490939440969307\n",
            "Loss SS11:  0.08505490798901807\n",
            "Loss SS22:  0.07348468212403934\n",
            "Loss SS33:  0.08549982653002898\n",
            "Loss SS44:  0.07578500945849612\n",
            "Loss SS55:  0.08790497567051012\n",
            "Loss SS66:  0.08862291928037067\n",
            "Loss SS77:  0.08752736693928602\n",
            "Loss SS88:  0.09478040960552067\n",
            "Loss SS11:  0.08517769856703239\n",
            "Loss SS22:  0.07351857028715976\n",
            "Loss SS33:  0.08546301658891699\n",
            "Loss SS44:  0.07587136752452714\n",
            "Loss SS55:  0.08794249323763458\n",
            "Loss SS66:  0.0886796388867911\n",
            "Loss SS77:  0.08754766602533144\n",
            "Loss SS88:  0.09475835558570576\n",
            "Loss SS11:  0.08522250925757222\n",
            "Loss SS22:  0.0735810042656574\n",
            "Loss SS33:  0.08561491428576794\n",
            "Loss SS44:  0.07590527515026302\n",
            "Loss SS55:  0.0879942043540404\n",
            "Loss SS66:  0.08892662375141255\n",
            "Loss SS77:  0.08764787312737855\n",
            "Loss SS88:  0.09501042583144408\n",
            "Loss SS11:  0.08531823610744603\n",
            "Loss SS22:  0.07357546628957175\n",
            "Loss SS33:  0.08560350465517108\n",
            "Loss SS44:  0.07587620917942833\n",
            "Loss SS55:  0.08809098351338376\n",
            "Loss SS66:  0.08888157808602451\n",
            "Loss SS77:  0.08762120096588452\n",
            "Loss SS88:  0.09487854575496972\n",
            "Loss SS11:  0.08522488905110927\n",
            "Loss SS22:  0.07344711014935058\n",
            "Loss SS33:  0.08558339055423951\n",
            "Loss SS44:  0.07580294112683876\n",
            "Loss SS55:  0.08802209988571823\n",
            "Loss SS66:  0.08878804256965876\n",
            "Loss SS77:  0.08755656923511787\n",
            "Loss SS88:  0.09461395751529185\n",
            "Loss SS11:  0.08534142976145136\n",
            "Loss SS22:  0.07348248217475377\n",
            "Loss SS33:  0.08569779151882338\n",
            "Loss SS44:  0.07586376385246853\n",
            "Loss SS55:  0.08809107653448515\n",
            "Loss SS66:  0.08876538200077609\n",
            "Loss SS77:  0.08762482920446871\n",
            "Loss SS88:  0.09458974732602497\n",
            "Loss SS11:  0.08527020704890306\n",
            "Loss SS22:  0.0734530921766585\n",
            "Loss SS33:  0.08560567869160471\n",
            "Loss SS44:  0.07578839960415197\n",
            "Loss SS55:  0.08802216441519671\n",
            "Loss SS66:  0.08863107157797973\n",
            "Loss SS77:  0.08762208124752131\n",
            "Loss SS88:  0.09446337722309406\n",
            "Loss SS11:  0.08532639697762179\n",
            "Loss SS22:  0.07344916330710534\n",
            "Loss SS33:  0.08564662553173356\n",
            "Loss SS44:  0.07573372881608276\n",
            "Loss SS55:  0.08802800990444475\n",
            "Loss SS66:  0.08863072427224554\n",
            "Loss SS77:  0.08761420922457648\n",
            "Loss SS88:  0.09444726983910082\n",
            "Loss SS11:  0.08542805360338288\n",
            "Loss SS22:  0.07346714484343501\n",
            "Loss SS33:  0.08570803943862262\n",
            "Loss SS44:  0.07574368522972123\n",
            "Loss SS55:  0.08804673508361534\n",
            "Loss SS66:  0.08869722910076805\n",
            "Loss SS77:  0.08769628827982819\n",
            "Loss SS88:  0.09448253053460705\n",
            "Loss SS11:  0.08550232529144869\n",
            "Loss SS22:  0.0734378110070473\n",
            "Loss SS33:  0.08569922248660032\n",
            "Loss SS44:  0.07568344636884752\n",
            "Loss SS55:  0.08791730022496463\n",
            "Loss SS66:  0.0885896454062158\n",
            "Loss SS77:  0.0876551943307438\n",
            "Loss SS88:  0.09444342646489844\n",
            "Loss SS11:  0.0854633732063751\n",
            "Loss SS22:  0.07341692565546203\n",
            "Loss SS33:  0.08563729546301449\n",
            "Loss SS44:  0.07564737534306121\n",
            "Loss SS55:  0.08782286861675448\n",
            "Loss SS66:  0.08850018826697394\n",
            "Loss SS77:  0.08762282964514916\n",
            "Loss SS88:  0.09437568118228425\n",
            "Loss SS11:  0.08536296271354821\n",
            "Loss SS22:  0.07344068303274044\n",
            "Loss SS33:  0.08554806949428999\n",
            "Loss SS44:  0.07555673041642494\n",
            "Loss SS55:  0.08771750755275641\n",
            "Loss SS66:  0.08837549087256584\n",
            "Loss SS77:  0.08749594320384223\n",
            "Loss SS88:  0.09426104844476921\n",
            "Loss SS11:  0.08536257859691025\n",
            "Loss SS22:  0.07336084762840625\n",
            "Loss SS33:  0.08553753915192831\n",
            "Loss SS44:  0.07557803195189028\n",
            "Loss SS55:  0.08760691125450841\n",
            "Loss SS66:  0.08832655580299895\n",
            "Loss SS77:  0.08748427624135371\n",
            "Loss SS88:  0.0941997763827024\n",
            "Loss SS11:  0.08552382472075726\n",
            "Loss SS22:  0.07339740305804553\n",
            "Loss SS33:  0.08556269408387139\n",
            "Loss SS44:  0.07565294829501476\n",
            "Loss SS55:  0.08758885959660323\n",
            "Loss SS66:  0.0883538705712542\n",
            "Loss SS77:  0.0875141692726392\n",
            "Loss SS88:  0.0942058715031034\n",
            "Loss SS11:  0.08551396870047506\n",
            "Loss SS22:  0.07339222675728682\n",
            "Loss SS33:  0.08557344325920091\n",
            "Loss SS44:  0.07566181513188529\n",
            "Loss SS55:  0.08758461178074207\n",
            "Loss SS66:  0.08834461764480075\n",
            "Loss SS77:  0.08746854562103894\n",
            "Loss SS88:  0.09409759164654136\n",
            "Loss SS11:  0.08553597824802875\n",
            "Loss SS22:  0.07338908706701\n",
            "Loss SS33:  0.08557831023481283\n",
            "Loss SS44:  0.07565262594266912\n",
            "Loss SS55:  0.08761661991724776\n",
            "Loss SS66:  0.0883901430351434\n",
            "Loss SS77:  0.08753878375484371\n",
            "Loss SS88:  0.09421392137321893\n",
            "Loss SS11:  0.08549842037900142\n",
            "Loss SS22:  0.07333998232561309\n",
            "Loss SS33:  0.08556537788248947\n",
            "Loss SS44:  0.07557540303938903\n",
            "Loss SS55:  0.08759715217932748\n",
            "Loss SS66:  0.08832499054703524\n",
            "Loss SS77:  0.0874140631848311\n",
            "Loss SS88:  0.09404360298974884\n",
            "Loss SS11:  0.08545447789690122\n",
            "Loss SS22:  0.07329491937275376\n",
            "Loss SS33:  0.08554232632424556\n",
            "Loss SS44:  0.07556241841948762\n",
            "Loss SS55:  0.08755327266157349\n",
            "Loss SS66:  0.08830841966902588\n",
            "Loss SS77:  0.08732390707852888\n",
            "Loss SS88:  0.09399967061148749\n",
            "Loss SS11:  0.08554236952653217\n",
            "Loss SS22:  0.07332906413600344\n",
            "Loss SS33:  0.08559075767740178\n",
            "Loss SS44:  0.07561940746072127\n",
            "Loss SS55:  0.08765222855787848\n",
            "Loss SS66:  0.08837122787327037\n",
            "Loss SS77:  0.08735681075743192\n",
            "Loss SS88:  0.09407478240535423\n",
            "Loss SS11:  0.0854886886128376\n",
            "Loss SS22:  0.07333952479403862\n",
            "Loss SS33:  0.08565159659194325\n",
            "Loss SS44:  0.0756196996556704\n",
            "Loss SS55:  0.08772351137016962\n",
            "Loss SS66:  0.08841630769590493\n",
            "Loss SS77:  0.08734548392497536\n",
            "Loss SS88:  0.09404262641763997\n",
            "Loss SS11:  0.08545688033546865\n",
            "Loss SS22:  0.07335436269799109\n",
            "Loss SS33:  0.08564331671994203\n",
            "Loss SS44:  0.07565516121131853\n",
            "Loss SS55:  0.0877602041685151\n",
            "Loss SS66:  0.08842462864397944\n",
            "Loss SS77:  0.08735120790019917\n",
            "Loss SS88:  0.09397949094858392\n",
            "Loss SS11:  0.08545218356991509\n",
            "Loss SS22:  0.07334998596978534\n",
            "Loss SS33:  0.08567282419628512\n",
            "Loss SS44:  0.07569628791291104\n",
            "Loss SS55:  0.08774349321136851\n",
            "Loss SS66:  0.08846979752461777\n",
            "Loss SS77:  0.08731074996772774\n",
            "Loss SS88:  0.09399604389982263\n",
            "Loss SS11:  0.08542587553471509\n",
            "Loss SS22:  0.07332619533033817\n",
            "Loss SS33:  0.08567356318235397\n",
            "Loss SS44:  0.07571927367002075\n",
            "Loss SS55:  0.08772103873872465\n",
            "Loss SS66:  0.08841938580065783\n",
            "Loss SS77:  0.08733500936305692\n",
            "Loss SS88:  0.09401422092603812\n",
            "Validation: \n",
            " Loss SS11:  0.08080453425645828\n",
            " Loss SS22:  0.10877218842506409\n",
            " Loss SS33:  0.11616108566522598\n",
            " Loss SS55:  0.10126153379678726\n",
            " Loss SS66:  0.1195036843419075\n",
            " Loss SS77:  0.11773237586021423\n",
            " Loss SS88:  0.11946246773004532\n",
            " Loss SS99:  0.1536150723695755\n",
            " Loss SS11:  0.09707130988438924\n",
            " Loss SS22:  0.11508481381904512\n",
            " Loss SS33:  0.12415711652664911\n",
            " Loss SS55:  0.11592091442573638\n",
            " Loss SS66:  0.13812732448180517\n",
            " Loss SS77:  0.1337463039727438\n",
            " Loss SS88:  0.14222471841744014\n",
            " Loss SS99:  0.15809070141542525\n",
            " Loss SS11:  0.09414040333614117\n",
            " Loss SS22:  0.11226644239774565\n",
            " Loss SS33:  0.12342323490032335\n",
            " Loss SS55:  0.1149687688888573\n",
            " Loss SS66:  0.13718735244942876\n",
            " Loss SS77:  0.13521753651339832\n",
            " Loss SS88:  0.1420014155347173\n",
            " Loss SS99:  0.15726150644988549\n",
            " Loss SS11:  0.09310913843209626\n",
            " Loss SS22:  0.11087550906861415\n",
            " Loss SS33:  0.12148881239480659\n",
            " Loss SS55:  0.11390561818099412\n",
            " Loss SS66:  0.135766295624561\n",
            " Loss SS77:  0.1333030897085784\n",
            " Loss SS88:  0.14114330721194626\n",
            " Loss SS99:  0.15428319133696008\n",
            " Loss SS11:  0.09236615574286308\n",
            " Loss SS22:  0.11050992265895561\n",
            " Loss SS33:  0.12152858032488528\n",
            " Loss SS55:  0.11403429958923364\n",
            " Loss SS66:  0.13473265774456072\n",
            " Loss SS77:  0.1331046298146248\n",
            " Loss SS88:  0.1398488184736099\n",
            " Loss SS99:  0.15449370830147355\n",
            "\n",
            "Epoch: 58\n",
            "Loss SS11:  0.0881795585155487\n",
            "Loss SS22:  0.07368677109479904\n",
            "Loss SS33:  0.08044031262397766\n",
            "Loss SS44:  0.07974272221326828\n",
            "Loss SS55:  0.08497634530067444\n",
            "Loss SS66:  0.07530179619789124\n",
            "Loss SS77:  0.08732537180185318\n",
            "Loss SS88:  0.09665901958942413\n",
            "Loss SS11:  0.08700608597560362\n",
            "Loss SS22:  0.07692745531147177\n",
            "Loss SS33:  0.08583085171201012\n",
            "Loss SS44:  0.07880520549687473\n",
            "Loss SS55:  0.08976115828210657\n",
            "Loss SS66:  0.08842307058247653\n",
            "Loss SS77:  0.09269517118280585\n",
            "Loss SS88:  0.09740167720751329\n",
            "Loss SS11:  0.08492076574336915\n",
            "Loss SS22:  0.07549159370717548\n",
            "Loss SS33:  0.08620518623363405\n",
            "Loss SS44:  0.0764356418734505\n",
            "Loss SS55:  0.08912907576277143\n",
            "Loss SS66:  0.08989572773377101\n",
            "Loss SS77:  0.09083317113774163\n",
            "Loss SS88:  0.09544276836372557\n",
            "Loss SS11:  0.08495856340854399\n",
            "Loss SS22:  0.07450004955453257\n",
            "Loss SS33:  0.08565672511054624\n",
            "Loss SS44:  0.07528180773219754\n",
            "Loss SS55:  0.08734947827554518\n",
            "Loss SS66:  0.08885699390403685\n",
            "Loss SS77:  0.08816039129610985\n",
            "Loss SS88:  0.09457336942995748\n",
            "Loss SS11:  0.08540991257603575\n",
            "Loss SS22:  0.07468097239005857\n",
            "Loss SS33:  0.0856472348294607\n",
            "Loss SS44:  0.07574364488444678\n",
            "Loss SS55:  0.08748340788410931\n",
            "Loss SS66:  0.0889881680287966\n",
            "Loss SS77:  0.08771906484191011\n",
            "Loss SS88:  0.09526164320910849\n",
            "Loss SS11:  0.08567403680553623\n",
            "Loss SS22:  0.07431760006675533\n",
            "Loss SS33:  0.08614557104952195\n",
            "Loss SS44:  0.07600825352996003\n",
            "Loss SS55:  0.0882016496331084\n",
            "Loss SS66:  0.08885513567457012\n",
            "Loss SS77:  0.08739011384108487\n",
            "Loss SS88:  0.09512011677611108\n",
            "Loss SS11:  0.08556762252186166\n",
            "Loss SS22:  0.07428980594287153\n",
            "Loss SS33:  0.08657436512532782\n",
            "Loss SS44:  0.07611155045814201\n",
            "Loss SS55:  0.08802755250305426\n",
            "Loss SS66:  0.08881328853427387\n",
            "Loss SS77:  0.0873662460534299\n",
            "Loss SS88:  0.09482584246357934\n",
            "Loss SS11:  0.08540869556682211\n",
            "Loss SS22:  0.07410267121355299\n",
            "Loss SS33:  0.08612237786743003\n",
            "Loss SS44:  0.07596725498286772\n",
            "Loss SS55:  0.08774030166612544\n",
            "Loss SS66:  0.08871215183130453\n",
            "Loss SS77:  0.08681930286783568\n",
            "Loss SS88:  0.09446883327524427\n",
            "Loss SS11:  0.08603518676978571\n",
            "Loss SS22:  0.07409519095111776\n",
            "Loss SS33:  0.08615083164638943\n",
            "Loss SS44:  0.07603165948832477\n",
            "Loss SS55:  0.08792813435012911\n",
            "Loss SS66:  0.08861873611623858\n",
            "Loss SS77:  0.0870655761272819\n",
            "Loss SS88:  0.09439974958881919\n",
            "Loss SS11:  0.08660706001651156\n",
            "Loss SS22:  0.07388103466767532\n",
            "Loss SS33:  0.08633344579528976\n",
            "Loss SS44:  0.07591306532804783\n",
            "Loss SS55:  0.08818583493376826\n",
            "Loss SS66:  0.08870854351546738\n",
            "Loss SS77:  0.08720155895411313\n",
            "Loss SS88:  0.09443675767589402\n",
            "Loss SS11:  0.08632795320879115\n",
            "Loss SS22:  0.07353107498423887\n",
            "Loss SS33:  0.08568028412242927\n",
            "Loss SS44:  0.07574735537613972\n",
            "Loss SS55:  0.08754189541139225\n",
            "Loss SS66:  0.08802292355806521\n",
            "Loss SS77:  0.08651967272900118\n",
            "Loss SS88:  0.09398981213274568\n",
            "Loss SS11:  0.08624729587956592\n",
            "Loss SS22:  0.07302154644249796\n",
            "Loss SS33:  0.0852726443528055\n",
            "Loss SS44:  0.07560312298235593\n",
            "Loss SS55:  0.08723562244359437\n",
            "Loss SS66:  0.08767603854606817\n",
            "Loss SS77:  0.08622354208617597\n",
            "Loss SS88:  0.09360820967871863\n",
            "Loss SS11:  0.08623906163391003\n",
            "Loss SS22:  0.07308447320968652\n",
            "Loss SS33:  0.0851982246068391\n",
            "Loss SS44:  0.07541146274071095\n",
            "Loss SS55:  0.08724253125919783\n",
            "Loss SS66:  0.08763014783790289\n",
            "Loss SS77:  0.08606877664396585\n",
            "Loss SS88:  0.09344320066950539\n",
            "Loss SS11:  0.08629700063749124\n",
            "Loss SS22:  0.07315931443608444\n",
            "Loss SS33:  0.08552090468183729\n",
            "Loss SS44:  0.07561204266798405\n",
            "Loss SS55:  0.08734037465494097\n",
            "Loss SS66:  0.08786175013270997\n",
            "Loss SS77:  0.08621845542247059\n",
            "Loss SS88:  0.09360201781942644\n",
            "Loss SS11:  0.08641708420312151\n",
            "Loss SS22:  0.07316981522855184\n",
            "Loss SS33:  0.08533096664869194\n",
            "Loss SS44:  0.07562209874497237\n",
            "Loss SS55:  0.08722127202554797\n",
            "Loss SS66:  0.08795382668997379\n",
            "Loss SS77:  0.0862111324536885\n",
            "Loss SS88:  0.0935392660874847\n",
            "Loss SS11:  0.08636786507454929\n",
            "Loss SS22:  0.07332297046946375\n",
            "Loss SS33:  0.08542684576665328\n",
            "Loss SS44:  0.07584908972216757\n",
            "Loss SS55:  0.08727732670820312\n",
            "Loss SS66:  0.08806632528241896\n",
            "Loss SS77:  0.0863478302166162\n",
            "Loss SS88:  0.09348839047728785\n",
            "Loss SS11:  0.08602945506572723\n",
            "Loss SS22:  0.0732916747422322\n",
            "Loss SS33:  0.08522765252808606\n",
            "Loss SS44:  0.07570234665311641\n",
            "Loss SS55:  0.08697882984180628\n",
            "Loss SS66:  0.08785314330403109\n",
            "Loss SS77:  0.08634999514736744\n",
            "Loss SS88:  0.09349109861791505\n",
            "Loss SS11:  0.08590966933651974\n",
            "Loss SS22:  0.07307174372655606\n",
            "Loss SS33:  0.08512610116461564\n",
            "Loss SS44:  0.07559550346599685\n",
            "Loss SS55:  0.086870747351507\n",
            "Loss SS66:  0.08761512849763123\n",
            "Loss SS77:  0.08627857346283763\n",
            "Loss SS88:  0.0932939055196026\n",
            "Loss SS11:  0.08592242809290386\n",
            "Loss SS22:  0.07306525710126313\n",
            "Loss SS33:  0.0852517748858718\n",
            "Loss SS44:  0.07567577889364069\n",
            "Loss SS55:  0.08690917195536155\n",
            "Loss SS66:  0.08758257370627387\n",
            "Loss SS77:  0.0863986959608879\n",
            "Loss SS88:  0.0933431962000731\n",
            "Loss SS11:  0.0858582811751915\n",
            "Loss SS22:  0.07301551922765702\n",
            "Loss SS33:  0.08531616052797951\n",
            "Loss SS44:  0.07560103837186129\n",
            "Loss SS55:  0.08679551315245204\n",
            "Loss SS66:  0.08781675237635668\n",
            "Loss SS77:  0.08648664406768938\n",
            "Loss SS88:  0.09344229464911666\n",
            "Loss SS11:  0.08581236457053702\n",
            "Loss SS22:  0.07290810761760123\n",
            "Loss SS33:  0.08532726473698568\n",
            "Loss SS44:  0.07552455819735479\n",
            "Loss SS55:  0.08674050567310247\n",
            "Loss SS66:  0.08785873462459934\n",
            "Loss SS77:  0.08640083618721559\n",
            "Loss SS88:  0.0933539243924677\n",
            "Loss SS11:  0.08579731817352829\n",
            "Loss SS22:  0.07276991366351385\n",
            "Loss SS33:  0.08525679318755158\n",
            "Loss SS44:  0.07555011732247768\n",
            "Loss SS55:  0.08682402569394541\n",
            "Loss SS66:  0.08779962629221062\n",
            "Loss SS77:  0.08646205547861578\n",
            "Loss SS88:  0.09338913540139583\n",
            "Loss SS11:  0.08579718503612199\n",
            "Loss SS22:  0.07284919246694081\n",
            "Loss SS33:  0.08524894985745396\n",
            "Loss SS44:  0.07565038133732874\n",
            "Loss SS55:  0.08697024999161111\n",
            "Loss SS66:  0.08803955624006453\n",
            "Loss SS77:  0.08664611767454924\n",
            "Loss SS88:  0.09362712556420408\n",
            "Loss SS11:  0.0858477173429547\n",
            "Loss SS22:  0.07287830904577718\n",
            "Loss SS33:  0.08530190862812005\n",
            "Loss SS44:  0.0757995035128418\n",
            "Loss SS55:  0.08695071938879047\n",
            "Loss SS66:  0.08817525807913247\n",
            "Loss SS77:  0.08680007919604645\n",
            "Loss SS88:  0.09370137206016681\n",
            "Loss SS11:  0.08612278610838894\n",
            "Loss SS22:  0.07301299057560838\n",
            "Loss SS33:  0.08545195785857335\n",
            "Loss SS44:  0.07588282587679095\n",
            "Loss SS55:  0.08707127438788592\n",
            "Loss SS66:  0.08841106811870678\n",
            "Loss SS77:  0.08693758586877609\n",
            "Loss SS88:  0.09398417196456822\n",
            "Loss SS11:  0.08595190790901146\n",
            "Loss SS22:  0.0730167974187321\n",
            "Loss SS33:  0.08551668104125684\n",
            "Loss SS44:  0.0759233554669348\n",
            "Loss SS55:  0.0871683401890485\n",
            "Loss SS66:  0.08845788590699077\n",
            "Loss SS77:  0.0869812626406491\n",
            "Loss SS88:  0.09410830364521756\n",
            "Loss SS11:  0.08583761471898163\n",
            "Loss SS22:  0.07317560932588303\n",
            "Loss SS33:  0.08556455954680954\n",
            "Loss SS44:  0.07596170118656652\n",
            "Loss SS55:  0.08721977069802668\n",
            "Loss SS66:  0.08842177061057183\n",
            "Loss SS77:  0.08696971685265216\n",
            "Loss SS88:  0.09412758623274807\n",
            "Loss SS11:  0.0857911020572335\n",
            "Loss SS22:  0.07324379007589774\n",
            "Loss SS33:  0.08566314360037501\n",
            "Loss SS44:  0.07594250974235059\n",
            "Loss SS55:  0.08733060675573524\n",
            "Loss SS66:  0.08837160099696409\n",
            "Loss SS77:  0.08698915948617063\n",
            "Loss SS88:  0.09413578548224649\n",
            "Loss SS11:  0.08577528829252168\n",
            "Loss SS22:  0.0732315315125888\n",
            "Loss SS33:  0.08580421297098394\n",
            "Loss SS44:  0.07605748799931113\n",
            "Loss SS55:  0.08744488264105922\n",
            "Loss SS66:  0.08856516387114745\n",
            "Loss SS77:  0.0869996323386121\n",
            "Loss SS88:  0.09416763086760171\n",
            "Loss SS11:  0.08571696837026228\n",
            "Loss SS22:  0.07328618416415457\n",
            "Loss SS33:  0.08585401659005697\n",
            "Loss SS44:  0.07610743514604584\n",
            "Loss SS55:  0.08748619673178368\n",
            "Loss SS66:  0.0886718168137819\n",
            "Loss SS77:  0.08719100262905724\n",
            "Loss SS88:  0.0942628835279917\n",
            "Loss SS11:  0.08572896879376764\n",
            "Loss SS22:  0.07326778592510873\n",
            "Loss SS33:  0.08567584661203761\n",
            "Loss SS44:  0.07607655371987938\n",
            "Loss SS55:  0.08745385733652748\n",
            "Loss SS66:  0.08856121428385129\n",
            "Loss SS77:  0.08718109863541056\n",
            "Loss SS88:  0.09424530323656691\n",
            "Loss SS11:  0.08564062533942066\n",
            "Loss SS22:  0.07320800990201652\n",
            "Loss SS33:  0.08559506299075972\n",
            "Loss SS44:  0.0760604385466246\n",
            "Loss SS55:  0.08729876513170659\n",
            "Loss SS66:  0.08858610735158062\n",
            "Loss SS77:  0.08710364458357789\n",
            "Loss SS88:  0.09410768002271652\n",
            "Loss SS11:  0.08567470794897585\n",
            "Loss SS22:  0.0732212300496495\n",
            "Loss SS33:  0.08561872847587149\n",
            "Loss SS44:  0.07605521177809187\n",
            "Loss SS55:  0.08735131548943921\n",
            "Loss SS66:  0.08856327938513592\n",
            "Loss SS77:  0.08714873768458857\n",
            "Loss SS88:  0.09413210673959826\n",
            "Loss SS11:  0.08569252554023374\n",
            "Loss SS22:  0.07324168078173683\n",
            "Loss SS33:  0.08556389406863656\n",
            "Loss SS44:  0.07595422279843751\n",
            "Loss SS55:  0.08732048737228457\n",
            "Loss SS66:  0.08841554014704378\n",
            "Loss SS77:  0.0871835589814042\n",
            "Loss SS88:  0.0941023317213505\n",
            "Loss SS11:  0.08568075207496319\n",
            "Loss SS22:  0.07325292377571556\n",
            "Loss SS33:  0.08562697853641776\n",
            "Loss SS44:  0.07597751586624255\n",
            "Loss SS55:  0.08737783225750294\n",
            "Loss SS66:  0.08847462527912733\n",
            "Loss SS77:  0.0872139771444357\n",
            "Loss SS88:  0.09411742568802624\n",
            "Loss SS11:  0.08561152981811779\n",
            "Loss SS22:  0.07326977436783647\n",
            "Loss SS33:  0.08555877961197131\n",
            "Loss SS44:  0.07595895562968363\n",
            "Loss SS55:  0.08732552009175645\n",
            "Loss SS66:  0.08841231681852259\n",
            "Loss SS77:  0.08716529009179172\n",
            "Loss SS88:  0.09408350985104542\n",
            "Loss SS11:  0.08561625001420604\n",
            "Loss SS22:  0.07321839632048502\n",
            "Loss SS33:  0.08546879892740554\n",
            "Loss SS44:  0.07588954341089627\n",
            "Loss SS55:  0.08723362539366011\n",
            "Loss SS66:  0.08831108340843893\n",
            "Loss SS77:  0.08713047761583592\n",
            "Loss SS88:  0.0940396429718036\n",
            "Loss SS11:  0.08558808712946436\n",
            "Loss SS22:  0.07318105360849526\n",
            "Loss SS33:  0.08544862681040867\n",
            "Loss SS44:  0.07584544193913352\n",
            "Loss SS55:  0.0871073218649931\n",
            "Loss SS66:  0.08822200256537877\n",
            "Loss SS77:  0.08712705325164563\n",
            "Loss SS88:  0.09409810584590762\n",
            "Loss SS11:  0.08564010964525653\n",
            "Loss SS22:  0.07318703428380133\n",
            "Loss SS33:  0.08534076955570323\n",
            "Loss SS44:  0.07577493828855787\n",
            "Loss SS55:  0.08704464393728988\n",
            "Loss SS66:  0.08812600706662406\n",
            "Loss SS77:  0.08713139488002447\n",
            "Loss SS88:  0.09403768836904386\n",
            "Loss SS11:  0.08566215415211285\n",
            "Loss SS22:  0.07310568626083987\n",
            "Loss SS33:  0.08528306753472294\n",
            "Loss SS44:  0.07574217262513497\n",
            "Loss SS55:  0.08697222072221435\n",
            "Loss SS66:  0.08801865455744516\n",
            "Loss SS77:  0.08710647835527234\n",
            "Loss SS88:  0.09391978190606817\n",
            "Loss SS11:  0.08580800732992534\n",
            "Loss SS22:  0.0731066782166833\n",
            "Loss SS33:  0.08522652802138851\n",
            "Loss SS44:  0.07577182840267917\n",
            "Loss SS55:  0.08697002661644372\n",
            "Loss SS66:  0.08799044522189738\n",
            "Loss SS77:  0.0871227316725581\n",
            "Loss SS88:  0.09391672160485737\n",
            "Loss SS11:  0.0858570686983366\n",
            "Loss SS22:  0.07308210880289402\n",
            "Loss SS33:  0.08518771338673113\n",
            "Loss SS44:  0.07575646228635108\n",
            "Loss SS55:  0.08689078710375041\n",
            "Loss SS66:  0.08793443285490765\n",
            "Loss SS77:  0.08699954136154657\n",
            "Loss SS88:  0.09385392232967989\n",
            "Loss SS11:  0.08591454023066156\n",
            "Loss SS22:  0.07315092234741764\n",
            "Loss SS33:  0.08523134799616637\n",
            "Loss SS44:  0.07581437933933423\n",
            "Loss SS55:  0.08692109131756418\n",
            "Loss SS66:  0.08797617957456096\n",
            "Loss SS77:  0.08707000603656022\n",
            "Loss SS88:  0.09392566840892166\n",
            "Loss SS11:  0.08586843951395659\n",
            "Loss SS22:  0.07317491493721573\n",
            "Loss SS33:  0.08521714015688808\n",
            "Loss SS44:  0.07581177283847691\n",
            "Loss SS55:  0.08686298740435643\n",
            "Loss SS66:  0.08791600297401787\n",
            "Loss SS77:  0.08700017733045631\n",
            "Loss SS88:  0.09393797396507175\n",
            "Loss SS11:  0.08584037762718136\n",
            "Loss SS22:  0.07319343354460063\n",
            "Loss SS33:  0.08523819957967518\n",
            "Loss SS44:  0.07581290405860293\n",
            "Loss SS55:  0.08689835625373317\n",
            "Loss SS66:  0.08793257421849807\n",
            "Loss SS77:  0.08705008812164233\n",
            "Loss SS88:  0.09396464808457562\n",
            "Loss SS11:  0.08589372655042259\n",
            "Loss SS22:  0.07319308829776729\n",
            "Loss SS33:  0.08523498261615865\n",
            "Loss SS44:  0.07584727174724284\n",
            "Loss SS55:  0.08686291333907459\n",
            "Loss SS66:  0.08794760781685158\n",
            "Loss SS77:  0.08708053764509256\n",
            "Loss SS88:  0.09404056289640075\n",
            "Loss SS11:  0.08584218515879162\n",
            "Loss SS22:  0.07319202035697836\n",
            "Loss SS33:  0.08525518593812713\n",
            "Loss SS44:  0.07583623684118472\n",
            "Loss SS55:  0.08690942791563311\n",
            "Loss SS66:  0.0879559275934598\n",
            "Loss SS77:  0.08711568272618048\n",
            "Loss SS88:  0.09400855678733673\n",
            "Loss SS11:  0.08578162086680689\n",
            "Loss SS22:  0.07317600406374142\n",
            "Loss SS33:  0.08523367271776412\n",
            "Loss SS44:  0.07584388520877072\n",
            "Loss SS55:  0.08693064871044422\n",
            "Loss SS66:  0.08796790311797156\n",
            "Loss SS77:  0.0870998527878409\n",
            "Loss SS88:  0.09395230029988441\n",
            "Loss SS11:  0.08572150512817248\n",
            "Loss SS22:  0.07316583697829326\n",
            "Loss SS33:  0.0852465766584675\n",
            "Loss SS44:  0.0757842443403236\n",
            "Loss SS55:  0.08690360904111684\n",
            "Loss SS66:  0.08792343523485001\n",
            "Loss SS77:  0.08707990655346373\n",
            "Loss SS88:  0.09397464728838689\n",
            "Loss SS11:  0.0856081882677593\n",
            "Loss SS22:  0.07308864351475797\n",
            "Loss SS33:  0.08519735678180658\n",
            "Loss SS44:  0.07572787640475935\n",
            "Loss SS55:  0.08680997292706534\n",
            "Loss SS66:  0.08786761706084195\n",
            "Loss SS77:  0.08712016780483262\n",
            "Loss SS88:  0.09392183688289037\n",
            "Validation: \n",
            " Loss SS11:  0.08131472766399384\n",
            " Loss SS22:  0.1070866584777832\n",
            " Loss SS33:  0.11022840440273285\n",
            " Loss SS55:  0.10642499476671219\n",
            " Loss SS66:  0.1332385540008545\n",
            " Loss SS77:  0.12280884385108948\n",
            " Loss SS88:  0.12067233771085739\n",
            " Loss SS99:  0.15436752140522003\n",
            " Loss SS11:  0.09660312285025914\n",
            " Loss SS22:  0.11398636336837496\n",
            " Loss SS33:  0.12121634575582686\n",
            " Loss SS55:  0.11830361329373859\n",
            " Loss SS66:  0.14439918171791805\n",
            " Loss SS77:  0.1360829607361839\n",
            " Loss SS88:  0.13699398509093694\n",
            " Loss SS99:  0.15310655037562051\n",
            " Loss SS11:  0.09337997109424777\n",
            " Loss SS22:  0.11092615927137979\n",
            " Loss SS33:  0.12030247508025751\n",
            " Loss SS55:  0.1174990961827883\n",
            " Loss SS66:  0.1431074842083745\n",
            " Loss SS77:  0.13730779726330827\n",
            " Loss SS88:  0.13817849700770726\n",
            " Loss SS99:  0.15291768203421338\n",
            " Loss SS11:  0.09256346939040012\n",
            " Loss SS22:  0.10981769114732742\n",
            " Loss SS33:  0.11874686328114056\n",
            " Loss SS55:  0.11654413967836098\n",
            " Loss SS66:  0.1413466372206563\n",
            " Loss SS77:  0.13532100947665388\n",
            " Loss SS88:  0.13713593842064747\n",
            " Loss SS99:  0.15022829382634553\n",
            " Loss SS11:  0.09178235409436403\n",
            " Loss SS22:  0.10939985283730942\n",
            " Loss SS33:  0.11831214398513605\n",
            " Loss SS55:  0.11661682563063538\n",
            " Loss SS66:  0.14012002678197108\n",
            " Loss SS77:  0.13492425246003234\n",
            " Loss SS88:  0.13597049996440794\n",
            " Loss SS99:  0.15004590567615297\n",
            "\n",
            "Epoch: 59\n",
            "Loss SS11:  0.09561732411384583\n",
            "Loss SS22:  0.07775523513555527\n",
            "Loss SS33:  0.0769183412194252\n",
            "Loss SS44:  0.0750216692686081\n",
            "Loss SS55:  0.07927942276000977\n",
            "Loss SS66:  0.08917047083377838\n",
            "Loss SS77:  0.0895274430513382\n",
            "Loss SS88:  0.08906730264425278\n",
            "Loss SS11:  0.08467161790891127\n",
            "Loss SS22:  0.07509997487068176\n",
            "Loss SS33:  0.0830734751441262\n",
            "Loss SS44:  0.07561958174813878\n",
            "Loss SS55:  0.0853962315754457\n",
            "Loss SS66:  0.09128058092160658\n",
            "Loss SS77:  0.08866811340505426\n",
            "Loss SS88:  0.09103738245638934\n",
            "Loss SS11:  0.08364745264961607\n",
            "Loss SS22:  0.0736049527213687\n",
            "Loss SS33:  0.08512840703839347\n",
            "Loss SS44:  0.07505046735916819\n",
            "Loss SS55:  0.08625643877756028\n",
            "Loss SS66:  0.09054278901645116\n",
            "Loss SS77:  0.08827386938390278\n",
            "Loss SS88:  0.09163353592157364\n",
            "Loss SS11:  0.08430090546607971\n",
            "Loss SS22:  0.07232394273723325\n",
            "Loss SS33:  0.08458662826207376\n",
            "Loss SS44:  0.07409693468962947\n",
            "Loss SS55:  0.08498022008326746\n",
            "Loss SS66:  0.08809747426740584\n",
            "Loss SS77:  0.08652770423120068\n",
            "Loss SS88:  0.09136566471668982\n",
            "Loss SS11:  0.08517384401908736\n",
            "Loss SS22:  0.07238451873020428\n",
            "Loss SS33:  0.08448444270506138\n",
            "Loss SS44:  0.07431868590959688\n",
            "Loss SS55:  0.08537257281018466\n",
            "Loss SS66:  0.0876436718716854\n",
            "Loss SS77:  0.08636983956505613\n",
            "Loss SS88:  0.09112248388005466\n",
            "Loss SS11:  0.08469534007941976\n",
            "Loss SS22:  0.07270484395763453\n",
            "Loss SS33:  0.08498640855153401\n",
            "Loss SS44:  0.07436332574077681\n",
            "Loss SS55:  0.08610863618406595\n",
            "Loss SS66:  0.08733844537945355\n",
            "Loss SS77:  0.08646861083951651\n",
            "Loss SS88:  0.09114847303021188\n",
            "Loss SS11:  0.08481169382079704\n",
            "Loss SS22:  0.07290881804999758\n",
            "Loss SS33:  0.08457800794820317\n",
            "Loss SS44:  0.07455570336248053\n",
            "Loss SS55:  0.08578363146449698\n",
            "Loss SS66:  0.08748896224576919\n",
            "Loss SS77:  0.08662777760478317\n",
            "Loss SS88:  0.09205301570110634\n",
            "Loss SS11:  0.08474630537167402\n",
            "Loss SS22:  0.07321653982073488\n",
            "Loss SS33:  0.08456257520846917\n",
            "Loss SS44:  0.07453253214627924\n",
            "Loss SS55:  0.0860920013256476\n",
            "Loss SS66:  0.08749209701175421\n",
            "Loss SS77:  0.08628775305311445\n",
            "Loss SS88:  0.09185727832602783\n",
            "Loss SS11:  0.08459667208386057\n",
            "Loss SS22:  0.0731601171011542\n",
            "Loss SS33:  0.08427765790695026\n",
            "Loss SS44:  0.07466943378065839\n",
            "Loss SS55:  0.0864858638357233\n",
            "Loss SS66:  0.08728348896091367\n",
            "Loss SS77:  0.08626037376162446\n",
            "Loss SS88:  0.09198920511537129\n",
            "Loss SS11:  0.08501829346130182\n",
            "Loss SS22:  0.07316882961562701\n",
            "Loss SS33:  0.08431109630472057\n",
            "Loss SS44:  0.07474263241657844\n",
            "Loss SS55:  0.08629182038398889\n",
            "Loss SS66:  0.08714956380836256\n",
            "Loss SS77:  0.08635935465713124\n",
            "Loss SS88:  0.09246070511065997\n",
            "Loss SS11:  0.08483462640554598\n",
            "Loss SS22:  0.0728035452637342\n",
            "Loss SS33:  0.08389230500353445\n",
            "Loss SS44:  0.07463039904095159\n",
            "Loss SS55:  0.08598382815276043\n",
            "Loss SS66:  0.08672958959152202\n",
            "Loss SS77:  0.08600325875058032\n",
            "Loss SS88:  0.09186876705377409\n",
            "Loss SS11:  0.08438806410308357\n",
            "Loss SS22:  0.07253945236270493\n",
            "Loss SS33:  0.08369299225710533\n",
            "Loss SS44:  0.07448397439208117\n",
            "Loss SS55:  0.08557403047342559\n",
            "Loss SS66:  0.08656189805484032\n",
            "Loss SS77:  0.08583677103659054\n",
            "Loss SS88:  0.09146113754124255\n",
            "Loss SS11:  0.0844646436739559\n",
            "Loss SS22:  0.07250203823378264\n",
            "Loss SS33:  0.083712910325074\n",
            "Loss SS44:  0.07453944647977175\n",
            "Loss SS55:  0.08586457273191657\n",
            "Loss SS66:  0.08650788295367533\n",
            "Loss SS77:  0.08601035793458135\n",
            "Loss SS88:  0.09153042899921907\n",
            "Loss SS11:  0.08465686156094529\n",
            "Loss SS22:  0.07242223441259552\n",
            "Loss SS33:  0.08384256690513087\n",
            "Loss SS44:  0.07481954033479436\n",
            "Loss SS55:  0.08593612243883482\n",
            "Loss SS66:  0.08661616474162531\n",
            "Loss SS77:  0.08616232087138954\n",
            "Loss SS88:  0.09165447210992565\n",
            "Loss SS11:  0.08487525407938247\n",
            "Loss SS22:  0.07240991983959015\n",
            "Loss SS33:  0.08387206167194015\n",
            "Loss SS44:  0.07508176800312726\n",
            "Loss SS55:  0.08586041550052927\n",
            "Loss SS66:  0.0866399792069239\n",
            "Loss SS77:  0.08624718903649783\n",
            "Loss SS88:  0.09168341618480412\n",
            "Loss SS11:  0.08489318700223569\n",
            "Loss SS22:  0.07234493428409494\n",
            "Loss SS33:  0.0839189308269924\n",
            "Loss SS44:  0.07512518440354739\n",
            "Loss SS55:  0.08600074462345894\n",
            "Loss SS66:  0.08665721532919549\n",
            "Loss SS77:  0.0861369813021445\n",
            "Loss SS88:  0.09174449060926375\n",
            "Loss SS11:  0.08455738605734725\n",
            "Loss SS22:  0.07217820192345921\n",
            "Loss SS33:  0.08379568585327693\n",
            "Loss SS44:  0.0748785103653899\n",
            "Loss SS55:  0.08584515680992826\n",
            "Loss SS66:  0.08671738318404797\n",
            "Loss SS77:  0.08600575717524712\n",
            "Loss SS88:  0.09173869702571667\n",
            "Loss SS11:  0.0843056547972891\n",
            "Loss SS22:  0.07212652294346464\n",
            "Loss SS33:  0.08378692197869396\n",
            "Loss SS44:  0.07490381666007098\n",
            "Loss SS55:  0.08600282093934845\n",
            "Loss SS66:  0.086756998073985\n",
            "Loss SS77:  0.08593566602433635\n",
            "Loss SS88:  0.09173971314353553\n",
            "Loss SS11:  0.08439070385628643\n",
            "Loss SS22:  0.07210181392111831\n",
            "Loss SS33:  0.08399195520423394\n",
            "Loss SS44:  0.07501470324331226\n",
            "Loss SS55:  0.08612822210261835\n",
            "Loss SS66:  0.08690562461456541\n",
            "Loss SS77:  0.08585018380570807\n",
            "Loss SS88:  0.09185388009192536\n",
            "Loss SS11:  0.08415205238377237\n",
            "Loss SS22:  0.07207885458675355\n",
            "Loss SS33:  0.08396080417158716\n",
            "Loss SS44:  0.07513817081822775\n",
            "Loss SS55:  0.08609590324431814\n",
            "Loss SS66:  0.08709261917475006\n",
            "Loss SS77:  0.08584658286646399\n",
            "Loss SS88:  0.09201744027162721\n",
            "Loss SS11:  0.08412764851578433\n",
            "Loss SS22:  0.07217265854575741\n",
            "Loss SS33:  0.08401321931116616\n",
            "Loss SS44:  0.07501770450330492\n",
            "Loss SS55:  0.0861688131716714\n",
            "Loss SS66:  0.08712423147995081\n",
            "Loss SS77:  0.08594167276994506\n",
            "Loss SS88:  0.09211142824508657\n",
            "Loss SS11:  0.08429430930111645\n",
            "Loss SS22:  0.07219390454591733\n",
            "Loss SS33:  0.08403941190920734\n",
            "Loss SS44:  0.07501969668325655\n",
            "Loss SS55:  0.08612311133558716\n",
            "Loss SS66:  0.08708737924765636\n",
            "Loss SS77:  0.08602912416814062\n",
            "Loss SS88:  0.09209538674891278\n",
            "Loss SS11:  0.08450883785389128\n",
            "Loss SS22:  0.0722767981401396\n",
            "Loss SS33:  0.0842257800482517\n",
            "Loss SS44:  0.07511591208399152\n",
            "Loss SS55:  0.08633800157729317\n",
            "Loss SS66:  0.08740669084350448\n",
            "Loss SS77:  0.08627699025615848\n",
            "Loss SS88:  0.09227402929671749\n",
            "Loss SS11:  0.08476658462187944\n",
            "Loss SS22:  0.07238253341479735\n",
            "Loss SS33:  0.08425272317417773\n",
            "Loss SS44:  0.0752944117855458\n",
            "Loss SS55:  0.08650681986050172\n",
            "Loss SS66:  0.08758017372388344\n",
            "Loss SS77:  0.08654726083789553\n",
            "Loss SS88:  0.09234157330417014\n",
            "Loss SS11:  0.08489706209586369\n",
            "Loss SS22:  0.0725226832314151\n",
            "Loss SS33:  0.08435831793981964\n",
            "Loss SS44:  0.07546318178664105\n",
            "Loss SS55:  0.08659295227641386\n",
            "Loss SS66:  0.08769071405112001\n",
            "Loss SS77:  0.08666012293941253\n",
            "Loss SS88:  0.09258292330993162\n",
            "Loss SS11:  0.08475006644825536\n",
            "Loss SS22:  0.07264471184684936\n",
            "Loss SS33:  0.08451759408550909\n",
            "Loss SS44:  0.07548274410020307\n",
            "Loss SS55:  0.0867400344505728\n",
            "Loss SS66:  0.08773821393450892\n",
            "Loss SS77:  0.0869205574768473\n",
            "Loss SS88:  0.09277771989187872\n",
            "Loss SS11:  0.08474138122180412\n",
            "Loss SS22:  0.07269474096110955\n",
            "Loss SS33:  0.0846312249871506\n",
            "Loss SS44:  0.07543548209400013\n",
            "Loss SS55:  0.08684626998115773\n",
            "Loss SS66:  0.08775473597291786\n",
            "Loss SS77:  0.0869562927048325\n",
            "Loss SS88:  0.09270218985290819\n",
            "Loss SS11:  0.08466174535975685\n",
            "Loss SS22:  0.07272836315785827\n",
            "Loss SS33:  0.08468357875008424\n",
            "Loss SS44:  0.07546984771355932\n",
            "Loss SS55:  0.08688090213770357\n",
            "Loss SS66:  0.08771756580065097\n",
            "Loss SS77:  0.08689890007361274\n",
            "Loss SS88:  0.09264492108813072\n",
            "Loss SS11:  0.08464574294158148\n",
            "Loss SS22:  0.0727046505412173\n",
            "Loss SS33:  0.0847014436146967\n",
            "Loss SS44:  0.07545828383111021\n",
            "Loss SS55:  0.0868008248375404\n",
            "Loss SS66:  0.08777714496829755\n",
            "Loss SS77:  0.08686895363589622\n",
            "Loss SS88:  0.09256958425893479\n",
            "Loss SS11:  0.08456776117029059\n",
            "Loss SS22:  0.07275443247605845\n",
            "Loss SS33:  0.08486461496025426\n",
            "Loss SS44:  0.07541148322298355\n",
            "Loss SS55:  0.08677530314299658\n",
            "Loss SS66:  0.08785967682440256\n",
            "Loss SS77:  0.08698072461123318\n",
            "Loss SS88:  0.09267013594252137\n",
            "Loss SS11:  0.084572789652205\n",
            "Loss SS22:  0.07272476944267948\n",
            "Loss SS33:  0.08477306989736336\n",
            "Loss SS44:  0.07539906826773751\n",
            "Loss SS55:  0.08674921548346745\n",
            "Loss SS66:  0.08782544276643037\n",
            "Loss SS77:  0.08705385848632287\n",
            "Loss SS88:  0.09258059991455553\n",
            "Loss SS11:  0.08454978228861112\n",
            "Loss SS22:  0.07266811658763042\n",
            "Loss SS33:  0.08470271633177325\n",
            "Loss SS44:  0.07533703151479411\n",
            "Loss SS55:  0.08669137259963242\n",
            "Loss SS66:  0.08771762786091715\n",
            "Loss SS77:  0.08697656280933086\n",
            "Loss SS88:  0.09252362319798331\n",
            "Loss SS11:  0.08460592664197969\n",
            "Loss SS22:  0.07268222271699772\n",
            "Loss SS33:  0.08476801959339332\n",
            "Loss SS44:  0.07539943906237774\n",
            "Loss SS55:  0.0867424733774313\n",
            "Loss SS66:  0.08771984657487394\n",
            "Loss SS77:  0.08710976478838103\n",
            "Loss SS88:  0.09256763526490916\n",
            "Loss SS11:  0.08457488589146346\n",
            "Loss SS22:  0.0726686611073795\n",
            "Loss SS33:  0.08473290223909649\n",
            "Loss SS44:  0.07529455894925083\n",
            "Loss SS55:  0.08671632186253625\n",
            "Loss SS66:  0.08761423893745572\n",
            "Loss SS77:  0.08703923817397605\n",
            "Loss SS88:  0.09257972368751048\n",
            "Loss SS11:  0.08462366697725313\n",
            "Loss SS22:  0.07269686837393867\n",
            "Loss SS33:  0.08476779992279769\n",
            "Loss SS44:  0.07527685708358141\n",
            "Loss SS55:  0.08672717213630676\n",
            "Loss SS66:  0.0875783772849617\n",
            "Loss SS77:  0.08697595879916221\n",
            "Loss SS88:  0.09258944626427816\n",
            "Loss SS11:  0.08457427310553031\n",
            "Loss SS22:  0.0726967670363069\n",
            "Loss SS33:  0.0847436389607242\n",
            "Loss SS44:  0.07521752224561155\n",
            "Loss SS55:  0.08670128060457034\n",
            "Loss SS66:  0.08763519710624999\n",
            "Loss SS77:  0.08693480587158448\n",
            "Loss SS88:  0.0925354826611671\n",
            "Loss SS11:  0.08458132406185868\n",
            "Loss SS22:  0.07264383456094443\n",
            "Loss SS33:  0.08464122596622504\n",
            "Loss SS44:  0.0750960980818047\n",
            "Loss SS55:  0.0866343818006423\n",
            "Loss SS66:  0.08748525036421509\n",
            "Loss SS77:  0.08694297715072156\n",
            "Loss SS88:  0.09249378546783468\n",
            "Loss SS11:  0.08459703222599955\n",
            "Loss SS22:  0.07262966487647067\n",
            "Loss SS33:  0.08457730104017129\n",
            "Loss SS44:  0.07509610071938957\n",
            "Loss SS55:  0.08658288531708268\n",
            "Loss SS66:  0.0874443267675423\n",
            "Loss SS77:  0.08689687850902665\n",
            "Loss SS88:  0.09253576763395993\n",
            "Loss SS11:  0.08468782322926158\n",
            "Loss SS22:  0.07259607089199419\n",
            "Loss SS33:  0.08449110867861374\n",
            "Loss SS44:  0.07508846780094575\n",
            "Loss SS55:  0.08656113446149927\n",
            "Loss SS66:  0.08733488552839425\n",
            "Loss SS77:  0.08685640906646183\n",
            "Loss SS88:  0.09252040864523314\n",
            "Loss SS11:  0.08463447714400718\n",
            "Loss SS22:  0.07260696356520628\n",
            "Loss SS33:  0.08443904218390165\n",
            "Loss SS44:  0.07509001722687955\n",
            "Loss SS55:  0.0864707439032662\n",
            "Loss SS66:  0.08729649457099188\n",
            "Loss SS77:  0.0867494426839187\n",
            "Loss SS88:  0.092455238046701\n",
            "Loss SS11:  0.08472192534559088\n",
            "Loss SS22:  0.07266083213865311\n",
            "Loss SS33:  0.08439878353900149\n",
            "Loss SS44:  0.07509593025518772\n",
            "Loss SS55:  0.08649964658473792\n",
            "Loss SS66:  0.087314111094671\n",
            "Loss SS77:  0.08673816950912784\n",
            "Loss SS88:  0.09253778886468036\n",
            "Loss SS11:  0.08476355082724796\n",
            "Loss SS22:  0.07264745225472752\n",
            "Loss SS33:  0.08436905634374224\n",
            "Loss SS44:  0.07511919491228686\n",
            "Loss SS55:  0.08649815093281786\n",
            "Loss SS66:  0.08730995571671321\n",
            "Loss SS77:  0.08671346836136495\n",
            "Loss SS88:  0.0925489050628495\n",
            "Loss SS11:  0.08478204960848543\n",
            "Loss SS22:  0.07268150199443597\n",
            "Loss SS33:  0.08439972375535058\n",
            "Loss SS44:  0.07514792295823187\n",
            "Loss SS55:  0.08656431338141479\n",
            "Loss SS66:  0.08737837379873507\n",
            "Loss SS77:  0.08677108297367843\n",
            "Loss SS88:  0.09257123944949085\n",
            "Loss SS11:  0.08475691837252154\n",
            "Loss SS22:  0.07259706528792526\n",
            "Loss SS33:  0.08436316725411161\n",
            "Loss SS44:  0.07505715933137869\n",
            "Loss SS55:  0.08650108426809311\n",
            "Loss SS66:  0.087298092719712\n",
            "Loss SS77:  0.08665871642650971\n",
            "Loss SS88:  0.09251254172930862\n",
            "Loss SS11:  0.08472227463984436\n",
            "Loss SS22:  0.0725660808806787\n",
            "Loss SS33:  0.08444103737131538\n",
            "Loss SS44:  0.07507669604689626\n",
            "Loss SS55:  0.08649427102468993\n",
            "Loss SS66:  0.08734870904765162\n",
            "Loss SS77:  0.08671249151905648\n",
            "Loss SS88:  0.09253388067154117\n",
            "Loss SS11:  0.08477345077597645\n",
            "Loss SS22:  0.07257507952908983\n",
            "Loss SS33:  0.08449039457906905\n",
            "Loss SS44:  0.07508392871549548\n",
            "Loss SS55:  0.08651693849235839\n",
            "Loss SS66:  0.08736913718960503\n",
            "Loss SS77:  0.08671975099458927\n",
            "Loss SS88:  0.09258736518032007\n",
            "Loss SS11:  0.08472497247304937\n",
            "Loss SS22:  0.07256978778511997\n",
            "Loss SS33:  0.08454219277729957\n",
            "Loss SS44:  0.07503838176129955\n",
            "Loss SS55:  0.08662013127721054\n",
            "Loss SS66:  0.0873764030263636\n",
            "Loss SS77:  0.08668772932828894\n",
            "Loss SS88:  0.09263268289299693\n",
            "Loss SS11:  0.08467849614513907\n",
            "Loss SS22:  0.07257476941763469\n",
            "Loss SS33:  0.08460152320041778\n",
            "Loss SS44:  0.07501927669893657\n",
            "Loss SS55:  0.08655549158715899\n",
            "Loss SS66:  0.08735214948907273\n",
            "Loss SS77:  0.08667373864450778\n",
            "Loss SS88:  0.09258707160339741\n",
            "Loss SS11:  0.0846687680780268\n",
            "Loss SS22:  0.07256911201654254\n",
            "Loss SS33:  0.08459231866186721\n",
            "Loss SS44:  0.07500285754394631\n",
            "Loss SS55:  0.08653893437925843\n",
            "Loss SS66:  0.08738246245654357\n",
            "Loss SS77:  0.08666016159892825\n",
            "Loss SS88:  0.09268959390150534\n",
            "Loss SS11:  0.08463541865955788\n",
            "Loss SS22:  0.07254416950031842\n",
            "Loss SS33:  0.08457489534757288\n",
            "Loss SS44:  0.07498851324414527\n",
            "Loss SS55:  0.0864765467596394\n",
            "Loss SS66:  0.08736982950732082\n",
            "Loss SS77:  0.08665987812817218\n",
            "Loss SS88:  0.09264828109935443\n",
            "Validation: \n",
            " Loss SS11:  0.07637932896614075\n",
            " Loss SS22:  0.10677491873502731\n",
            " Loss SS33:  0.11292939633131027\n",
            " Loss SS55:  0.10240358859300613\n",
            " Loss SS66:  0.12480705231428146\n",
            " Loss SS77:  0.11240144819021225\n",
            " Loss SS88:  0.11979690939188004\n",
            " Loss SS99:  0.1580406129360199\n",
            " Loss SS11:  0.09263351488681067\n",
            " Loss SS22:  0.1145140290969894\n",
            " Loss SS33:  0.11924729531719572\n",
            " Loss SS55:  0.11722410044499806\n",
            " Loss SS66:  0.13778608363299144\n",
            " Loss SS77:  0.13135825594266257\n",
            " Loss SS88:  0.14005102891297566\n",
            " Loss SS99:  0.15581405091853368\n",
            " Loss SS11:  0.08992106939961271\n",
            " Loss SS22:  0.1120339523001415\n",
            " Loss SS33:  0.11893083791180355\n",
            " Loss SS55:  0.11619605723677612\n",
            " Loss SS66:  0.13719249026077548\n",
            " Loss SS77:  0.132954536051285\n",
            " Loss SS88:  0.14163201865626546\n",
            " Loss SS99:  0.15591493794103947\n",
            " Loss SS11:  0.08916329055047426\n",
            " Loss SS22:  0.11078760492019966\n",
            " Loss SS33:  0.11730660351573444\n",
            " Loss SS55:  0.1151598616213095\n",
            " Loss SS66:  0.13563545341374444\n",
            " Loss SS77:  0.13128165729710314\n",
            " Loss SS88:  0.14090019635489728\n",
            " Loss SS99:  0.1527129484981787\n",
            " Loss SS11:  0.08842859233235136\n",
            " Loss SS22:  0.11051499604442973\n",
            " Loss SS33:  0.11710956913453562\n",
            " Loss SS55:  0.11524368390256975\n",
            " Loss SS66:  0.13445418374038037\n",
            " Loss SS77:  0.1310150690468741\n",
            " Loss SS88:  0.1398914341389397\n",
            " Loss SS99:  0.15272968731544637\n",
            "\n",
            "Epoch: 60\n",
            "Loss SS11:  0.07672508805990219\n",
            "Loss SS22:  0.06839433312416077\n",
            "Loss SS33:  0.08364177495241165\n",
            "Loss SS44:  0.07476307451725006\n",
            "Loss SS55:  0.0799519494175911\n",
            "Loss SS66:  0.08691198378801346\n",
            "Loss SS77:  0.0890972763299942\n",
            "Loss SS88:  0.0900978147983551\n",
            "Loss SS11:  0.08528257838704369\n",
            "Loss SS22:  0.07432065023617311\n",
            "Loss SS33:  0.08602693541483446\n",
            "Loss SS44:  0.07485059851949866\n",
            "Loss SS55:  0.0911143801429055\n",
            "Loss SS66:  0.08836413039402528\n",
            "Loss SS77:  0.09299196438355879\n",
            "Loss SS88:  0.09514898332682523\n",
            "Loss SS11:  0.0846347879795801\n",
            "Loss SS22:  0.07345406498227801\n",
            "Loss SS33:  0.08673435981784548\n",
            "Loss SS44:  0.07559613422268913\n",
            "Loss SS55:  0.08861471322320756\n",
            "Loss SS66:  0.09010229153292519\n",
            "Loss SS77:  0.09085820402417864\n",
            "Loss SS88:  0.09415291391667865\n",
            "Loss SS11:  0.08458002272152132\n",
            "Loss SS22:  0.07262437129693647\n",
            "Loss SS33:  0.08595785618789735\n",
            "Loss SS44:  0.0749961731414641\n",
            "Loss SS55:  0.08656940801489738\n",
            "Loss SS66:  0.0879189996950088\n",
            "Loss SS77:  0.08973508977120923\n",
            "Loss SS88:  0.09244615632680155\n",
            "Loss SS11:  0.0853448690074246\n",
            "Loss SS22:  0.07262467265855975\n",
            "Loss SS33:  0.08568293528585899\n",
            "Loss SS44:  0.07512171094010515\n",
            "Loss SS55:  0.08700491924111436\n",
            "Loss SS66:  0.0874538227188878\n",
            "Loss SS77:  0.08861042740868359\n",
            "Loss SS88:  0.0927431567049608\n",
            "Loss SS11:  0.08554258358244803\n",
            "Loss SS22:  0.07300115771153394\n",
            "Loss SS33:  0.08642355279595244\n",
            "Loss SS44:  0.0756698298980208\n",
            "Loss SS55:  0.08777306287312041\n",
            "Loss SS66:  0.08819197688032598\n",
            "Loss SS77:  0.08824982055846382\n",
            "Loss SS88:  0.09345358040402918\n",
            "Loss SS11:  0.08566624414725382\n",
            "Loss SS22:  0.0731569698355237\n",
            "Loss SS33:  0.0860588181702817\n",
            "Loss SS44:  0.0753917293470414\n",
            "Loss SS55:  0.0873343865158128\n",
            "Loss SS66:  0.08855980121698535\n",
            "Loss SS77:  0.08795346783809975\n",
            "Loss SS88:  0.09362917161378705\n",
            "Loss SS11:  0.08522527696381152\n",
            "Loss SS22:  0.07289878362920922\n",
            "Loss SS33:  0.0855746134905748\n",
            "Loss SS44:  0.07551730581572358\n",
            "Loss SS55:  0.08676363034567362\n",
            "Loss SS66:  0.08814571073777239\n",
            "Loss SS77:  0.087598291920944\n",
            "Loss SS88:  0.09367614974018554\n",
            "Loss SS11:  0.08489435993962818\n",
            "Loss SS22:  0.07298456132411957\n",
            "Loss SS33:  0.0856406511164006\n",
            "Loss SS44:  0.0755686229209841\n",
            "Loss SS55:  0.08670989019267353\n",
            "Loss SS66:  0.08810657051242428\n",
            "Loss SS77:  0.08749060995048946\n",
            "Loss SS88:  0.09375040297522957\n",
            "Loss SS11:  0.08514353141679869\n",
            "Loss SS22:  0.07286957966593596\n",
            "Loss SS33:  0.0855943548319104\n",
            "Loss SS44:  0.07561570602458911\n",
            "Loss SS55:  0.08668649622372218\n",
            "Loss SS66:  0.08832324550047026\n",
            "Loss SS77:  0.08713823606024731\n",
            "Loss SS88:  0.09350747836160135\n",
            "Loss SS11:  0.08490913649006646\n",
            "Loss SS22:  0.07257501932211441\n",
            "Loss SS33:  0.08527188512063262\n",
            "Loss SS44:  0.07535184285428265\n",
            "Loss SS55:  0.08632536763601964\n",
            "Loss SS66:  0.0878695866996699\n",
            "Loss SS77:  0.08648271748039982\n",
            "Loss SS88:  0.09323259789754848\n",
            "Loss SS11:  0.08499198024337357\n",
            "Loss SS22:  0.07232241631225422\n",
            "Loss SS33:  0.08507831985349054\n",
            "Loss SS44:  0.07518325840030704\n",
            "Loss SS55:  0.08611057275855863\n",
            "Loss SS66:  0.0874603364515949\n",
            "Loss SS77:  0.08609996138660757\n",
            "Loss SS88:  0.0929617536766035\n",
            "Loss SS11:  0.08481489700719344\n",
            "Loss SS22:  0.07234032276617594\n",
            "Loss SS33:  0.08509307822658996\n",
            "Loss SS44:  0.0751303404815926\n",
            "Loss SS55:  0.08611550255994166\n",
            "Loss SS66:  0.0874552216288472\n",
            "Loss SS77:  0.08605128305017455\n",
            "Loss SS88:  0.09285023802321804\n",
            "Loss SS11:  0.0844817679119474\n",
            "Loss SS22:  0.07238984685249002\n",
            "Loss SS33:  0.08512463602628417\n",
            "Loss SS44:  0.07523209635765497\n",
            "Loss SS55:  0.08623046268716113\n",
            "Loss SS66:  0.08745728438819637\n",
            "Loss SS77:  0.08611770116418373\n",
            "Loss SS88:  0.09300734097038517\n",
            "Loss SS11:  0.08446394604571322\n",
            "Loss SS22:  0.07227616485023329\n",
            "Loss SS33:  0.08510433475599222\n",
            "Loss SS44:  0.07522840914151348\n",
            "Loss SS55:  0.08619672350638302\n",
            "Loss SS66:  0.08761816191757825\n",
            "Loss SS77:  0.08602409815111904\n",
            "Loss SS88:  0.09280134372888728\n",
            "Loss SS11:  0.08469713804934988\n",
            "Loss SS22:  0.07243899850557182\n",
            "Loss SS33:  0.0851300491007748\n",
            "Loss SS44:  0.07535059320808246\n",
            "Loss SS55:  0.08638273401568268\n",
            "Loss SS66:  0.0877525100348801\n",
            "Loss SS77:  0.08592973027797725\n",
            "Loss SS88:  0.09298834801707047\n",
            "Loss SS11:  0.0845470301205327\n",
            "Loss SS22:  0.072197251484631\n",
            "Loss SS33:  0.08494842589271735\n",
            "Loss SS44:  0.0750393824686545\n",
            "Loss SS55:  0.08627343089869303\n",
            "Loss SS66:  0.08747842176730589\n",
            "Loss SS77:  0.08563261108924143\n",
            "Loss SS88:  0.09281879469104436\n",
            "Loss SS11:  0.08435885446984866\n",
            "Loss SS22:  0.07212546816346241\n",
            "Loss SS33:  0.0848162720600764\n",
            "Loss SS44:  0.07502958163270476\n",
            "Loss SS55:  0.08631760535532967\n",
            "Loss SS66:  0.08732510558520144\n",
            "Loss SS77:  0.08560094389824839\n",
            "Loss SS88:  0.092749431816458\n",
            "Loss SS11:  0.08443757916353026\n",
            "Loss SS22:  0.07221271785089324\n",
            "Loss SS33:  0.0849476737788369\n",
            "Loss SS44:  0.07501254800851173\n",
            "Loss SS55:  0.08642616793924933\n",
            "Loss SS66:  0.08728707429453812\n",
            "Loss SS77:  0.08560134165853427\n",
            "Loss SS88:  0.09269290207499299\n",
            "Loss SS11:  0.08455469900088784\n",
            "Loss SS22:  0.07214529006815082\n",
            "Loss SS33:  0.08488017835542169\n",
            "Loss SS44:  0.0749606699804673\n",
            "Loss SS55:  0.08631059983793977\n",
            "Loss SS66:  0.08742027940394366\n",
            "Loss SS77:  0.08548383044166714\n",
            "Loss SS88:  0.09274317620624423\n",
            "Loss SS11:  0.08460309526961834\n",
            "Loss SS22:  0.07213979735229145\n",
            "Loss SS33:  0.08486052265214683\n",
            "Loss SS44:  0.0748261093389039\n",
            "Loss SS55:  0.08632655293490757\n",
            "Loss SS66:  0.08741219905181903\n",
            "Loss SS77:  0.08570091385598207\n",
            "Loss SS88:  0.09274045908035923\n",
            "Loss SS11:  0.08487570794272761\n",
            "Loss SS22:  0.0721181241786593\n",
            "Loss SS33:  0.08488615058468417\n",
            "Loss SS44:  0.07485032334076284\n",
            "Loss SS55:  0.08634945308851405\n",
            "Loss SS66:  0.0874039488028011\n",
            "Loss SS77:  0.08581903470918466\n",
            "Loss SS88:  0.09271229062035186\n",
            "Loss SS11:  0.08517362372907578\n",
            "Loss SS22:  0.07217366017434931\n",
            "Loss SS33:  0.0850644809155982\n",
            "Loss SS44:  0.07490992837699291\n",
            "Loss SS55:  0.08653220217422122\n",
            "Loss SS66:  0.08761175209445651\n",
            "Loss SS77:  0.08607786081360476\n",
            "Loss SS88:  0.09287491750932926\n",
            "Loss SS11:  0.08527804636981064\n",
            "Loss SS22:  0.07225412880251934\n",
            "Loss SS33:  0.08505534509688745\n",
            "Loss SS44:  0.07496340909884089\n",
            "Loss SS55:  0.08646599009826586\n",
            "Loss SS66:  0.08754345229564807\n",
            "Loss SS77:  0.08623152023011987\n",
            "Loss SS88:  0.0929069932856601\n",
            "Loss SS11:  0.08540972037805067\n",
            "Loss SS22:  0.07239729934275398\n",
            "Loss SS33:  0.08512886839288894\n",
            "Loss SS44:  0.07497330929666635\n",
            "Loss SS55:  0.0865388261579379\n",
            "Loss SS66:  0.0878002488081386\n",
            "Loss SS77:  0.08631311721821544\n",
            "Loss SS88:  0.09307378966407658\n",
            "Loss SS11:  0.08527958523704711\n",
            "Loss SS22:  0.07239569683533266\n",
            "Loss SS33:  0.08499564177011588\n",
            "Loss SS44:  0.07503545371837825\n",
            "Loss SS55:  0.08660520304484197\n",
            "Loss SS66:  0.0877626035733527\n",
            "Loss SS77:  0.08631966604417064\n",
            "Loss SS88:  0.0931150480154976\n",
            "Loss SS11:  0.08526101114649426\n",
            "Loss SS22:  0.07247951925086335\n",
            "Loss SS33:  0.08512291120957598\n",
            "Loss SS44:  0.07515036461739248\n",
            "Loss SS55:  0.08682205031315486\n",
            "Loss SS66:  0.08790088407838025\n",
            "Loss SS77:  0.08638305533891437\n",
            "Loss SS88:  0.09318367845710666\n",
            "Loss SS11:  0.0852180269866412\n",
            "Loss SS22:  0.07261883697001696\n",
            "Loss SS33:  0.08517733007339534\n",
            "Loss SS44:  0.07516921840855556\n",
            "Loss SS55:  0.08691852009164451\n",
            "Loss SS66:  0.08785625016557334\n",
            "Loss SS77:  0.0863842032437395\n",
            "Loss SS88:  0.09315786532700282\n",
            "Loss SS11:  0.08522432101260725\n",
            "Loss SS22:  0.07260018902635235\n",
            "Loss SS33:  0.08514522201650083\n",
            "Loss SS44:  0.0751553739278554\n",
            "Loss SS55:  0.08701397908115727\n",
            "Loss SS66:  0.08788304775953293\n",
            "Loss SS77:  0.08631571016383766\n",
            "Loss SS88:  0.0932389114091829\n",
            "Loss SS11:  0.08508401036364925\n",
            "Loss SS22:  0.07259547006326034\n",
            "Loss SS33:  0.08524939474166464\n",
            "Loss SS44:  0.07519655396899406\n",
            "Loss SS55:  0.08713645275515788\n",
            "Loss SS66:  0.08782339597895383\n",
            "Loss SS77:  0.08644141179999125\n",
            "Loss SS88:  0.09325158224789958\n",
            "Loss SS11:  0.08518701952656242\n",
            "Loss SS22:  0.07260896845958954\n",
            "Loss SS33:  0.08528207673186876\n",
            "Loss SS44:  0.07512870359163348\n",
            "Loss SS55:  0.08713218552428623\n",
            "Loss SS66:  0.087855717940782\n",
            "Loss SS77:  0.08633834903323373\n",
            "Loss SS88:  0.09314424646059144\n",
            "Loss SS11:  0.08521115899660962\n",
            "Loss SS22:  0.07251449027532933\n",
            "Loss SS33:  0.08518764081971054\n",
            "Loss SS44:  0.07506549684181091\n",
            "Loss SS55:  0.0870554618225987\n",
            "Loss SS66:  0.08778337638381976\n",
            "Loss SS77:  0.08623718000780731\n",
            "Loss SS88:  0.09304133943997779\n",
            "Loss SS11:  0.08526647893996253\n",
            "Loss SS22:  0.07246456272041314\n",
            "Loss SS33:  0.08520997106750434\n",
            "Loss SS44:  0.07503160117107018\n",
            "Loss SS55:  0.08711246832787434\n",
            "Loss SS66:  0.087774088612041\n",
            "Loss SS77:  0.08623583979231546\n",
            "Loss SS88:  0.09315487352487083\n",
            "Loss SS11:  0.08524075821267153\n",
            "Loss SS22:  0.0724831583636765\n",
            "Loss SS33:  0.08509780034377856\n",
            "Loss SS44:  0.0749595980989969\n",
            "Loss SS55:  0.08699800488271742\n",
            "Loss SS66:  0.08765770269844828\n",
            "Loss SS77:  0.0861232555794932\n",
            "Loss SS88:  0.09302758580788385\n",
            "Loss SS11:  0.08543414954653233\n",
            "Loss SS22:  0.07247697277939565\n",
            "Loss SS33:  0.0851640143702107\n",
            "Loss SS44:  0.07494566021514429\n",
            "Loss SS55:  0.0870410912722087\n",
            "Loss SS66:  0.08767802405916701\n",
            "Loss SS77:  0.08615016452203399\n",
            "Loss SS88:  0.09312812265150708\n",
            "Loss SS11:  0.08538342778838938\n",
            "Loss SS22:  0.07250300836231974\n",
            "Loss SS33:  0.08516568249022519\n",
            "Loss SS44:  0.07491029526221107\n",
            "Loss SS55:  0.0870590346873316\n",
            "Loss SS66:  0.0877186810104256\n",
            "Loss SS77:  0.0861579692389211\n",
            "Loss SS88:  0.09304893880742907\n",
            "Loss SS11:  0.08538337402726805\n",
            "Loss SS22:  0.0724299831155925\n",
            "Loss SS33:  0.08505304073115134\n",
            "Loss SS44:  0.07487765025770565\n",
            "Loss SS55:  0.08696738731662983\n",
            "Loss SS66:  0.08760785777895734\n",
            "Loss SS77:  0.08604102620953008\n",
            "Loss SS88:  0.09299200833091445\n",
            "Loss SS11:  0.08534972512256103\n",
            "Loss SS22:  0.0724338124583995\n",
            "Loss SS33:  0.08506806769139683\n",
            "Loss SS44:  0.07488860644781686\n",
            "Loss SS55:  0.08688689308426772\n",
            "Loss SS66:  0.08757699076136488\n",
            "Loss SS77:  0.08604580481258042\n",
            "Loss SS88:  0.09306136504659113\n",
            "Loss SS11:  0.08527228009356601\n",
            "Loss SS22:  0.0723794659800104\n",
            "Loss SS33:  0.08495847309824675\n",
            "Loss SS44:  0.0748411192218932\n",
            "Loss SS55:  0.0867469326600315\n",
            "Loss SS66:  0.08745737835334667\n",
            "Loss SS77:  0.08599721053688544\n",
            "Loss SS88:  0.09296773728032125\n",
            "Loss SS11:  0.08521667995568737\n",
            "Loss SS22:  0.07233020661355895\n",
            "Loss SS33:  0.08488571607624479\n",
            "Loss SS44:  0.07480128146612736\n",
            "Loss SS55:  0.08663767675304657\n",
            "Loss SS66:  0.08739639069799267\n",
            "Loss SS77:  0.08593218842202135\n",
            "Loss SS88:  0.09292703908880043\n",
            "Loss SS11:  0.08526224958852045\n",
            "Loss SS22:  0.07239308557531185\n",
            "Loss SS33:  0.08494833572547038\n",
            "Loss SS44:  0.07484594454751942\n",
            "Loss SS55:  0.08667486229739582\n",
            "Loss SS66:  0.08744573827247668\n",
            "Loss SS77:  0.08600238413361837\n",
            "Loss SS88:  0.09302192521362827\n",
            "Loss SS11:  0.0851799515223271\n",
            "Loss SS22:  0.07243352008126948\n",
            "Loss SS33:  0.08492829066921034\n",
            "Loss SS44:  0.07484880596435563\n",
            "Loss SS55:  0.08661316694134344\n",
            "Loss SS66:  0.08748861235496191\n",
            "Loss SS77:  0.08591516939537949\n",
            "Loss SS88:  0.09300030419861313\n",
            "Loss SS11:  0.0852386191522707\n",
            "Loss SS22:  0.0724816431696228\n",
            "Loss SS33:  0.0849689330525183\n",
            "Loss SS44:  0.07488168434476626\n",
            "Loss SS55:  0.08667791151660638\n",
            "Loss SS66:  0.08750698788007479\n",
            "Loss SS77:  0.08600892662789646\n",
            "Loss SS88:  0.09313235040798323\n",
            "Loss SS11:  0.08517717328052234\n",
            "Loss SS22:  0.07242181652556162\n",
            "Loss SS33:  0.08490682891170276\n",
            "Loss SS44:  0.07485454536438542\n",
            "Loss SS55:  0.0865839129430512\n",
            "Loss SS66:  0.08745251056627995\n",
            "Loss SS77:  0.08593384802272314\n",
            "Loss SS88:  0.09305698308756623\n",
            "Loss SS11:  0.08516257295127358\n",
            "Loss SS22:  0.07238907046835709\n",
            "Loss SS33:  0.0849451009614938\n",
            "Loss SS44:  0.07483382127722915\n",
            "Loss SS55:  0.08659739461210039\n",
            "Loss SS66:  0.0874460147790898\n",
            "Loss SS77:  0.08594408575572124\n",
            "Loss SS88:  0.09309522101397957\n",
            "Loss SS11:  0.08521690639252144\n",
            "Loss SS22:  0.07235868912842744\n",
            "Loss SS33:  0.08496791902442201\n",
            "Loss SS44:  0.07486682459546827\n",
            "Loss SS55:  0.0866709061280851\n",
            "Loss SS66:  0.08751163215370242\n",
            "Loss SS77:  0.08598181841576971\n",
            "Loss SS88:  0.0931462001906266\n",
            "Loss SS11:  0.08511650363496483\n",
            "Loss SS22:  0.07229687449665752\n",
            "Loss SS33:  0.08499927434124813\n",
            "Loss SS44:  0.07483111785769204\n",
            "Loss SS55:  0.08674305593372685\n",
            "Loss SS66:  0.0875057517480178\n",
            "Loss SS77:  0.08598358024097576\n",
            "Loss SS88:  0.09313933502826152\n",
            "Loss SS11:  0.08509170895169495\n",
            "Loss SS22:  0.0722702852353422\n",
            "Loss SS33:  0.08492516422537481\n",
            "Loss SS44:  0.07481165709579067\n",
            "Loss SS55:  0.08673596970594613\n",
            "Loss SS66:  0.08748375059692723\n",
            "Loss SS77:  0.08598124134629143\n",
            "Loss SS88:  0.09308341037948673\n",
            "Loss SS11:  0.08509267279549101\n",
            "Loss SS22:  0.07228714480159684\n",
            "Loss SS33:  0.0849296294942715\n",
            "Loss SS44:  0.0748052438120832\n",
            "Loss SS55:  0.08666563802350336\n",
            "Loss SS66:  0.08753509220053401\n",
            "Loss SS77:  0.08600216010380188\n",
            "Loss SS88:  0.0930654502739034\n",
            "Loss SS11:  0.0851438586010224\n",
            "Loss SS22:  0.07222543458042709\n",
            "Loss SS33:  0.08491732383454162\n",
            "Loss SS44:  0.07480726530680588\n",
            "Loss SS55:  0.08664070471908798\n",
            "Loss SS66:  0.08753223703555808\n",
            "Loss SS77:  0.08600572387397654\n",
            "Loss SS88:  0.09308878063791395\n",
            "Validation: \n",
            " Loss SS11:  0.08365841954946518\n",
            " Loss SS22:  0.10761956870555878\n",
            " Loss SS33:  0.1117439791560173\n",
            " Loss SS55:  0.10348308086395264\n",
            " Loss SS66:  0.1244552955031395\n",
            " Loss SS77:  0.11330394446849823\n",
            " Loss SS88:  0.1173764020204544\n",
            " Loss SS99:  0.1536172777414322\n",
            " Loss SS11:  0.0967834350608644\n",
            " Loss SS22:  0.11449473102887471\n",
            " Loss SS33:  0.1220983692577907\n",
            " Loss SS55:  0.11843443378096535\n",
            " Loss SS66:  0.1383308544754982\n",
            " Loss SS77:  0.1309649028948375\n",
            " Loss SS88:  0.1389886655268215\n",
            " Loss SS99:  0.14957040406408764\n",
            " Loss SS11:  0.09345243871212006\n",
            " Loss SS22:  0.11199771758259797\n",
            " Loss SS33:  0.12172531481923127\n",
            " Loss SS55:  0.11741999863851361\n",
            " Loss SS66:  0.13779252785735013\n",
            " Loss SS77:  0.13228344626542998\n",
            " Loss SS88:  0.14031494045402945\n",
            " Loss SS99:  0.14896734676709988\n",
            " Loss SS11:  0.09262143454102219\n",
            " Loss SS22:  0.11068333734254368\n",
            " Loss SS33:  0.12005408059378139\n",
            " Loss SS55:  0.11608798704186424\n",
            " Loss SS66:  0.13589913410241486\n",
            " Loss SS77:  0.13037250716178145\n",
            " Loss SS88:  0.1391009264549271\n",
            " Loss SS99:  0.14587051983250945\n",
            " Loss SS11:  0.09173475003536836\n",
            " Loss SS22:  0.11017405729234954\n",
            " Loss SS33:  0.11964639359050327\n",
            " Loss SS55:  0.11619998100731108\n",
            " Loss SS66:  0.13483985320285516\n",
            " Loss SS77:  0.1299234637875616\n",
            " Loss SS88:  0.13783089899354511\n",
            " Loss SS99:  0.1456783692593928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net4(nn.Module):\n",
        "    def __init__(self, ss11,ss22,ss33,ss44,ss55,ss66,ss77,ss88):\n",
        "        super(Net4, self).__init__()\n",
        "        self.ss11 = ss11\n",
        "        self.ss22 = ss22\n",
        "        self.ss33 = ss33\n",
        "        self.ss44 = ss44\n",
        "        self.ss55 = ss55\n",
        "        self.ss66 = ss66\n",
        "        self.ss77 = ss77\n",
        "        self.ss88 = ss88\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.ss11(x)\n",
        "        out2 = self.ss22(x)\n",
        "        out3 = self.ss33(x)\n",
        "        out4 = self.ss44(x)\n",
        "        out5 = self.ss55(x)\n",
        "        out6 = self.ss66(x)\n",
        "        out7 = self.ss77(x)\n",
        "        out8 = self.ss88(x)\n",
        "\n",
        "        out = torch.cat((out1,out2,out3,out4,out5,out6,out7,out8),1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "net8students = Net4( ss11,ss22,ss33,ss44,ss55,ss66,ss77,ss88)\n",
        "net8students.to(device)\n",
        "summary(net8students, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPbCl6H0KSJX",
        "outputId": "bedbc962-0ba1-4314-dfa0-d7be215b6397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "              VGG-40                   [-1, 64]               0\n",
            "           Conv2d-41           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-42           [-1, 32, 32, 32]              64\n",
            "             ReLU-43           [-1, 32, 32, 32]               0\n",
            "           Conv2d-44           [-1, 32, 32, 32]           9,248\n",
            "      BatchNorm2d-45           [-1, 32, 32, 32]              64\n",
            "             ReLU-46           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-47           [-1, 32, 16, 16]               0\n",
            "           Conv2d-48           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-49           [-1, 32, 16, 16]              64\n",
            "             ReLU-50           [-1, 32, 16, 16]               0\n",
            "           Conv2d-51           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-52           [-1, 32, 16, 16]              64\n",
            "             ReLU-53           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-54             [-1, 32, 8, 8]               0\n",
            "           Conv2d-55             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-56             [-1, 32, 8, 8]              64\n",
            "             ReLU-57             [-1, 32, 8, 8]               0\n",
            "           Conv2d-58             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-59             [-1, 64, 8, 8]             128\n",
            "             ReLU-60             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-61             [-1, 64, 4, 4]               0\n",
            "           Conv2d-62             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-63             [-1, 64, 4, 4]             128\n",
            "             ReLU-64             [-1, 64, 4, 4]               0\n",
            "           Conv2d-65             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-66             [-1, 64, 4, 4]             128\n",
            "             ReLU-67             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-68             [-1, 64, 2, 2]               0\n",
            "           Conv2d-69             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-70             [-1, 64, 2, 2]             128\n",
            "             ReLU-71             [-1, 64, 2, 2]               0\n",
            "           Conv2d-72             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-73             [-1, 64, 2, 2]             128\n",
            "             ReLU-74             [-1, 64, 2, 2]               0\n",
            "           Conv2d-75             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-76             [-1, 64, 2, 2]             128\n",
            "             ReLU-77             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-78             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-79             [-1, 64, 1, 1]               0\n",
            "              VGG-80                   [-1, 64]               0\n",
            "           Conv2d-81           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-82           [-1, 32, 32, 32]              64\n",
            "             ReLU-83           [-1, 32, 32, 32]               0\n",
            "           Conv2d-84           [-1, 32, 32, 32]           9,248\n",
            "      BatchNorm2d-85           [-1, 32, 32, 32]              64\n",
            "             ReLU-86           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-87           [-1, 32, 16, 16]               0\n",
            "           Conv2d-88           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-89           [-1, 32, 16, 16]              64\n",
            "             ReLU-90           [-1, 32, 16, 16]               0\n",
            "           Conv2d-91           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-92           [-1, 32, 16, 16]              64\n",
            "             ReLU-93           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-94             [-1, 32, 8, 8]               0\n",
            "           Conv2d-95             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-96             [-1, 32, 8, 8]              64\n",
            "             ReLU-97             [-1, 32, 8, 8]               0\n",
            "           Conv2d-98             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-99             [-1, 64, 8, 8]             128\n",
            "            ReLU-100             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-101             [-1, 64, 4, 4]               0\n",
            "          Conv2d-102             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-103             [-1, 64, 4, 4]             128\n",
            "            ReLU-104             [-1, 64, 4, 4]               0\n",
            "          Conv2d-105             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-106             [-1, 64, 4, 4]             128\n",
            "            ReLU-107             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-108             [-1, 64, 2, 2]               0\n",
            "          Conv2d-109             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-110             [-1, 64, 2, 2]             128\n",
            "            ReLU-111             [-1, 64, 2, 2]               0\n",
            "          Conv2d-112             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-113             [-1, 64, 2, 2]             128\n",
            "            ReLU-114             [-1, 64, 2, 2]               0\n",
            "          Conv2d-115             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-116             [-1, 64, 2, 2]             128\n",
            "            ReLU-117             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-118             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-119             [-1, 64, 1, 1]               0\n",
            "             VGG-120                   [-1, 64]               0\n",
            "          Conv2d-121           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-122           [-1, 32, 32, 32]              64\n",
            "            ReLU-123           [-1, 32, 32, 32]               0\n",
            "          Conv2d-124           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-125           [-1, 32, 32, 32]              64\n",
            "            ReLU-126           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-127           [-1, 32, 16, 16]               0\n",
            "          Conv2d-128           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-129           [-1, 32, 16, 16]              64\n",
            "            ReLU-130           [-1, 32, 16, 16]               0\n",
            "          Conv2d-131           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-132           [-1, 32, 16, 16]              64\n",
            "            ReLU-133           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-134             [-1, 32, 8, 8]               0\n",
            "          Conv2d-135             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-136             [-1, 32, 8, 8]              64\n",
            "            ReLU-137             [-1, 32, 8, 8]               0\n",
            "          Conv2d-138             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-139             [-1, 64, 8, 8]             128\n",
            "            ReLU-140             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-141             [-1, 64, 4, 4]               0\n",
            "          Conv2d-142             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-143             [-1, 64, 4, 4]             128\n",
            "            ReLU-144             [-1, 64, 4, 4]               0\n",
            "          Conv2d-145             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-146             [-1, 64, 4, 4]             128\n",
            "            ReLU-147             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-148             [-1, 64, 2, 2]               0\n",
            "          Conv2d-149             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-150             [-1, 64, 2, 2]             128\n",
            "            ReLU-151             [-1, 64, 2, 2]               0\n",
            "          Conv2d-152             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-153             [-1, 64, 2, 2]             128\n",
            "            ReLU-154             [-1, 64, 2, 2]               0\n",
            "          Conv2d-155             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-156             [-1, 64, 2, 2]             128\n",
            "            ReLU-157             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-158             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-159             [-1, 64, 1, 1]               0\n",
            "             VGG-160                   [-1, 64]               0\n",
            "          Conv2d-161           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-162           [-1, 32, 32, 32]              64\n",
            "            ReLU-163           [-1, 32, 32, 32]               0\n",
            "          Conv2d-164           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-165           [-1, 32, 32, 32]              64\n",
            "            ReLU-166           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-167           [-1, 32, 16, 16]               0\n",
            "          Conv2d-168           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-169           [-1, 32, 16, 16]              64\n",
            "            ReLU-170           [-1, 32, 16, 16]               0\n",
            "          Conv2d-171           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-172           [-1, 32, 16, 16]              64\n",
            "            ReLU-173           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-174             [-1, 32, 8, 8]               0\n",
            "          Conv2d-175             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-176             [-1, 32, 8, 8]              64\n",
            "            ReLU-177             [-1, 32, 8, 8]               0\n",
            "          Conv2d-178             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-179             [-1, 64, 8, 8]             128\n",
            "            ReLU-180             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-181             [-1, 64, 4, 4]               0\n",
            "          Conv2d-182             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-183             [-1, 64, 4, 4]             128\n",
            "            ReLU-184             [-1, 64, 4, 4]               0\n",
            "          Conv2d-185             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-186             [-1, 64, 4, 4]             128\n",
            "            ReLU-187             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-188             [-1, 64, 2, 2]               0\n",
            "          Conv2d-189             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-190             [-1, 64, 2, 2]             128\n",
            "            ReLU-191             [-1, 64, 2, 2]               0\n",
            "          Conv2d-192             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-193             [-1, 64, 2, 2]             128\n",
            "            ReLU-194             [-1, 64, 2, 2]               0\n",
            "          Conv2d-195             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-196             [-1, 64, 2, 2]             128\n",
            "            ReLU-197             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-198             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-199             [-1, 64, 1, 1]               0\n",
            "             VGG-200                   [-1, 64]               0\n",
            "          Conv2d-201           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-202           [-1, 32, 32, 32]              64\n",
            "            ReLU-203           [-1, 32, 32, 32]               0\n",
            "          Conv2d-204           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-205           [-1, 32, 32, 32]              64\n",
            "            ReLU-206           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-207           [-1, 32, 16, 16]               0\n",
            "          Conv2d-208           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-209           [-1, 32, 16, 16]              64\n",
            "            ReLU-210           [-1, 32, 16, 16]               0\n",
            "          Conv2d-211           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-212           [-1, 32, 16, 16]              64\n",
            "            ReLU-213           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-214             [-1, 32, 8, 8]               0\n",
            "          Conv2d-215             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-216             [-1, 32, 8, 8]              64\n",
            "            ReLU-217             [-1, 32, 8, 8]               0\n",
            "          Conv2d-218             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-219             [-1, 64, 8, 8]             128\n",
            "            ReLU-220             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-221             [-1, 64, 4, 4]               0\n",
            "          Conv2d-222             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-223             [-1, 64, 4, 4]             128\n",
            "            ReLU-224             [-1, 64, 4, 4]               0\n",
            "          Conv2d-225             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-226             [-1, 64, 4, 4]             128\n",
            "            ReLU-227             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-228             [-1, 64, 2, 2]               0\n",
            "          Conv2d-229             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-230             [-1, 64, 2, 2]             128\n",
            "            ReLU-231             [-1, 64, 2, 2]               0\n",
            "          Conv2d-232             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-233             [-1, 64, 2, 2]             128\n",
            "            ReLU-234             [-1, 64, 2, 2]               0\n",
            "          Conv2d-235             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-236             [-1, 64, 2, 2]             128\n",
            "            ReLU-237             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-238             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-239             [-1, 64, 1, 1]               0\n",
            "             VGG-240                   [-1, 64]               0\n",
            "          Conv2d-241           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-242           [-1, 32, 32, 32]              64\n",
            "            ReLU-243           [-1, 32, 32, 32]               0\n",
            "          Conv2d-244           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-245           [-1, 32, 32, 32]              64\n",
            "            ReLU-246           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-247           [-1, 32, 16, 16]               0\n",
            "          Conv2d-248           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-249           [-1, 32, 16, 16]              64\n",
            "            ReLU-250           [-1, 32, 16, 16]               0\n",
            "          Conv2d-251           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-252           [-1, 32, 16, 16]              64\n",
            "            ReLU-253           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-254             [-1, 32, 8, 8]               0\n",
            "          Conv2d-255             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-256             [-1, 32, 8, 8]              64\n",
            "            ReLU-257             [-1, 32, 8, 8]               0\n",
            "          Conv2d-258             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-259             [-1, 64, 8, 8]             128\n",
            "            ReLU-260             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-261             [-1, 64, 4, 4]               0\n",
            "          Conv2d-262             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-263             [-1, 64, 4, 4]             128\n",
            "            ReLU-264             [-1, 64, 4, 4]               0\n",
            "          Conv2d-265             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-266             [-1, 64, 4, 4]             128\n",
            "            ReLU-267             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-268             [-1, 64, 2, 2]               0\n",
            "          Conv2d-269             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-270             [-1, 64, 2, 2]             128\n",
            "            ReLU-271             [-1, 64, 2, 2]               0\n",
            "          Conv2d-272             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-273             [-1, 64, 2, 2]             128\n",
            "            ReLU-274             [-1, 64, 2, 2]               0\n",
            "          Conv2d-275             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-276             [-1, 64, 2, 2]             128\n",
            "            ReLU-277             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-278             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-279             [-1, 64, 1, 1]               0\n",
            "             VGG-280                   [-1, 64]               0\n",
            "          Conv2d-281           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-282           [-1, 32, 32, 32]              64\n",
            "            ReLU-283           [-1, 32, 32, 32]               0\n",
            "          Conv2d-284           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-285           [-1, 32, 32, 32]              64\n",
            "            ReLU-286           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-287           [-1, 32, 16, 16]               0\n",
            "          Conv2d-288           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-289           [-1, 32, 16, 16]              64\n",
            "            ReLU-290           [-1, 32, 16, 16]               0\n",
            "          Conv2d-291           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-292           [-1, 32, 16, 16]              64\n",
            "            ReLU-293           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-294             [-1, 32, 8, 8]               0\n",
            "          Conv2d-295             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-296             [-1, 32, 8, 8]              64\n",
            "            ReLU-297             [-1, 32, 8, 8]               0\n",
            "          Conv2d-298             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-299             [-1, 64, 8, 8]             128\n",
            "            ReLU-300             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-301             [-1, 64, 4, 4]               0\n",
            "          Conv2d-302             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-303             [-1, 64, 4, 4]             128\n",
            "            ReLU-304             [-1, 64, 4, 4]               0\n",
            "          Conv2d-305             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-306             [-1, 64, 4, 4]             128\n",
            "            ReLU-307             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-308             [-1, 64, 2, 2]               0\n",
            "          Conv2d-309             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-310             [-1, 64, 2, 2]             128\n",
            "            ReLU-311             [-1, 64, 2, 2]               0\n",
            "          Conv2d-312             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-313             [-1, 64, 2, 2]             128\n",
            "            ReLU-314             [-1, 64, 2, 2]               0\n",
            "          Conv2d-315             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-316             [-1, 64, 2, 2]             128\n",
            "            ReLU-317             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-318             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-319             [-1, 64, 1, 1]               0\n",
            "             VGG-320                   [-1, 64]               0\n",
            "          Linear-321                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 1,942,026\n",
            "Trainable params: 1,942,026\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 17.36\n",
            "Params size (MB): 7.41\n",
            "Estimated Total Size (MB): 24.78\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for m in net8students.modules():\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        m.weight.requires_grad = False\n",
        "        if m.bias is not None:\n",
        "            m.bias.requires_grad = False\n",
        "net8students = net8students.to(device)\n",
        "summary(net8students, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZAB4QuQLbvG",
        "outputId": "0819c1be-d115-422c-f9be-e47b792531f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "            Conv2d-8           [-1, 32, 16, 16]           9,248\n",
            "       BatchNorm2d-9           [-1, 32, 16, 16]              64\n",
            "             ReLU-10           [-1, 32, 16, 16]               0\n",
            "           Conv2d-11           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-12           [-1, 32, 16, 16]              64\n",
            "             ReLU-13           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-14             [-1, 32, 8, 8]               0\n",
            "           Conv2d-15             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-16             [-1, 32, 8, 8]              64\n",
            "             ReLU-17             [-1, 32, 8, 8]               0\n",
            "           Conv2d-18             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-19             [-1, 64, 8, 8]             128\n",
            "             ReLU-20             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-21             [-1, 64, 4, 4]               0\n",
            "           Conv2d-22             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-23             [-1, 64, 4, 4]             128\n",
            "             ReLU-24             [-1, 64, 4, 4]               0\n",
            "           Conv2d-25             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-26             [-1, 64, 4, 4]             128\n",
            "             ReLU-27             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-28             [-1, 64, 2, 2]               0\n",
            "           Conv2d-29             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-30             [-1, 64, 2, 2]             128\n",
            "             ReLU-31             [-1, 64, 2, 2]               0\n",
            "           Conv2d-32             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-33             [-1, 64, 2, 2]             128\n",
            "             ReLU-34             [-1, 64, 2, 2]               0\n",
            "           Conv2d-35             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-36             [-1, 64, 2, 2]             128\n",
            "             ReLU-37             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-38             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "              VGG-40                   [-1, 64]               0\n",
            "           Conv2d-41           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-42           [-1, 32, 32, 32]              64\n",
            "             ReLU-43           [-1, 32, 32, 32]               0\n",
            "           Conv2d-44           [-1, 32, 32, 32]           9,248\n",
            "      BatchNorm2d-45           [-1, 32, 32, 32]              64\n",
            "             ReLU-46           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-47           [-1, 32, 16, 16]               0\n",
            "           Conv2d-48           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-49           [-1, 32, 16, 16]              64\n",
            "             ReLU-50           [-1, 32, 16, 16]               0\n",
            "           Conv2d-51           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-52           [-1, 32, 16, 16]              64\n",
            "             ReLU-53           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-54             [-1, 32, 8, 8]               0\n",
            "           Conv2d-55             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-56             [-1, 32, 8, 8]              64\n",
            "             ReLU-57             [-1, 32, 8, 8]               0\n",
            "           Conv2d-58             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-59             [-1, 64, 8, 8]             128\n",
            "             ReLU-60             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-61             [-1, 64, 4, 4]               0\n",
            "           Conv2d-62             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-63             [-1, 64, 4, 4]             128\n",
            "             ReLU-64             [-1, 64, 4, 4]               0\n",
            "           Conv2d-65             [-1, 64, 4, 4]          36,928\n",
            "      BatchNorm2d-66             [-1, 64, 4, 4]             128\n",
            "             ReLU-67             [-1, 64, 4, 4]               0\n",
            "        MaxPool2d-68             [-1, 64, 2, 2]               0\n",
            "           Conv2d-69             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-70             [-1, 64, 2, 2]             128\n",
            "             ReLU-71             [-1, 64, 2, 2]               0\n",
            "           Conv2d-72             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-73             [-1, 64, 2, 2]             128\n",
            "             ReLU-74             [-1, 64, 2, 2]               0\n",
            "           Conv2d-75             [-1, 64, 2, 2]          36,928\n",
            "      BatchNorm2d-76             [-1, 64, 2, 2]             128\n",
            "             ReLU-77             [-1, 64, 2, 2]               0\n",
            "        MaxPool2d-78             [-1, 64, 1, 1]               0\n",
            "        AvgPool2d-79             [-1, 64, 1, 1]               0\n",
            "              VGG-80                   [-1, 64]               0\n",
            "           Conv2d-81           [-1, 32, 32, 32]             896\n",
            "      BatchNorm2d-82           [-1, 32, 32, 32]              64\n",
            "             ReLU-83           [-1, 32, 32, 32]               0\n",
            "           Conv2d-84           [-1, 32, 32, 32]           9,248\n",
            "      BatchNorm2d-85           [-1, 32, 32, 32]              64\n",
            "             ReLU-86           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-87           [-1, 32, 16, 16]               0\n",
            "           Conv2d-88           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-89           [-1, 32, 16, 16]              64\n",
            "             ReLU-90           [-1, 32, 16, 16]               0\n",
            "           Conv2d-91           [-1, 32, 16, 16]           9,248\n",
            "      BatchNorm2d-92           [-1, 32, 16, 16]              64\n",
            "             ReLU-93           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-94             [-1, 32, 8, 8]               0\n",
            "           Conv2d-95             [-1, 32, 8, 8]           9,248\n",
            "      BatchNorm2d-96             [-1, 32, 8, 8]              64\n",
            "             ReLU-97             [-1, 32, 8, 8]               0\n",
            "           Conv2d-98             [-1, 64, 8, 8]          18,496\n",
            "      BatchNorm2d-99             [-1, 64, 8, 8]             128\n",
            "            ReLU-100             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-101             [-1, 64, 4, 4]               0\n",
            "          Conv2d-102             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-103             [-1, 64, 4, 4]             128\n",
            "            ReLU-104             [-1, 64, 4, 4]               0\n",
            "          Conv2d-105             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-106             [-1, 64, 4, 4]             128\n",
            "            ReLU-107             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-108             [-1, 64, 2, 2]               0\n",
            "          Conv2d-109             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-110             [-1, 64, 2, 2]             128\n",
            "            ReLU-111             [-1, 64, 2, 2]               0\n",
            "          Conv2d-112             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-113             [-1, 64, 2, 2]             128\n",
            "            ReLU-114             [-1, 64, 2, 2]               0\n",
            "          Conv2d-115             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-116             [-1, 64, 2, 2]             128\n",
            "            ReLU-117             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-118             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-119             [-1, 64, 1, 1]               0\n",
            "             VGG-120                   [-1, 64]               0\n",
            "          Conv2d-121           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-122           [-1, 32, 32, 32]              64\n",
            "            ReLU-123           [-1, 32, 32, 32]               0\n",
            "          Conv2d-124           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-125           [-1, 32, 32, 32]              64\n",
            "            ReLU-126           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-127           [-1, 32, 16, 16]               0\n",
            "          Conv2d-128           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-129           [-1, 32, 16, 16]              64\n",
            "            ReLU-130           [-1, 32, 16, 16]               0\n",
            "          Conv2d-131           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-132           [-1, 32, 16, 16]              64\n",
            "            ReLU-133           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-134             [-1, 32, 8, 8]               0\n",
            "          Conv2d-135             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-136             [-1, 32, 8, 8]              64\n",
            "            ReLU-137             [-1, 32, 8, 8]               0\n",
            "          Conv2d-138             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-139             [-1, 64, 8, 8]             128\n",
            "            ReLU-140             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-141             [-1, 64, 4, 4]               0\n",
            "          Conv2d-142             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-143             [-1, 64, 4, 4]             128\n",
            "            ReLU-144             [-1, 64, 4, 4]               0\n",
            "          Conv2d-145             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-146             [-1, 64, 4, 4]             128\n",
            "            ReLU-147             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-148             [-1, 64, 2, 2]               0\n",
            "          Conv2d-149             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-150             [-1, 64, 2, 2]             128\n",
            "            ReLU-151             [-1, 64, 2, 2]               0\n",
            "          Conv2d-152             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-153             [-1, 64, 2, 2]             128\n",
            "            ReLU-154             [-1, 64, 2, 2]               0\n",
            "          Conv2d-155             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-156             [-1, 64, 2, 2]             128\n",
            "            ReLU-157             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-158             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-159             [-1, 64, 1, 1]               0\n",
            "             VGG-160                   [-1, 64]               0\n",
            "          Conv2d-161           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-162           [-1, 32, 32, 32]              64\n",
            "            ReLU-163           [-1, 32, 32, 32]               0\n",
            "          Conv2d-164           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-165           [-1, 32, 32, 32]              64\n",
            "            ReLU-166           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-167           [-1, 32, 16, 16]               0\n",
            "          Conv2d-168           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-169           [-1, 32, 16, 16]              64\n",
            "            ReLU-170           [-1, 32, 16, 16]               0\n",
            "          Conv2d-171           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-172           [-1, 32, 16, 16]              64\n",
            "            ReLU-173           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-174             [-1, 32, 8, 8]               0\n",
            "          Conv2d-175             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-176             [-1, 32, 8, 8]              64\n",
            "            ReLU-177             [-1, 32, 8, 8]               0\n",
            "          Conv2d-178             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-179             [-1, 64, 8, 8]             128\n",
            "            ReLU-180             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-181             [-1, 64, 4, 4]               0\n",
            "          Conv2d-182             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-183             [-1, 64, 4, 4]             128\n",
            "            ReLU-184             [-1, 64, 4, 4]               0\n",
            "          Conv2d-185             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-186             [-1, 64, 4, 4]             128\n",
            "            ReLU-187             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-188             [-1, 64, 2, 2]               0\n",
            "          Conv2d-189             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-190             [-1, 64, 2, 2]             128\n",
            "            ReLU-191             [-1, 64, 2, 2]               0\n",
            "          Conv2d-192             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-193             [-1, 64, 2, 2]             128\n",
            "            ReLU-194             [-1, 64, 2, 2]               0\n",
            "          Conv2d-195             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-196             [-1, 64, 2, 2]             128\n",
            "            ReLU-197             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-198             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-199             [-1, 64, 1, 1]               0\n",
            "             VGG-200                   [-1, 64]               0\n",
            "          Conv2d-201           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-202           [-1, 32, 32, 32]              64\n",
            "            ReLU-203           [-1, 32, 32, 32]               0\n",
            "          Conv2d-204           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-205           [-1, 32, 32, 32]              64\n",
            "            ReLU-206           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-207           [-1, 32, 16, 16]               0\n",
            "          Conv2d-208           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-209           [-1, 32, 16, 16]              64\n",
            "            ReLU-210           [-1, 32, 16, 16]               0\n",
            "          Conv2d-211           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-212           [-1, 32, 16, 16]              64\n",
            "            ReLU-213           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-214             [-1, 32, 8, 8]               0\n",
            "          Conv2d-215             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-216             [-1, 32, 8, 8]              64\n",
            "            ReLU-217             [-1, 32, 8, 8]               0\n",
            "          Conv2d-218             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-219             [-1, 64, 8, 8]             128\n",
            "            ReLU-220             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-221             [-1, 64, 4, 4]               0\n",
            "          Conv2d-222             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-223             [-1, 64, 4, 4]             128\n",
            "            ReLU-224             [-1, 64, 4, 4]               0\n",
            "          Conv2d-225             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-226             [-1, 64, 4, 4]             128\n",
            "            ReLU-227             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-228             [-1, 64, 2, 2]               0\n",
            "          Conv2d-229             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-230             [-1, 64, 2, 2]             128\n",
            "            ReLU-231             [-1, 64, 2, 2]               0\n",
            "          Conv2d-232             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-233             [-1, 64, 2, 2]             128\n",
            "            ReLU-234             [-1, 64, 2, 2]               0\n",
            "          Conv2d-235             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-236             [-1, 64, 2, 2]             128\n",
            "            ReLU-237             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-238             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-239             [-1, 64, 1, 1]               0\n",
            "             VGG-240                   [-1, 64]               0\n",
            "          Conv2d-241           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-242           [-1, 32, 32, 32]              64\n",
            "            ReLU-243           [-1, 32, 32, 32]               0\n",
            "          Conv2d-244           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-245           [-1, 32, 32, 32]              64\n",
            "            ReLU-246           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-247           [-1, 32, 16, 16]               0\n",
            "          Conv2d-248           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-249           [-1, 32, 16, 16]              64\n",
            "            ReLU-250           [-1, 32, 16, 16]               0\n",
            "          Conv2d-251           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-252           [-1, 32, 16, 16]              64\n",
            "            ReLU-253           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-254             [-1, 32, 8, 8]               0\n",
            "          Conv2d-255             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-256             [-1, 32, 8, 8]              64\n",
            "            ReLU-257             [-1, 32, 8, 8]               0\n",
            "          Conv2d-258             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-259             [-1, 64, 8, 8]             128\n",
            "            ReLU-260             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-261             [-1, 64, 4, 4]               0\n",
            "          Conv2d-262             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-263             [-1, 64, 4, 4]             128\n",
            "            ReLU-264             [-1, 64, 4, 4]               0\n",
            "          Conv2d-265             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-266             [-1, 64, 4, 4]             128\n",
            "            ReLU-267             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-268             [-1, 64, 2, 2]               0\n",
            "          Conv2d-269             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-270             [-1, 64, 2, 2]             128\n",
            "            ReLU-271             [-1, 64, 2, 2]               0\n",
            "          Conv2d-272             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-273             [-1, 64, 2, 2]             128\n",
            "            ReLU-274             [-1, 64, 2, 2]               0\n",
            "          Conv2d-275             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-276             [-1, 64, 2, 2]             128\n",
            "            ReLU-277             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-278             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-279             [-1, 64, 1, 1]               0\n",
            "             VGG-280                   [-1, 64]               0\n",
            "          Conv2d-281           [-1, 32, 32, 32]             896\n",
            "     BatchNorm2d-282           [-1, 32, 32, 32]              64\n",
            "            ReLU-283           [-1, 32, 32, 32]               0\n",
            "          Conv2d-284           [-1, 32, 32, 32]           9,248\n",
            "     BatchNorm2d-285           [-1, 32, 32, 32]              64\n",
            "            ReLU-286           [-1, 32, 32, 32]               0\n",
            "       MaxPool2d-287           [-1, 32, 16, 16]               0\n",
            "          Conv2d-288           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-289           [-1, 32, 16, 16]              64\n",
            "            ReLU-290           [-1, 32, 16, 16]               0\n",
            "          Conv2d-291           [-1, 32, 16, 16]           9,248\n",
            "     BatchNorm2d-292           [-1, 32, 16, 16]              64\n",
            "            ReLU-293           [-1, 32, 16, 16]               0\n",
            "       MaxPool2d-294             [-1, 32, 8, 8]               0\n",
            "          Conv2d-295             [-1, 32, 8, 8]           9,248\n",
            "     BatchNorm2d-296             [-1, 32, 8, 8]              64\n",
            "            ReLU-297             [-1, 32, 8, 8]               0\n",
            "          Conv2d-298             [-1, 64, 8, 8]          18,496\n",
            "     BatchNorm2d-299             [-1, 64, 8, 8]             128\n",
            "            ReLU-300             [-1, 64, 8, 8]               0\n",
            "       MaxPool2d-301             [-1, 64, 4, 4]               0\n",
            "          Conv2d-302             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-303             [-1, 64, 4, 4]             128\n",
            "            ReLU-304             [-1, 64, 4, 4]               0\n",
            "          Conv2d-305             [-1, 64, 4, 4]          36,928\n",
            "     BatchNorm2d-306             [-1, 64, 4, 4]             128\n",
            "            ReLU-307             [-1, 64, 4, 4]               0\n",
            "       MaxPool2d-308             [-1, 64, 2, 2]               0\n",
            "          Conv2d-309             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-310             [-1, 64, 2, 2]             128\n",
            "            ReLU-311             [-1, 64, 2, 2]               0\n",
            "          Conv2d-312             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-313             [-1, 64, 2, 2]             128\n",
            "            ReLU-314             [-1, 64, 2, 2]               0\n",
            "          Conv2d-315             [-1, 64, 2, 2]          36,928\n",
            "     BatchNorm2d-316             [-1, 64, 2, 2]             128\n",
            "            ReLU-317             [-1, 64, 2, 2]               0\n",
            "       MaxPool2d-318             [-1, 64, 1, 1]               0\n",
            "       AvgPool2d-319             [-1, 64, 1, 1]               0\n",
            "             VGG-320                   [-1, 64]               0\n",
            "          Linear-321                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 1,942,026\n",
            "Trainable params: 13,834\n",
            "Non-trainable params: 1,928,192\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 17.36\n",
            "Params size (MB): 7.41\n",
            "Estimated Total Size (MB): 24.78\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net8students.parameters(), lr=0.0001)\n",
        "\n",
        "def train81(epoch):\n",
        "    print('\\nEpoch: %d' % (epoch+1))\n",
        "    net8students.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs = Variable(inputs, requires_grad=False)\n",
        "        targets = Variable(targets)\n",
        "        net8students.zero_grad()\n",
        "        outputs = net8students(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if(batch_idx % 200 == 0):\n",
        "          print(\"Accuracy : \",100.*correct/total,\" Loss : \", train_loss/(batch_idx+1))\n",
        "def test82(epoch):\n",
        "    net8students.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net8students(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if(batch_idx % 20 == 0):\n",
        "              print(\"Accuracy : \",100.*correct/total,\" Loss : \", test_loss/(batch_idx+1))"
      ],
      "metadata": {
        "id": "mrvLxB_eNVcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_acc = 0\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "    train81(epoch)\n",
        "    print(\"Validation: \")\n",
        "    test82(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tGEXdFBNpfV",
        "outputId": "4d5cc797-6bd5-4e43-d76b-7284d8ad6f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Accuracy :  1.0  Loss :  2.861071825027466\n",
            "Accuracy :  66.94029850746269  Loss :  1.3400109238292448\n",
            "Accuracy :  75.63092269326684  Loss :  0.976042689973874\n",
            "Validation: \n",
            "Accuracy :  84.0  Loss :  0.49758201837539673\n",
            "Accuracy :  83.42857142857143  Loss :  0.5150319692634401\n",
            "Accuracy :  83.2439024390244  Loss :  0.5227911850301231\n",
            "Accuracy :  83.55737704918033  Loss :  0.5188690609619265\n",
            "Accuracy :  83.50617283950618  Loss :  0.5178100301159753\n",
            "\n",
            "Epoch: 2\n",
            "Accuracy :  87.0  Loss :  0.48380985856056213\n",
            "Accuracy :  84.64676616915423  Loss :  0.482476782739459\n",
            "Accuracy :  84.99750623441396  Loss :  0.4653062862425374\n",
            "Validation: \n",
            "Accuracy :  84.0  Loss :  0.4409956634044647\n",
            "Accuracy :  83.85714285714286  Loss :  0.4674651835645948\n",
            "Accuracy :  83.92682926829268  Loss :  0.47694505060591347\n",
            "Accuracy :  84.21311475409836  Loss :  0.4718351730557739\n",
            "Accuracy :  84.07407407407408  Loss :  0.4702553690215688\n",
            "\n",
            "Epoch: 3\n",
            "Accuracy :  87.0  Loss :  0.4026736319065094\n",
            "Accuracy :  85.11442786069652  Loss :  0.4374388768453503\n",
            "Accuracy :  85.29426433915212  Loss :  0.4320414249885112\n",
            "Validation: \n",
            "Accuracy :  85.0  Loss :  0.424715131521225\n",
            "Accuracy :  83.80952380952381  Loss :  0.4588079892453693\n",
            "Accuracy :  83.97560975609755  Loss :  0.46983417723237014\n",
            "Accuracy :  84.21311475409836  Loss :  0.46409152570318\n",
            "Accuracy :  84.11111111111111  Loss :  0.46223579733460035\n",
            "\n",
            "Epoch: 4\n",
            "Accuracy :  87.0  Loss :  0.35152146220207214\n",
            "Accuracy :  85.20398009950249  Loss :  0.4254137288723419\n",
            "Accuracy :  85.39401496259352  Loss :  0.42230874073327035\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.42004573345184326\n",
            "Accuracy :  84.04761904761905  Loss :  0.456759797675269\n",
            "Accuracy :  84.21951219512195  Loss :  0.46740445494651794\n",
            "Accuracy :  84.47540983606558  Loss :  0.4612250337835218\n",
            "Accuracy :  84.41975308641975  Loss :  0.4589558173844844\n",
            "\n",
            "Epoch: 5\n",
            "Accuracy :  90.0  Loss :  0.40824562311172485\n",
            "Accuracy :  85.61194029850746  Loss :  0.4194904137310104\n",
            "Accuracy :  85.65835411471322  Loss :  0.4162801148513903\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4160558581352234\n",
            "Accuracy :  84.19047619047619  Loss :  0.4546851430620466\n",
            "Accuracy :  84.26829268292683  Loss :  0.46578029597677834\n",
            "Accuracy :  84.62295081967213  Loss :  0.45933580887122233\n",
            "Accuracy :  84.55555555555556  Loss :  0.45700014962090385\n",
            "\n",
            "Epoch: 6\n",
            "Accuracy :  87.0  Loss :  0.3902616798877716\n",
            "Accuracy :  85.66169154228855  Loss :  0.41715415622761\n",
            "Accuracy :  85.72319201995012  Loss :  0.41536447402099125\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.41063544154167175\n",
            "Accuracy :  84.33333333333333  Loss :  0.4530359719480787\n",
            "Accuracy :  84.39024390243902  Loss :  0.4640336152983875\n",
            "Accuracy :  84.68852459016394  Loss :  0.4573577603355783\n",
            "Accuracy :  84.5925925925926  Loss :  0.45496032267440983\n",
            "\n",
            "Epoch: 7\n",
            "Accuracy :  88.0  Loss :  0.35703450441360474\n",
            "Accuracy :  85.69651741293532  Loss :  0.4151153196742879\n",
            "Accuracy :  85.84039900249377  Loss :  0.41140232809315297\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4075841009616852\n",
            "Accuracy :  84.42857142857143  Loss :  0.4517169012909844\n",
            "Accuracy :  84.41463414634147  Loss :  0.46263967854220694\n",
            "Accuracy :  84.73770491803279  Loss :  0.45585271178698933\n",
            "Accuracy :  84.71604938271605  Loss :  0.4529635254983549\n",
            "\n",
            "Epoch: 8\n",
            "Accuracy :  88.0  Loss :  0.3683801591396332\n",
            "Accuracy :  85.7860696517413  Loss :  0.411059002007418\n",
            "Accuracy :  85.80798004987531  Loss :  0.4093083085264648\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4056413769721985\n",
            "Accuracy :  84.47619047619048  Loss :  0.4511075701032366\n",
            "Accuracy :  84.53658536585365  Loss :  0.4613937742826415\n",
            "Accuracy :  84.75409836065573  Loss :  0.45491954342263646\n",
            "Accuracy :  84.75308641975309  Loss :  0.452197543632837\n",
            "\n",
            "Epoch: 9\n",
            "Accuracy :  88.0  Loss :  0.378551185131073\n",
            "Accuracy :  85.91044776119404  Loss :  0.4109454610157962\n",
            "Accuracy :  86.05486284289277  Loss :  0.4073991485293072\n",
            "Validation: \n",
            "Accuracy :  86.0  Loss :  0.4051992893218994\n",
            "Accuracy :  84.47619047619048  Loss :  0.45009045160952066\n",
            "Accuracy :  84.63414634146342  Loss :  0.45981585361608646\n",
            "Accuracy :  84.85245901639344  Loss :  0.4533666364970754\n",
            "Accuracy :  84.87654320987654  Loss :  0.45048683164296327\n",
            "\n",
            "Epoch: 10\n",
            "Accuracy :  89.0  Loss :  0.37348079681396484\n",
            "Accuracy :  86.05472636815921  Loss :  0.4058325543480726\n",
            "Accuracy :  86.01745635910224  Loss :  0.4047034237524518\n",
            "Validation: \n",
            "Accuracy :  87.0  Loss :  0.40113773941993713\n",
            "Accuracy :  84.85714285714286  Loss :  0.44897291773841497\n",
            "Accuracy :  84.65853658536585  Loss :  0.45924876957404903\n",
            "Accuracy :  84.93442622950819  Loss :  0.4527653309165454\n",
            "Accuracy :  84.92592592592592  Loss :  0.4497330564039725\n"
          ]
        }
      ]
    }
  ]
}